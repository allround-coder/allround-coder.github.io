{"pageProps":{"post":{"title":"파이썬으로 AI 텍스트-비디오 모델 처음부터 구축하는 방법","description":"","date":"2024-06-22 04:36","slug":"2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython","content":"\n\n![Building an AI Text-to-Video Model from Scratch Using Python](/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_0.png)\n\n애정하는 여러분, 반가워요!\n\n오픈AI의 Sora, 안정성 AI의 Stable Video Diffusion 등 2024년에 가장 인기 있는 인공지능 트렌드 중 하나인 텍스트-비디오 모델들이 대규모 언어 모델 이후 많이 등장하고 있습니다. 이 블로그에서는 제가 직접 작성한 작은 규모의 텍스트-비디오 모델을 만들어보려고 해요. 텍스트 프롬프트를 입력하면 학습된 모델이 해당 프롬프트를 기반으로 비디오를 생성할 거예요. 여기서는 이론적 개념을 이해하는 것부터 전체 아키텍처 코딩하고 최종 결과물을 생성하는 과정까지 모두 다루고 있어요.\n\n저는 고급 GPU를 갖고 있지 않아서 작은 규모 아키텍처를 코딩했어요. 다양한 프로세서에서 모델을 학습하는데 걸리는 시간을 비교해보겠습니다:\n\nCPU에서 실행하면 모델 학습에 훨씬 오랜 시간이 걸릴 거예요. 코드 변경을 빠르게 테스트하고 결과를 확인하려면 CPU는 최적의 선택이 아닙니다. 더 효율적이고 빠른 학습을 위해 Colab이나 Kaggle의 T4 GPU를 사용하는 것을 권장해요.\n\n더 궁금한 점이 있으시면 언제든지 물어주세요!\n\n<div class=\"content-ad\"></div>\n\n이 블로그에서 코드를 복사하여 붙여 넣는 것을 피하려면 GitHub 저장소에 노트북 파일과 모든 코드 및 정보가 포함되어 있습니다.\n\n여기에는 처음부터 Stable Diffusion을 만드는 방법을 안내하는 블로그 링크가 있습니다:\n\n# 목차\n\n- 무엇을 만들고 있는가\n- 필수 조건\n- GAN 아키텍처 이해하기\n∘ GAN이란?\n∘ 실세계 응용\n∘ GAN이 어떻게 작동하는가?\n∘ GAN 훈련 예제\n- 준비 단계 설정\n- 훈련 데이터 코딩\n- 교육 데이터 전 처리\n- 텍스트 임베딩 레이어 구현\n- Generator 레이어 구현\n- Discriminator 레이어 구현\n- 교육 매개변수 코딩\n- 훈련 루프 코딩\n- 훈련된 모델 저장\n- AI 비디오 생성\n- 빠진 것은 무엇일까?\n- 나에 대해\n\n<div class=\"content-ad\"></div>\n\n# 무엇을 구축하고 있는가\n\n우리는 전통적인 기계 학습 또는 딥 러닝 모델과 유사한 방식을 따를 것입니다. 데이터 세트에서 훈련을 받은 후 보이지 않는 데이터에서 테스트하는 것이 기본적인 접근 방식입니다. 텍스트에서 비디오로 변환하는 맥락에서, 우리는 10만 개의 비디오 데이터 세트를 사용하여 개가 공을 가져오고 고양이가 쥐를 쫓는 내용을 가르치겠습니다. 우리 모델을 훈련시켜서 고양이가 공을 가져오거나 개가 쥐를 쫓는 비디오를 생성하도록 할 것입니다.\n\n이러한 훈련 데이터 세트는 인터넷에서 쉽게 이용할 수 있지만, 필요한 컴퓨팅 파워는 극히 높습니다. 따라서 우리는 Python 코드에서 생성된 움직이는 물체들의 비디오 데이터 세트를 사용할 것입니다.\n\n저희는 OpenAI Sora가 사용하는 확산 모델 대신 GAN(생성 적대적 신경망) 아키텍처를 사용하여 모델을 만들 것입니다. 확산 모델을 사용하려고 했지만, 메모리 요구 사항 때문에 그 기능이 중단되었습니다. GAN은 반면에 훈련과 테스트가 더 쉽고 빠릅니다.\n\n<div class=\"content-ad\"></div>\n\n# 사전 요구 사항\n\n우리는 OOP (Object-Oriented Programming)을 사용할 것이기 때문에, 기본적인 이해를 갖고 있어야 하며 뉴럴 네트워크에 대한 이해도 필요합니다. GANs (Generative Adversarial Networks)에 대한 지식은 필수는 아니지만, 여기서 그 아키텍처를 다룰 것이므로 참고하시면 좋습니다.\n\n# GAN 아키텍처 이해\n\nGAN을 이해하는 것은 중요합니다. 우리의 아키텍처의 많은 부분이 이에 의존하기 때문입니다. 무엇인지, 그 구성 요소는 무엇인지 등을 탐구해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## GAN이란 무엇인가요?\n\nGenerative Adversarial Network (GAN)은 주어진 데이터셋으로부터 새로운 데이터(예: 이미지 또는 음악)를 생성하는 하나의 신경망과 그 데이터가 실제인지 가짜인지 구별하려는 다른 신경망이 경쟁하는 딥 러닝 모델입니다. 이 과정은 생성된 데이터가 원본과 구별할 수 없을 때까지 계속됩니다.\n\n## 실제 세계 응용\n\n- 이미지 생성: GAN은 텍스트 프롬프트로 현실적인 이미지를 생성하거나 기존 이미지를 수정하여 해상도를 향상시키거나 흑백 사진에 색상을 추가합니다.\n- 데이터 증강: 다른 기계 학습 모델을 훈련하기 위해 합성 데이터를 생성하며, 예를 들어 사기 탐지 시스템을 위해 사기 거래 데이터를 생성합니다.\n- 누락된 정보 완성: GAN은 누락된 데이터를 채울 수 있으며, 에너지 응용 프로그램에 대한 지형 지도에서 장력 이미지를 생성하는 등의 작업을 수행할 수 있습니다.\n- 3D 모델 생성: 2D 이미지를 3D 모델로 변환하여, 수술 계획을 위해 현실적인 장기 이미지를 생성하는 의료 분야와 같은 분야에서 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n## GAN이 작동하는 방식은?\n\nGAN은 생성자(generator)와 판별자(discriminator) 두 개의 딥 뉴럴 네트워크로 구성되어 있습니다. 이 두 네트워크는 적대적인 설정에서 함께 훈련되며, 하나는 새로운 데이터를 생성하고 다른 하나는 데이터가 진짜인지 가짜인지 판별합니다.\n\n다음은 GAN 작동 방식의 간단한 개요입니다:\n\n- 훈련 데이터 분석: 생성자는 훈련 데이터를 분석하여 데이터 특성을 식별하고, 판별자는 독립적으로 동일한 데이터를 분석하여 해당 특성을 학습합니다.\n- 데이터 수정: 생성자는 데이터의 일부 특성에 잡음(랜덤 변경)을 추가합니다.\n- 데이터 전달: 수정된 데이터는 그런 다음 판별자에게 전달됩니다.\n- 확률 계산: 판별자는 생성된 데이터가 원본 데이터셋에서 온 확률을 계산합니다.\n- 피드백 루프: 판별자는 생성자에게 피드백을 제공하여 다음 주기에서 잡음을 줄이도록 안내합니다.\n- 적대적 훈련: 생성자는 판별자의 오류를 최대화하려고 하고, 판별자는 자신의 에러를 최소화하려고 합니다. 많은 훈련 반복을 통해 두 네트워크는 개선되고 발전합니다.\n- 평형 상태: 판별자가 더 이상 진짜와 생성된 데이터를 구별하지 못할 때까지 훈련이 계속되며, 이는 생성자가 현실적인 데이터를 생성하는 것을 성공적으로 배웠음을 나타냅니다. 이 시점에서 훈련 과정이 완료됩니다.\n\n<div class=\"content-ad\"></div>\n\n## GAN 훈련 예시\n\n이미지 간 변환을 예로 들어 GAN 모델을 설명해 보겠습니다. 이때는 인간의 얼굴을 수정하는 데 초점을 맞춥니다.\n\n- 입력 이미지: 입력은 실제 인간 얼굴 이미지입니다.\n- 속성 수정: 생성자는 눈에 선글라스를 추가하는 등 얼굴의 속성을 수정합니다.\n- 생성된 이미지: 생성자는 선글라스가 추가된 이미지 세트를 생성합니다.\n- 판별자의 역할: 판별자는 실제 이미지(선글라스를 쓴 사람)와 생성된 이미지(선글라스가 추가된 얼굴)의 혼합을 받습니다.\n- 평가: 판별자는 실제 이미지와 생성된 이미지를 구별하려고 합니다.\n- 피드백 루프: 만약 판별자가 가짜 이미지를 올바르게 식별하면 생성자는 더 현실적인 이미지를 만들기 위해 매개변수를 조정합니다. 생성자가 판별자를 성공적으로 속이면 판별자는 감지를 개선하기 위해 매개변수를 업데이트합니다.\n\n이 적대적 프로세스를 통해 두 네트워크가 계속해서 발전합니다. 생성자는 현실적인 이미지를 만드는 데 더 잘해지고 판별자는 가짜를 식별하는 데 더 잘해지며 균형이 이루어질 때까지 계속 발전합니다. 판별자가 더 이상 실제 이미지와 생성된 이미지를 구별하지 못할 정도로 적절한 평형점에 도달하면 GAN이 성공적으로 현실적인 수정을 만들기 위해 배웠다고 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 무대를 준비합니다\n\n파이썬 라이브러리 그룹과 작업할 것입니다. 그러니, 이제 그들을 가져오겠습니다.\n\n```python\n# 운영 체제와 상호 작용하기 위한 운영 체제 모듈\nimport os\n\n# 무작위 숫자 생성을 위한 모듈\nimport random\n\n# 숫자 연산을 위한 모듈\nimport numpy as np\n\n# 이미지 처리를 위한 OpenCV 라이브러리\nimport cv2\n\n# 이미지 처리를 위한 Python Imaging Library\nfrom PIL import Image, ImageDraw, ImageFont\n\n# 딥 러닝을 위한 PyTorch 라이브러리\nimport torch\n\n# PyTorch에서 사용자 정의 데이터셋 만들기 위한 Dataset 클래스\nfrom torch.utils.data import Dataset\n\n# 이미지 변환을 위한 모듈\nimport torchvision.transforms as transforms\n\n# PyTorch의 신경망 모듈\nimport torch.nn as nn\n\n# PyTorch의 최적화 알고리즘\nimport torch.optim as optim\n\n# PyTorch에서 시퀀스를 패딩하는 함수\nfrom torch.nn.utils.rnn import pad_sequence\n\n# PyTorch에서 이미지 저장하는 함수\nfrom torchvision.utils import save_image\n\n# 그래프 및 이미지 플로팅을 위한 모듈\nimport matplotlib.pyplot as plt\n\n# IPython 환경에서 풍부한 콘텐츠 표시를 위한 모듈\nfrom IPython.display import clear_output, display, HTML\n\n# 이진 데이터를 텍스트로 인코딩 및 디코딩하는 모듈\nimport base64\n```\n\n모든 라이브러리를 가져왔으니, 다음 단계는 GAN 아키텍처를 훈련할 때 사용할 훈련 데이터를 정의하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 훈련 데이터 코딩하기\n\n적어도 10,000개의 비디오가 훈련 데이터로 필요해요. 왜냐하면 작은 숫자로 테스트해봤더니 결과가 매우 좋지 않았어요. 거의 아무것도 보이지 않았죠. 다음으로 중요한 질문은 무엇일까요? 이 비디오들은 무엇에 관한 걸까요? 우리의 훈련 비디오 데이터셋은 서로 다른 방향으로 움직이는 원을 포함해요. 그래서 이제 코드를 작성하고 10,000개의 비디오를 생성해보죠.\n\n```js\n# 'training_dataset'이름의 디렉토리 생성\nos.makedirs('training_dataset', exist_ok=True)\n\n# 데이터셋을 생성할 비디오 수 정의\nnum_videos = 10000\n\n# 비디오 당 프레임 수 정의 (1초 비디오)\nframes_per_video = 10\n\n# 데이터셋 내 각 이미지의 크기 정의\nimg_size = (64, 64)\n\n# 모양의 크기 정의 (원)\nshape_size = 10\r\n```\n\n기본 매개변수를 설정한 후, 다음은 훈련 데이터셋의 텍스트 프롬프트를 정의해야해요.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 원에 대한 텍스트 프롬프트와 해당 움직임을 정의합니다.\nprompts_and_movements = [\n    (\"circle moving down\", \"circle\", \"down\"),  # 원을 아래로 움직입니다\n    (\"circle moving left\", \"circle\", \"left\"),  # 원을 왼쪽으로 움직입니다\n    (\"circle moving right\", \"circle\", \"right\"),  # 원을 오른쪽으로 움직입니다\n    (\"circle moving diagonally up-right\", \"circle\", \"diagonal_up_right\"),  # 원을 대각선으로 위쪽 오른쪽으로 움직입니다\n    (\"circle moving diagonally down-left\", \"circle\", \"diagonal_down_left\"),  # 원을 대각선으로 아래쪽 왼쪽으로 움직입니다\n    (\"circle moving diagonally up-left\", \"circle\", \"diagonal_up_left\"),  # 원을 대각선으로 위쪽 왼쪽으로 움직입니다\n    (\"circle moving diagonally down-right\", \"circle\", \"diagonal_down_right\"),  # 원을 대각선으로 아래쪽 오른쪽으로 움직입니다\n    (\"circle rotating clockwise\", \"circle\", \"rotate_clockwise\"),  # 원을 시계방향으로 회전시킵니다\n    (\"circle rotating counter-clockwise\", \"circle\", \"rotate_counter_clockwise\"),  # 원을 반시계방향으로 회전시킵니다\n    (\"circle shrinking\", \"circle\", \"shrink\"),  # 원을 축소시킵니다\n    (\"circle expanding\", \"circle\", \"expand\"),  # 원을 확대시킵니다\n    (\"circle bouncing vertically\", \"circle\", \"bounce_vertical\"),  # 원을 수직으로 튕기게 합니다\n    (\"circle bouncing horizontally\", \"circle\", \"bounce_horizontal\"),  # 원을 수평으로 튕기게 합니다\n    (\"circle zigzagging vertically\", \"circle\", \"zigzag_vertical\"),  # 원을 수직으로 지그재그로 움직입니다\n    (\"circle zigzagging horizontally\", \"circle\", \"zigzag_horizontal\"),  # 원을 수평으로 지그재그로 움직입니다\n    (\"circle moving up-left\", \"circle\", \"up_left\"),  # 원을 왼쪽 위로 움직입니다\n    (\"circle moving down-right\", \"circle\", \"down_right\"),  # 원을 오른쪽 아래로 움직입니다\n    (\"circle moving down-left\", \"circle\", \"down_left\"),  # 원을 왼쪽 아래로 움직입니다\n]\n```\n\n이제 이러한 프롬프트를 사용하여 원의 여러 움직임을 정의했습니다. 다음으로 프롬프트를 기반으로 이 원을 움직이는 수학적 방정식을 코딩해야 합니다.\n\n```js\n# 매개변수가 있는 함수 정의\ndef create_image_with_moving_shape(size, frame_num, shape, direction):\n  \n    # 특정 크기와 흰색 배경으로 새로운 RGB 이미지를 생성합니다\n    img = Image.new('RGB', size, color=(255, 255, 255))  \n\n    # 이미지에 대한 그리기 컨텍스트를 생성합니다\n    draw = ImageDraw.Draw(img)  \n\n    # 이미지의 중앙 좌표를 계산합니다\n    center_x, center_y = size[0] // 2, size[1] // 2  \n\n    # 모든 움직임의 중심으로 위치를 초기화합니다\n    position = (center_x, center_y)  \n\n    # 각 방향을 해당 위치 조정이나 이미지 변환으로 매핑하는 딕셔너리를 정의합니다\n    direction_map = {  \n        # 프레임 번호에 따라 아래로 위치 조정\n        \"down\": (0, frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 왼쪽으로 위치 조정\n        \"left\": (-frame_num * 5 % size[0], 0),  \n        # 프레임 번호에 따라 오른쪽으로 위치 조정\n        \"right\": (frame_num * 5 % size[0], 0),  \n        # 대각선 위 오른쪽으로 위치 조정\n        \"diagonal_up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 대각선 아래 왼쪽으로 위치 조정\n        \"diagonal_down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n        # 대각선 위 왼쪽으로 위치 조정\n        \"diagonal_up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 대각선 아래 오른쪽으로 위치 조정\n        \"diagonal_down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 이미지를 시계방향으로 회전\n        \"rotate_clockwise\": img.rotate(frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),  \n        # 프레임 번호에 따라 이미지를 반시계방향으로 회전\n        \"rotate_counter_clockwise\": img.rotate(-frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),  \n        # 상하로 튕기는 효과를 위해 위치 조정\n        \"bounce_vertical\": (0, center_y - abs(frame_num * 5 % size[1] - center_y)),  \n        # 좌우로 튕기는 효과를 위해 위치 조정\n        \"bounce_horizontal\": (center_x - abs(frame_num * 5 % size[0] - center_x), 0),  \n        # 상하로 지그재그 효과를 위해 위치 조정\n        \"zigzag_vertical\": (0, center_y - frame_num * 5 % size[1]) if frame_num % 2 == 0 else (0, center_y + frame_num * 5 % size[1]),  \n        # 좌우로 지그재그 효과를 위해 위치 조정\n        \"zigzag_horizontal\": (center_x - frame_num * 5 % size[0], center_y) if frame_num % 2 == 0 else (center_x + frame_num * 5 % size[0], center_y),  \n        # 프레임 번호에 따라 오른쪽 위로 위치 조정\n        \"up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 왼쪽 위로 위치 조정\n        \"up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 오른쪽 아래로 위치 조정\n        \"down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 왼쪽 아래로 위치 조정\n        \"down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1])  \n    }\n\n    # direction_map에 방향이 있는지 확인합니다\n    if direction in direction_map:  \n        # 방향이 위치 조정에 매핑되는지 확인합니다\n        if isinstance(direction_map[direction], tuple):  \n            # 조정에 따라 위치를 업데이트합니다\n            position = tuple(np.add(position, direction_map[direction]))  \n        else:  # 방향이 이미지 변환에 매핑되는 경우\n            # 변환에 따라 이미지를 업데이트합니다\n            img = direction_map[direction]  \n\n    # 이미지를 numpy 배열로 반환합니다\n    return np.array(img)\n```\n\n위 함수는 선택한 방향에 따라 각 프레임마다 원을 움직이는 데 사용됩니다. 모든 비디오를 생성하기\n\n<div class=\"content-ad\"></div>\n\n```js\n# 비디오 수만큼 반복하여 생성\nfor i in range(num_videos):\n    # 미리 정의된 목록에서 무작위로 프롬프트와 이동 선택\n    prompt, shape, direction = random.choice(prompts_and_movements)\n    \n    # 현재 비디오를 위한 디렉터리 생성\n    video_dir = f'training_dataset/video_{i}'\n    os.makedirs(video_dir, exist_ok=True)\n    \n    # 선택된 프롬프트를 비디오 디렉터리 내의 텍스트 파일에 작성\n    with open(f'{video_dir}/prompt.txt', 'w') as f:\n        f.write(prompt)\n    \n    # 현재 비디오의 프레임 생성\n    for frame_num in range(frames_per_video):\n        # 현재 프레임 번호, 모양, 방향을 기반으로 이동 모양이 있는 이미지 생성\n        img = create_image_with_moving_shape(img_size, frame_num, shape, direction)\n        \n        # 생성된 이미지를 비디오 디렉터리에 PNG 파일로 저장\n        cv2.imwrite(f'{video_dir}/frame_{frame_num}.png', img)\r\n```\n\n위의 코드를 실행하면 전체 훈련 데이터 세트를 생성합니다. 훈련 데이터 세트 파일 구조는 다음과 같습니다.\n\n![Training Dataset Structure](/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_1.png)\n\n각 훈련 비디오 폴더에는 프레임과 텍스트 프롬프트가 포함되어 있습니다. 훈련 데이터세트 샘플을 살펴보겠습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![https://miro.medium.com/v2/resize:fit:1400/1*mzizetR6zOyIheNFtKpo0A.gif](https://miro.medium.com/v2/resize:fit:1400/1*mzizetR6zOyIheNFtKpo0A.gif)\n\n훈련 데이터셋에서 원이 위로 올라가고 오른쪽으로 이동하는 동작을 포함시키지 않았습니다. 이것을 보고 우리가 훈련한 모델을 보지 않은 데이터에서 테스트하는 프롬프트로 사용할 것입니다.\n\n한 가지 더 중요한 점은 우리의 훈련 데이터에는 장면에서 멀어지는 물체나 카메라 앞에 부분적으로 나타나는 많은 샘플이 포함되어 있다는 것입니다. 이는 OpenAI 소라 데모 비디오에서 관찰한 것과 유사합니다.\n\n![https://miro.medium.com/v2/resize:fit:1400/1*RP5M_TEt2H4Mo6OhnlcRLA.gif](https://miro.medium.com/v2/resize:fit:1400/1*RP5M_TEt2H4Mo6OhnlcRLA.gif)\n\n\n<div class=\"content-ad\"></div>\n\n우리가 교육 데이터에 이러한 샘플을 포함시킨 이유는 우리의 모델이 원이 극단적인 구석에서 장면에 들어오더라도 모양을 유지할 수 있는지 테스트하기 위해서입니다.\n\n이제 교육 데이터가 생성되었으므로 교육 비디오를 PyTorch와 같은 딥 러닝 프레임워크에서 주로 사용되는 기본 데이터 유형인 텐서로 변환해야 합니다. 또한, 정규화와 같은 변환 작업을 수행하여 데이터를 더 작은 범위로 스케일링하여 교육 아키텍처의 수렴과 안정성을 개선하는 데 도움이 됩니다.\n\n# 교육 데이터 전처리\n\n텍스트-비디오 작업을 위한 데이터 세트 클래스를 작성해야 합니다. 이 클래스는 교육 데이터 세트 디렉토리에서 비디오 프레임과 해당 텍스트 프롬프트를 읽어 PyTorch에서 사용할 수 있도록 만들어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n# torch.utils.data.Dataset 클래스를 상속받아 데이터셋 클래스 정의\nclass TextToVideoDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        # 루트 디렉토리와 옵션으로 주어진 변형(transform)으로 데이터셋 초기화\n        self.root_dir = root_dir\n        self.transform = transform\n        # 루트 디렉토리의 모든 하위 디렉토리 나열\n        self.video_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n        # 프레임 경로와 해당 프롬프트를 저장할 리스트 초기화\n        self.frame_paths = []\n        self.prompts = []\n\n        # 각 비디오 디렉토리마다 반복\n        for video_dir in self.video_dirs:\n            # 비디오 디렉토리에 있는 모든 PNG 파일 나열하고 파일 경로 저장\n            frames = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.png')]\n            self.frame_paths.extend(frames)\n            # 비디오 디렉토리에 있는 프롬프트 텍스트 파일 읽어서 내용 저장\n            with open(os.path.join(video_dir, 'prompt.txt'), 'r') as f:\n                prompt = f.read().strip()\n            # 각 프레임에 해당하는 프롬프트를 반복해서 prompts 리스트에 저장\n            self.prompts.extend([prompt] * len(frames))\n\n    # 데이터셋 전체 샘플 수 반환\n    def __len__(self):\n        return len(self.frame_paths)\n\n    # 주어진 인덱스에 해당하는 샘플 반환\n    def __getitem__(self, idx):\n        # 주어진 인덱스에 해당하는 프레임 경로 가져오기\n        frame_path = self.frame_paths[idx]\n        # PIL (Python Imaging Library)을 사용하여 이미지 열기\n        image = Image.open(frame_path)\n        # 주어진 인덱스에 해당하는 프롬프트 가져오기\n        prompt = self.prompts[idx]\n\n        # 지정된 경우 변형 적용\n        if self.transform:\n            image = self.transform(image)\n\n        # 변형된 이미지와 프롬프트 반환\n        return image, prompt\n```\n\n코드 아키텍처를 작성하기 전에 훈련 데이터를 정규화해야 합니다. 배치 크기는 16으로 설정하고 데이터를 섞어 더 많은 무작위성을 도입할 것입니다.\n\n```python\n# 데이터에 적용할 변형 세트 정의\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # PIL 이미지 또는 numpy.ndarray를 텐서로 변환\n    transforms.Normalize((0.5,), (0.5,))  # 평균과 표준편차를 사용하여 이미지 정규화\n])\n\n# 정의된 변형을 사용하여 데이터셋 로드\ndataset = TextToVideoDataset(root_dir='training_dataset', transform=transform)\n# 데이터셋을 반복할 데이터로더 생성\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n```\n\n# 텍스트 임베딩 레이어 구현하기\n\n\n<div class=\"content-ad\"></div>\n\n트랜스포머 아키텍처에서 본 적이 있을 수 있어요. 텍스트 입력을 임베딩으로 변환하고 멀티 헤드 어텐션에서 추가로 처리할 수 있어요. 여기서는 텍스트 임베딩 레이어를 코드로 작성해야 합니다. 이 레이어를 기반으로 GAN 아키텍처 훈련이 임베딩 데이터와 이미지 텐서에서 이루어질 거에요.\n\n```js\n# 텍스트 임베딩을 위한 클래스 정의\nclass TextEmbedding(nn.Module):\n    # vocab_size와 embed_size 매개변수를 사용하는 생성자 메서드\n    def __init__(self, vocab_size, embed_size):\n        # 슈퍼 클래스 생성자 호출\n        super(TextEmbedding, self).__init__()\n        # 임베딩 레이어 초기화\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n\n    # 순전파 메서드 정의\n    def forward(self, x):\n        # 입력의 임베딩 표현 반환\n        return self.embedding(x)\n```\n\n어휘 사전 크기는 훈련 데이터에 기반하여 결정될 거에요. 임베딩 크기는 10일 거에요. 더 큰 데이터셋을 다룬다면 Hugging Face에서 제공하는 임베딩 모델을 선택해서 사용할 수도 있어요.\n\n# 생성자 레이어 구현하기\n\n<div class=\"content-ad\"></div>\n\n이제 GANs에서 생성자가 하는 역할을 이미 알고 있기 때문에, 이 층을 코딩하고 내용을 이해해봅시다. \n\n```python\nclass Generator(nn.Module):\n    def __init__(self, text_embed_size):\n        super(Generator, self).__init__()\n        \n        # 노이즈와 텍스트 임베딩을 입력으로 받는 완전 연결층\n        self.fc1 = nn.Linear(100 + text_embed_size, 256 * 8 * 8)\n        \n        # 입력을 업샘플링하는 전치 합성곱층\n        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n        self.deconv3 = nn.ConvTranspose2d(64, 3, 4, 2, 1)  # RGB 이미지에 대한 출력은 3채널이 됩니다\n        \n        # 활성화 함수\n        self.relu = nn.ReLU(True)  # ReLU 활성화 함수\n        self.tanh = nn.Tanh()       # 최종 출력을 -1과 1 사이의 값으로 만들기 위한 Tanh 활성화 함수\n\n    def forward(self, noise, text_embed):\n        # 채널 차원을 따라 노이즈와 텍스트 임베딩을 연결합니다\n        x = torch.cat((noise, text_embed), dim=1)\n        \n        # 완전 연결층 다음에 4D 텐서로 재구성합니다\n        x = self.fc1(x).view(-1, 256, 8, 8)\n        \n        # ReLU 활성화를 사용하여 전치 합성곱층을 통한 업샘플링\n        x = self.relu(self.deconv1(x))\n        x = self.relu(self.deconv2(x))\n        \n        # 최종 층에서 출력 값을 -1과 1 사이로 만들기 위해 Tanh 활성화를 사용합니다 (이미지용)\n        x = self.tanh(self.deconv3(x))\n        \n        return x\n```\n\n이 생성자 클래스는 무작위 노이즈와 텍스트 임베딩의 결합에서 비디오 프레임을 생성하는 역할을 합니다. 목표는 주어진 텍스트 설명에 조건부로 현실적인 비디오 프레임을 생성하는 것입니다. 네트워크는 노이즈 벡터와 텍스트 임베딩을 하나의 특성 벡터로 결합하는 완전 연결층 (nn.Linear)으로 시작합니다. 이 벡터는 재구성되고, 레이어는 원하는 비디오 프레임 크기로 특성 맵을 점진적으로 업샘플링하는 일련의 전치 합성곱층 (nn.ConvTranspose2d)을 통해 전달됩니다.\n\n이 레이어들은 비선형성을 위해 ReLU 활성화 함수(nn.ReLU)를 사용하고, 최종 레이어는 출력을 [-1, 1] 범위로 스케일링하기 위해 Tanh 활성화(nn.Tanh)를 사용합니다. 따라서 생성자는 추상적이고 고차원의 입력을 시각적으로 입력 텍스트를 잘 나타내는 일관된 비디오 프레임으로 변환합니다.\n\n<div class=\"content-ad\"></div>\n\n# 판별자 레이어 구현\n제너레이터 레이어를 코딩한 후에는 이제 판별자 부분을 구현해야 합니다.\n\n```js\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        # 입력 이미지를 처리하기 위한 합성곱 레이어\n        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)   # 3개의 입력 채널 (RGB), 64개의 출력 채널, 커널 크기 4x4, 스트라이드 2, 패딩 1\n        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1) # 64개의 입력 채널, 128개의 출력 채널, 커널 크기 4x4, 스트라이드 2, 패딩 1\n        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1) # 128개의 입력 채널, 256개의 출력 채널, 커널 크기 4x4, 스트라이드 2, 패딩 1\n        \n        # 분류를 위한 완전 연결 레이어\n        self.fc1 = nn.Linear(256 * 8 * 8, 1)  # 입력 크기 256x8x8 (마지막 컨볼루션 레이어의 출력 크기), 출력 크기 1 (이진 분류)\n        \n        # 활성화 함수\n        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)  # 음의 기울기 0.2를 가지는 Leaky ReLU 활성화 함수\n        self.sigmoid = nn.Sigmoid()  # 최종 출력을 위한 시그모이드 활성화 함수 (확률)\n\n    def forward(self, input):\n        # LeakyReLU 활성화 함수를 사용하여 입력을 합성곱 레이어를 통과시킴\n        x = self.leaky_relu(self.conv1(input))\n        x = self.leaky_relu(self.conv2(x))\n        x = self.leaky_relu(self.conv3(x))\n        \n        # 컨볼루션 레이어의 출력을 펼침\n        x = x.view(-1, 256 * 8 * 8)\n        \n        # 이진 분류를 위해 시그모이드 활성화 함수를 사용하는 완전 연결 레이어를 통과시킴\n        x = self.sigmoid(self.fc1(x))\n        \n        return x\n```\n\n판별자 클래스는 실제 및 생성된 비디오 프레임을 구별하는 이진 분류기로 작동합니다. 비디오 프레임의 신뢰성을 평가하여 생성기가 더 현실적인 출력물을 생성할 수 있게 안내하는 것이 목적입니다. 이 네트워크는 입력 비디오 프레임에서 계층적 특성을 추출하는 합성곱 레이어(nn.Conv2d)로 구성되어 있으며, Leaky ReLU 활성화(nn.LeakyReLU)가 음의 값에 대해 작은 경사도를 허용하면서 비선형성을 추가합니다. 특성 맵은 그 후 펼쳐지고 완전 연결 레이어(nn.Linear)를 통과한 후 시그모이드 활성화(nn.Sigmoid)로 끝나며, 이는 프레임이 실제인지 가짜인지를 나타내는 확률 점수를 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n판별자를 올바르게 분류하도록 훈련함으로써 생성자는 더 설득력있는 비디오 프레임을 만들기 위해 동시에 훈련됩니다. 이것은 생성자가 판별자를 속이려고 할 때 발생합니다.\n\n# 코딩 훈련 매개변수\n\nGAN을 훈련하기 위해 손실 함수, 옵티마이저 등과 같은 기본 구성 요소를 설정해야 합니다. \n\n```js\n# GPU 사용 가능 여부 확인\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 텍스트 프롬프트를 위한 간단한 어휘 생성\nall_prompts = [prompt for prompt, _, _ in prompts_and_movements]  # prompts_and_movements 리스트에서 모든 프롬프트 추출\nvocab = {word: idx for idx, word in enumerate(set(\" \".join(all_prompts).split()))}  # 각 고유 단어에 인덱스가 할당된 어휘 사전 생성\nvocab_size = len(vocab)  # 어휘 크기\nembed_size = 10  # 텍스트 임베딩 벡터 크기\n\ndef encode_text(prompt):\n    # 주어진 프롬프트를 어휘를 사용하여 인덱스의 텐서로 인코딩\n    return torch.tensor([vocab[word] for word in prompt.split()])\n\n# 모델, 손실 함수, 옵티마이저 초기화\ntext_embedding = TextEmbedding(vocab_size, embed_size).to(device)  # vocab_size 및 embed_size로 TextEmbedding 모델 초기화\nnetG = Generator(embed_size).to(device)  # embed_size로 Generator 모델 초기화\nnetD = Discriminator().to(device)  # Discriminator 모델 초기화\ncriterion = nn.BCELoss().to(device)  # 이진 교차 엔트로피 손실 함수\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))  # 판별자에 대한 Adam 옵티마이저\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))  # 생성자에 대한 Adam 옵티마이저\n```\n\n<div class=\"content-ad\"></div>\n\n이 부분은 사용 가능한 GPU에서 코드를 실행할 수 있도록 변환해야 하는 부분입니다. 우리는 vocab_size를 찾기 위해 코드를 작성했고, 생성자와 구분자 모두 ADAM 옵티마이저를 사용하고 있습니다. 원하는 경우 자체 옵티마이저를 선택할 수 있습니다. 여기서 학습률을 0.0002와 같이 작은 값으로 설정하고, 임베딩 크기는 다른 Hugging Face 모델과 비교했을 때 훨씬 작은 10으로 설정하였습니다.\n\n# 훈련 루프 코딩\n\n다른 모든 신경망과 마찬가지로 GAN 아키텍처 훈련을 유사한 방식으로 코딩할 것입니다.\n\n```js\n# 에폭 수\nnum_epochs = 13\n\n# 각 에폭을 반복\nfor epoch in range(num_epochs):\n    # 각 데이터 배치를 반복\n    for i, (data, prompts) in enumerate(dataloader):\n        # 실제 데이터를 장치로 이동\n        real_data = data.to(device)\n        \n        # 프롬프트를 리스트로 변환\n        prompts = [prompt for prompt in prompts]\n\n        # 구분자 업데이트\n        netD.zero_grad()  # 구분자의 기울기를 0으로 초기화\n        batch_size = real_data.size(0)  # 배치 크기 가져오기\n        labels = torch.ones(batch_size, 1).to(device)  # 실제 데이터용 레이블 생성 (1)\n        output = netD(real_data)  # 실제 데이터를 구분자를 통과시키면서 순전파\n        lossD_real = criterion(output, labels)  # 실제 데이터에 대한 손실 계산\n        lossD_real.backward()  # 기울기 계산을 위한 역전파\n       \n        # 가짜 데이터 생성\n        noise = torch.randn(batch_size, 100).to(device)  # 임의의 노이즈 생성\n        text_embeds = torch.stack([text_embedding(encode_text(prompt).to(device)).mean(dim=0) for prompt in prompts])  # 프롬프트를 텍스트 임베딩으로 변환\n        fake_data = netG(noise, text_embeds)  # 노이즈와 텍스트 임베딩으로부터 가짜 데이터 생성\n        labels = torch.zeros(batch_size, 1).to(device)  # 가짜 데이터용 레이블 생성 (0)\n        output = netD(fake_data.detach())  # 가짜 데이터를 구분자를 통과시키면서 순전파 (detached를 사용하여 생성자로 그라디언트가 흐르는 것을 방지)\n        lossD_fake = criterion(output, labels)  # 가짜 데이터에 대한 손실 계산\n        lossD_fake.backward()  # 기울기 계산을 위한 역전파\n        optimizerD.step()  # 구분자 매개변수 업데이트\n\n        # 생성자 업데이트\n        netG.zero_grad()  # 생성자의 기울기를 0으로 초기화\n        labels = torch.ones(batch_size, 1).to(device)  # 구분자를 속이기 위한 가짜 데이터용 레이블 생성 (1)\n        output = netD(fake_data)  # 가짜 데이터(이제 업데이트됨)를 구분자를 통과시키면서 순전파\n        lossG = criterion(output, labels)  # 생성자의 손실 계산 (구분자의 반응에 따른)\n        lossG.backward()  # 기울기 계산을 위한 역전파\n        optimizerG.step()  # 생성자 매개변수 업데이트\n    \n    # 에폭 정보 출력\n    print(f\"에폭 [{epoch + 1}/{num_epochs}] 손실 D: {lossD_real + lossD_fake}, 손실 G: {lossG}\")\r\n```\n\n<div class=\"content-ad\"></div>\n\n백프로패게이션을 통해 생성자와 식별자의 손실이 조정될 것입니다. 우리는 훈련 루프에 13개의 에포크를 사용했습니다. 다양한 값을 테스트해 보았지만, 에포크를 13보다 높게 설정해도 결과에 큰 차이가 나타나지 않았습니다. 게다가, 과적합의 위험이 있습니다. 더 많은 움직임과 모양을 포함하는 더 다양한 데이터셋이 있다면 더 높은 에포크 값을 사용할 수 있겠지만, 현재 상황에서는 그렇지 않습니다.\n\n이 코드를 실행하면 훈련이 시작되고 각 에포크 이후에 생성자와 식별자의 손실이 출력됩니다.\n\n```js\n## 출력 ##\n\n에포크 [1/13] 손실 D: 0.8798642754554749, 손실 G: 1.300612449645996\n에포크 [2/13] 손실 D: 0.8235711455345154, 손실 G: 1.3729925155639648\n에포크 [3/13] 손실 D: 0.6098687052726746, 손실 G: 1.3266581296920776\n\n...\n```\n\n# 훈련된 모델 저장하기\n\n<div class=\"content-ad\"></div>\n\n훈련이 완료되면 훈련된 GAN 아키텍처의 판별자와 생성자를 저장해야 합니다. 이 작업은 단 두 줄의 코드로 간단히 수행할 수 있습니다.\n\n```js\n# 생성자 모델의 상태 사전을 'generator.pth'라는 파일로 저장합니다\ntorch.save(netG.state_dict(), 'generator.pth')\n\n# 판별자 모델의 상태 사전을 'discriminator.pth'라는 파일로 저장합니다\ntorch.save(netD.state_dict(), 'discriminator.pth')\n```\n\n# AI 비디오 생성\n\n토론한 바와 같이, 훈련되지 않은 데이터에 모델을 테스트하는 접근 방식은 개가 공을 던지고, 고양이가 쥐를 쫓는 훈련 데이터의 예와 비교됩니다. 따라서 테스트 프롬프트는 고양이가 공을 던지거나, 개가 쥐를 쫓는 등의 시나리오를 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n특정 경우에서, 원이 위로 움직이고 오른쪽으로 이동하는 움직임은 우리의 훈련 데이터에 포함되어 있지 않아서 모델은 이 특정 움직임을 모르고 있습니다. 그러나, 다른 움직임에는 훈련을 받았습니다. 이 움직임을 사용하여 훈련된 모델을 테스트하고 성능을 관찰할 수 있습니다.\n\n```js\n# 주어진 텍스트 코멘트를 기반으로 비디오를 생성하는 추론 함수\ndef generate_video(text_prompt, num_frames=10):\n    # 텍스트 코멘트를 기반으로 생성된 비디오 프레임을 담을 디렉토리 생성\n    os.makedirs(f'generated_video_{text_prompt.replace(\" \", \"_\")}', exist_ok=True)\n    \n    # 텍스트 코멘트를 텍스트 임베딩 텐서로 인코딩\n    text_embed = text_embedding(encode_text(text_prompt).to(device)).mean(dim=0).unsqueeze(0)\n    \n    # 비디오 프레임 생성\n    for frame_num in range(num_frames):\n        # 임의의 노이즈 생성\n        noise = torch.randn(1, 100).to(device)\n        \n        # Generator 네트워크를 사용하여 가짜 프레임 생성\n        with torch.no_grad():\n            fake_frame = netG(noise, text_embed)\n        \n        # 생성된 가짜 프레임을 이미지 파일로 저장\n        save_image(fake_frame, f'generated_video_{text_prompt.replace(\" \", \"_\")}/frame_{frame_num}.png')\n\n# 특정 텍스트 코멘트와 함께 generate_video 함수 사용 예시\ngenerate_video('circle moving up-right')\n```\n\n위의 코드를 실행하면 생성된 비디오의 모든 프레임을 포함한 디렉토리가 생성됩니다. 이러한 프레임을 모두 하나의 짧은 비디오로 합치기 위해 약간의 코드를 사용해야 합니다.\n\n```js\n# PNG 프레임을 포함한 폴더 경로 정의\nfolder_path = 'generated_video_circle_moving_up-right'\n\n# 폴더 내 모든 PNG 파일 목록 가져오기\nimage_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n\n# 이름을 기준으로 이미지 정렬 (순차적 번호로 가정)\nimage_files.sort()\n\n# 프레임을 저장할 리스트 생성\nframes = []\n\n# 각 이미지를 읽어서 frames 리스트에 추가\nfor image_file in image_files:\n  image_path = os.path.join(folder_path, image_file)\n  frame = cv2.imread(image_path)\n  frames.append(frame)\n\n# 편리한 처리를 위해 frames 리스트를 넘파이 배열로 변환\nframes = np.array(frames)\n\n# 프레임 속도 정의 (초당 프레임 수)\nfps = 10\n\n# 비디오 작성자 객체 생성\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('generated_video.avi', fourcc, fps, (frames[0].shape[1], frames[0].shape[0]))\n\n# 각 프레임을 비디오에 작성\nfor frame in frames:\n  out.write(frame)\n\n# 비디오 작성자 해제\nout.release()\n```\n\n<div class=\"content-ad\"></div>\n\n새로 생성된 비디오가 있는 폴더 경로로 가리키도록 해주세요. 이 코드를 실행한 후에는 AI 비디오가 성공적으로 생성되었습니다. 어떻게 보이는지 함께 확인해봅시다.\n\n![AI 동영상](https://miro.medium.com/v2/resize:fit:1400/1*AUioBh9zHkh2c3f3nGtpsQ.gif)\n\n동일한 에포크 횟수로 여러 번 훈련을 수행했습니다. 양쪽 경우 모두 원이 반 이상 나타나면서 아래에서 시작합니다. 좋은 점은 우리 모델이 양쪽 경우 모두 오른쪽 위로 이동을 시도했다는 것입니다. 예를 들어, 시도 1에서는 원이 대각선으로 위로 이동한 다음 위로 이동했으며, 시도 2에서는 크기가 작아지면서 대각선으로 이동했습니다. 원이 왼쪽으로 이동하거나 완전히 사라지는 경우는 없었습니다. 이것은 좋은 조짐입니다.\n\n# 무엇이 부족할까요?\n\n<div class=\"content-ad\"></div>\n\n저는 이 아키텍처의 다양한 측면을 테스트해본 결과, 훈련 데이터가 중요하다는 것을 발견했습니다. 데이터셋에 더 많은 동작과 모양을 포함시키면 변별성을 높이고 모델의 성능을 향상시킬 수 있습니다. 데이터가 코드를 통해 생성되기 때문에 더 다양한 데이터를 생성하는 데 많은 시간이 걸리지 않습니다. 대신에 논리를 정제하는 데 집중할 수 있습니다.\n\n뿐만 아니라, 이 블로그에서 논의된 GAN 아키텍처는 비교적 직관적입니다. 고급 기술을 통합하거나 기본 신경망 임베딩 대신 언어 모델 임베딩(LLM)을 사용함으로써 보다 복잡하게 만들 수 있습니다. 또한, 임베딩 크기와 같은 매개변수를 조정하는 것이 모델의 효과를 크게 좌우할 수 있습니다.\n\n# 나에 대해\n\n저는 데이터 과학 석사 학위를 가지고 있으며 NLP와 AI 분야에서 두 년 이상 일해왔습니다. 저를 고용하거나 AI 관련 문의 사항이 있으면 언제든지 저에게 물어보세요! 모든 질문에 대해 이메일로 답변드립니다.\n\n<div class=\"content-ad\"></div>\n\n제 LinkedIn 프로필에 연락하세요: [링크](https://www.linkedin.com/in/fareed-khan-dev/)\n\n이메일로 연락하세요: fareedhassankhan12@gmail.com","ogImage":{"url":"/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_0.png"},"coverImage":"/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_0.png","tag":["Tech"],"readingTime":26},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p><img src=\"/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_0.png\" alt=\"Building an AI Text-to-Video Model from Scratch Using Python\"></p>\n<p>애정하는 여러분, 반가워요!</p>\n<p>오픈AI의 Sora, 안정성 AI의 Stable Video Diffusion 등 2024년에 가장 인기 있는 인공지능 트렌드 중 하나인 텍스트-비디오 모델들이 대규모 언어 모델 이후 많이 등장하고 있습니다. 이 블로그에서는 제가 직접 작성한 작은 규모의 텍스트-비디오 모델을 만들어보려고 해요. 텍스트 프롬프트를 입력하면 학습된 모델이 해당 프롬프트를 기반으로 비디오를 생성할 거예요. 여기서는 이론적 개념을 이해하는 것부터 전체 아키텍처 코딩하고 최종 결과물을 생성하는 과정까지 모두 다루고 있어요.</p>\n<p>저는 고급 GPU를 갖고 있지 않아서 작은 규모 아키텍처를 코딩했어요. 다양한 프로세서에서 모델을 학습하는데 걸리는 시간을 비교해보겠습니다:</p>\n<p>CPU에서 실행하면 모델 학습에 훨씬 오랜 시간이 걸릴 거예요. 코드 변경을 빠르게 테스트하고 결과를 확인하려면 CPU는 최적의 선택이 아닙니다. 더 효율적이고 빠른 학습을 위해 Colab이나 Kaggle의 T4 GPU를 사용하는 것을 권장해요.</p>\n<p>더 궁금한 점이 있으시면 언제든지 물어주세요!</p>\n<p>이 블로그에서 코드를 복사하여 붙여 넣는 것을 피하려면 GitHub 저장소에 노트북 파일과 모든 코드 및 정보가 포함되어 있습니다.</p>\n<p>여기에는 처음부터 Stable Diffusion을 만드는 방법을 안내하는 블로그 링크가 있습니다:</p>\n<h1>목차</h1>\n<ul>\n<li>무엇을 만들고 있는가</li>\n<li>필수 조건</li>\n<li>GAN 아키텍처 이해하기\n∘ GAN이란?\n∘ 실세계 응용\n∘ GAN이 어떻게 작동하는가?\n∘ GAN 훈련 예제</li>\n<li>준비 단계 설정</li>\n<li>훈련 데이터 코딩</li>\n<li>교육 데이터 전 처리</li>\n<li>텍스트 임베딩 레이어 구현</li>\n<li>Generator 레이어 구현</li>\n<li>Discriminator 레이어 구현</li>\n<li>교육 매개변수 코딩</li>\n<li>훈련 루프 코딩</li>\n<li>훈련된 모델 저장</li>\n<li>AI 비디오 생성</li>\n<li>빠진 것은 무엇일까?</li>\n<li>나에 대해</li>\n</ul>\n<h1>무엇을 구축하고 있는가</h1>\n<p>우리는 전통적인 기계 학습 또는 딥 러닝 모델과 유사한 방식을 따를 것입니다. 데이터 세트에서 훈련을 받은 후 보이지 않는 데이터에서 테스트하는 것이 기본적인 접근 방식입니다. 텍스트에서 비디오로 변환하는 맥락에서, 우리는 10만 개의 비디오 데이터 세트를 사용하여 개가 공을 가져오고 고양이가 쥐를 쫓는 내용을 가르치겠습니다. 우리 모델을 훈련시켜서 고양이가 공을 가져오거나 개가 쥐를 쫓는 비디오를 생성하도록 할 것입니다.</p>\n<p>이러한 훈련 데이터 세트는 인터넷에서 쉽게 이용할 수 있지만, 필요한 컴퓨팅 파워는 극히 높습니다. 따라서 우리는 Python 코드에서 생성된 움직이는 물체들의 비디오 데이터 세트를 사용할 것입니다.</p>\n<p>저희는 OpenAI Sora가 사용하는 확산 모델 대신 GAN(생성 적대적 신경망) 아키텍처를 사용하여 모델을 만들 것입니다. 확산 모델을 사용하려고 했지만, 메모리 요구 사항 때문에 그 기능이 중단되었습니다. GAN은 반면에 훈련과 테스트가 더 쉽고 빠릅니다.</p>\n<h1>사전 요구 사항</h1>\n<p>우리는 OOP (Object-Oriented Programming)을 사용할 것이기 때문에, 기본적인 이해를 갖고 있어야 하며 뉴럴 네트워크에 대한 이해도 필요합니다. GANs (Generative Adversarial Networks)에 대한 지식은 필수는 아니지만, 여기서 그 아키텍처를 다룰 것이므로 참고하시면 좋습니다.</p>\n<h1>GAN 아키텍처 이해</h1>\n<p>GAN을 이해하는 것은 중요합니다. 우리의 아키텍처의 많은 부분이 이에 의존하기 때문입니다. 무엇인지, 그 구성 요소는 무엇인지 등을 탐구해 보겠습니다.</p>\n<h2>GAN이란 무엇인가요?</h2>\n<p>Generative Adversarial Network (GAN)은 주어진 데이터셋으로부터 새로운 데이터(예: 이미지 또는 음악)를 생성하는 하나의 신경망과 그 데이터가 실제인지 가짜인지 구별하려는 다른 신경망이 경쟁하는 딥 러닝 모델입니다. 이 과정은 생성된 데이터가 원본과 구별할 수 없을 때까지 계속됩니다.</p>\n<h2>실제 세계 응용</h2>\n<ul>\n<li>이미지 생성: GAN은 텍스트 프롬프트로 현실적인 이미지를 생성하거나 기존 이미지를 수정하여 해상도를 향상시키거나 흑백 사진에 색상을 추가합니다.</li>\n<li>데이터 증강: 다른 기계 학습 모델을 훈련하기 위해 합성 데이터를 생성하며, 예를 들어 사기 탐지 시스템을 위해 사기 거래 데이터를 생성합니다.</li>\n<li>누락된 정보 완성: GAN은 누락된 데이터를 채울 수 있으며, 에너지 응용 프로그램에 대한 지형 지도에서 장력 이미지를 생성하는 등의 작업을 수행할 수 있습니다.</li>\n<li>3D 모델 생성: 2D 이미지를 3D 모델로 변환하여, 수술 계획을 위해 현실적인 장기 이미지를 생성하는 의료 분야와 같은 분야에서 유용합니다.</li>\n</ul>\n<h2>GAN이 작동하는 방식은?</h2>\n<p>GAN은 생성자(generator)와 판별자(discriminator) 두 개의 딥 뉴럴 네트워크로 구성되어 있습니다. 이 두 네트워크는 적대적인 설정에서 함께 훈련되며, 하나는 새로운 데이터를 생성하고 다른 하나는 데이터가 진짜인지 가짜인지 판별합니다.</p>\n<p>다음은 GAN 작동 방식의 간단한 개요입니다:</p>\n<ul>\n<li>훈련 데이터 분석: 생성자는 훈련 데이터를 분석하여 데이터 특성을 식별하고, 판별자는 독립적으로 동일한 데이터를 분석하여 해당 특성을 학습합니다.</li>\n<li>데이터 수정: 생성자는 데이터의 일부 특성에 잡음(랜덤 변경)을 추가합니다.</li>\n<li>데이터 전달: 수정된 데이터는 그런 다음 판별자에게 전달됩니다.</li>\n<li>확률 계산: 판별자는 생성된 데이터가 원본 데이터셋에서 온 확률을 계산합니다.</li>\n<li>피드백 루프: 판별자는 생성자에게 피드백을 제공하여 다음 주기에서 잡음을 줄이도록 안내합니다.</li>\n<li>적대적 훈련: 생성자는 판별자의 오류를 최대화하려고 하고, 판별자는 자신의 에러를 최소화하려고 합니다. 많은 훈련 반복을 통해 두 네트워크는 개선되고 발전합니다.</li>\n<li>평형 상태: 판별자가 더 이상 진짜와 생성된 데이터를 구별하지 못할 때까지 훈련이 계속되며, 이는 생성자가 현실적인 데이터를 생성하는 것을 성공적으로 배웠음을 나타냅니다. 이 시점에서 훈련 과정이 완료됩니다.</li>\n</ul>\n<h2>GAN 훈련 예시</h2>\n<p>이미지 간 변환을 예로 들어 GAN 모델을 설명해 보겠습니다. 이때는 인간의 얼굴을 수정하는 데 초점을 맞춥니다.</p>\n<ul>\n<li>입력 이미지: 입력은 실제 인간 얼굴 이미지입니다.</li>\n<li>속성 수정: 생성자는 눈에 선글라스를 추가하는 등 얼굴의 속성을 수정합니다.</li>\n<li>생성된 이미지: 생성자는 선글라스가 추가된 이미지 세트를 생성합니다.</li>\n<li>판별자의 역할: 판별자는 실제 이미지(선글라스를 쓴 사람)와 생성된 이미지(선글라스가 추가된 얼굴)의 혼합을 받습니다.</li>\n<li>평가: 판별자는 실제 이미지와 생성된 이미지를 구별하려고 합니다.</li>\n<li>피드백 루프: 만약 판별자가 가짜 이미지를 올바르게 식별하면 생성자는 더 현실적인 이미지를 만들기 위해 매개변수를 조정합니다. 생성자가 판별자를 성공적으로 속이면 판별자는 감지를 개선하기 위해 매개변수를 업데이트합니다.</li>\n</ul>\n<p>이 적대적 프로세스를 통해 두 네트워크가 계속해서 발전합니다. 생성자는 현실적인 이미지를 만드는 데 더 잘해지고 판별자는 가짜를 식별하는 데 더 잘해지며 균형이 이루어질 때까지 계속 발전합니다. 판별자가 더 이상 실제 이미지와 생성된 이미지를 구별하지 못할 정도로 적절한 평형점에 도달하면 GAN이 성공적으로 현실적인 수정을 만들기 위해 배웠다고 볼 수 있습니다.</p>\n<h1>무대를 준비합니다</h1>\n<p>파이썬 라이브러리 그룹과 작업할 것입니다. 그러니, 이제 그들을 가져오겠습니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># 운영 체제와 상호 작용하기 위한 운영 체제 모듈</span>\n<span class=\"hljs-keyword\">import</span> os\n\n<span class=\"hljs-comment\"># 무작위 숫자 생성을 위한 모듈</span>\n<span class=\"hljs-keyword\">import</span> random\n\n<span class=\"hljs-comment\"># 숫자 연산을 위한 모듈</span>\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n\n<span class=\"hljs-comment\"># 이미지 처리를 위한 OpenCV 라이브러리</span>\n<span class=\"hljs-keyword\">import</span> cv2\n\n<span class=\"hljs-comment\"># 이미지 처리를 위한 Python Imaging Library</span>\n<span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\">import</span> Image, ImageDraw, ImageFont\n\n<span class=\"hljs-comment\"># 딥 러닝을 위한 PyTorch 라이브러리</span>\n<span class=\"hljs-keyword\">import</span> torch\n\n<span class=\"hljs-comment\"># PyTorch에서 사용자 정의 데이터셋 만들기 위한 Dataset 클래스</span>\n<span class=\"hljs-keyword\">from</span> torch.utils.data <span class=\"hljs-keyword\">import</span> Dataset\n\n<span class=\"hljs-comment\"># 이미지 변환을 위한 모듈</span>\n<span class=\"hljs-keyword\">import</span> torchvision.transforms <span class=\"hljs-keyword\">as</span> transforms\n\n<span class=\"hljs-comment\"># PyTorch의 신경망 모듈</span>\n<span class=\"hljs-keyword\">import</span> torch.nn <span class=\"hljs-keyword\">as</span> nn\n\n<span class=\"hljs-comment\"># PyTorch의 최적화 알고리즘</span>\n<span class=\"hljs-keyword\">import</span> torch.optim <span class=\"hljs-keyword\">as</span> optim\n\n<span class=\"hljs-comment\"># PyTorch에서 시퀀스를 패딩하는 함수</span>\n<span class=\"hljs-keyword\">from</span> torch.nn.utils.rnn <span class=\"hljs-keyword\">import</span> pad_sequence\n\n<span class=\"hljs-comment\"># PyTorch에서 이미지 저장하는 함수</span>\n<span class=\"hljs-keyword\">from</span> torchvision.utils <span class=\"hljs-keyword\">import</span> save_image\n\n<span class=\"hljs-comment\"># 그래프 및 이미지 플로팅을 위한 모듈</span>\n<span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt\n\n<span class=\"hljs-comment\"># IPython 환경에서 풍부한 콘텐츠 표시를 위한 모듈</span>\n<span class=\"hljs-keyword\">from</span> IPython.display <span class=\"hljs-keyword\">import</span> clear_output, display, HTML\n\n<span class=\"hljs-comment\"># 이진 데이터를 텍스트로 인코딩 및 디코딩하는 모듈</span>\n<span class=\"hljs-keyword\">import</span> base64\n</code></pre>\n<p>모든 라이브러리를 가져왔으니, 다음 단계는 GAN 아키텍처를 훈련할 때 사용할 훈련 데이터를 정의하는 것입니다.</p>\n<h1>훈련 데이터 코딩하기</h1>\n<p>적어도 10,000개의 비디오가 훈련 데이터로 필요해요. 왜냐하면 작은 숫자로 테스트해봤더니 결과가 매우 좋지 않았어요. 거의 아무것도 보이지 않았죠. 다음으로 중요한 질문은 무엇일까요? 이 비디오들은 무엇에 관한 걸까요? 우리의 훈련 비디오 데이터셋은 서로 다른 방향으로 움직이는 원을 포함해요. 그래서 이제 코드를 작성하고 10,000개의 비디오를 생성해보죠.</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-string\">'training_dataset'</span>이름의 디렉토리 생성\nos.<span class=\"hljs-title function_\">makedirs</span>(<span class=\"hljs-string\">'training_dataset'</span>, exist_ok=<span class=\"hljs-title class_\">True</span>)\n\n# 데이터셋을 생성할 비디오 수 정의\nnum_videos = <span class=\"hljs-number\">10000</span>\n\n# 비디오 당 프레임 수 정의 (<span class=\"hljs-number\">1</span>초 비디오)\nframes_per_video = <span class=\"hljs-number\">10</span>\n\n# 데이터셋 내 각 이미지의 크기 정의\nimg_size = (<span class=\"hljs-number\">64</span>, <span class=\"hljs-number\">64</span>)\n\n# 모양의 크기 정의 (원)\nshape_size = <span class=\"hljs-number\">10</span>\n</code></pre>\n<p>기본 매개변수를 설정한 후, 다음은 훈련 데이터셋의 텍스트 프롬프트를 정의해야해요.</p>\n<pre><code class=\"hljs language-js\"># 원에 대한 텍스트 프롬프트와 해당 움직임을 정의합니다.\nprompts_and_movements = [\n    (<span class=\"hljs-string\">\"circle moving down\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"down\"</span>),  # 원을 아래로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving left\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"left\"</span>),  # 원을 왼쪽으로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving right\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"right\"</span>),  # 원을 오른쪽으로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving diagonally up-right\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"diagonal_up_right\"</span>),  # 원을 대각선으로 위쪽 오른쪽으로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving diagonally down-left\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"diagonal_down_left\"</span>),  # 원을 대각선으로 아래쪽 왼쪽으로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving diagonally up-left\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"diagonal_up_left\"</span>),  # 원을 대각선으로 위쪽 왼쪽으로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving diagonally down-right\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"diagonal_down_right\"</span>),  # 원을 대각선으로 아래쪽 오른쪽으로 움직입니다\n    (<span class=\"hljs-string\">\"circle rotating clockwise\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"rotate_clockwise\"</span>),  # 원을 시계방향으로 회전시킵니다\n    (<span class=\"hljs-string\">\"circle rotating counter-clockwise\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"rotate_counter_clockwise\"</span>),  # 원을 반시계방향으로 회전시킵니다\n    (<span class=\"hljs-string\">\"circle shrinking\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"shrink\"</span>),  # 원을 축소시킵니다\n    (<span class=\"hljs-string\">\"circle expanding\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"expand\"</span>),  # 원을 확대시킵니다\n    (<span class=\"hljs-string\">\"circle bouncing vertically\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"bounce_vertical\"</span>),  # 원을 수직으로 튕기게 합니다\n    (<span class=\"hljs-string\">\"circle bouncing horizontally\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"bounce_horizontal\"</span>),  # 원을 수평으로 튕기게 합니다\n    (<span class=\"hljs-string\">\"circle zigzagging vertically\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"zigzag_vertical\"</span>),  # 원을 수직으로 지그재그로 움직입니다\n    (<span class=\"hljs-string\">\"circle zigzagging horizontally\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"zigzag_horizontal\"</span>),  # 원을 수평으로 지그재그로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving up-left\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"up_left\"</span>),  # 원을 왼쪽 위로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving down-right\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"down_right\"</span>),  # 원을 오른쪽 아래로 움직입니다\n    (<span class=\"hljs-string\">\"circle moving down-left\"</span>, <span class=\"hljs-string\">\"circle\"</span>, <span class=\"hljs-string\">\"down_left\"</span>),  # 원을 왼쪽 아래로 움직입니다\n]\n</code></pre>\n<p>이제 이러한 프롬프트를 사용하여 원의 여러 움직임을 정의했습니다. 다음으로 프롬프트를 기반으로 이 원을 움직이는 수학적 방정식을 코딩해야 합니다.</p>\n<pre><code class=\"hljs language-js\"># 매개변수가 있는 함수 정의\ndef <span class=\"hljs-title function_\">create_image_with_moving_shape</span>(size, frame_num, shape, direction):\n  \n    # 특정 크기와 흰색 배경으로 새로운 <span class=\"hljs-variable constant_\">RGB</span> 이미지를 생성합니다\n    img = <span class=\"hljs-title class_\">Image</span>.<span class=\"hljs-title function_\">new</span>(<span class=\"hljs-string\">'RGB'</span>, size, color=(<span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">255</span>))  \n\n    # 이미지에 대한 그리기 컨텍스트를 생성합니다\n    draw = <span class=\"hljs-title class_\">ImageDraw</span>.<span class=\"hljs-title class_\">Draw</span>(img)  \n\n    # 이미지의 중앙 좌표를 계산합니다\n    center_x, center_y = size[<span class=\"hljs-number\">0</span>] <span class=\"hljs-comment\">// 2, size[1] // 2  </span>\n\n    # 모든 움직임의 중심으로 위치를 초기화합니다\n    position = (center_x, center_y)  \n\n    # 각 방향을 해당 위치 조정이나 이미지 변환으로 매핑하는 딕셔너리를 정의합니다\n    direction_map = {  \n        # 프레임 번호에 따라 아래로 위치 조정\n        <span class=\"hljs-string\">\"down\"</span>: (<span class=\"hljs-number\">0</span>, frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 프레임 번호에 따라 왼쪽으로 위치 조정\n        <span class=\"hljs-string\">\"left\"</span>: (-frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], <span class=\"hljs-number\">0</span>),  \n        # 프레임 번호에 따라 오른쪽으로 위치 조정\n        <span class=\"hljs-string\">\"right\"</span>: (frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], <span class=\"hljs-number\">0</span>),  \n        # 대각선 위 오른쪽으로 위치 조정\n        <span class=\"hljs-string\">\"diagonal_up_right\"</span>: (frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], -frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 대각선 아래 왼쪽으로 위치 조정\n        <span class=\"hljs-string\">\"diagonal_down_left\"</span>: (-frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 대각선 위 왼쪽으로 위치 조정\n        <span class=\"hljs-string\">\"diagonal_up_left\"</span>: (-frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], -frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 대각선 아래 오른쪽으로 위치 조정\n        <span class=\"hljs-string\">\"diagonal_down_right\"</span>: (frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 프레임 번호에 따라 이미지를 시계방향으로 회전\n        <span class=\"hljs-string\">\"rotate_clockwise\"</span>: img.<span class=\"hljs-title function_\">rotate</span>(frame_num * <span class=\"hljs-number\">10</span> % <span class=\"hljs-number\">360</span>, center=(center_x, center_y), fillcolor=(<span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">255</span>)),  \n        # 프레임 번호에 따라 이미지를 반시계방향으로 회전\n        <span class=\"hljs-string\">\"rotate_counter_clockwise\"</span>: img.<span class=\"hljs-title function_\">rotate</span>(-frame_num * <span class=\"hljs-number\">10</span> % <span class=\"hljs-number\">360</span>, center=(center_x, center_y), fillcolor=(<span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">255</span>, <span class=\"hljs-number\">255</span>)),  \n        # 상하로 튕기는 효과를 위해 위치 조정\n        <span class=\"hljs-string\">\"bounce_vertical\"</span>: (<span class=\"hljs-number\">0</span>, center_y - <span class=\"hljs-title function_\">abs</span>(frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>] - center_y)),  \n        # 좌우로 튕기는 효과를 위해 위치 조정\n        <span class=\"hljs-string\">\"bounce_horizontal\"</span>: (center_x - <span class=\"hljs-title function_\">abs</span>(frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>] - center_x), <span class=\"hljs-number\">0</span>),  \n        # 상하로 지그재그 효과를 위해 위치 조정\n        <span class=\"hljs-string\">\"zigzag_vertical\"</span>: (<span class=\"hljs-number\">0</span>, center_y - frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]) <span class=\"hljs-keyword\">if</span> frame_num % <span class=\"hljs-number\">2</span> == <span class=\"hljs-number\">0</span> <span class=\"hljs-keyword\">else</span> (<span class=\"hljs-number\">0</span>, center_y + frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 좌우로 지그재그 효과를 위해 위치 조정\n        <span class=\"hljs-string\">\"zigzag_horizontal\"</span>: (center_x - frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], center_y) <span class=\"hljs-keyword\">if</span> frame_num % <span class=\"hljs-number\">2</span> == <span class=\"hljs-number\">0</span> <span class=\"hljs-keyword\">else</span> (center_x + frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], center_y),  \n        # 프레임 번호에 따라 오른쪽 위로 위치 조정\n        <span class=\"hljs-string\">\"up_right\"</span>: (frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], -frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 프레임 번호에 따라 왼쪽 위로 위치 조정\n        <span class=\"hljs-string\">\"up_left\"</span>: (-frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], -frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 프레임 번호에 따라 오른쪽 아래로 위치 조정\n        <span class=\"hljs-string\">\"down_right\"</span>: (frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>]),  \n        # 프레임 번호에 따라 왼쪽 아래로 위치 조정\n        <span class=\"hljs-string\">\"down_left\"</span>: (-frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">0</span>], frame_num * <span class=\"hljs-number\">5</span> % size[<span class=\"hljs-number\">1</span>])  \n    }\n\n    # direction_map에 방향이 있는지 확인합니다\n    <span class=\"hljs-keyword\">if</span> direction <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">direction_map</span>:  \n        # 방향이 위치 조정에 매핑되는지 확인합니다\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">isinstance</span>(direction_map[direction], tuple):  \n            # 조정에 따라 위치를 업데이트합니다\n            position = <span class=\"hljs-title function_\">tuple</span>(np.<span class=\"hljs-title function_\">add</span>(position, direction_map[direction]))  \n        <span class=\"hljs-attr\">else</span>:  # 방향이 이미지 변환에 매핑되는 경우\n            # 변환에 따라 이미지를 업데이트합니다\n            img = direction_map[direction]  \n\n    # 이미지를 numpy 배열로 반환합니다\n    <span class=\"hljs-keyword\">return</span> np.<span class=\"hljs-title function_\">array</span>(img)\n</code></pre>\n<p>위 함수는 선택한 방향에 따라 각 프레임마다 원을 움직이는 데 사용됩니다. 모든 비디오를 생성하기</p>\n<pre><code class=\"hljs language-js\"># 비디오 수만큼 반복하여 생성\n<span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(num_videos):\n    # 미리 정의된 목록에서 무작위로 프롬프트와 이동 선택\n    prompt, shape, direction = random.<span class=\"hljs-title function_\">choice</span>(prompts_and_movements)\n    \n    # 현재 비디오를 위한 디렉터리 생성\n    video_dir = f<span class=\"hljs-string\">'training_dataset/video_{i}'</span>\n    os.<span class=\"hljs-title function_\">makedirs</span>(video_dir, exist_ok=<span class=\"hljs-title class_\">True</span>)\n    \n    # 선택된 프롬프트를 비디오 디렉터리 내의 텍스트 파일에 작성\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-title function_\">open</span>(f<span class=\"hljs-string\">'{video_dir}/prompt.txt'</span>, <span class=\"hljs-string\">'w'</span>) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">f</span>:\n        f.<span class=\"hljs-title function_\">write</span>(prompt)\n    \n    # 현재 비디오의 프레임 생성\n    <span class=\"hljs-keyword\">for</span> frame_num <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(frames_per_video):\n        # 현재 프레임 번호, 모양, 방향을 기반으로 이동 모양이 있는 이미지 생성\n        img = <span class=\"hljs-title function_\">create_image_with_moving_shape</span>(img_size, frame_num, shape, direction)\n        \n        # 생성된 이미지를 비디오 디렉터리에 <span class=\"hljs-variable constant_\">PNG</span> 파일로 저장\n        cv2.<span class=\"hljs-title function_\">imwrite</span>(f<span class=\"hljs-string\">'{video_dir}/frame_{frame_num}.png'</span>, img)\n</code></pre>\n<p>위의 코드를 실행하면 전체 훈련 데이터 세트를 생성합니다. 훈련 데이터 세트 파일 구조는 다음과 같습니다.</p>\n<p><img src=\"/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_1.png\" alt=\"Training Dataset Structure\"></p>\n<p>각 훈련 비디오 폴더에는 프레임과 텍스트 프롬프트가 포함되어 있습니다. 훈련 데이터세트 샘플을 살펴보겠습니다.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*mzizetR6zOyIheNFtKpo0A.gif\" alt=\"https://miro.medium.com/v2/resize:fit:1400/1*mzizetR6zOyIheNFtKpo0A.gif\"></p>\n<p>훈련 데이터셋에서 원이 위로 올라가고 오른쪽으로 이동하는 동작을 포함시키지 않았습니다. 이것을 보고 우리가 훈련한 모델을 보지 않은 데이터에서 테스트하는 프롬프트로 사용할 것입니다.</p>\n<p>한 가지 더 중요한 점은 우리의 훈련 데이터에는 장면에서 멀어지는 물체나 카메라 앞에 부분적으로 나타나는 많은 샘플이 포함되어 있다는 것입니다. 이는 OpenAI 소라 데모 비디오에서 관찰한 것과 유사합니다.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*RP5M_TEt2H4Mo6OhnlcRLA.gif\" alt=\"https://miro.medium.com/v2/resize:fit:1400/1*RP5M_TEt2H4Mo6OhnlcRLA.gif\"></p>\n<p>우리가 교육 데이터에 이러한 샘플을 포함시킨 이유는 우리의 모델이 원이 극단적인 구석에서 장면에 들어오더라도 모양을 유지할 수 있는지 테스트하기 위해서입니다.</p>\n<p>이제 교육 데이터가 생성되었으므로 교육 비디오를 PyTorch와 같은 딥 러닝 프레임워크에서 주로 사용되는 기본 데이터 유형인 텐서로 변환해야 합니다. 또한, 정규화와 같은 변환 작업을 수행하여 데이터를 더 작은 범위로 스케일링하여 교육 아키텍처의 수렴과 안정성을 개선하는 데 도움이 됩니다.</p>\n<h1>교육 데이터 전처리</h1>\n<p>텍스트-비디오 작업을 위한 데이터 세트 클래스를 작성해야 합니다. 이 클래스는 교육 데이터 세트 디렉토리에서 비디오 프레임과 해당 텍스트 프롬프트를 읽어 PyTorch에서 사용할 수 있도록 만들어야 합니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># torch.utils.data.Dataset 클래스를 상속받아 데이터셋 클래스 정의</span>\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">TextToVideoDataset</span>(<span class=\"hljs-title class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, root_dir, transform=<span class=\"hljs-literal\">None</span></span>):\n        <span class=\"hljs-comment\"># 루트 디렉토리와 옵션으로 주어진 변형(transform)으로 데이터셋 초기화</span>\n        self.root_dir = root_dir\n        self.transform = transform\n        <span class=\"hljs-comment\"># 루트 디렉토리의 모든 하위 디렉토리 나열</span>\n        self.video_dirs = [os.path.join(root_dir, d) <span class=\"hljs-keyword\">for</span> d <span class=\"hljs-keyword\">in</span> os.listdir(root_dir) <span class=\"hljs-keyword\">if</span> os.path.isdir(os.path.join(root_dir, d))]\n        <span class=\"hljs-comment\"># 프레임 경로와 해당 프롬프트를 저장할 리스트 초기화</span>\n        self.frame_paths = []\n        self.prompts = []\n\n        <span class=\"hljs-comment\"># 각 비디오 디렉토리마다 반복</span>\n        <span class=\"hljs-keyword\">for</span> video_dir <span class=\"hljs-keyword\">in</span> self.video_dirs:\n            <span class=\"hljs-comment\"># 비디오 디렉토리에 있는 모든 PNG 파일 나열하고 파일 경로 저장</span>\n            frames = [os.path.join(video_dir, f) <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> os.listdir(video_dir) <span class=\"hljs-keyword\">if</span> f.endswith(<span class=\"hljs-string\">'.png'</span>)]\n            self.frame_paths.extend(frames)\n            <span class=\"hljs-comment\"># 비디오 디렉토리에 있는 프롬프트 텍스트 파일 읽어서 내용 저장</span>\n            <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(os.path.join(video_dir, <span class=\"hljs-string\">'prompt.txt'</span>), <span class=\"hljs-string\">'r'</span>) <span class=\"hljs-keyword\">as</span> f:\n                prompt = f.read().strip()\n            <span class=\"hljs-comment\"># 각 프레임에 해당하는 프롬프트를 반복해서 prompts 리스트에 저장</span>\n            self.prompts.extend([prompt] * <span class=\"hljs-built_in\">len</span>(frames))\n\n    <span class=\"hljs-comment\"># 데이터셋 전체 샘플 수 반환</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"hljs-params\">self</span>):\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">len</span>(self.frame_paths)\n\n    <span class=\"hljs-comment\"># 주어진 인덱스에 해당하는 샘플 반환</span>\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span class=\"hljs-params\">self, idx</span>):\n        <span class=\"hljs-comment\"># 주어진 인덱스에 해당하는 프레임 경로 가져오기</span>\n        frame_path = self.frame_paths[idx]\n        <span class=\"hljs-comment\"># PIL (Python Imaging Library)을 사용하여 이미지 열기</span>\n        image = Image.<span class=\"hljs-built_in\">open</span>(frame_path)\n        <span class=\"hljs-comment\"># 주어진 인덱스에 해당하는 프롬프트 가져오기</span>\n        prompt = self.prompts[idx]\n\n        <span class=\"hljs-comment\"># 지정된 경우 변형 적용</span>\n        <span class=\"hljs-keyword\">if</span> self.transform:\n            image = self.transform(image)\n\n        <span class=\"hljs-comment\"># 변형된 이미지와 프롬프트 반환</span>\n        <span class=\"hljs-keyword\">return</span> image, prompt\n</code></pre>\n<p>코드 아키텍처를 작성하기 전에 훈련 데이터를 정규화해야 합니다. 배치 크기는 16으로 설정하고 데이터를 섞어 더 많은 무작위성을 도입할 것입니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># 데이터에 적용할 변형 세트 정의</span>\ntransform = transforms.Compose([\n    transforms.ToTensor(),  <span class=\"hljs-comment\"># PIL 이미지 또는 numpy.ndarray를 텐서로 변환</span>\n    transforms.Normalize((<span class=\"hljs-number\">0.5</span>,), (<span class=\"hljs-number\">0.5</span>,))  <span class=\"hljs-comment\"># 평균과 표준편차를 사용하여 이미지 정규화</span>\n])\n\n<span class=\"hljs-comment\"># 정의된 변형을 사용하여 데이터셋 로드</span>\ndataset = TextToVideoDataset(root_dir=<span class=\"hljs-string\">'training_dataset'</span>, transform=transform)\n<span class=\"hljs-comment\"># 데이터셋을 반복할 데이터로더 생성</span>\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class=\"hljs-number\">16</span>, shuffle=<span class=\"hljs-literal\">True</span>)\n</code></pre>\n<h1>텍스트 임베딩 레이어 구현하기</h1>\n<p>트랜스포머 아키텍처에서 본 적이 있을 수 있어요. 텍스트 입력을 임베딩으로 변환하고 멀티 헤드 어텐션에서 추가로 처리할 수 있어요. 여기서는 텍스트 임베딩 레이어를 코드로 작성해야 합니다. 이 레이어를 기반으로 GAN 아키텍처 훈련이 임베딩 데이터와 이미지 텐서에서 이루어질 거에요.</p>\n<pre><code class=\"hljs language-js\"># 텍스트 임베딩을 위한 클래스 정의\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">TextEmbedding</span>(nn.<span class=\"hljs-property\">Module</span>):\n    # vocab_size와 embed_size 매개변수를 사용하는 생성자 메서드\n    def <span class=\"hljs-title function_\">__init__</span>(self, vocab_size, embed_size):\n        # 슈퍼 클래스 생성자 호출\n        <span class=\"hljs-variable language_\">super</span>(<span class=\"hljs-title class_\">TextEmbedding</span>, self).<span class=\"hljs-title function_\">__init__</span>()\n        # 임베딩 레이어 초기화\n        self.<span class=\"hljs-property\">embedding</span> = nn.<span class=\"hljs-title class_\">Embedding</span>(vocab_size, embed_size)\n\n    # 순전파 메서드 정의\n    def <span class=\"hljs-title function_\">forward</span>(self, x):\n        # 입력의 임베딩 표현 반환\n        <span class=\"hljs-keyword\">return</span> self.<span class=\"hljs-title function_\">embedding</span>(x)\n</code></pre>\n<p>어휘 사전 크기는 훈련 데이터에 기반하여 결정될 거에요. 임베딩 크기는 10일 거에요. 더 큰 데이터셋을 다룬다면 Hugging Face에서 제공하는 임베딩 모델을 선택해서 사용할 수도 있어요.</p>\n<h1>생성자 레이어 구현하기</h1>\n<p>이제 GANs에서 생성자가 하는 역할을 이미 알고 있기 때문에, 이 층을 코딩하고 내용을 이해해봅시다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Generator</span>(nn.Module):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, text_embed_size</span>):\n        <span class=\"hljs-built_in\">super</span>(Generator, self).__init__()\n        \n        <span class=\"hljs-comment\"># 노이즈와 텍스트 임베딩을 입력으로 받는 완전 연결층</span>\n        self.fc1 = nn.Linear(<span class=\"hljs-number\">100</span> + text_embed_size, <span class=\"hljs-number\">256</span> * <span class=\"hljs-number\">8</span> * <span class=\"hljs-number\">8</span>)\n        \n        <span class=\"hljs-comment\"># 입력을 업샘플링하는 전치 합성곱층</span>\n        self.deconv1 = nn.ConvTranspose2d(<span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">128</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>)\n        self.deconv2 = nn.ConvTranspose2d(<span class=\"hljs-number\">128</span>, <span class=\"hljs-number\">64</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>)\n        self.deconv3 = nn.ConvTranspose2d(<span class=\"hljs-number\">64</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>)  <span class=\"hljs-comment\"># RGB 이미지에 대한 출력은 3채널이 됩니다</span>\n        \n        <span class=\"hljs-comment\"># 활성화 함수</span>\n        self.relu = nn.ReLU(<span class=\"hljs-literal\">True</span>)  <span class=\"hljs-comment\"># ReLU 활성화 함수</span>\n        self.tanh = nn.Tanh()       <span class=\"hljs-comment\"># 최종 출력을 -1과 1 사이의 값으로 만들기 위한 Tanh 활성화 함수</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, noise, text_embed</span>):\n        <span class=\"hljs-comment\"># 채널 차원을 따라 노이즈와 텍스트 임베딩을 연결합니다</span>\n        x = torch.cat((noise, text_embed), dim=<span class=\"hljs-number\">1</span>)\n        \n        <span class=\"hljs-comment\"># 완전 연결층 다음에 4D 텐서로 재구성합니다</span>\n        x = self.fc1(x).view(-<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">8</span>)\n        \n        <span class=\"hljs-comment\"># ReLU 활성화를 사용하여 전치 합성곱층을 통한 업샘플링</span>\n        x = self.relu(self.deconv1(x))\n        x = self.relu(self.deconv2(x))\n        \n        <span class=\"hljs-comment\"># 최종 층에서 출력 값을 -1과 1 사이로 만들기 위해 Tanh 활성화를 사용합니다 (이미지용)</span>\n        x = self.tanh(self.deconv3(x))\n        \n        <span class=\"hljs-keyword\">return</span> x\n</code></pre>\n<p>이 생성자 클래스는 무작위 노이즈와 텍스트 임베딩의 결합에서 비디오 프레임을 생성하는 역할을 합니다. 목표는 주어진 텍스트 설명에 조건부로 현실적인 비디오 프레임을 생성하는 것입니다. 네트워크는 노이즈 벡터와 텍스트 임베딩을 하나의 특성 벡터로 결합하는 완전 연결층 (nn.Linear)으로 시작합니다. 이 벡터는 재구성되고, 레이어는 원하는 비디오 프레임 크기로 특성 맵을 점진적으로 업샘플링하는 일련의 전치 합성곱층 (nn.ConvTranspose2d)을 통해 전달됩니다.</p>\n<p>이 레이어들은 비선형성을 위해 ReLU 활성화 함수(nn.ReLU)를 사용하고, 최종 레이어는 출력을 [-1, 1] 범위로 스케일링하기 위해 Tanh 활성화(nn.Tanh)를 사용합니다. 따라서 생성자는 추상적이고 고차원의 입력을 시각적으로 입력 텍스트를 잘 나타내는 일관된 비디오 프레임으로 변환합니다.</p>\n<h1>판별자 레이어 구현</h1>\n<p>제너레이터 레이어를 코딩한 후에는 이제 판별자 부분을 구현해야 합니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Discriminator</span>(nn.<span class=\"hljs-property\">Module</span>):\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n        <span class=\"hljs-variable language_\">super</span>(<span class=\"hljs-title class_\">Discriminator</span>, self).<span class=\"hljs-title function_\">__init__</span>()\n        \n        # 입력 이미지를 처리하기 위한 합성곱 레이어\n        self.<span class=\"hljs-property\">conv1</span> = nn.<span class=\"hljs-title class_\">Conv2</span>d(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">64</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>)   # <span class=\"hljs-number\">3</span>개의 입력 채널 (<span class=\"hljs-variable constant_\">RGB</span>), <span class=\"hljs-number\">64</span>개의 출력 채널, 커널 크기 4x4, 스트라이드 <span class=\"hljs-number\">2</span>, 패딩 <span class=\"hljs-number\">1</span>\n        self.<span class=\"hljs-property\">conv2</span> = nn.<span class=\"hljs-title class_\">Conv2</span>d(<span class=\"hljs-number\">64</span>, <span class=\"hljs-number\">128</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>) # <span class=\"hljs-number\">64</span>개의 입력 채널, <span class=\"hljs-number\">128</span>개의 출력 채널, 커널 크기 4x4, 스트라이드 <span class=\"hljs-number\">2</span>, 패딩 <span class=\"hljs-number\">1</span>\n        self.<span class=\"hljs-property\">conv3</span> = nn.<span class=\"hljs-title class_\">Conv2</span>d(<span class=\"hljs-number\">128</span>, <span class=\"hljs-number\">256</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>) # <span class=\"hljs-number\">128</span>개의 입력 채널, <span class=\"hljs-number\">256</span>개의 출력 채널, 커널 크기 4x4, 스트라이드 <span class=\"hljs-number\">2</span>, 패딩 <span class=\"hljs-number\">1</span>\n        \n        # 분류를 위한 완전 연결 레이어\n        self.<span class=\"hljs-property\">fc1</span> = nn.<span class=\"hljs-title class_\">Linear</span>(<span class=\"hljs-number\">256</span> * <span class=\"hljs-number\">8</span> * <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">1</span>)  # 입력 크기 256x8x8 (마지막 컨볼루션 레이어의 출력 크기), 출력 크기 <span class=\"hljs-number\">1</span> (이진 분류)\n        \n        # 활성화 함수\n        self.<span class=\"hljs-property\">leaky_relu</span> = nn.<span class=\"hljs-title class_\">LeakyReLU</span>(<span class=\"hljs-number\">0.2</span>, inplace=<span class=\"hljs-title class_\">True</span>)  # 음의 기울기 <span class=\"hljs-number\">0.2</span>를 가지는 <span class=\"hljs-title class_\">Leaky</span> <span class=\"hljs-title class_\">ReLU</span> 활성화 함수\n        self.<span class=\"hljs-property\">sigmoid</span> = nn.<span class=\"hljs-title class_\">Sigmoid</span>()  # 최종 출력을 위한 시그모이드 활성화 함수 (확률)\n\n    def <span class=\"hljs-title function_\">forward</span>(self, input):\n        # <span class=\"hljs-title class_\">LeakyReLU</span> 활성화 함수를 사용하여 입력을 합성곱 레이어를 통과시킴\n        x = self.<span class=\"hljs-title function_\">leaky_relu</span>(self.<span class=\"hljs-title function_\">conv1</span>(input))\n        x = self.<span class=\"hljs-title function_\">leaky_relu</span>(self.<span class=\"hljs-title function_\">conv2</span>(x))\n        x = self.<span class=\"hljs-title function_\">leaky_relu</span>(self.<span class=\"hljs-title function_\">conv3</span>(x))\n        \n        # 컨볼루션 레이어의 출력을 펼침\n        x = x.<span class=\"hljs-title function_\">view</span>(-<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">256</span> * <span class=\"hljs-number\">8</span> * <span class=\"hljs-number\">8</span>)\n        \n        # 이진 분류를 위해 시그모이드 활성화 함수를 사용하는 완전 연결 레이어를 통과시킴\n        x = self.<span class=\"hljs-title function_\">sigmoid</span>(self.<span class=\"hljs-title function_\">fc1</span>(x))\n        \n        <span class=\"hljs-keyword\">return</span> x\n</code></pre>\n<p>판별자 클래스는 실제 및 생성된 비디오 프레임을 구별하는 이진 분류기로 작동합니다. 비디오 프레임의 신뢰성을 평가하여 생성기가 더 현실적인 출력물을 생성할 수 있게 안내하는 것이 목적입니다. 이 네트워크는 입력 비디오 프레임에서 계층적 특성을 추출하는 합성곱 레이어(nn.Conv2d)로 구성되어 있으며, Leaky ReLU 활성화(nn.LeakyReLU)가 음의 값에 대해 작은 경사도를 허용하면서 비선형성을 추가합니다. 특성 맵은 그 후 펼쳐지고 완전 연결 레이어(nn.Linear)를 통과한 후 시그모이드 활성화(nn.Sigmoid)로 끝나며, 이는 프레임이 실제인지 가짜인지를 나타내는 확률 점수를 출력합니다.</p>\n<p>판별자를 올바르게 분류하도록 훈련함으로써 생성자는 더 설득력있는 비디오 프레임을 만들기 위해 동시에 훈련됩니다. 이것은 생성자가 판별자를 속이려고 할 때 발생합니다.</p>\n<h1>코딩 훈련 매개변수</h1>\n<p>GAN을 훈련하기 위해 손실 함수, 옵티마이저 등과 같은 기본 구성 요소를 설정해야 합니다.</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-variable constant_\">GPU</span> 사용 가능 여부 확인\ndevice = torch.<span class=\"hljs-title function_\">device</span>(<span class=\"hljs-string\">\"cuda\"</span> <span class=\"hljs-keyword\">if</span> torch.<span class=\"hljs-property\">cuda</span>.<span class=\"hljs-title function_\">is_available</span>() <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"cpu\"</span>)\n\n# 텍스트 프롬프트를 위한 간단한 어휘 생성\nall_prompts = [prompt <span class=\"hljs-keyword\">for</span> prompt, _, _ <span class=\"hljs-keyword\">in</span> prompts_and_movements]  # prompts_and_movements 리스트에서 모든 프롬프트 추출\nvocab = {<span class=\"hljs-attr\">word</span>: idx <span class=\"hljs-keyword\">for</span> idx, word <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">enumerate</span>(<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\" \"</span>.<span class=\"hljs-title function_\">join</span>(all_prompts).<span class=\"hljs-title function_\">split</span>()))}  # 각 고유 단어에 인덱스가 할당된 어휘 사전 생성\nvocab_size = <span class=\"hljs-title function_\">len</span>(vocab)  # 어휘 크기\nembed_size = <span class=\"hljs-number\">10</span>  # 텍스트 임베딩 벡터 크기\n\ndef <span class=\"hljs-title function_\">encode_text</span>(prompt):\n    # 주어진 프롬프트를 어휘를 사용하여 인덱스의 텐서로 인코딩\n    <span class=\"hljs-keyword\">return</span> torch.<span class=\"hljs-title function_\">tensor</span>([vocab[word] <span class=\"hljs-keyword\">for</span> word <span class=\"hljs-keyword\">in</span> prompt.<span class=\"hljs-title function_\">split</span>()])\n\n# 모델, 손실 함수, 옵티마이저 초기화\ntext_embedding = <span class=\"hljs-title class_\">TextEmbedding</span>(vocab_size, embed_size).<span class=\"hljs-title function_\">to</span>(device)  # vocab_size 및 embed_size로 <span class=\"hljs-title class_\">TextEmbedding</span> 모델 초기화\nnetG = <span class=\"hljs-title class_\">Generator</span>(embed_size).<span class=\"hljs-title function_\">to</span>(device)  # embed_size로 <span class=\"hljs-title class_\">Generator</span> 모델 초기화\nnetD = <span class=\"hljs-title class_\">Discriminator</span>().<span class=\"hljs-title function_\">to</span>(device)  # <span class=\"hljs-title class_\">Discriminator</span> 모델 초기화\ncriterion = nn.<span class=\"hljs-title class_\">BCELoss</span>().<span class=\"hljs-title function_\">to</span>(device)  # 이진 교차 엔트로피 손실 함수\noptimizerD = optim.<span class=\"hljs-title class_\">Adam</span>(netD.<span class=\"hljs-title function_\">parameters</span>(), lr=<span class=\"hljs-number\">0.0002</span>, betas=(<span class=\"hljs-number\">0.5</span>, <span class=\"hljs-number\">0.999</span>))  # 판별자에 대한 <span class=\"hljs-title class_\">Adam</span> 옵티마이저\noptimizerG = optim.<span class=\"hljs-title class_\">Adam</span>(netG.<span class=\"hljs-title function_\">parameters</span>(), lr=<span class=\"hljs-number\">0.0002</span>, betas=(<span class=\"hljs-number\">0.5</span>, <span class=\"hljs-number\">0.999</span>))  # 생성자에 대한 <span class=\"hljs-title class_\">Adam</span> 옵티마이저\n</code></pre>\n<p>이 부분은 사용 가능한 GPU에서 코드를 실행할 수 있도록 변환해야 하는 부분입니다. 우리는 vocab_size를 찾기 위해 코드를 작성했고, 생성자와 구분자 모두 ADAM 옵티마이저를 사용하고 있습니다. 원하는 경우 자체 옵티마이저를 선택할 수 있습니다. 여기서 학습률을 0.0002와 같이 작은 값으로 설정하고, 임베딩 크기는 다른 Hugging Face 모델과 비교했을 때 훨씬 작은 10으로 설정하였습니다.</p>\n<h1>훈련 루프 코딩</h1>\n<p>다른 모든 신경망과 마찬가지로 GAN 아키텍처 훈련을 유사한 방식으로 코딩할 것입니다.</p>\n<pre><code class=\"hljs language-js\"># 에폭 수\nnum_epochs = <span class=\"hljs-number\">13</span>\n\n# 각 에폭을 반복\n<span class=\"hljs-keyword\">for</span> epoch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(num_epochs):\n    # 각 데이터 배치를 반복\n    <span class=\"hljs-keyword\">for</span> i, (data, prompts) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">enumerate</span>(dataloader):\n        # 실제 데이터를 장치로 이동\n        real_data = data.<span class=\"hljs-title function_\">to</span>(device)\n        \n        # 프롬프트를 리스트로 변환\n        prompts = [prompt <span class=\"hljs-keyword\">for</span> prompt <span class=\"hljs-keyword\">in</span> prompts]\n\n        # 구분자 업데이트\n        netD.<span class=\"hljs-title function_\">zero_grad</span>()  # 구분자의 기울기를 <span class=\"hljs-number\">0</span>으로 초기화\n        batch_size = real_data.<span class=\"hljs-title function_\">size</span>(<span class=\"hljs-number\">0</span>)  # 배치 크기 가져오기\n        labels = torch.<span class=\"hljs-title function_\">ones</span>(batch_size, <span class=\"hljs-number\">1</span>).<span class=\"hljs-title function_\">to</span>(device)  # 실제 데이터용 레이블 생성 (<span class=\"hljs-number\">1</span>)\n        output = <span class=\"hljs-title function_\">netD</span>(real_data)  # 실제 데이터를 구분자를 통과시키면서 순전파\n        lossD_real = <span class=\"hljs-title function_\">criterion</span>(output, labels)  # 실제 데이터에 대한 손실 계산\n        lossD_real.<span class=\"hljs-title function_\">backward</span>()  # 기울기 계산을 위한 역전파\n       \n        # 가짜 데이터 생성\n        noise = torch.<span class=\"hljs-title function_\">randn</span>(batch_size, <span class=\"hljs-number\">100</span>).<span class=\"hljs-title function_\">to</span>(device)  # 임의의 노이즈 생성\n        text_embeds = torch.<span class=\"hljs-title function_\">stack</span>([<span class=\"hljs-title function_\">text_embedding</span>(<span class=\"hljs-title function_\">encode_text</span>(prompt).<span class=\"hljs-title function_\">to</span>(device)).<span class=\"hljs-title function_\">mean</span>(dim=<span class=\"hljs-number\">0</span>) <span class=\"hljs-keyword\">for</span> prompt <span class=\"hljs-keyword\">in</span> prompts])  # 프롬프트를 텍스트 임베딩으로 변환\n        fake_data = <span class=\"hljs-title function_\">netG</span>(noise, text_embeds)  # 노이즈와 텍스트 임베딩으로부터 가짜 데이터 생성\n        labels = torch.<span class=\"hljs-title function_\">zeros</span>(batch_size, <span class=\"hljs-number\">1</span>).<span class=\"hljs-title function_\">to</span>(device)  # 가짜 데이터용 레이블 생성 (<span class=\"hljs-number\">0</span>)\n        output = <span class=\"hljs-title function_\">netD</span>(fake_data.<span class=\"hljs-title function_\">detach</span>())  # 가짜 데이터를 구분자를 통과시키면서 순전파 (detached를 사용하여 생성자로 그라디언트가 흐르는 것을 방지)\n        lossD_fake = <span class=\"hljs-title function_\">criterion</span>(output, labels)  # 가짜 데이터에 대한 손실 계산\n        lossD_fake.<span class=\"hljs-title function_\">backward</span>()  # 기울기 계산을 위한 역전파\n        optimizerD.<span class=\"hljs-title function_\">step</span>()  # 구분자 매개변수 업데이트\n\n        # 생성자 업데이트\n        netG.<span class=\"hljs-title function_\">zero_grad</span>()  # 생성자의 기울기를 <span class=\"hljs-number\">0</span>으로 초기화\n        labels = torch.<span class=\"hljs-title function_\">ones</span>(batch_size, <span class=\"hljs-number\">1</span>).<span class=\"hljs-title function_\">to</span>(device)  # 구분자를 속이기 위한 가짜 데이터용 레이블 생성 (<span class=\"hljs-number\">1</span>)\n        output = <span class=\"hljs-title function_\">netD</span>(fake_data)  # 가짜 데이터(이제 업데이트됨)를 구분자를 통과시키면서 순전파\n        lossG = <span class=\"hljs-title function_\">criterion</span>(output, labels)  # 생성자의 손실 계산 (구분자의 반응에 따른)\n        lossG.<span class=\"hljs-title function_\">backward</span>()  # 기울기 계산을 위한 역전파\n        optimizerG.<span class=\"hljs-title function_\">step</span>()  # 생성자 매개변수 업데이트\n    \n    # 에폭 정보 출력\n    <span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">\"에폭 [{epoch + 1}/{num_epochs}] 손실 D: {lossD_real + lossD_fake}, 손실 G: {lossG}\"</span>)\n</code></pre>\n<p>백프로패게이션을 통해 생성자와 식별자의 손실이 조정될 것입니다. 우리는 훈련 루프에 13개의 에포크를 사용했습니다. 다양한 값을 테스트해 보았지만, 에포크를 13보다 높게 설정해도 결과에 큰 차이가 나타나지 않았습니다. 게다가, 과적합의 위험이 있습니다. 더 많은 움직임과 모양을 포함하는 더 다양한 데이터셋이 있다면 더 높은 에포크 값을 사용할 수 있겠지만, 현재 상황에서는 그렇지 않습니다.</p>\n<p>이 코드를 실행하면 훈련이 시작되고 각 에포크 이후에 생성자와 식별자의 손실이 출력됩니다.</p>\n<pre><code class=\"hljs language-js\">## 출력 ##\n\n에포크 [<span class=\"hljs-number\">1</span>/<span class=\"hljs-number\">13</span>] 손실 <span class=\"hljs-attr\">D</span>: <span class=\"hljs-number\">0.8798642754554749</span>, 손실 <span class=\"hljs-attr\">G</span>: <span class=\"hljs-number\">1.300612449645996</span>\n에포크 [<span class=\"hljs-number\">2</span>/<span class=\"hljs-number\">13</span>] 손실 <span class=\"hljs-attr\">D</span>: <span class=\"hljs-number\">0.8235711455345154</span>, 손실 <span class=\"hljs-attr\">G</span>: <span class=\"hljs-number\">1.3729925155639648</span>\n에포크 [<span class=\"hljs-number\">3</span>/<span class=\"hljs-number\">13</span>] 손실 <span class=\"hljs-attr\">D</span>: <span class=\"hljs-number\">0.6098687052726746</span>, 손실 <span class=\"hljs-attr\">G</span>: <span class=\"hljs-number\">1.3266581296920776</span>\n\n...\n</code></pre>\n<h1>훈련된 모델 저장하기</h1>\n<p>훈련이 완료되면 훈련된 GAN 아키텍처의 판별자와 생성자를 저장해야 합니다. 이 작업은 단 두 줄의 코드로 간단히 수행할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\"># 생성자 모델의 상태 사전을 <span class=\"hljs-string\">'generator.pth'</span>라는 파일로 저장합니다\ntorch.<span class=\"hljs-title function_\">save</span>(netG.<span class=\"hljs-title function_\">state_dict</span>(), <span class=\"hljs-string\">'generator.pth'</span>)\n\n# 판별자 모델의 상태 사전을 <span class=\"hljs-string\">'discriminator.pth'</span>라는 파일로 저장합니다\ntorch.<span class=\"hljs-title function_\">save</span>(netD.<span class=\"hljs-title function_\">state_dict</span>(), <span class=\"hljs-string\">'discriminator.pth'</span>)\n</code></pre>\n<h1>AI 비디오 생성</h1>\n<p>토론한 바와 같이, 훈련되지 않은 데이터에 모델을 테스트하는 접근 방식은 개가 공을 던지고, 고양이가 쥐를 쫓는 훈련 데이터의 예와 비교됩니다. 따라서 테스트 프롬프트는 고양이가 공을 던지거나, 개가 쥐를 쫓는 등의 시나리오를 포함할 수 있습니다.</p>\n<p>특정 경우에서, 원이 위로 움직이고 오른쪽으로 이동하는 움직임은 우리의 훈련 데이터에 포함되어 있지 않아서 모델은 이 특정 움직임을 모르고 있습니다. 그러나, 다른 움직임에는 훈련을 받았습니다. 이 움직임을 사용하여 훈련된 모델을 테스트하고 성능을 관찰할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\"># 주어진 텍스트 코멘트를 기반으로 비디오를 생성하는 추론 함수\ndef <span class=\"hljs-title function_\">generate_video</span>(text_prompt, num_frames=<span class=\"hljs-number\">10</span>):\n    # 텍스트 코멘트를 기반으로 생성된 비디오 프레임을 담을 디렉토리 생성\n    os.<span class=\"hljs-title function_\">makedirs</span>(f<span class=\"hljs-string\">'generated_video_{text_prompt.replace(\" \", \"_\")}'</span>, exist_ok=<span class=\"hljs-title class_\">True</span>)\n    \n    # 텍스트 코멘트를 텍스트 임베딩 텐서로 인코딩\n    text_embed = <span class=\"hljs-title function_\">text_embedding</span>(<span class=\"hljs-title function_\">encode_text</span>(text_prompt).<span class=\"hljs-title function_\">to</span>(device)).<span class=\"hljs-title function_\">mean</span>(dim=<span class=\"hljs-number\">0</span>).<span class=\"hljs-title function_\">unsqueeze</span>(<span class=\"hljs-number\">0</span>)\n    \n    # 비디오 프레임 생성\n    <span class=\"hljs-keyword\">for</span> frame_num <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(num_frames):\n        # 임의의 노이즈 생성\n        noise = torch.<span class=\"hljs-title function_\">randn</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">100</span>).<span class=\"hljs-title function_\">to</span>(device)\n        \n        # <span class=\"hljs-title class_\">Generator</span> 네트워크를 사용하여 가짜 프레임 생성\n        <span class=\"hljs-keyword\">with</span> torch.<span class=\"hljs-title function_\">no_grad</span>():\n            fake_frame = <span class=\"hljs-title function_\">netG</span>(noise, text_embed)\n        \n        # 생성된 가짜 프레임을 이미지 파일로 저장\n        <span class=\"hljs-title function_\">save_image</span>(fake_frame, f<span class=\"hljs-string\">'generated_video_{text_prompt.replace(\" \", \"_\")}/frame_{frame_num}.png'</span>)\n\n# 특정 텍스트 코멘트와 함께 generate_video 함수 사용 예시\n<span class=\"hljs-title function_\">generate_video</span>(<span class=\"hljs-string\">'circle moving up-right'</span>)\n</code></pre>\n<p>위의 코드를 실행하면 생성된 비디오의 모든 프레임을 포함한 디렉토리가 생성됩니다. 이러한 프레임을 모두 하나의 짧은 비디오로 합치기 위해 약간의 코드를 사용해야 합니다.</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-variable constant_\">PNG</span> 프레임을 포함한 폴더 경로 정의\nfolder_path = <span class=\"hljs-string\">'generated_video_circle_moving_up-right'</span>\n\n# 폴더 내 모든 <span class=\"hljs-variable constant_\">PNG</span> 파일 목록 가져오기\nimage_files = [f <span class=\"hljs-keyword\">for</span> f <span class=\"hljs-keyword\">in</span> os.<span class=\"hljs-title function_\">listdir</span>(folder_path) <span class=\"hljs-keyword\">if</span> f.<span class=\"hljs-title function_\">endswith</span>(<span class=\"hljs-string\">'.png'</span>)]\n\n# 이름을 기준으로 이미지 정렬 (순차적 번호로 가정)\nimage_files.<span class=\"hljs-title function_\">sort</span>()\n\n# 프레임을 저장할 리스트 생성\nframes = []\n\n# 각 이미지를 읽어서 frames 리스트에 추가\n<span class=\"hljs-keyword\">for</span> image_file <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">image_files</span>:\n  image_path = os.<span class=\"hljs-property\">path</span>.<span class=\"hljs-title function_\">join</span>(folder_path, image_file)\n  frame = cv2.<span class=\"hljs-title function_\">imread</span>(image_path)\n  frames.<span class=\"hljs-title function_\">append</span>(frame)\n\n# 편리한 처리를 위해 frames 리스트를 넘파이 배열로 변환\nframes = np.<span class=\"hljs-title function_\">array</span>(frames)\n\n# 프레임 속도 정의 (초당 프레임 수)\nfps = <span class=\"hljs-number\">10</span>\n\n# 비디오 작성자 객체 생성\nfourcc = cv2.<span class=\"hljs-title class_\">VideoWriter</span>_fourcc(*<span class=\"hljs-string\">'XVID'</span>)\nout = cv2.<span class=\"hljs-title class_\">VideoWriter</span>(<span class=\"hljs-string\">'generated_video.avi'</span>, fourcc, fps, (frames[<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">shape</span>[<span class=\"hljs-number\">1</span>], frames[<span class=\"hljs-number\">0</span>].<span class=\"hljs-property\">shape</span>[<span class=\"hljs-number\">0</span>]))\n\n# 각 프레임을 비디오에 작성\n<span class=\"hljs-keyword\">for</span> frame <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">frames</span>:\n  out.<span class=\"hljs-title function_\">write</span>(frame)\n\n# 비디오 작성자 해제\nout.<span class=\"hljs-title function_\">release</span>()\n</code></pre>\n<p>새로 생성된 비디오가 있는 폴더 경로로 가리키도록 해주세요. 이 코드를 실행한 후에는 AI 비디오가 성공적으로 생성되었습니다. 어떻게 보이는지 함께 확인해봅시다.</p>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*AUioBh9zHkh2c3f3nGtpsQ.gif\" alt=\"AI 동영상\"></p>\n<p>동일한 에포크 횟수로 여러 번 훈련을 수행했습니다. 양쪽 경우 모두 원이 반 이상 나타나면서 아래에서 시작합니다. 좋은 점은 우리 모델이 양쪽 경우 모두 오른쪽 위로 이동을 시도했다는 것입니다. 예를 들어, 시도 1에서는 원이 대각선으로 위로 이동한 다음 위로 이동했으며, 시도 2에서는 크기가 작아지면서 대각선으로 이동했습니다. 원이 왼쪽으로 이동하거나 완전히 사라지는 경우는 없었습니다. 이것은 좋은 조짐입니다.</p>\n<h1>무엇이 부족할까요?</h1>\n<p>저는 이 아키텍처의 다양한 측면을 테스트해본 결과, 훈련 데이터가 중요하다는 것을 발견했습니다. 데이터셋에 더 많은 동작과 모양을 포함시키면 변별성을 높이고 모델의 성능을 향상시킬 수 있습니다. 데이터가 코드를 통해 생성되기 때문에 더 다양한 데이터를 생성하는 데 많은 시간이 걸리지 않습니다. 대신에 논리를 정제하는 데 집중할 수 있습니다.</p>\n<p>뿐만 아니라, 이 블로그에서 논의된 GAN 아키텍처는 비교적 직관적입니다. 고급 기술을 통합하거나 기본 신경망 임베딩 대신 언어 모델 임베딩(LLM)을 사용함으로써 보다 복잡하게 만들 수 있습니다. 또한, 임베딩 크기와 같은 매개변수를 조정하는 것이 모델의 효과를 크게 좌우할 수 있습니다.</p>\n<h1>나에 대해</h1>\n<p>저는 데이터 과학 석사 학위를 가지고 있으며 NLP와 AI 분야에서 두 년 이상 일해왔습니다. 저를 고용하거나 AI 관련 문의 사항이 있으면 언제든지 저에게 물어보세요! 모든 질문에 대해 이메일로 답변드립니다.</p>\n<p>제 LinkedIn 프로필에 연락하세요: <a href=\"https://www.linkedin.com/in/fareed-khan-dev/\" rel=\"nofollow\" target=\"_blank\">링크</a></p>\n<p>이메일로 연락하세요: <a href=\"mailto:fareedhassankhan12@gmail.com\">fareedhassankhan12@gmail.com</a></p>\n</body>\n</html>\n"},"__N_SSG":true}