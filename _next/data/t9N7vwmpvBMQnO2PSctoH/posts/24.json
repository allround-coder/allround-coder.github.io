{"pageProps":{"posts":[{"title":"파이썬과 QGIS로 인도의 낙타 80 찾는 방법","description":"","date":"2024-06-22 02:56","slug":"2024-06-22-HereshowIusedPythonandQGIStofindout80ofIndiascamelpopulation","content":"\n\n## 파이썬 자동화\n\n\"안녕 Aayush, 내가 인도의 저소득 농촌 여성들을 위한 낙타 기반 생계 개선에 집중해야 할 곳을 알고 싶어\", 라는 요구가 왔어요. 라자스탄 출신인 내 친구가 말했어요. 그녀는 라자스탄의 사막 지역이 정답일 것이라고 알고 있었지만, 직감을 뒷받침할 공식 자료가 필요했어요. 그래서 제가 나서서 이 정보를 찾기로 했어요. \n\n## 단계 1\n\n인도 정부의 데이터 제공 플랫폼인 data.gov.in 에서 2019년 20번째 가축 조사 자료 시트 제2020년 승업부와 가축전문부, 수산부, 가축전문부의 자료를 찾아보세요.\n\n<div class=\"content-ad\"></div>\n\n## 단계 2\n\n개발자 도구를 사용하여 한 번에 모든 파일에 액세스할 수 있는 URL을 얻는 cURL을 얻으세요. 이렇게 하지 않았다면 각 파일을 개별적으로 클릭하여 양식을 작성하고 캡차를 입력한 다음 CSV를 수동으로 다운로드해야 했을 것입니다. 이 방법으로 제게 많은 시간을 절약했어요.\n\n## Step 3\n\nPostman에 URL을 게시하고 해당하는 Python 요청 코드를 가져와서 응답을 구문 분석하세요.\n\n<div class=\"content-ad\"></div>\n\n## 단계 4\n\n저는 Python을 사용하여 응답을 구문 분석하고, 이 코드를 사용하여 모든 URL 링크를 가져왔어요.\n\n```js\nfrom pprint import pprint\nimport os\nimport pandas as pd\nimport requests\n\nurl = \"https://data.gov.in/backend/dmspublic/v1/resources?filters[catalog_reference]=6885101&offset=0&limit=35&sort[changed]=desc&filters[domain_visibility]=4\"\n\npayload = {}\nheaders = {\n  'Accept': 'application/json, text/plain, */*',\n  'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8,de;q=0.7',\n  'Connection': 'keep-alive',\n  'Cookie': 'fontSize=67.5; citrix_ns_id=AAA7TRx1ZjuD0ksAAAAAADuMGtjGAxHPGX4gOzVglnj-t-2_KYp3QS5pOwB3wsrGOw==jyF1Zg==9zCLz_Tsia4CNE6H2-pAKy8Ou1w=; citrix_ns_id=AAA7TRx1ZjuD0ksAAAAAADuMGtjGAxHPGX4gOzVglnj-t-2_KYp3QS5pOwB3wsrGOw==uiR1Zg==JTC1HaNqvL2oNi2kwWYolcsi_TU=',\n  'Referer': 'https://data.gov.in/catalog/20th-livestock-census',\n  'Sec-Fetch-Dest': 'empty',\n  'Sec-Fetch-Mode': 'cors',\n  'Sec-Fetch-Site': 'same-origin',\n  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',\n  'dnt': '1',\n  'sec-ch-ua': '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n  'sec-ch-ua-mobile': '?0',\n  'sec-ch-ua-platform': '\"macOS\"',\n  'sec-gpc': '1'\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, data=payload)\n```\n\n```js\nrjson = response.json()\nrows = rjson['data']['rows']\nurls = []\nfor row in rows:\n    url = \"https://\"+row['datafile'][0]\n    print(url)\n    urls.append(url)\n```\n\n<div class=\"content-ad\"></div>\n\n최종 출력물은 CSV를 얻기 위해 구문 분석한 URL 목록입니다.\n\n![이미지](/assets/img/2024-06-22-HereshowIusedPythonandQGIStofindout80ofIndiascamelpopulation_0.png)\n\n## 단계 5\n\n모든 파일을 다운로드하여 연결하여 최종 데이터 프레임을 얻었습니다. 그 후에는 80%의 기준점을 사용하여 낙타 인구의 80%를 보유한 지역을 파악했습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nsave_directory = './csv_files/'\n\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n\ndef download_file(url, save_directory):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # 요청이 성공적인지 확인\n        file_name = os.path.join(save_directory, url.split('/')[-1])\n        with open(file_name, 'wb') as file:\n            file.write(response.content)\n        print(f\"다운로드 완료: {file_name}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"{url}을(를) 다운로드하는 데 실패했습니다: {e}\")\n\nfor url in urls:\n    download_file(url, save_directory)\n\ncsv_directory = save_directory\n\ndataframes = []\n\nall_headers = set()\n\nfor filename in os.listdir(csv_directory):\n    if filename.endswith('.csv'):\n        file_path = os.path.join(csv_directory, filename)\n        df = pd.read_csv(file_path)\n        dataframes.append(df)\n        all_headers.update(df.columns)\n\n# 모든 데이터 프레임이 동일한 헤더를 갖고 있는지 확인\nheaders_match = all(len(df.columns.difference(all_headers)) == 0 for df in dataframes)\n\nif headers_match:\n    combined_df = pd.concat(dataframes, ignore_index=True)\nelse:\n    combined_df = pd.DataFrame(columns=all_headers)\n    for df in dataframes:\n        df = df.reindex(columns=all_headers)  # 모든 열이 존재하는지 확인\n        combined_df = pd.concat([combined_df, df], ignore_index=True)\n\ncombined_csv_path = os.path.join(csv_directory, 'combined.csv')\ncombined_df.to_csv(combined_csv_path, index=False)\n\nprint(f\"결합된 CSV가 저장되었습니다: {combined_csv_path}\")\r\n```\n\n```js\r\ndf['camel'] = pd.to_numeric(df['camel'], errors='coerce')\ndf_sorted = df.sort_values(by='camel', ascending=False).reset_index(drop=True)\ndf_sorted['cumulative_sum'] = df_sorted['camel'].cumsum()\ntotal_camels = df_sorted['camel'].sum()\nthreshold = 0.8 * total_camels\ndf_sorted['cumulative_percentage'] = df_sorted['cumulative_sum'] / total_camels\ndistricts_80_percent = df_sorted[df_sorted['cumulative_percentage'] <= 0.8]\nprint(districts_80_percent[['state_name','district_name', 'camel', 'cumulative_sum', 'cumulative_percentage']])\r\n```\n\n<img src=\"/assets/img/2024-06-22-HereshowIusedPythonandQGIStofindout80ofIndiascamelpopulation_1.png\" />\n\n## 단계 6\r\n\n\n<div class=\"content-ad\"></div>\n\n인도의 지구 행정 구역 geojson 파일을 받았어요. 이 파일을 이용하여 18개의 지구를 필터링하는 기능을 만들고, QGIS에서 이를 시각화하며 배경 레이어를 구글 위성 지도로 설정하여 최종 결과물을 얻었어요.\n\n만약 어떤 단계에 대해 더 알고 싶다면 언제든지 저에게 문의해주세요. 이것은 공개 데이터를 활용하여 결정을 내리는 데 있어 있는 가능성 중 하나의 작은 예시에 불과해요. 즐기세요!","ogImage":{"url":"/assets/img/2024-06-22-HereshowIusedPythonandQGIStofindout80ofIndiascamelpopulation_0.png"},"coverImage":"/assets/img/2024-06-22-HereshowIusedPythonandQGIStofindout80ofIndiascamelpopulation_0.png","tag":["Tech"],"readingTime":5},{"title":"시도해볼 만한 10가지 파이썬 자동화 스크립트","description":"","date":"2024-06-22 02:55","slug":"2024-06-22-10PythonScriptsforAutomationYouShouldTry","content":"\n\n<img src=\"/assets/img/2024-06-22-10PythonScriptsforAutomationYouShouldTry_0.png\" />\n\n매일 반복되는 지루한 작업을 자동화하면 어떨까요? 엑셀 파일 편집, 이메일 보내기, WhatsApp 메시지 보내기 및 일상적인 작업을 처리하는 봇 만들기와 같은 작업을 자동화할 수 있습니다. 이 스크립트에서는 매뉴얼로 수행하는 매일 작업을 자동화할 수 있는 10가지 Python 스크립트를 소개합니다. 그러니 이 글을 즐겨찾기에 추가하고 시작해보세요.\n\n# 👉 AI 이미지 생성기\n\n상상한 것으로부터 아름다운 이미지를 만들고 싶다면 Getimg.ai API를 활용한 Python 스크립트가 있습니다. 이 API는 매달 100개의 무료 텍스트-이미지 크레딧을 제공하며 이를 활용하여 이미지를 생성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n단순하고 효과적인 이미지를 생성해 보세요. 웹사이트에서 무료 API를 쉽게 얻을 수 있어요.😄\n\n```js\n# AI 이미지 생성기\n# pip install requests\n# pip install pillow\n\nimport requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\n\ndef Imagine(prompt):\n\n    url = \"https://api.getimg.ai/v1/stable-diffusion-xl/text-to-image\"\n\n    payload = { \"prompt\": prompt }\n    headers = {\n        \"accept\": \"application/json\",\n        \"content-type\": \"application/json\",\n        \"authorization\": \"Bearer 여기에 API 키 입력\"\n    }\n\n    response = requests.post(url, json=payload, headers=headers)\n    content = response.json()\n\n    # 바이트를 이미지로 변환\n    image = content[\"image\"]\n    image = base64.b64decode(image)\n    img = Image.open(BytesIO(image))\n    \n    # 이미지 저장\n    img.save(\"Image.png\")\n\n\nImagine(\"아름다운 바다 위 석양과 생동적인 하늘\")    \n```\n\n# 👉 눈길을 끄는 이메일 전송\n\n일반적인 텍스트 이메일을 보내는 데 파이썬을 사용하고 계실 텐데, 이메일 마케팅에서와 같이 고급 이메일을 보낼 수도 있어요. 이 Python 스크립트는 Smtplib와 Email 모듈을 사용하여 HTML로 멋진 이메일을 만드는 방법을 보여줄 거예요.\n\n<div class=\"content-ad\"></div>\n\n아래 코드는 디자인 HTML을 자유롭게 해보세요😄.\n\n```js\n# 눈에 띄는 이메일 보내기\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nyour_email = \"mymail@xyz.com\"\nyour_password = \"mypass\"\nreceiver_email = \"me@xzy.com\"\nmsg = MIMEMultipart()\nmsg['From'] = your_email\nmsg['To'] = receiver_email\nmsg['Subject'] = \"Just a Test\"\n# HTML 내용 생성\nhtml = \"\"\"\\\n<html>\n  <body>\n    <h1 style=\"color:blue;\">이것은 테스트 이메일입니다</h1>\n    <p>이 이메일은 Python을 사용하여 전송되었습니다!</p>\n    <p>HTML 이메일에서 할 수 있는 멋진 기능들:</p>\n    <ul>\n      <li>CSS로 텍스트 스타일링</li>\n      <li>이미지 삽입</li>\n      <li>링크 추가</li>\n    </ul>\n    <p>최고의 문의,<br>Python 스크립트</p>\n  </body>\n</html>\n\"\"\"\n# 이메일에 HTML 내용 첨부\nmsg.attach(MIMEText(html, 'html'))\n# 이메일 보내기\nserver = smtplib.SMTP('smtp.gmail.com', 587)\nserver.starttls()\nserver.login(your_email, your_password)\nserver.sendmail(your_email, receiver_email, msg.as_string())\n# 서버 종료\nprint(\"이메일이 성공적으로 전송되었습니다!\")\nserver.quit()\n```\n\n# 👉 사진 압축\n\n사진 크기가 크고 품질을 유지한 채로 크기를 줄이고 싶다면 이 Python 스크립트가 맞을 것입니다. 이 스크립트는 이미지 처리 기술을 사용하여 이미지 크기를 줄이고 최상의 품질을 유지하는 훌륭한 모듈인 Imageio를 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 사진 압축기\n# pip install imageio\nimport imageio\ndef Compress_Photo(image, quality=85):\n    img = imageio.imread(image)\n    \n    output_name = 'compressed.jpg'\n    imageio.imwrite(output_name, img, quality=quality)\n    \n    print(\"이미지 압축 완료...\")\nif __name__ == '__main__':\n    Compress_Photo('photo.jpg')\n```\n\n# 👉 Whatsapp Bot\n\n기본 시간에 자동으로 메시지를 보낼 수 있는 WhatsApp 봇이 필요하거나 번호로 대량 WhatsApp 마케팅 메시지를 보내고 싶으신가요? 이 파이썬 스크립트는 Pywhatkit를 사용하여 이 작업을 수행하는데 도움이 됩니다. 아래에 빌드하는 데 도움이 되는 네 가지 함수를 설명했습니다. 자유롭게 사용해보세요.\n\n- 대량 메시징에 편리함\n- 마케팅에 유용\n- 예약 메시징에 유용함\n- 기타 많은 기능\n\n\n<div class=\"content-ad\"></div>\n\n\n# Whatsapp Bot\n\n```python\n# Whatsapp Bot\n# pip install pywhatkit\nimport pywhatkit as whatbot\ndef send_message(ph, msg, hr, min):\n    whatbot.sendwhatmsg(ph, msg, hr, min)\n    print(\"Message sent successfully\")\ndef send_image(ph, img_file, caption):\n    whatbot.sendwhats_image(ph, img_file, caption)\n    print(\"Image sent successfully\")\ndef send_msg_to_group(group_name, msg):\n    whatbot.sendwhatmsg_to_group_instantly(group_name, msg)\n    print(\"Message sent successfully\")\ndef send_msg_to_group_delay(group_name, msg, hr, min):\n    whatbot.sendwhatmsg_to_group(group_name, msg, hr, min)\n    print(\"Message sent successfully\")\nsend_message(\"+1234567890\", \"Hello\", 12, 30)\nsend_image(\"+1234567890\", \"image.jpg\", \"This is an image\")\nsend_msg_to_group(\"Group Name\", \"Yoo!\")\nsend_msg_to_group_delay(\"Group Name\", \"Hello\", 12, 30)\n```\n\n# 👉 Excel Bot\n\nIf you want to read, write, and edit your Excel files programmatically, you can bookmark this automation script that uses the popular Openpyxl module, my favorite for automating any Excel task. This module helps you read, write, create, style the Excel file, and more. \n\nThe script below is a beginner's guide to automating your Excel tasks with the basics.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n# Excel Bot\n# pip install openpyxl\nimport openpyxl\nfrom openpyxl.styles import *\n# Excel 파일 로드\nwb = openpyxl.load_workbook(\"data.xlsx\")\n# 시트 로드\nsheet = wb.active\n# 특정 열 값 가져오기\nfor x in sheet[\"A\"]:\n    print(x.value)\n# 특정 행 값 가져오기\nfor x in sheet[1]:\n    print(x.value)\n# 특정 셀 값 가져오기\nprint(sheet[\"A1\"].value)\n# 특정 행과 열 값 가져오기\nprint(sheet.cell(row=1, column=1).value)\n# 최대 행과 열 값 가져오기\nprint(sheet.max_row)\nprint(sheet.max_column)\n# 쓰기 및 추가\nsheet[\"A1\"] = \"Hello\"\n# 특정 셀에 쓰기\nsheet.cell(row=2, column=2).value = \"World\"\n# 데이터를 시트에 추가\nsheet.append([1, 2, 3, 4, 5])\n# 셀 병합\nsheet.merge_cells(\"A1:B1\")\n# 글꼴 변경\nsheet[\"A1\"].font = Font(bold=True)\n# 셀 배경색 변경\nsheet[\"A1\"].fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n# 파일 저장\nwb.save(\"data.xlsx\")\r\n```\n\n# 👉 간단한 PDF에서 텍스트 추출\n\nPDF 파일에서 텍스트를 가져오고 싶다면 PyMuPDF 모듈을 사용하는 아래 자동화 스크립트를 사용해보세요. 이 모듈은 PDF의 페이지를 반복하고 서식을 유지한 채 텍스트를 추출하는 가장 좋은 방법을 제공합니다. 아래에 사용할 수 있는 코드를 안내해드릴게요.\n\n```js\n# 간단한 PDF에서 텍스트 추출\n# pip install PyMuPDF\nimport fitz\ndef PDF_To_Text(pdf_File):\n    data = \"\"\n    pdf_doc = fitz.open(pdf_File)\n    for page in pdf_doc:\n        data += page.get_text(\"text\") \n    return data\nif __name__ == \"__main__\":\n    pdf_File = \"test.pdf\"\n    print(PDF_To_Text(pdf_File))\r\n```\n\n<div class=\"content-ad\"></div>\n\n# 👉 파이썬 비디오 레코더\n\n스크린 녹화 소프트웨어를 구입할 필요가 없어요. 여러분들만의 것을 만들 수 있으니까요. 이 자동화 스크립트는 OpenCV, Numpy 및 Pyautogui를 사용하여 화면을 최고 해상도로 60FPS로 녹화합니다. Fpsm 해상도 또는 시작 및 정지 버튼도 편집할 수 있어요. 아래 코드를 그대로 복사하고 사용하세요 😉.\n\n```js\n# 파이썬 스크린 레코더\n# pip install opencv-python\n# pip install numpy\n# pip install pyautogui\nimport cv2\nimport numpy as np\nimport pyautogui\n# 해상도 지정\nresolution = (1920, 1080)\n# 비디오 코덱 지정\ncodec = cv2.VideoWriter_fourcc(*\"XVID\")\nfilename = \"Recording.avi\"\nfps = 60.0\nout = cv2.VideoWriter(filename, codec, fps, resolution)\ncv2.namedWindow(\"실시간\", cv2.WINDOW_NORMAL)\ncv2.resizeWindow(\"실시간\", 480, 270)\nwhile True:\n    img = pyautogui.screenshot()\n    \n    frame = np.array(img)\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    \n    out.write(frame)\n    cv2.imshow(\"실시간\", frame)\n    key = cv2.waitKey(1)\n    \n    if key == ord(\"s\"):\n        break\n    elif key == ord(\"r\"):\n        continue\n# 비디오 레코더 해제\nout.release()\ncv2.destroyAllWindows()\n```\n\n# 👉 맞춤법 검사 봇\n\n<div class=\"content-ad\"></div>\n\n온라인 맞춤법 검사기 웹 앱이 필요하지 않습니다. 이제 파이썬으로 직접 만들 수 있습니다. 맞춤법 검사를 위해 많은 텍스트가 있으면, 온라인 검사기에 하나하나 입력하는 대신 자동으로 처리할 수 있습니다. 이 파이썬 스크립트는 Autocorrect 모듈을 사용하여 맞춤법 검사 작업을 자동화할 것입니다.\n\n```python\n# 맞춤법 검사 봇\n# pip install autocorrect\nfrom autocorrect import Speller\n\ndef SpellChecker(text):\n    spell = Speller(lang='en')\n    corrected = spell(text)\n    print(f\"원본: {text}\")\n    print(f\"수정된 내용: {corrected}\")\n\nif __name__ == \"__main__\":\n    text = \"I am goinng to the markeet\"\n    SpellChecker(text)\n```\n\n# 👉 무료 클라우드 저장 공간\n\n이 자동화 스크립트는 Pydrive2 모듈을 사용하여 파일을 Google 드라이브에 저장할 것이며, 이는 클라우드 저장 공간처럼 활용할 수 있게 됩니다. 이 스크립트는 파일을 Google 드라이브에 업로드하거나 다운로드할 수 있도록 디자인되었습니다.\n\n<div class=\"content-ad\"></div>\n\n이 스크립트는 여러 파일이 있는 것을 상상할 때 유용합니다. 단 한 번 클릭으로 모든 파일을 Google Drive에 업로드하고 필요할 때 언제든지 다시 다운로드할 수 있습니다.\n\n```js\n# 무료 클라우드 저장소\n# pip install PyDrive2\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\ndef Upload_File(filename):\n    auth = GoogleAuth()\n    auth.LocalWebserverAuth()\n    drive = GoogleDrive(auth)\n    file = drive.CreateFile()\n    file.SetContentFile(filename)\n    file.Upload()\n    print(\"파일이 성공적으로 업로드되었습니다\")\ndef Download_File( filename):\n    auth = GoogleAuth()\n    auth.LocalWebserverAuth()\n    drive = GoogleDrive(auth)\n    # 검색으로 다운로드\n    file_list = drive.ListFile({'q': f\"title='{filename}'\"}).GetList()\n    for file in file_list:\n        file.GetContentFile(filename)\n        print(\"파일이 성공적으로 다운로드되었습니다\")\nif __name__ == \"__main__\":\n    Upload_File(\"Image.png\")\n    Download_File(\"Image.png\")\n```\n\n# 👉 속도 테스트 하기\n\n이 자동화 스크립트를 사용하여 인터넷 속도에 대해 최신 정보를 얻어보세요. 이 스크립트는 OKALA 속도 테스트에서 가져온 Speedtest 모듈을 사용하여 인터넷 연결의 다운로드, 업로드 및 핑을 테스트할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n내 인터넷 속도가 떨어지거나 핑이 불안할 때 알려주는 기능이 유용합니다. 지금 바로 시도해보세요! 😎\n\n```js\n# pip install speedtest-cli\nimport speedtest as st\ndef Speed_Test():\n    test = st.Speedtest()\n    # 다운로드 속도 확인\n    down_speed = test.download()\n    down_speed = round(down_speed / 10**6, 2)\n    print(\"다운로드 속도: \", down_speed)\n    # 업로드 속도 확인\n    up_speed = test.upload()\n    up_speed = round(up_speed / 10**6, 2)\n    print(\"업로드 속도: \", up_speed)\n    # 핑 확인\n    ping = test.results.ping\n    print(\"핑: \", ping)\nSpeed_Test()\n```\n\n# 👉 마지막으로\n\n글을 끝까지 읽어주셔서 감사합니다. 다음 프로젝트나 작업에 유용한 스크립트를 찾으셨기를 바랍니다. 공유하고 싶은 유용한 스크립트가 있으면 알려주세요.\n\n<div class=\"content-ad\"></div>\n\n아래는 시청자들이 가장 사랑하는 선택된 기사들입니다. 확인해보세요.\n\n# 간단명료한 언어로 🚀\n\nIn Plain English 커뮤니티에 참여해 주셔서 감사합니다! 떠나시기 전에:\n\n- 작가를 추천하고 팔로우해 주세요 ️👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | 뉴스레터\n- 다른 플랫폼에서도 만나보세요: CoFeed | Differ\n- 더 많은 컨텐츠는 PlainEnglish.io에서 확인하세요","ogImage":{"url":"/assets/img/2024-06-22-10PythonScriptsforAutomationYouShouldTry_0.png"},"coverImage":"/assets/img/2024-06-22-10PythonScriptsforAutomationYouShouldTry_0.png","tag":["Tech"],"readingTime":10},{"title":"해석 가능한 시계열 예측을 위한 Temporal Fusion Transformer TFT 사용법","description":"","date":"2024-06-22 02:49","slug":"2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions","content":"\n\n![2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_0](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_0.png)\n\n무료 샘플 eBook 장(chapters) 보기: [여기](https://github.com/dataman-git/modern-time-series/blob/main/20240522beauty_TOC.pdf)\n\nTeachable.com에서 eBook 구매: $22.50 [여기](https://drdataman.teachable.com/p/home)\n\nAmazon.com에서 인쇄판 구매: $65 [여기](https://a.co/d/25FVsMx)\n\n<div class=\"content-ad\"></div>\n\n우리는 Walmart, Target 및 Best Buy와 같은 소매업체에서 쇼핑을 합니다. Macy's, Nordstrom 및 Sears와 같은 백화점에서 물건을 사거나 Kroger, Safeway 및 Whole Foods와 같은 슈퍼마켓을 방문하기도 합니다. Amazon 및 eBay와 같은 온라인 소매업체에서 제품을 구매하기도 합니다. 우리는 제품이 정확한 시간에 문 앞으로 도착하기를 기대합니다. 제품이 품절일 경우 실망스럽게 여깁니다. 이러한 비즈니스들은 여러 가지 공통점이 있습니다: 수백 개에서 수천 개의 제품을 판매하며, 모두 계획 지침을 위해 좋은 데이터 과학 모델에 의존합니다. 이러한 종류의 데이터 과학 모델은 수천 개의 제품에 대한 예측을 제공해야 합니다. 예측은 하나의 기간만이 아니라 여러 기간이어야 합니다. 예측은 위험 완화를 위한 예측 구간을 가져야 합니다.\n\n이 챕터에서는 여러 제품, 여러 기간 및 확률적 예측을 제공할 수 있는 Transformer 기반의 예측 모델을 소개합니다. 그것은 Temporal Fusion Transformer (TFT) 모델입니다. TFT는 특정 작업에서 다른 모델보다 우수한 성능을 보여준 바 있습니다. 그 효율성, 유연성 및 해석 가능성은 다양한 응용 분야에서 가치 있는 자산으로 만듭니다. 그리고 2019년에 Lim, Arik, Loeff, Pfister에 의해 소개된 것처럼 그 제목 또한 흥미로워합니다. \"Temporal\"은 시간 관련 데이터나 시간 종속성이 있는 순차 데이터를 처리한다는 것을 나타냅니다. “Fusion”은 여러 데이터 소스나 특성에서 정보를 혼합하는 설계를 포착합니다. 그리고 “Transformer”는 Transformer 기반 모델이기 때문에 사용되었습니다. 문헌 \"Attention is All You Need\" (2017)의 Transformer 모델은 모든 현대 대형 언어 모델 (LLM)의 백본입니다. 이전 챕터인 \"RNN/LSTM에서 Temporal Fusion Transformers 및 Lag-Llama\"를 참고하실 수 있습니다.\n\n오늘날의 데이터 과학 모델은 복잡한 기계이며, 아마도 아직 완전히 알아내기 어려운 미로의 영아 단계에 있을지도 모릅니다. 한편, 데이터 과학 커뮤니티는 모델의 투명성과 해석 가능성을 요구합니다. 모델이 예측을 어떻게 하는지에 대한 질문을 합니다. 모델 해석 가능성은 책 \"The explainable AI\"에서 다루었듯이 책 \"An Explanation for eXplainable AI\"에서 다루었듯이 책 \"An Explanation for eXplainable AI\"에서 다루었듯이 책 \"An Explanation for eXplainable AI\" 모델의 책임 있는 예측을 보장하기 위한 활발한 연구 분야입니다. TFT의 주요 특징인 모델 해석 가능성은 모델이 예측을 어떻게 하는지에 대한 통찰력을 제공합니다. 이는 미래 값을 예측하는 데 가장 영향력 있는 과거 시간 단계가 무엇인지 설명해줍니다. 코드 예제에서 나중에 보게 될 변수 중요도 플롯은 모델이 예측하는 방식을 설명합니다.\n\n이 챕터에서는 TFT 모델을 구축하기 위한 실제 데이터 케이스를 따라가보겠습니다. 다음 주제들을 다룰 예정입니다:\n\n<div class=\"content-ad\"></div>\n\n- 글로벌 시계열 모델 구축\n- TFT의 구조\n- 소프트웨어 요구 사항\n- 데이터\n- 데이터 Darts Python 라이브러리로 변환\n- 모델링\n- 예측\n- 그래프 그리기\n- 모델 해석 가능성\n\n이 장을 완료하면 TFT를 미래 사례에 적용하고 TFT의 혜택을 설명할 수 있을 것입니다.\n\n글로벌 시계열 모델 구축\n\nWalmart이나 Amazon의 수천 가지 제품은 수천 개의 시계열을 의미합니다. 모든 시계열을 함께 모델링하면 모델은 전역 모델입니다. 각 시계열을 단일 변수 시계열 모델로 모델링하는 경우 지역 모델입니다. 실제로 제품이나 서비스의 계층구조에 따라 별도의 글로벌 모델을 구축할 수 있습니다. 각 글로벌 모델은 제품 범주를 제공하며 관련 제품이 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n글로벌 모델의 장점은 무엇인가요? 글로벌 모델은 여러 시계열 간의 공통 패턴과 관계를 포착할 수 있어 개별 시계열의 예측 정확도를 향상시킬 수 있습니다. 글로벌 모델은 새로운 제품과 같이 데이터가 제한적인 경우 유용할 수 있습니다. 글로벌 모델은 새 제품이 동일 범주 내 유사한 제품의 기능을 활용할 수 있도록 합니다. 한편 로컬 모델은 단일 시계열에 훈련된 것이기 때문에 독특한 패턴을 포착할 수 있습니다. 시계열 간에 상당한 차이가 있는 경우 로컬 모델이 유용할 수 있습니다.\n\nTemporal Fusion Transformer(TFT) 모델은 글로벌 모델로, 각 시계열을 독립적으로 모델링하는 대신 다른 시계열 간의 관계를 모델링합니다. TFT의 아이디어는 시계열 간의 관계를 모든 시리즈에 걸친 기본 패턴과 트렌드를 포착하는 공유 표현으로 캡처할 수 있다는 것입니다. 모든 시리즈의 결합 분포를 모델링함으로써 TFT는 단일 시계열만 고려하는 로컬 모델로는 모델링하기 어려운 복잡한 패턴과 관계를 포착할 수 있습니다.\n\nTFT 아키텍처에 대한 다음 섹션이 상당히 길다는 점을 알려드리고 싶습니다. 대신 모델링을 학습할 \"소프트웨어 요구사항\" 섹션으로 건너뛰어보세요. 이후 다시 TFT 아키텍처로 돌아오셔도 됩니다.\n\nTFT 아키텍처\n\n<div class=\"content-ad\"></div>\n\n(A) 도표는 원본 논문의 TFT 아키텍처를 보여줍니다 [1]. 이 다이어그램이 조금 복잡해 보일 수 있어요. 하나씩 차근차근 블록을 설명해 드릴게요.\n\n![이미지](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_1.png)\n\n(A) 도표는 아래에서부터 읽어야 해요. 먼저 아래의 입력값부터 시작해요. 그 다음으로 'Variable Selection' 상자를 위로 한 단계 올려 보세요. 그 다음으로 'Encoders' 상자를 위로 한 단계 올려가는 식으로, 계속 해 보세요. 최종 출력은 맨 위에 있는 분위수 예측입니다.\n\n입력 데이터\n\n<div class=\"content-ad\"></div>\n\n시계열 데이터는 크게 세 가지 유형으로 그룹화될 수 있어요:\n\n- 첫 번째 그룹은 시간이 지나도 변하지 않는 정적 메타데이터입니다. 예를 들어 상점 위치나 제품 카테고리와 같은 정보가 여기에 속해요.\n- 두 번째 그룹은 k 기간 전의 입력 데이터에 해당해요.\n- 세 번째 그룹은 공휴일 플래그, 요일 또는 월, 예정된 프로모션 이벤트와 같은 다양한 공변수에 해당해요. 우리는 미래를 예측할 것이기 때문에 이러한 공변수들은 t+𝛕 기간까지 준비되어 있거나 알려져 있어야 해요.\n\n변수 선택 네트워크\n\n모든 입력 데이터가 목표 데이터를 예측하는 데 필요하지는 않아요. 변수 선택 네트워크(VSNs)는 어떤 입력 데이터가 예측에 가장 관련성이 있는지를 결정해요. 이는 TFT의 직관적인 설계입니다. VSNs는 각 시간 단계마다 예측에 가장 관련된 입력 데이터 하위 집합을 동적으로 선택해요. 이 동적 기능 선택 메커니즘은 모델이 예측 정확도를 향상시킬 수 있게 해줘요. 그러나 미리 어떤 입력이 목표와 관련이 있는지 또는 선형인지 비선형인지 관계가 명확하지 않아요. 어떻게 관련 입력을 식별할 수 있을까요? VSNs는 모델이 필요에 따라 유연하게 입력 변수를 선택하고 정보를 제거할 수 있게 해줘요. (B) 그림에 Variable Selection Network가 나와요. 각 입력 특징에 대해 게이트된 잔차 네트워크가 있어요. 다이어그램의 \"변수 선택 가중치\"는 변수 중요도에 대한 가중치입니다. 가중치는 훈련 과정 중에 결정돼요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_2.png\" />\n\nGRN을 이해해 봅시다.\n\n게이트된 잔여 네트워크 (GRN)\n\n게이트된 잔여 네트워크 (GRN)는 TFT 전반에서 복잡한 시계열 데이터의 시간적 패턴 및 의존성을 캡처하기 위해 사용됩니다. GRN은 게이트 메커니즘과 잔여 연결을 가지고 있습니다. 게이팅 메커니즘을 통해 모델은 각 레이어에서 다른 특징들의 중요성을 적응적으로 조절할 수 있습니다. 이러한 게이팅 함수는 일반적으로 시그모이드 활성화 함수의 형태를 취하며 0과 1 사이의 값을 생성합니다. 이들은 이전 레이어에서 얼마나 많은 정보가 다음 레이어로 전달되어야 하는지를 결정하며, 필요에 따라 정보를 선택적으로 보존하거나 버릴 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Temporal Fusion Transformer for Interpretable Time Series Predictions](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_3.png)\n\nFigure (C) has a dashed line for the residual connection. The output of the previous layer(s) is added to the output of the current layer. This mechanism helps address the vanishing gradient problem. Also, with the additions of the outputs of previous and subsequent layers through residual connections, the GRN has mixed the input features and can capture the non-linear interactions between features. That’s why the name “Fusion” was coined.\n\nAfter explaining the row of VSNs in Figure (A), let’s move up one row to the Static Covariate Encoders, as shown in Figure (D).\n\n![Temporal Fusion Transformer for Interpretable Time Series Predictions](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n정적 공변량 인코더\n\n그림 (D)의 정적 공변량 인코더는 범주형 정적 공변량을 숫자로 변환합니다. 이 임베딩 프로세스는 자연어 처리의 단어 임베딩과 유사합니다. 각 범주형 변수는 임베딩 공간에 있는 고차원 벡터에 매핑됩니다. 이 임베딩 프로세스는 다양한 범주 간 의미 관계를 포착합니다. 텍스트 표현과 단어 임베딩에 대한 친절한 설명을 제공하는 책 \"The Handbook of NLP with Gensim\" [3]의 1장에서 3장을 읽는 것이 좋습니다.\n\nLSTM 인코더\n\n18장 \"From RNN/LSTM to Temporal Fusion Transformers and Lag-Llama\"에서 우리는 시계열 데이터에 대해 Transformer 모델을 직접 사용하지 않는 이유를 설명했습니다. 시계열 데이터와 언어 데이터는 다르기 때문입니다. 시계열 데이터를 어떻게 인코딩할까요? 시계열 데이터는 독특한 시간적 종속성과 패턴을 가지고 있습니다. 그림 (D)에 나타난 TFT의 인코더는 LSTM(Long Short-Term Memory) 네트워크입니다. LSTM 인코더는 순차 데이터를 효과적으로 모델링하는 능력으로 알려진 순환 신경망(RNN) 아키텍처의 일종입니다. LSTM 인코더가 입력 시계열 데이터를 처리할 때, 관련 특징을 추출하고 데이터에 있는 시간적 동적의 의미 있는 표현을 학습합니다.\n\n<div class=\"content-ad\"></div>\n\n정적 공변량 인코더에서 LSTM 인코더로 향하는 화살표에 대해 알아보셨을 겁니다. Figure (D)에서 보이는 노란색 화살표는 정적 공변량의 수치 표현이 LSTM 인코더 내의 시간적 특성과 연결된다는 것을 의미합니다. 이 연결 과정은 정적 공변량과 시간적 특성 모두로부터 정보를 결합합니다. 이는 모델이 정확한 예측을 위해 시간적 및 정적 정보를 모두 활용할 수 있도록 도와줍니다. 예를 들어 코드 예제에서 정적 공변량 중 하나인 매장(store)이 있습니다. 매장의 매출은 매장 위치에 따라 다를 수 있습니다. 매장 정보를 과거 시간적 특성과 결합하는 것은 모델이 매장별 매출을 정확하게 예측할 수 있도록 유연성을 제공합니다.\n\nLSTM 디코더\n\nLSTM 디코더는 Figure (D)의 오른쪽에 있습니다. 왼쪽 화살표에서 LSTM 인코더의 정보를 가져옵니다. 또한 앞으로의 𝛕 기간을 위한 공변량이 담긴 아래쪽 화살표에서도 정보를 받습니다. LSTM 인코더와 마찬가지로 LSTM 디코더는 데이터 내의 시간 의존성과 패턴을 캡처할 수 있는 능력을 갖추고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 Figure (E)에 나와 있는 Figure (A)의 정적 보강으로 이동합시다.\n\n![image](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_5.png)\n\nGRNs를 활용한 정적 보강\n\nLSTM 인코더로부터 입력을 받는 GRN 세트와 LSTM 디코더로부터 입력을 받는 다른 GRN 세트가 있습니다. 두 세트의 GRN은 벡터 형태의 정적 공변량도 입력으로 사용합니다. 다시 말해, GRN의 정적 보강은 TFT가 시간 의존성과 정적 공변량 정보를 결합하여 보다 정확한 예측을 할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n이제 Figure (A)에 표시된 Figure (F)의 Temporal Self-Attention으로 이동해 보겠습니다.\n\n![이미지](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_6.png)\n\nTemporal Multi-head Self-Attention\n\nTemporal self-attention 메커니즘은 모델이 동일한 입력 시퀀스에서 다른 시간 단계에 주의를 기울이도록하고 그들 사이의 복잡한 관계를 학습할 수 있게 합니다. Temporal self-attention 메커니즘은 transformer 아키텍처에서 사용되는 표준 self-attention 메커니즘과 유사하게 작동합니다. 입력 시퀀스의 각 시간 단계에서 temporal self-attention 메커니즘은 입력 시퀀스를 인코딩할 때 각 시간 단계에 얼마나 집중해야하는지 결정하는 attention weights를 계산합니다. 예측 작업에 더 관련이 있거나 정보를 전달하는 시간 단계는 더 높은 attention weights를 받고, 덜 관련이있는 시간 단계는 더 낮은 attention weights를 받습니다. 이러한 weights는 TFT가 모델 해석력을 수행하는 데 필요한 구성 요소입니다. 코드 예제에서 self-attention weights를 시각화하는 방법을 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n셀프 어텐션 매커니즘은 멀티헤드입니다. 이것은 모델이 입력 시퀀스의 다양한 측면에 동시에 주의를 기울일 수 있도록 합니다. 단일 셀프 어텐션은 동일한 시계열 세그먼트 내의 일부에 주의를 기울이는 것을 의미합니다. 마찬가지로, 한 번만 주의를 계산하는 대신 멀티헤드는 모델이 여러 번 동시에 주의를 계산할 수 있도록 합니다. 각 어텐션 헤드는 입력 시퀀스 내에서 다른 유형의 관계를 포착하기 위한 고유한 어텐션 가중치 집합을 학습합니다. 여러 어텐션 헤드를 사용함으로써 모델은 다양한 패턴과 종속성을 더 효과적으로 포착할 수 있습니다.\n\n그 다음, Figure (A)에 있는 Position-wise Feed-forward Network로 이동해 보겠습니다. Figure (G)에 표시된 것입니다.\n\n![이미지](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_7.png)\n\nPosition-wise Feed-forward Network\n\n<div class=\"content-ad\"></div>\n\n전통적인 피드포워드 네트워크와 달리 TFT(Temporary Fluctuations in Temperature)의 위치별 FFN은 입력 시퀀스의 각 위치를 독립적으로 처리합니다. 이 위치별 처리는 모델이 입력 시퀀스 내에서 위치별 정보 및 상호 작용을 포착할 수 있도록 합니다. 피드포워드 네트워크는 비선형 활성화 함수(예: ReLU 또는 GELU)를 적용합니다.\n\n추가 & 정규화\n\n\"추가 & 정규화\" 블록이 몇 개 있습니다. \"추가 & 정규화\" 기술은 TFT 모델의 교육 안정성과 수렴 속도를 향상시킵니다. \"추가\" 작업의 목적은 모델이 입력으로부터 원래 정보를 유지하면서 출력으로부터 변환된 정보를 통합하는 것입니다. 레이어의 출력에 입력을 추가함으로써 모델은 원래 정보가 변환 과정 전체에서 보존되도록 보장할 수 있습니다. 이는 기울기 소멸 문제를 완화하는 데 도움이 됩니다.\n\n다중 기간에 대한 분위 회귀 출력\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 모델은 동시에 여러 미래 시간 단계에 대한 분위수 예측을 생성합니다. 분위 회귀는 다양한 불확실성 수준을 양적화하기 위해 여러 분위수(예: 10, 50, 90 백분위수)를 추정합니다. 그림 (D)에 설명된 것처럼, 이 부분은 \"다중 기간 확률적 예측을 위한 선형 회귀\"와 \"다중 기간 시계열 확률적 예측을 위한 Tree-based XGB, LightGBM 및 CatBoost 모델\"의 분위 회귀 기법과 동일합니다.\n\n![이미지](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_8.png)\n\n최적화\n\nTFT의 최적화 알고리즘은 표준 신경망 알고리즘입니다. 확률적 경사 하강법(SGD)이나 그 변형을 사용합니다. 실제 값과 예측 값 사이의 손실 함수를 정의합니다. 역전파를 사용하여 모델 매개변수에 대한 손실의 그래디언트를 계산합니다. 그래디언트는 학습률로 모수를 업데이트하는 데 사용되며, 학습률은 단계 크기를 결정합니다. 미니 배치 학습은 학습 속도를 높이고 일반화를 향상시키기 위해 사용됩니다. 오버피팅을 방지하기 위해 L1 또는 L2 정칙화와 같은 정칙화 기법을 적용할 수 있습니다. 학습률 스케줄링을 통해 학습률을 시간에 따라 조정합니다. 최적화 알고리즘은 이러한 단계를 반복적으로 수행하며 최대 에폭 수나 원하는 성능 수준과 같은 중지 기준을 충족할 때까지 작동합니다.\n\n<div class=\"content-ad\"></div>\n\n실제 데이터 케이스를 사용하여 TFT 모델을 구축해 보겠습니다. Python 노트북은 이 Github 링크를 통해 다운로드할 수 있습니다.\n\n소프트웨어 요구 사항\n\nDarts Python 라이브러리를 설치해야 합니다. 이 책은 Darts 데이터 형식을 설명하는 \"시계열 데이터 형식을 쉽게 만드는 방법\" 장을 별도로 제공합니다. 또한 이 책은 다음 장에서 Darts를 자세히 설명합니다:\n\n- 시계열 데이터 형식을 쉽게 만드는 방법\n- 다중 기간 확률 예측을 위한 선형 회귀\n- 다중 기간 시계열 확률 예측을 위한 Tree-based XGB, LightGBM 및 CatBoost 모델\n- 응용: 아마존의 DeepAR을 활용한 주식 예측\n\n<div class=\"content-ad\"></div>\n\n필요한 라이브러리를 가져오겠습니다.\n\n```js\nimport pandas as pd\nimport numpy as np\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\n\nfrom darts import TimeSeries\nfrom darts.dataprocessing.pipeline import Pipeline\nfrom darts.models import TFTModel\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.utils.timeseries_generation import datetime_attribute_timeseries\nfrom darts.utils.likelihood_models import QuantileRegression\nfrom darts.dataprocessing.transformers import StaticCovariatesTransformer, MissingValuesFiller\n```\n\n데이터 준비\n\n에콰도르의 Favorita 스토어에서의 상점 매출 데이터를 사용할 것입니다. 이 데이터셋은 Kaggle에서 제공됩니다. 데이터셋에는 체인 스토어에서 판매되는 수천 가지 제품 군이 포함되어 있습니다. 훈련 데이터에는 날짜, 상점 및 제품 정보, 해당 제품이 프로모션 중인지 여부, 그리고 매출 숫자가 포함되어 있습니다. 추가 파일에는 모델을 구축하는 데 유용할 수 있는 보조 정보가 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 코드는 여러 파일을 병합합니다.\n\n```js\n# CSV 파일 읽기\npath = 'data/store-sales-time-series-forecasting'\ndata = pd.read_csv(path + '/train.csv', delimiter=\",\")\nholidays = pd.read_csv(path + '/holidays_events.csv', delimiter=\",\").drop('type', axis=1)\nstores = pd.read_csv(path + '/stores.csv', delimiter=\",\")\ntransactions = pd.read_csv(path + '/transactions.csv', delimiter=\",\")\n# 파일 병합\nholidays['holiday_flag'] = 1\ndata = data.merge(holidays, on='date', how='left')\ndata = data.merge(stores, on='store_nbr', how='left')\ndata = data[data['date'] != '2013-01-01'] # 잘못된 데이터\ndata = data.merge(transactions, on=['date', 'store_nbr'], how='left')\n# 기본 데이터 조작\ndata['date'] = pd.to_datetime(data[\"date\"])\ndata = data.drop_duplicates(subset=['date','store_nbr', 'family'], keep='last') \ndata.loc[data['holiday_flag'].isna(),'holiday_flag'] = 0\ndata['year'] = data['date'].dt.year \ndata.columns\n```\n\n데이터에는 다음과 같은 열이 있습니다:\n\n- store_nbr: 상점 번호\n- family: 제품 패밀리\n- sales: 상품 패밀리의 상점 및 날짜별 총 매출\n- onpromotion: 상품 패밀리 중 특정 날짜에 할인된 제품의 총 수\n- holiday_flag: 휴일 플래그\n\n<div class=\"content-ad\"></div>\n\n데이터에는 각 매장의 제품이 시계열 데이터이기 때문에 수백 개의 시계열이 있습니다. 설명을 위해, 우리는 전역 모델을 구축하기 위해 네 개의 가장 큰 매장과 네 개의 가장 큰 제품 카테고리를 선택할 것입니다. 앞서 언급한 코드는 16개의 시계열 데이터를 선택한 다음 이를 훈련 및 테스트 데이터로 분할합니다.\n\n데이터 변환을 Darts로\n\n여러 시계열을 위한 전역 모델을 구축하려면 다수의 시계열을 포함하는 데이터를 구조화해야 합니다. 이는 \"Time Series Data Formats Made Easy\" 장에서 설명된대로 Darts에 의해 편리하게 처리됩니다. 시계열의 가장 세부 레벨은 매장 및 제품 카테고리 수준이므로 \"store_nbr\" 및 \"family\"로 그룹화를 지정합니다. 이 두 변수는 단순히 그룹화 변수뿐만 아니라 예측 변수로도 사용될 수 있습니다. 예를 들어 특정 매장이나 제품 카테고리가 다른 매장이나 제품 카테고리보다 더 많이 팔릴 수 있습니다. 이 두 변수는 매장 및 제품별 정보를 포착할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nTIME_COL = \"date\"\nTARGET = \"sales\"\nSTATIC_COLS = ['store_nbr', 'family'] \nFREQ = \"D\"\nFORECAST_HORIZON = test['date'].nunique() \nCOVARIATES = ['onpromotion','holiday_flag']\nSCALER = Scaler()\nTRANSFORMER = StaticCovariatesTransformer()\nPIPELINE = Pipeline([SCALER, TRANSFORMER])\n```\n\n타겟은 \"sales\"이고, 공변량은 \"onpromotion\"과 \"holiday_flag\"입니다. Darts의 .from_group_dataframe() 함수는 누락된 값이나 값을 외삽화할 수 있는 편리한 도구입니다.\n\n```js\n# 학습 및 테스트 데이터셋을 읽고 변환합니다\ntrain_darts = TimeSeries.from_group_dataframe(df=train, \n                                              group_cols=STATIC_COLS, \n                                              time_col=TIME_COL, \n                                              value_cols=TARGET, \n                                              freq=FREQ, \n                                              fill_missing_dates=True, \n                                              fillna_value=0)\ntest_darts = TimeSeries.from_group_dataframe(df=test, \n                                             group_cols=GROUP_COLS, \n                                             time_col=TIME_COL, \n                                             value_cols=TARGET, \n                                             freq=FREQ, \n                                             fill_missing_dates=True, \n                                             fillna_value=0)\n\n[len(train_darts[0]), len(test_darts[0])] # [561, 32]는 학습 데이터와 테스트 데이터의 기간 수입니다\n```\n\n시간 인덱스에는 많은 숨은 정보가 포함되어 있습니다. 특정 이벤트가 특정 날에 발생할 수 있습니다. 예를 들어, 고객들은 주말에 일반적으로 더 많이 쇼핑하며, 여름 달은 보통 야외 제품에 대한 수요가 더 많습니다. 시간 인덱스를 사용하여 더 많은 공변량을 생성할 수 있습니다. 앞에서 제공한 코드는 연도의 12개월과 52주에 대한 지표를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ncreate_covariates = []\nfor ts in train_darts:\n    # 월과 주를 공선변수로 추가\n    covariate = datetime_attribute_timeseries(\n        ts,\n        attribute=\"month\",\n        one_hot=True,\n        cyclic=False,\n        add_length=FORECAST_HORIZON,\n    )\n    covariate = covariate.stack(\n        datetime_attribute_timeseries(\n            ts,\n            attribute=\"week\",\n            one_hot=True,\n            cyclic=False,\n            add_length=FORECAST_HORIZON,\n        )\n    )\n    store = ts.static_covariates['store_nbr'].item()\n    family = ts.static_covariates['family'].item()\n    \n    # 공선변수 생성\n    other_cov = TimeSeries.from_dataframe(data[(data['store_nbr'] == store) & (data['family'] == family)], time_col=TIME_COL, value_cols=COVARIATES, freq=FREQ, fill_missing_dates=True)\n    covariate = covariate.stack(MissingValuesFiller().transform(other_cov))\n\n    create_covariates.append(covariate)\n\ncreate_covariates[0].columns\n\n#Index(['month_0', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5',\n#       'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n#       'week_0', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6',\n#       'week_7', 'week_8', 'week_9', 'week_10', 'week_11', 'week_12',\n#       'week_13', 'week_14', 'week_15', 'week_16', 'week_17', 'week_18',\n#       'week_19', 'week_20', 'week_21', 'week_22', 'week_23', 'week_24',\n#       'week_25', 'week_26', 'week_27', 'week_28', 'week_29', 'week_30',\n#       'week_31', 'week_32', 'week_33', 'week_34', 'week_35', 'week_36',\n#       'week_37', 'week_38', 'week_39', 'week_40', 'week_41', 'week_42',\n#       'week_43', 'week_44', 'week_45', 'week_46', 'week_47', 'week_48',\n#       'week_49', 'week_50', 'week_51', 'onpromotion', 'holiday_flag'],\n#      dtype='object', name='component')\n```\n\n어떻게 보이나요? Darts 데이터를 다시 Pandas 데이터 프레임으로 변환하여 확인할 수 있습니다. 이들은 단순한 이진 지표들입니다.\n\n```js\nTimeSeries.pd_dataframe(create_covariates[0]).tail()\n```\n\n<img src=\"/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_9.png\" />\n\n<div class=\"content-ad\"></div>\n\n\n마찬가지로, 대상 데이터 \"sales\"를 Pandas 데이터 프레임으로 변환하여 살펴볼 수 있습니다:\n\n```js\nTimeSeries.pd_dataframe(train_darts[15]).tail()\n```\n\n![이미지](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_10.png)\n\n모델링 전에 데이터를 표준화하려고 합니다. 이는 많은 데이터 과학 모델에서 흔한 실천법입니다. 앞의 코드는 훈련 데이터에 따라 스케일러를 작성합니다. 이 스케일러는 나중에 테스트 데이터에 적용될 것입니다. 입문자가 훈련 및 테스트 데이터를 독립적으로 스케일링하는 실수를 저지를 수 있습니다. 이러한 오류를 피하고 싶다면 \"경력에 영향을 줄 수 있는 치명적인 모델링 실수를 피하세요\"라는 게시물을 참조할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 데이터 스케일링 및 정적 공변량 변환\n# PIPELINE에 SCALER이 포함되어 있기 때문에 SCALER이 먼저 온다.\ntrain_transformed = PIPELINE.fit_transform(train_darts)\n# 공변량 스케일링\ncovariates_transformed = SCALER.fit_transform(create_covariates)\n```\n\n이제 모델을 구축해 봅시다.\n\n모델링\n\n모델 선언은 일반 신경망 하이퍼파라미터와 시계열 특정 하이퍼파라미터를 포함합니다. 가독성을 높이기 위해 하이퍼파라미터를 다음과 같이 그룹화했습니다:\n\n\n<div class=\"content-ad\"></div>\n\n- 데이터 준비를 위한 하이퍼파라미터: 이 그룹은 코드에서 input_chunk_length와 output_chunk_length를 가리킵니다. 이들은 단변량 시리즈에서 샘플을 생성하는 데 관련됩니다. 설명을 위해, 도식 (G)은 y0부터 y15까지의 시리즈에서 생성된 샘플을 보여줍니다. 각 샘플은 입력 청크와 출력 청크를 포함합니다. 입력 청크의 길이가 5이고 출력 청크의 길이가 2라고 가정합시다. 첫 번째 샘플은 입력 청크로 y0 ~ y4를, 출력 청크로 y5, y6을 갖습니다. 시리즈를 따라 창이 이동하여 시리즈의 끝까지 샘플을 생성합니다.\n\n![이미지](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_11.png)\n\n- 시계열을 위한 하이퍼파라미터: 예측 구간을 생성하기 위해 분위 회귀를 사용할 것입니다. 이것은 모델이 예측 불확실성을 생성하는 데 중요한 기능입니다.\n- 모델 아키텍처를 위한 하이퍼파라미터: 숨겨진 레이어 수, 어텐션 헤드 수, LSTM 레이어 수를 포함한 모델 스펙을 다양하게 조정할 수 있습니다.\n- 최적화를 위한 하이퍼파라미터: 이는 표준 신경망 하이퍼파라미터입니다. 이러한 하이퍼파라미터에 대한 설명은 별지에 추가했습니다.\n\n```js\nTFT_params = {\n    # 데이터 준비를 위한 하이퍼파라미터\n    \"input_chunk_length\": 52, # 과거를 바라보는 주 수 \n    \"output_chunk_length\": FORECAST_HORIZON,\n\n    # 시계열 하이퍼파라미터\n    \"likelihood\": QuantileRegression(quantiles=[0.25, 0.5, 0.75]),\n    \n    # 모델 아키텍처를 위한 하이퍼파라미터\n    \"use_static_covariates\": True,\n    \"hidden_size\": 4,\n    \"lstm_layers\": 4,\n    \"num_attention_heads\": 4,\n\n    # 최적화를 위한 하이퍼파라미터\n    \"dropout\": 0.1,\n    \"batch_size\": 16,\n    \"n_epochs\": 60,\n    \"random_state\": 42,\n    \"optimizer_kwargs\": {\"lr\": 1e-3},\n}\n\ntft_model = TFTModel(**TFT_params)\ntft_model.fit(train_transformed, # 훈련 기간\n              future_covariates=covariates_transformed, # 전체 기간\n              verbose=False)\n```  \n\n<div class=\"content-ad\"></div>\n\n모델이 훈련되면 예측에 사용할 것입니다.\n\n예측\n\n예측 단계는 간단합니다. 기억해야 할 한 가지는 \"future_covariates\"입니다. 이미 미래 공변량을 포함하고 있으며, 월 1-12, 주 1-52 및 휴일 플래그와 같은 다른 알려진 공변량을 포함합니다. 또한, 외부에서 가져올 \"onpromotion\" 플래그와 같은 여러 다른 공변량도 포함됩니다.\n\n예측된 값은 스케일된 값입니다. 스케일된 값을 다시 원래 스케일로 역변환하는 것을 기억해 주세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 스케일링된 예측값을 가져옵니다\nscaled_pred = tft_model.predict(n=FORECAST_HORIZON, \n                                series=train_transformed, # 훈련 기간\n                                num_samples=50, \n                                future_covariates=covariates_transformed # 전체 기간\n                               )\n\n# 스케일링된 예측값을 일반 스케일로 변환합니다\nprediction = PIPELINE.inverse_transform(scaled_pred)\n```\n\n지금까지 우리는 우리가 다루는 글로벌 모델을 완성하고 불확실성을 고려한 예측을 제공했습니다.\n\n플로팅\n\n실제 매출, 예상 매출 및 25% 및 75%의 예측 구간을 플롯팅해 봅시다. 아래 함수는 상점의 4가지 제품 패밀리를 플롯팅합니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\ndef plot_it():\n    fig, axs = plt.subplots(2, 2, figsize=(10, 6), dpi=100)\n    ax0 = axs[0,0]\n    ax1 = axs[0,1]\n    ax2 = axs[1,0]\n    ax3 = axs[1,1]\n    \n    plt.suptitle(\"Store:\" +  str(store) , fontsize=12)\n    \n    val0[: pred0.end_time()].plot(ax=ax0, label=\"actual\", marker='o', linewidth=1)\n    pred0.plot(ax = ax0, low_quantile=0.25, high_quantile=0.75, label=\"prediction\", marker='o',linewidth=1,alpha=0.2 )\n    ax0.title.set_text('Product: '+family[0])\n    \n    val1[: pred1.end_time()].plot(ax=ax1, label=\"actual\", marker='o', linewidth=1)\n    pred1.plot(ax = ax1, low_quantile=0.25, high_quantile=0.75, label=\"prediction\", marker='o',linewidth=1,alpha=0.2 )\n    ax1.title.set_text('Product: '+family[1])\n    \n    val2[: pred2.end_time()].plot(ax=ax2, label=\"actual\", marker='o', linewidth=1)\n    pred2.plot(ax = ax2, low_quantile=0.25, high_quantile=0.75, label=\"prediction\", marker='o',linewidth=1,alpha=0.2 )\n    ax2.title.set_text('Product: '+family[2])\n    \n    val3[: pred3.end_time()].plot(ax=ax3, label=\"actual\", marker='o', linewidth=1)\n    pred3.plot(ax = ax3, low_quantile=0.25, high_quantile=0.75, label=\"prediction\", marker='o',linewidth=1,alpha=0.2 )\n    ax3.title.set_text('Product: '+family[3])\n    fig.tight_layout()\n    plt.show()\n\n\nstore_nbr = [44, 45, 47, 3]\nfamily = ['GROCERY I', 'BEVERAGES', 'PRODUCE', 'CLEANING']\n\nfor i in range(0,16,4):\n    k = int(i/4)\n    store = store_nbr[k]\n    pred0 = prediction[i]\n    pred1 = prediction[i+1]\n    pred2 = prediction[i+2]\n    pred3 = prediction[i+3]\n    val0 = test_darts[i]\n    val1 = test_darts[i+1]\n    val2 = test_darts[i+2]\n    val3 = test_darts[i+3]\n    plot_it()\n```\n\n아래 4개의 그림은 가게 44와 45의 GroceryI 및 Beverages 제품군에 대한 실제 값과 예측 값입니다.\n\n![image](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_12.png)\n\n![image](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_13.png)\n\n\n<div class=\"content-ad\"></div>\n\nTFT의 중요한 기능 중 하나는 모델 해석 가능성입니다. 함께 알아봐요.\n\n모델 해석 가능성\n\nTFT는 자기 주의 메커니즘을 통해 해석 가능성을 제공합니다. 자기 주의 메커니즘은 예측을 할 때 입력 시퀀스의 서로 다른 부분에 주의를 기울입니다. 특정 기능이나 시간 단계에 주의를 기울여 모델이 예측 과정에서 상대적인 중요성을 강조할 수 있습니다. 이를 통해 예측을 이끄는 근본적인 요소를 이해하고 데이터에 대한 통찰력을 얻을 수 있습니다. 이를 수행하기 위해 TFTExplainer() 함수를 사용할 것입니다.\n\n```js\nfrom darts.explainability import TFTExplainer\n\nexplainer = TFTExplainer(\n    tft_model,\n    background_series=train_transformed[1],\n    background_future_covariates=dynamic_covariates_transformed[1],\n)\nexplainability_result = explainer.explain()\n```\n\n<div class=\"content-ad\"></div>\n\n몇 분은 \"explainer\" 함수 이름이 SHAP 값의 explainer 함수와 달라 보일 수 있습니다. 비록 다른 기능을 하지만, 모델 자체를 설명하는 데 동일한 목표를 가지고 있습니다. SHAP explainer나 기타 기술은 \"설명 가능한 AI에 대한 설명\"과 \"설명 가능한 AI\" 책에서 찾을 수 있습니다.\n\n이제 주의 가중치를 시각화할 준비가 되었습니다. 이러한 시각화는 다른 변수나 시간 단계간의 패턴과 관계를 확인할 수 있습니다. 첫 번째 변수 중요도 차트는 인코더 중요도입니다.\n\n인코더 변수 중요도는 각 입력 변수가 예측의 정확성에 얼마나 기여하는지를 측정합니다. 이는 모델이 각 시간 단계에서 가장 관련성 있는 입력 변수에 초점을 맞출 수 있도록 해주는 주의 메커니즘을 사용하여 계산됩니다.\n\n```python\nplt.rcParams[\"figure.figsize\"] = (10,5)\nplt.barh(data=explainer._encoder_importance.melt().sort_values(by='value').tail(10), y='variable', width='value')\nplt.xlabel('중요도')\nplt.ylabel('특성')\nplt.title('Encoder 중요도')\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n\n![TFT Model](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_14.png)\n\nTFT 모델에서 디코더는 여러 계층으로 구성되어 있으며, 각 계층은 셀프 어텐션 메커니즘 다음에 피드 포워드 신경망(FFNN)이 이어집니다. 셀프 어텐션 메커니즘을 통해 모델은 입력 시퀀스의 여러 부분에 주의를 기울이고 출력 시퀀스를 생성할 때 그 중요성을 가중 평가할 수 있습니다. FFNN은 셀프 어텐션 메커니즘의 출력을 처리하고 현재 시간 단계의 최종 출력을 생성합니다.\n\n다음으로 중요한 변수 그래프가 디코더 중요도 차트입니다. 디코더 변수 중요도는 모델이 입력 시퀀스를 활용하여 출력 시퀀스를 생성하는 방식을 이해하는 데 유용합니다. 디코더 변수 중요도는 디코더의 각 변수에 할당된 어텐션 가중치를 분석하여 계산됩니다. 어텐션 가중치를 사용하여 각 변수의 중요도 점수가 계산되며, 이는 출력 시퀀스를 생성할 때 디코더에서 변수가 얼마나 중요한지 나타냅니다. 중요도 점수는 다음과 같이 계산됩니다:\n\n중요도 점수 = ∑ (어텐션 가중치 * 어텐션 헤드의 중요성)\n\n\n<div class=\"content-ad\"></div>\n\n```js\nplt.rcParams[\"figure.figsize\"] = (10,5)\nplt.barh(data=explainer._decoder_importance.melt().sort_values(by='value').tail(10), y='variable', width='value')\nplt.xlabel('중요도')\nplt.ylabel('특성')\nplt.title('디코더 중요도')\nplt.show()\n```\n\n위 출력에서 \"month5\", \"week_39\", \"week_29\" 등이 상위 입력 변수로 표시됩니다.\n\n<img src=\"/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_15.png\" />\n\n모델의 두 정적 변수의 효과를 검토할 수 있습니다. 그래프에서 \"family\"가 \"store\"보다 상대적으로 중요한 지표임을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n```py\nplt.rcParams[\"figure.figsize\"] = (10,5)\nplt.barh(data=explainer._static_covariates_importance.melt().sort_values(by='value').tail(10), y='variable', width='value')\nplt.xlabel('중요도')\nplt.ylabel('특성')\nplt.title('고정 Cov 중요도')\nplt.show()\r\n```\n\n<img src=\"/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_16.png\" />\n\nTFT 모델에서는 다중 헤드 어텐션이라는 기술을 사용하여 주의 시각화가 이루어집니다. 다중 헤드 어텐션을 통해 모델은 서로 다른 표현 공간에서 서로 다른 위치의 정보를 동시에 고려할 수 있습니다. 주의 가중치는 학습 중에 학습되고 입력 순서의 가중 합을 계산하는 데 사용되며, 그것은 비선형 활성화 함수를 통해 출력을 생성하기 위해 전달됩니다.\n\n미래 예측을 위한 주의 가중치를 시각화할 수도 있습니다. 그림(F)은 피라미드의 빛 굴절과 같은 그래프로, 미래를 위한 주의 가중치를 보여줍니다. 가까운 미래에 대한 예측은 더 높은 주의 가중치를 가지며, 먼 미래에 대한 예측은 더 낮은 주의 가중치를 가집니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nexplainer.plot_attention(explainability_result, plot_type=\"all\", show_index_as='time')\n```\n\n![Image](/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_17.png)\n\n내 하드웨어\n\nTFT는 계산 성능을 요구합니다. 저는 TFT를 Apple 2020 Mac Mini M1 칩 (8GB RAM, 512GB SSD 저장 용량)에서 실행했습니다. TFT를 세밀하게 조정하고 Figure (H) 및 (I)를 얻기 위해 더 나아가 데이터를 두 가게와 두 제품 패밀리로 제한하여 최대 60회 에포크까지 실행했습니다.\n\n<div class=\"content-ad\"></div>\n\n결론\n\n이 장에서는 Temporal Fusion Transformer (TFT) 기술을 설명했습니다. 이 장은 Temporal Fusion Transformer의 네 가지 중요한 측면을 강조했습니다: Multi-Horizon Forecasting, Interpretability, Temporal Fusion Mechanism 및 Transformer Architecture. 이 장은 공변량을 사용하여 전역 모델을 구축하는 방법을 보여주었습니다. 그런 다음 TFT의 모델 해석 특성에 대한 설명을 제공했습니다.\n\n부록\n\nTFT는 신경망 모델이므로 표준 신경망 하이퍼파라미터를 사용합니다. 여기서 \"드롭아웃\", \"배치 크기\" 및 \"에포크\" 개념을 이해하게 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n드롭아웃\n\n신경망에서 드롭아웃은 오버피팅을 방지하고 모델의 일반화 성능을 향상시키는 정규화 기술입니다. 오버피팅은 모델이 훈련 데이터에 너무 꼭 맞아 맞춰져, 새로운 보이지 않는 데이터에서 성능이 나빠지는 현상을 말합니다.\n\n드롭아웃은 훈련 중에 층(layer) 내의 일부 뉴런(neuron)을 사용자가 지정한 확률로 랜덤하게 제거하면서 동작합니다. 이는 제거된 뉴런을 대체하기 위해 나머지 뉴런이 더 견고한 특징을 학습하도록 만듭니다.\n\n테스트 중에는 드롭아웉되지 않은 상태로 모든 뉴런이 사용됩니다. 제거된 뉴런은 실제로 네트워크에서 제거되지 않으므로, 이는 특정 세트의 뉴런에 의존하지 않고 예측을 만드는 데 도움이 됩니다. 이를 통해 오버피팅을 방지할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n드롭아웃률, 즉 뉴런이 중단되는 확률은 일반적으로 0.1과 0.5 사이로 설정됩니다. 높은 드롭아웃률은 일반화 성능을 개선할 수 있지만, 학습 중에 일부 뉴런이 효과적으로 네트워크에서 제거되어 모델 용량이 감소할 수도 있습니다.\n\n배치 크기\n\n배치 크기는 학습 프로세스의 각 반복에서 한 번의 전진 및 후진 패스를 통해 처리되는 훈련 예제의 수를 나타냅니다. 학습 중에 신경망은 다양한 입력 예제를 제시받는데, 각 입력은 네트워크를 통해 전달되어 출력을 계산합니다. 그런 다음 예측된 출력과 실제 출력 간의 차이에 기초하여 네트워크의 매개변수가 조정됩니다. 이와 같은 과정을 역전파라고합니다.\n\n배치 크기는 각 반복에서 매개변수를 업데이트하는 데 사용되는 입력 예제의 수를 결정합니다. 더 큰 배치 크기는 기울기의 더 신뢰할 수있는 추정을 제공할 수 있으며, 학습 프로세스의 정확성과 안정성을 향상시킬 수 있습니다. 그러나 큰 배치 크기는 더 많은 메모리 및 계산 리소스가 필요하므로 특정 응용 프로그램에는 제한이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n제가 설명하는 방법은 베이킹 스토리를 사용하여 신경망의 배치 크기 개념을 설명하는 것을 좋아해요. 상상해봐요, 베이킹을 원하는 많은 쿠키 반죽이 있다고 해봅시다. 이 쿠키를 작은 배치 또는 큰 배치로 베이킹할 수 있어요. 한 번에 큰 배치로 쿠키를 베이킹하면 굉장히 오랜 시간이 걸릴 거예요 (ㅋㅋ). 쿠키를 8개 또는 16개 배치로 나눌 수 있어요. 몇 가지 훈련 예시를 한 번에 처리할 수 있어요. 이렇게 하면 모델 매개변수를 더 자주 업데이트할 수 있고, 기울기는 각 작은 배치 후에 계산되고 적용됩니다. 이는 보다 빠른 수렴과 더 자주 무게 업데이트를 가져올 수 있지만, 샘플 크기가 작아서 매개변수 업데이트에 더 많은 노이즈가 발생할 수도 있어요.\n\n에포크\n\n에포크는 전체 훈련 데이터 세트를 한 번 통과하는 것을 의미해요. 각 에포크 동안 신경망은 훈련 데이터 세트의 각 예시를 예측하고 예측된 값과 실제 값 사이의 오차에 따라 가중치와 바이어스를 업데이트해요.\n\n책 \"이미지 분류를 위한 전이 학습\"에서는 1,000개의 이미지 데이터 세트로 에포크 개념을 설명해요. 각 에포크 중에 신경망은 1,000개의 이미지 각각을 예측하고 예측된 값과 실제 값 사이의 오차에 따라 가중치와 바이어스를 업데이트해요. 이 프로세스는 일정한 횟수의 에포크 동안 반복되어 신경망이 전체 훈련 데이터 세트를 여러 번 보게 될 때까지 지속돼요.\n\n<div class=\"content-ad\"></div>\n\n에포크의 수는 신경망이 전체 학습 데이터 세트를 몇 번 보게 될지를 결정합니다. 에포크 수가 너무 낮으면, 신경망은 데이터의 기저 패턴을 배울 충분한 기회가 없을 수 있으며, 정확하지 않은 예측을 할 수 있습니다. 반면에, 에포크 수가 너무 높으면, 신경망은 학습 데이터에 오버피팅될 수 있으며, 새로운 보이지 않는 데이터에 대해 정확하지 않은 예측을 할 수 있습니다.\n\n참고 자료\n\n- [1] Lim, B., Arik, S.Ö., Loeff, N., & Pfister, T. (2019). Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting. ArXiv, abs/1912.09363.\n- [2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems (p./pp. 5998–6008).\n- [3] Kuo, C. (2023). The Handbook of NLP with Gensim: Leverage topic modeling to uncover hidden patterns, themes, and valuable insights within textual data. Packt Publishing.\n\n샘플 eBook 장(chapter) 무료 다운로드: [https://github.com/dataman-git/modern-time-series/blob/main/20240522beauty_TOC.pdf](https://github.com/dataman-git/modern-time-series/blob/main/​20240522beauty_TOC.pdf)\n\n<div class=\"content-ad\"></div>\n\n- 아름다운 형식으로 책을 재현해준 The Innovation Press, LLC의 직원들에게 감사드립니다. 즐거운 독서 경험을 위해 Teachable 플랫폼을 선택하고 전 세계 독자들에게 분배하였습니다. 신용 카드 거래는 Teachable.com이 신뢰성 있고 안전하게 처리합니다.\n\nTeachable.com에서의 eBook: $22.50\nhttps://drdataman.teachable.com/p/home\n\nAmazon.com의 인쇄판: $65\nhttps://a.co/d/25FVsMx\n\n- 인쇄판은 광택 처리된 표지, 컬러 인쇄 및 아름다운 Springer 글꼴과 레이아웃을 채택하여 즐길 수 있는 독서를 제공합니다. 7.5 x 9.25 인치의 크기로 대부분의 책장에 맞습니다.\n- \"이 책은 시계열 분석과 예측 분석, 이상 감지에 대한 깊은 이해를 보여주는 Kuo의 증명서입니다. 이 책은 독자들이 현실 세계의 과제에 대처하는 데 필요한 기술을 제공합니다. 데이터 과학으로의 경력 전환을 원하는 사람들에게 특히 가치 있습니다. Kuo는 전통적이고 첨단 기술을 자세히 탐구합니다. Kuo는 최신 동향과 분야의 최신 발전을 반영하기 위해 신경망 및 기타 고급 알고리즘에 대한 토론을 통합합니다. 이를 통해 독자들이 확립된 방법뿐만 아니라 데이터 과학 분야의 가장 최신이고 혁신적인 기술과 소통할 준비가 되어 있음을 보장합니다. 이 책의 명확성과 접근성은 Kuo의 매력적인 글쓰기 스타일에 의해 향상됩니다. 그는 복잡한 수학적 및 통계적 개념을 해독하여 엄격성을 희생하지 않고 접근 가능하게 만들었습니다.\"\n\n<div class=\"content-ad\"></div>\n\n# 모던 시계열 예측: 예측 분석과 이상 감지를 위한\n\n제로장: 서문\n\n제1장: 소개\n\n제2장: 비즈니스 예측을 위한 선지자\n\n<div class=\"content-ad\"></div>\n\n## Chapter 3: 튜토리얼 I: 추세 + 계절성 + 휴일 및 이벤트\n\n## Chapter 4: 튜토리얼 II: 추세 + 계절성 + 휴일 및 이벤트 + 자기회귀(AR) + 지연 회귀자 + 미래 회귀자\n\n## Chapter 5: 시계열의 변곡점 탐지\n\n## Chapter 6: 시계열 확률 예측을 위한 몬테카를로 시뮬레이션\n\n<div class=\"content-ad\"></div>\n\n# 제 7장: 시계열 확률적 예측을 위한 분위 회귀\n\n# 제 8장: 시계열 확률적 예측을 위한 적응형 예측\n\n# 제 9장: 시계열 확률적 예측을 위한 적응형 분위 회귀\n\n# 제 10장: 자동 ARIMA!\n\n<div class=\"content-ad\"></div>\n\n# 챕터 11: 시계열 데이터 형식 간단히\n\n# 챕터 12: 다중 기간 확률 예측을 위한 선형 회귀\n\n# 챕터 13: 트리 기반 시계열 모델용 특성 공학\n\n# 챕터 14: 다중 기간 시계열 예측을 위한 주요 두 가지 전략\n\n<div class=\"content-ad\"></div>\n\n15장: Tree 기반 XGB, LightGBM 및 CatBoost 모델을 활용한 다기간 시계열 확률 예측\n\n16장: 시계열 모델링 기법의 진화\n\n17장: 시계열 확률 예측을 위한 Deep Learning 기반 DeepAR\n\n18장: 응용 – 주식 가격에 대한 확률 예측\n\n<div class=\"content-ad\"></div>\n\n# 19장: RNN에서 트랜스포머 기반 시계열 모델로\n\n# 20장: 해석 가능한 시계열 예측을 위한 Temporal Fusion Transformer\n\n# 21장: 시계열 예측을 위한 오픈 소스 Lag-Llama 튜토리얼","ogImage":{"url":"/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_0.png"},"coverImage":"/assets/img/2024-06-22-TemporalFusionTransformerforInterpretableTimeSeriesPredictions_0.png","tag":["Tech"],"readingTime":30},{"title":"Django 배우기 전에 꼭 마스터 해야 할 필수 Python 주제","description":"","date":"2024-06-22 02:47","slug":"2024-06-22-EssentialPythonTopicstoMasterBeforeLearningDjango","content":"\n\nDjango는 빠른 속도로 안전하고 유지보수가 쉬운 웹 사이트를 개발할 수 있도록 하는 Python의 강력하고 인기 있는 웹 프레임워크입니다. 그러나 Django를 최대한 활용하려면 Python에 튼튼한 기반을 갖추는 것이 중요합니다. 이 블로그 포스트는 Django에 뛰어들기 전에 반드시 숙달해야 할 주요 Python 주제를 안내해 드릴 것입니다.\n\n# 기본 Python 개념\n\n## 1. 구문과 의미론\n\nPython의 구문과 의미론을 이해하는 것이 첫걸음입니다. Python은 코드 블록을 정의하는 데 들여쓰기를 사용합니다. 이를 숙달함으로써 일반적인 함정을 피하고 코드를 더 읽기 쉽게 만들 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 들여쓰기: Python은 코드 블록을 정의하는 데 들여쓰기를 활용합니다.\n- 주석: 한 줄 주석은 #을 사용하고 여러 줄 주석은 삼중 따옴표를 활용합니다.\n\n## 2. 변수와 데이터 유형\n\n변수를 선언하고 사용하는 방법을 알아야 합니다.\n\n- 데이터 유형: 정수, 부동 소수점 수, 문자열, 부울린.\n- 컬렉션: 리스트, 튜플, 세트, 사전.\n\n<div class=\"content-ad\"></div>\n\n## 3. 연산자\n\n연산자는 변수와 값에 대한 작업을 수행하는 데 사용되는 기본 요소입니다.\n\n- 산술 연산자: +, -, *, / 등\n- 비교 연산자: ==, !=, `, ` 등\n- 논리 연산자: and, or, not\n- 할당 연산자: =, +=, -= 등\n- 비트 연산자: &, |, ^, ~, ``, ``\n\n## 4. 제어 흐름\n\n<div class=\"content-ad\"></div>\n\n프로그램의 흐름을 조절하기 위해 제어 흐름 문장을 사용합니다.\n\n- 조건문: if, elif, else.\n- 반복문: for, while.\n- 반복 제어: break, continue, pass.\n\n# 파이썬 중급 개념\n\n## 5. 함수\n\n<div class=\"content-ad\"></div>\n\n함수는 특정 작업을 수행하는 재사용 가능한 코드 조각입니다.\n\n- 정의 및 호출: def 키워드.\n- 인수: 위치, 키워드, 기본, 가변 길이.\n- 반환 값: return 문.\n- 람다 함수: 람다를 사용한 익명 함수.\n\n## 6. 모듈과 패키지\n\n모듈과 패키지를 사용하여 코드를 모듈화하면 관리하기 쉬워집니다.\n\n<div class=\"content-ad\"></div>\n\n- 모듈 가져오기: import 문.\n- 패키지 생성 및 사용: 모듈을 디렉토리에 구성하세요.\n- 표준 라이브러리 모듈: Python의 방대한 표준 라이브러리에 익숙해지세요.\n\n## 7. 파일 처리\n\n파일에서 읽고 쓰는 방법을 배웁니다.\n\n- 파일 읽고 쓰기: open(), read(), write(), close().\n- 파일 경로: 상대 경로와 절대 경로 이해하기.\n\n<div class=\"content-ad\"></div>\n\n## 8. 에러 및 예외 처리\n\n에러를 공손하게 처리하면 프로그램이 견고해집니다.\n\n- 예외 처리: try, except, finally 블록.\n- 사용자 정의 예외: 특정 시나리오에 대해 사용자 정의 예외를 생성하세요.\n\n# 고급 Python 개념\n\n<div class=\"content-ad\"></div>\n\n## 9. 객체 지향 프로그래밍 (OOP)\n\nOOP는 복잡한 프로그램을 조직화하는 데 도움이 됩니다.\n\n- 클래스와 객체: 클래스를 정의하고 인스턴스를 생성합니다.\n- 메소드와 속성: 클래스 내의 함수와 변수.\n- 상속: 기존 클래스를 재사용하고 확장합니다.\n- 다형성: 서로 다른 클래스를 교차로 사용할 수 있습니다.\n- 캡슐화: 특정 구성 요소에 대한 접근을 제한합니다.\n- 특별한 메소드: __init__, __str__, 등.\n\n## 10. Comprehensions\n\n<div class=\"content-ad\"></div>\n\n컴프리헨션은 컬렉션을 만드는 간결한 방법을 제공해요.\n\n- 리스트 컴프리헨션: [x for x in iterable].\n- 딕셔너리 컴프리헨션: 'k: v for k, v in iterable'.\n- 세트 컴프리헨션: 'x for x in iterable'.\n\n## 11. 데코레이터\n\n데코레이터는 함수나 클래스의 동작을 수정하는 역할을 해요.\n\n<div class=\"content-ad\"></div>\n\n- 기능 데코레이터: 다른 함수를 반환하는 함수들입니다.\n- 클래스 데코레이터: 클래스 동작을 수정합니다.\n\n## 12. 이터레이터와 제너레이터\n\n이터레이터와 제너레이터는 컬렉션을 간편하게 이터레이션할 수 있게 합니다.\n\n- 이터레이터: __iter__와 __next__ 메소드를 구현합니다.\n- 제너레이터: 값을 동적으로 생성하기 위해 yield를 사용합니다.\n- 제너레이터 표현식: 리스트 내포와 유사하지만 괄호를 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n# 추가 유용한 지식\n\n## 13. 정규 표현식\n\n정규 표현식(정규식)은 문자열 매칭과 조작에 강력합니다.\n\n- 기본 패턴: 일반적인 정규식 패턴을 배웁니다.\n- re 모듈: Python의 정규식 모듈을 사용하여 패턴 매칭을 수행합니다.\n\n<div class=\"content-ad\"></div>\n\n## 14. 자료 구조\n\n기본적인 자료 구조를 이해하는 것은 효율적인 프로그래밍에 중요합니다.\n\n- 스택과 큐: LIFO 및 FIFO 구조.\n- 연결 리스트: 순차적으로 연결된 노드.\n- 트리와 그래프: 계층적 및 네트워크화된 자료 구조.\n\n## 15. 동시성\n\n<div class=\"content-ad\"></div>\n\n동시에 여러 작업을 실행하는 것이 동시성에 관한 것이에요.\n\n- Threading: 병렬 실행을 위해 스레드를 실행합니다.\n- Multiprocessing: 여러 CPU 코어를 활용하기 위해 프로세스를 실행합니다.\n- asyncio: I/O 바운드 작업을 위한 비동기 프로그래밍입니다.\n\n# 실무 경험\n\n## 16. 가상 환경\n\n<div class=\"content-ad\"></div>\n\n가상 환경을 사용하여 프로젝트 종속성을 격리하세요.\n\n- 생성 및 관리: venv, virtualenv.\n\n## 17. Pip 및 패키지 관리\n\npip로 프로젝트 종속성을 관리하세요.\n\n<div class=\"content-ad\"></div>\n\n- 패키지 설치 및 관리: pip를 사용하여 Python 패키지를 설치하고 관리합니다.\n\n## 18. 웹 기본 개념\n\nDjango를 시작할 때 웹 개념에 대한 기본적인 이해가 도움이 됩니다.\n\n- HTTP/HTTPS: 웹 통신의 기본을 이해합니다.\n- REST API: RESTful API 작동 방식을 배웁니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터베이스 작업\n\n## 19. SQL 기본\n\n데이터베이스 작업을 위해 SQL 이해가 중요합니다.\n\n- CRUD 작업: 생성(Create), 읽기(Read), 업데이트(Update), 삭제(Delete).\n- 조인(Join): 여러 테이블에서 행을 결합합니다.\n- 인덱스(Index)와 트랜잭션(Transaction): 데이터베이스 작업을 최적화하고 관리합니다.\n\n<div class=\"content-ad\"></div>\n\n## 20. ORM (객체 관계 매핑)\n\nORM은 Django에서 데이터베이스 상호작용을 단순화합니다.\n\n- 기본 ORM 개념: ORM이 데이터베이스 테이블을 클래스로 매핑하는 방법을 이해합니다.\n- ORM 라이브러리: SQLAlchemy와 같은 라이브러리에 익숙해집니다.\n\n# 학습 자료\n\n<div class=\"content-ad\"></div>\n\n## 책\n\n- “Automate the Boring Stuff with Python” by Al Sweigart\n- “Python Crash Course” by Eric Matthes\n\n## 온라인 강좌\n\n- Coursera\n- Udemy\n- Codecademy\n\n<div class=\"content-ad\"></div>\n\n## 실습 플랫폼\n\n- LeetCode\n- HackerRank\n- Codewars\n\n# 결론\n\n이 Python 주제들을 숙달함으로써 Django에 대비할 준비가 충분해질 것입니다. 이 각 영역은 Django를 더 효과적으로 이해하고 사용하는데 도움이 되는 기초를 형성합니다. 즐거운 학습 되세요!","ogImage":{"url":"/assets/img/2024-06-22-EssentialPythonTopicstoMasterBeforeLearningDjango_0.png"},"coverImage":"/assets/img/2024-06-22-EssentialPythonTopicstoMasterBeforeLearningDjango_0.png","tag":["Tech"],"readingTime":5},{"title":"1분 만에 클라우드 아키텍처 다이어그램 만들기 이 도구 정말 빠름","description":"","date":"2024-06-22 02:46","slug":"2024-06-22-BuildcloudArchitectureDiagramsin1MinuteThisToolisCrazyFast","content":"\n\n차트태그를 마크다운 형식으로 변경해주세요.\n\n<div class=\"content-ad\"></div>\n\n스위치를 바꾸는 것은 무거운, 불편한 수트에서 매끄럽고 재빠른 도구 세트로 변하는 것 같았어요. 그래서 제가 경험한 것은:\n\n- 그리지 말고 다이어그램을 쓰기: 코드로 다이어그램을 만드는 것이 자연스럽고 빠르게 느껴졌어요. 까다로운 사용자 인터페이스와 씨름하지 않고 관계와 레이아웃을 정확하게 정의할 수 있었고, ChatGPT, BARD와 같은 생성적인 AI 도구들에게도 코드 생성을 요청할 수 있어요.\n- 재사용성: 다음 프로젝트에도 일부 요소를 재사용할 수 있어요.\n- 코드처럼 다이어그램 버전 관리: Git에 내 애플리케이션 코드와 함께 다이어그램을 저장하면 변경 사항을 추적하고 필요할 때 이전 버전으로 돌아가기 쉬워져요.\n- 다이어그램 생성 자동화: CI/CD 파이프라인에 다이어그램 생성을 추가하면 항상 최신의 다이어그램을 유지할 수 있어요. 이렇게 하면 수동 작업을 줄이고 오류를 감소시킬 수 있어요.\n- 다이어그램 쉽게 사용자 정의하기: 다이어그램 스타일과 요소를 손쉽게 조정하여 제 취향과 프로젝트 요구에 맞출 수 있었어요.\n\n# 준비 사항:\n\n## 1: Github에서 Diagrams 패키지 복제하기\n\n<div class=\"content-ad\"></div>\n\n```js \npip install diagrams \n```\n\n## 2: Graphviz 설치하기 (다이어그램 렌더링) 및 확인\n\n여기에서 Graphviz를 다운로드하고 설치하세요.\n\n# AI 사용 방법 (1분만에):\n\n<div class=\"content-ad\"></div>\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*2Zkdp3uGTEiSgChrGVw8cA.gif\" />\n\n## 단계 1: ChatGPT AI에게 다이어그램 코드를 생성해 달라고 요청하세요.\n\n해결책 세부 정보를 복사하여 붙여넣기하거나\n\n설명만 제공해주세요\n\n<div class=\"content-ad\"></div>\n\n## 지시 사항\n\n## 단계 2: 필요에 따라 AI가 제공한 코드를 수정하기\n\nAI가 제공한 코드 중 두 가지 오류를 수정해야 했습니다.\n\n- diagrams.aws.management 대신 diagrams.aws에서 KMS를 가져 오려고 했습니다.\n\n<div class=\"content-ad\"></div>\n\n보안.\n\n```js\ndiagrams.aws.management 모듈에서 KMS만 가져오셨네요.\ndiagrams.aws.general 모듈에서 InternetGateway, S3Bucket을 가져오셨네요.\n```\n\n```js\ndiagrams.aws.security 모듈에서 KMS를 가져오셨네요.\ndiagrams.aws.storage 모듈에서 S3를 가져오셨네요.\n```\n\n2. diagrams.aws.network에서 S3VPCEndpoint를 import하려고 했지만 S3VPCEndpoint가 존재하지 않아 Endpoint를 사용했습니다. 그리고 NatGateway에 관한 대소문자 문제가 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom diagrams.aws.network import VPC, PrivateSubnet, S3VPCEndpoint, NatGateway\ns3_endpoint = S3VPCEndpoint(\"S3 Gateway Endpoint\")\n```\n\n```js\nfrom diagrams.aws.network import VPC, PrivateSubnet, Endpoint, NATGateway\n\ns3_endpoint = Endpoint(\"S3 Gateway Endpoint\")\n```\n\n당신이 필요한 서비스가 패키지 내 어디에 정확히 위치하는지 확인할 수 있습니다.\n\n## 단계 3: 프로그램 실행하기\n\n<div class=\"content-ad\"></div>\n\n```bash\npython `filename`.py\n\n# 코드를 수동으로 작성하는 단계\n\n다이어그램 패키지를 배우고 수동으로 다이어그램을 만드는 것은 매우 쉽습니다. Diagram, Cluster, Edge 및 몇 가지 기호 등 약 6가지 항목에 대해 알고 있기만 하면 됩니다.\n\n“Diagram” — 당신의 다이어그램의 최상위 컨테이너\n```  \n\n<div class=\"content-ad\"></div>\n\n```js\n다음 코드에서,\n\nS3 to RDS는 저장할 이미지 파일의 이름을 나타냅니다\n\ndirection — 왼쪽에서 오른쪽으로(LR), 오른쪽에서 왼쪽으로, 위에서 아래로 컨테이너를 만들기 시작합니다. 필요한 경우 사용할 수 있는 옵션입니다.\n```\n\n<div class=\"content-ad\"></div>\n\n\"png\", \"jpg\", \"svg\", \"pdf\", \"dot\" 형식이 현재 지원됩니다.\n\n\"Cluster\" — 두 번째 수준 컨테이너(컨테이너의 이름 또는 레이블을 지정할 수 있습니다)\n\n```js\nwith Cluster(\"AWS\"):\n```\n\nEdge\n\n<div class=\"content-ad\"></div>\n\n\"``\" - 오른쪽으로 향하는 화살표 또는 가장자리\n\n```js\nevent_bridge >> Edge(label=\"triggers\") >> lambda1\n```\n\n\"``\" - 왼쪽으로 향하는 화살표 또는 가장자리\n\n\"-\" - 방향이 없는 엣지 또는 양방향\n\n<div class=\"content-ad\"></div>\n\n\ns3_raw_layer - Edge(label=\"push\") - lambda1\n\n\n# 샘플 출력:\n\n<img src=\"/assets/img/2024-06-22-BuildcloudArchitectureDiagramsin1MinuteThisToolisCrazyFast_0.png\" />\n\n# 다음 단계\n\n<div class=\"content-ad\"></div>\n\n잊지말고!\n\n![image](https://miro.medium.com/v2/resize:fit:960/0*BstxtFTCD4r-65Sd.gif)\n\n그리고,\n\n![image](/assets/img/2024-06-22-BuildcloudArchitectureDiagramsin1MinuteThisToolisCrazyFast_1.png)\n\n<div class=\"content-ad\"></div>\n\n그리고 만약 내 작업을 정말 좋아하신다면 커피 한 잔 사주실 수도 있어요 :).","ogImage":{"url":"/assets/img/2024-06-22-BuildcloudArchitectureDiagramsin1MinuteThisToolisCrazyFast_0.png"},"coverImage":"/assets/img/2024-06-22-BuildcloudArchitectureDiagramsin1MinuteThisToolisCrazyFast_0.png","tag":["Tech"],"readingTime":4},{"title":"파이썬에서 쓰레딩 사용 하는 방법","description":"","date":"2024-06-22 02:41","slug":"2024-06-22-ThreadinginPython","content":"\n\n<img src=\"/assets/img/2024-06-22-ThreadinginPython_0.png\" />\n\n# 소개\n\n이 게시물은 threading 모듈과 concurrent.futures 모듈의 ThreadPoolExecutor 클래스를 사용한 Python의 다중 스레딩에 대한 소개입니다.\n\n마지막에 있는 리소스 섹션에는 해당 주제를 깊이 파헤칠 수 있는 멋진 자료에 대한 링크가 있어요 🤓\n\n<div class=\"content-ad\"></div>\n\n관련 포스트\n\n- 병행성과 병렬성 소개\n- Python에서의 멀티프로세싱\n- Python에서의 ProcessPoolExecutor\n\n## 쓰레드란\n\n쓰레드는 프로세스 내에서 실행의 기본 단위입니다. 독립적인 실행 흐름으로, 동일한 프로세스 내의 다른 독립적인 실행 흐름과 동일한 주소 공간을 공유합니다. 프로세스는 하나 이상의 쓰레드를 가질 수 있으며, 이 중 하나는 메인 쓰레드입니다. 이는 Python 프로세스의 기본 쓰레드입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-ThreadinginPython_1.png\" />\n\n프로그램을 작성하여 여러 스레드를 활용하면 프로그램이 하나의 코어에서 동시에 실행될 수 있습니다. 코루틴을 사용하면 하나의 스레드 프로그램을 동시에 실행할 수도 있습니다.\n\nPython (CPython 구현) 프로세스 내의 스레드는 Python의 글로벌 인터프리터 락 (GIL) 때문에 다른 프로그래밍 언어의 스레드 (예: Java, C/C++, Go)와 달리 여러 코어가 있는 경우에도 병렬로 실행되지 않습니다. Python에서 CPU 바운드 작업이 필요하고 병렬 구현이 필요한 경우 multiprocessing 모듈이나 ProcessPoolExecutor 클래스 (Python의 Multiprocessing 참조)를 사용해야 합니다.\n\n프로그램을 작성한다고 상상해보세요. 실행이 시작되면 단일 프로세스가 될 것입니다. 또한 해당 프로세스는 두 개의 스레드를 갖게 될 것입니다. 두 개의 스레드가 있으면 동시성을 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n싱글 코어 CPU에서는 프로그램이 동시에 실행될 수 있습니다. 하나의 코어와 두 개의 스레드로, 스레드가 동일한 코어 내에서 서로 교환될 수 있습니다. 이를 컨텍스트 스위칭이라고 합니다.\n\n컨텍스트 스위칭 중에는 한 스레드가 CPU에서 스위칭되어 다른 스레드가 실행될 수 있도록 합니다. 이를 위해 프로세스나 스레드의 상태가 저장되어 나중에 복원되어 나중에 다시 실행될 수 있게 되며, 그 후 이전에 저장된 상태가 복원됩니다.\n\n컨텍스트 스위칭은 일반적으로 계산적으로 비용이 많이 듭니다. 프로세스나 스레드 간의 스위치 컨텍스트는 레지스터 및 다른 작업의 저장 및 로드에 일정 시간이 소요됩니다. 스레드 간의 컨텍스트 전환은 일반적으로 프로세스 간의 전환보다 빠릅니다.\n\n# 스레딩 사용 사례\n\n<div class=\"content-ad\"></div>\n\n다중 스레딩이 가장 적합한 작업은 I/O 바운드 작업입니다. 예를 들어, 스레드가 데이터베이스에 요청을 보내야 하는 명령을 실행하는 경우, 응답을 기다리는 스레드로 CPU 코어를 차단하는 것은 현명하지 않습니다. 대신에 첫 번째 스레드가 기다리는 동안에 다른 스레드가 코어를 사용할 수 있도록 하는 것이 자원을 더 잘 활용하는 방법입니다.\n\n아래 그림에서 빈 원은 스레드가 무언가 발생할 때까지 기다리는 I/O 작업을 나타냅니다. 첫 번째 I/O 작업이 시작될 때(빈 녹색 원), 운영 체제는 빠르게 대기 중인 스레드를 빨간색 스레드로 전환하여 계산 자원을 더 잘 할당합니다. 이것은 OS가 하는 결정이며, 개발자는 언제 스레드간 전환을 할지 결정할 수 없습니다.\n\n프로그램이 병렬로 여러 스레드를 사용하지 않고 대신에 단일 스레드 내에서 순차적으로 작업을 실행하는 경우, 녹색 작업을 완료하기를 기다려서 빨간 작업을 실행하기 시작해야 하므로, 두 작업을 완료하는 데 더 많은 시간이 소요됩니다.\n\n<div class=\"content-ad\"></div>\n\n`<img src=\"/assets/img/2024-06-22-ThreadinginPython_3.png\" />`\n\nI/O 작업을 다룰 때 멀티스레딩은 자원을 더 잘 할당할 수 있는 좋은 선택입니다.\n\n이제 멀티스레드 프로그램 구현 몇 가지를 살펴보겠습니다! 🥷🏽\n\n# Python 스레딩 초급 단계\n\n<div class=\"content-ad\"></div>\n\n먼저 I/O 바운드와 CPU 바운드 작업을 정의해 봅시다. io_bound_operation은 지정된 초 수만큼 \"잠들어\" 있습니다. cpu_bound_operation은 지정된 숫자 범위를 더합니다. 두 함수 모두 결과를 shared_list에 추가합니다. 동일한 프로세스의 스레드는 데이터를 공유할 수 있다는 것을 기억해 주세요.\n\n```js\nimport logging\nfrom threading import Thread\nfrom time import perf_counter, sleep\n\nfrom concurrency.utils import flaten_list_of_lists, get_saving_path, postprocess_times\nfrom concurrency.visualize import barh\n\n\nformat = \"%(asctime)s: %(message)s\"\nlogging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n\nshared_list = []  # 동일한 프로세스의 스레드는 데이터를 공유합니다.\n\ndef io_bound_operation(secs: float | int) -> None:\n    \"\"\"secs 초 동안 1개의 I/O 바운드 작업을 실행하고 결과를 shared_list에 추가합니다.\"\"\"\n    start = perf_counter()\n    sleep(secs)\n    finish = perf_counter()\n\n    shared_list.append([(start, finish)])\n\ndef cpu_bound_operation(n: int) -> None:\n    \"\"\"CPU 바운드 작업.\"\"\"\n    start = perf_counter()\n    count = 0\n    for i in range(n):\n        count += i\n    finish = perf_counter()\n\n    shared_list.append([(start, finish)])\n```\n\n이제 두 개의 새 스레드 t1과 t2를 생성할 것입니다. Thread 객체를 인스턴스화 할 때는 스레드에서 실행할 작업/함수인 target을 추가해야 합니다. 인자는 args 매개변수를 통해 전달될 수 있으며, 이는 Iterable 객체를 받습니다.\n\n이 예제에서는 I/O 바운드 작업이 1초 동안 지속되도록 하고, 프로세서가 이 100,000,000개의 숫자를 더하는 데 약 3.5초가 걸립니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef threading_two_threads():\n    # 두 개의 스레드 객체 생성\n    t1 = Thread(target=io_bound_operation, args=(1,))\n    t2 = Thread(target=cpu_bound_operation, args=(100000000,))\n\n    # 활동 시작 -> run() 메서드를 호출\n    t1.start()\n    sleep(0.1)\n    t2.start()\n\n    # 호출 스레드 블록 -> 스레드가 완료될 때까지 계속 실행됨\n    t1.join()\n    t2.join()\n\n    logging.info(f\"shared_list {shared_list}\")\n```\n\n그런 다음 스레드 활동을 시작해야 합니다. 이는 start() 메서드를 호출하여 수행됩니다. 이는 객체의 run() 메서드가 별도의 제어 스레드에서 호출되도록 정렬합니다.\n\n또한 sleep(0.1) 함수가 있어 두 번째 스레드가 조금 늦게 시작되도록합니다. 이를 통해 시각화를 더 잘할 수 있습니다.\n\n```js\ndef threading_two_threads():\n    # 두 개의 스레드 객체 생성\n    t1 = Thread(target=io_bound_operation, args=(1,))\n    t2 = Thread(target=cpu_bound_operation, args=(100000000,))\n\n    # 활동 시작 -> run() 메서드를 호출\n    t1.start()\n    sleep(0.1)\n    t2.start()\n\n    # 호출 스레드 블록 -> 스레드가 완료될 때까지 계속 실행됨\n    t1.join()\n    t2.join()\n\n    logging.info(f\"shared_list {shared_list}\")\n```\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 스레드 객체의 join() 메서드를 호출해야 스레드가 종료될 때까지 기다릴 수 있습니다.\n\n메인 스레드는 두 스레드가 모두 완료될 때까지 종료되지 않습니다.\n\n스레드를 결합하면 호출 중인 스레드(메인 스레드)가 join() 메서드가 호출된 스레드가 정상적으로 종료되거나 처리되지 않은 예외를 통해 또는 선택적으로 제한 시간이 발생할 때까지 블록됩니다.\n\n이 예제를 변경해보세요. 만약 두 join() 메서드의 주석 처리를 해도 프로그램은 예외를 발생시킬 것입니다. 왜냐하면 shared_list에는 아무것도 없기 때문에 postprocess_times 함수가 빈 목록을 색인화하려고 시도할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef threading_two_threads():\n    # 두 개의 스레드 객체를 생성합니다.\n    t1 = Thread(target=io_bound_operation, args=(1,))\n    t2 = Thread(target=cpu_bound_operation, args=(100000000,))\n\n    # 활동 시작 -> run() 메서드를 호출합니다.\n    t1.start()\n    sleep(0.1)\n    t2.start()\n\n    # 호출 스레드를 차단 -> 스레드가 완료될 때까지 계속 실행되지 않도록 합니다.\n    t1.join()\n    t2.join()\n\n    logging.info(f\"shared_list {shared_list}\")\n\n    # 차트 표시를 위한 일부 처리\n    start_points, end_points = postprocess_times(flaten_list_of_lists(shared_list))\n    # start_points, end_points = postprocess_times(shared_list)\n\n    barh(\n        title=\"동시 실행, 2개의 스레드, 1초의 I/O 바운드 작업 + 약 3.5초의 CPU 작업\",\n        start_points=start_points,\n        end_points=end_points,\n        path=get_saving_path(\"threading/images/first_multithreaded_program.png\"),\n        n=2,\n    )\n\nif __name__ == \"__main__\":\n    logging.info(f\"동시 작업 시작\")\n    threading_two_threads()\n    logging.info(f\"동시 작업 완료\")\n```\n\n아래 이미지는 각 스레드가 완료하는 데 소요된 시간을 보여줍니다. sleep 함수는 두 번째 스레드(cpu_bound_operation)가 조금 늦게 시작하도록 만듭니다. 그래프에서 첫 번째 스레드(0)가 시작한 후 0.1초 후 두 번째 스레드가 시작됩니다.\n\n<div class=\"content-ad\"></div>\n\nI/O 바운드 작업은 단 1초 동안 지속되고 io_bound_operation 함수는 해당 작업만 수행해야 합니다. 이 때 I/O 바운드 작업이 대기하는 동안(전체 1초 동안) CPU 바운드 작업이 실행될 수 있습니다. 이것이 CPU 바운드 작업(두 번째 스레드)이 약 3.5초 동안만 지속되며 I/O 바운드 작업에 의해 지연되지 않는 이유입니다.\n\n![image](/assets/img/2024-06-22-ThreadinginPython_4.png)\n\nThread 객체는 스레드를 만드는 가장 간단한 방법 중 하나이지만, 더 편리한 방법들이 있습니다. 그러나 더 자세히 파헤치기 전에 좀 더 간단한 예제를 살펴보겠습니다.\n\n# threading 모듈을 이용한 멀티스레딩 시간 시각화\n\n<div class=\"content-ad\"></div>\n\n## 예시 1–2 스레드\n\n- 스레드 1–1: 약 1초의 I/O-바운드 작업과 1초 정도의 CPU-바운드 작업\n- 스레드 2–1: 약 3.5초의 CPU-바운드 작업\n\n이제 첫 번째 스레드가 CPU-바운드 작업 1초 정도와 I/O-바운드 작업 1초로 구성된 작업을 실행하는 대신 I/O-바운드 작업만을 실행하는 작업이 아닌 경우를 고려해 봅시다.\n\n따라서 이제 두 개의 새로운 스레드를 생성하는 프로그램이 있습니다. 하나는 I/O-바운드 작업과 CPU-바운드 작업을 수행하고, 또 다른 하나는 약 3.5초의 CPU-바운드 작업을 수행합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\ndef cpu_io_bound_operations(secs: float | int, n: int) -> None:\n    \"\"\"한 가지 I/O 바운드 작업(초 단위)과 한 가지 CPU 바운드 작업을 실행하는 함수입니다. 결과는 shared_list에 추가됩니다.\"\"\"\n    start = perf_counter()\n    count = 0\n    for i in range(n):  # CPU 바운드\n        count += i\n    sleep(secs)  # I/O 바운드\n    finish = perf_counter()\n\n    shared_list.append([(start, finish)])\n```\n\n쓰레드 2는 프로세서에서 약 3.5초가 필요하며, 쓰레드 1은 1초만에 처리합니다.\n\n쓰레드 1이 1초만에 처리하는 이유는 CPU 바운드 작업 때문이며, I/O 바운드 작업의 대기 시간은 쓰레드 2가 활용합니다.\n\n쓰레드 2(3.5초) + 쓰레드 1(1초)을 더하면 4.5초의 CPU 작업 시간이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 그래프는 두 작업이 모두 4.5초 동안 실행된다는 것을 보여줍니다. 각 CPU 집약적 작업에 필요한 시간은 약간 다를 수 있습니다.\n\n<img src=\"/assets/img/2024-06-22-ThreadinginPython_5.png\" />\n\n그러나 스레드 1은 종료하는 데 3초가 걸립니다. 이는 우리가 컨텍스트 스위치가 언제 발생하는지 제어하지 않기 때문에, I/O 바운드 작업이 종료된 후에도 스레드 1이 프로세서를 사용하기 위해 얼마간의 대기 시간이 있을 수 있기 때문입니다. 컨텍스트 스위치는 개발자의 제어를 벗어나므로, 실제로 원하는 것보다 더 자주 발생할 수 있으며 다른 스레드로 스위치하고 싶지 않은 순간에 발생할 수 있습니다.\n\n이제 몇 가지 추가 예시를 빠르게 살펴보겠습니다! 이미 이해하셨다면이 부분을 건너뛰고 바로 다음 섹션인 ThreadPoolExecutor로 이동할 수 있습니다 🚀\n\n<div class=\"content-ad\"></div>\n\n## 예제 2-1 스레드\n\n- 10개의 IO 바운드 작업을 1초씩 순차적으로 수행합니다.\n\n여기서는 순차적 실행을 표현하고 더 많은 스레드를 생성할 필요가 없습니다. 메인 스레드 하나로 충분합니다.\n\n```python\ndef sequential(n: int = 10, secs: float | int = 1) -> None:\n    \"\"\"1개 스레드에서 n개의 I/O 바운드 작업을 secs 초 동안 순차적으로 수행하고 수평 막대 차트를 플롯합니다.\n    \"\"\"\n    # n개의 I/O 바운드 작업 수행, 각 작업에 대한 튜플 저장\n    times = [io_bound_operation(secs) for _ in range(n)]\n    start_points, end_points = postprocess_times(times)\n\n    barh(\n        title=\"순차 실행, 1개 스레드, 10개의 1초 IO 바운드 작업\",\n        start_points=start_points,\n        end_points=end_points,\n        path=get_saving_path(\"threading/images/ex_1_one_thread.png\"),\n    )\r\n```\n\n<div class=\"content-ad\"></div>\n\n위 그림에서 각 스레드가 작업을 수행했기 때문에 스레드가 막대로 표시되었습니다(입출력 바인드 및 CPU 바인드 작업을 결합해도 동일한 작업으로 간주했습니다).\n\n이제 10개의 다른 입출력 바인드 작업이 동일한 스레드에서 실행되므로 각 작업을 더 잘 시각화할 수 있습니다. 따라서 이 열개의 막대는 동일한 스레드에 속합니다.\n\n![이미지](/assets/img/2024-06-22-ThreadinginPython_6.png)\n\n## 예제 3–1 thread\n\n<div class=\"content-ad\"></div>\n\n- 2개의 CPU 바운드 작업\n\n만약 우리가 동일한 스레드에서 연속적으로 3.5초 정도 걸리는 CPU 바운드 작업 두 개를 실행한다면, 약 7초 정도 소요된다는 것을 확인할 수 있습니다.\n\n두 번째 작업을 시작하기 전에 첫 번째 작업이 완료되어야 합니다.\n\n```python\ndef sequential(counts: int, n: int = 10) -> None:\n    # n개의 CPU 바운드 작업 수행, 각 작업에 대한 튜플 저장\n    times = [cpu_bound_operation(counts) for _ in range(n)]\n    start_points, end_points = postprocess_times(times)\n```\n\n<div class=\"content-ad\"></div>\n\n![스레드](/assets/img/2024-06-22-ThreadinginPython_7.png)\n\n## 예제 4-2 스레드\n\n- 스레드 1–1: 대략 3.5초 소요되는 CPU 바운드 작업\n- 스레드 2–1: 대략 3.5초 소요되는 CPU 바운드 작업\n\n위의 두 CPU 바운드 작업은 동시에 실행될 때 매우 다른 차트를 보여줍니다. 두 작업은 모두 7초가 걸리는 것처럼 보이지만, 실제로는 각각 3.5초가 걸립니다. 그들은 서로 번갈아가며 작업을 완료할 때까지 전환됩니다.\n\n<div class=\"content-ad\"></div>\n\n멀티스레딩을 사용하는 방법이 제대로 되지 않았어요. 현재는 교육 목적으로만 사용하고 있어요. CPU 바운드 작업만 한다면 멀티스레딩을 사용해도 시간이 단축되지 않아요.\n\n```js\ndef thread_cpu_bound_operations(counts: int) -> None:\n    \"\"\"Run a CPU-bound task and append the results to shared_list.\"\"\"\n    shared_list.append([cpu_bound_operation(counts)])\n\n\ndef threading_two_threads() -> None:\n    # 두 개의 스레드 객체를 생성합니다. 각 스레드는 다섯 개의 I/O 바운드 작업을 수행할 거에요\n    t1 = Thread(target=thread_cpu_bound_operations, args=(100000000,))\n    t2 = Thread(target=thread_cpu_bound_operations, args=(100000000,))\n\n    # 활동 시작 -> run() 메서드를 호출합니다\n    t1.start()\n    t2.start()\n\n    # 호출한 스레드가 완료될 때까지 기다립니다 -> 스레드가 모두 끝날 때까지 진행을 막습니다\n    t1.join()\n    t2.join()\n```\n\n![이미지](/assets/img/2024-06-22-ThreadinginPython_8.png)\n\n## 예제 5-2 스레드\n\n<div class=\"content-ad\"></div>\n\n- 1초 동안 5개의 I/O 바운드 작업을 가진 스레드 1 실행\n- 1초 동안 5개의 I/O 바운드 작업을 가진 스레드 2 실행\n\n총 10개의 1초 동안 동작하는 I/O 바운드 작업과 두 개의 스레드가 있습니다. 각 스레드는 순차적으로 다섯 개의 I/O 바운드 작업을 실행하며, 두 그룹의 다섯 개의 작업은 동시에 실행됩니다.\n\n```python\ndef thread_io_bound_operations(n: int, secs: float | int) -> None:\n    \"\"\"n개의 I/O 바운드 작업을 secs 초 동안 실행하고 결과를 shared_list에 추가합니다.\"\"\"\n    shared_list.append([io_bound_operation(secs) for _ in range(n)])\n\n\ndef threading_two_threads() -> None:\n    # 각각 다섯 개의 I/O 바운드 작업을 수행할 두 개의 스레드 개체 생성\n    t1 = Thread(target=thread_io_bound_operations, args=(5, 1))\n    t2 = Thread(target=thread_io_bound_operations, args=(5, 1))\n\n    # 활동 시작 -> run() 메서드 호출\n    t1.start()\n    t2.start()\n\n    # 호출 스레드 블록 -> 스레드가 완료되지 않은 상태로 계속 실행되는 것을 방지\n    t1.join()\n    t2.join()\n```\n\n![Python에서 쓰레딩하기](/assets/img/2024-06-22-ThreadinginPython_9.png)\n\n<div class=\"content-ad\"></div>\n\n## 예제 6–10 스레드\n\n- 각 스레드 — 1초 동안의 1개의 I/O 바운드 작업\n\n지난 예제와 비슷하지만, 이제는 두 개가 아닌 열 개의 스레드가 있으며 각각은 1초 동안의 단 하나의 I/O 바운드 작업을 실행합니다.\n\n```js\ndef thread_io_bound_operations(n: int, secs: float | int) -> None:\n    \"\"\"n개의 secs 초 동안의 I/O 바운드 작업을 실행하고 결과를 shared_list에 추가합니다.\"\"\"\n    shared_list.append([io_bound_operation(secs) for _ in range(n)])\n\n\ndef threading_two_threads() -> None:\n    threads = []\n    # 열 개의 스레드 객체 생성, 각 스레드는 하나의 I/O 바운드 작업을 수행합니다\n    for _ in range(10):\n        t = Thread(target=thread_io_bound_operations, args=(1, 1))\n        t.start()\n        threads.append(t)\n\n    # 호출 스레드 블로킹 -> 스레드가 완료되지 않은 상태로 계속 실행되지 않도록 함\n    [thread.join() for thread in threads]\n```\n\n<div class=\"content-ad\"></div>\n\n\n![Example 7-2 threads](/assets/img/2024-06-22-ThreadinginPython_10.png)\n\n## Example 7-2 threads\n\n- Thread 1: CPU-bound task of approximately 3.5s\n- Thread 2: 5 I/O-bound tasks of 1s each\n\nNow we have two threads. Thread 1 executes a CPU-bound operation taking about 3.5 seconds, while thread 2 executes five I/O-bound tasks, each taking 1 second.\n\n\n<div class=\"content-ad\"></div>\n\nI/O 작업이 대기하는 동안 CPU 집약 작업이 실행됩니다. 매번 I/O 작업이 시작될 때마다 OS는 빠르게 스레드를 전환합니다.\n\n```js\ndef thread_io_bound_operations(n: int, secs: float | int) -> None:\n    \"\"\"Run n I/O-bound tasks of secs seconds and append the results to shared_list.\"\"\"\n    shared_list.append([io_bound_operation(secs) for _ in range(n)])\n\n\ndef thread_cpu_bound_operations(counts: int) -> None:\n    \"\"\"Run a CPU-bound task and append the results to shared_list.\"\"\"\n    shared_list.append([cpu_bound_operation(counts)])\n\n\ndef threading_two_threads() -> None:\n    # 두 개의 스레드 개체 생성, 각 스레드는 다섯 가지의 I/O 작업을 수행할 것입니다\n    t1 = Thread(target=thread_cpu_bound_operations, args=(100000000,))\n    t2 = Thread(target=thread_io_bound_operations, args=(5, 1))\n\n    # 활동 시작 -> run() 메서드 호출\n    t1.start()\n    t2.start()\n\n    # 호출 스레드 차단 -> 스레드가 완료될 때까지 계속 실행하지 못하도록 함\n    t1.join()\n    t2.join()\n```\n\n![image](/assets/img/2024-06-22-ThreadinginPython_11.png)\n\n## 예제 8-6 스레드\n\n<div class=\"content-ad\"></div>\n\n- Thread 1–1 CPU-bound task of 3.5s approx (bar 5)\n- Thread 2–1 CPU-bound task (bar 4)\n- Thread 3–1 CPU-bound task (bar 3)\n- Thread 4–1 I/O-bound task of 1s\n- Thread 5–1 I/O-bound task of 1s\n- Thread 6–1 I/O-bound task of 1s\n\n여기에서는 세 개의 스레드가 각각 하나의 I/O 작업을 수행하고, 세 개의 스레드가 각각 하나의 CPU 집약적인 작업을 수행합니다. 세 개의 CPU 집약적인 작업이 완료되기까지 걸리는 시간이 다릅니다.\n\n3.5초 동안 계속되는 가장 긴 작업이 처음에 시작됩니다(바 5). 다른 두 가지 CPU 집약적인 작업 때문에 거의 6초가 걸립니다.\n\n```python\ndef 스레드_io_bound_operations(n: int, secs: float | int) -> None:\n    \"\"\"n개의 secs 초동안 I/O-bound 작업을 실행하고 결과를 shared_list에 추가합니다.\"\"\"\n    shared_list.append([io_bound_operation(secs) for _ in range(n)])\n    \n\ndef 스레드_cpu_bound_operations(counts: int) -> None:\n    \"\"\"CPU-bound 작업을 실행하고 결과를 shared_list에 추가합니다.\"\"\"\n    shared_list.append([cpu_bound_operation(counts)])\n\n\ndef threading_six_threads() -> None:\n    # 두 가지 스레드 객체 생성, 각 스레드는 다섯 개의 I/O-bound 작업을 수행할 것임\n    t1 = Thread(target=thread_cpu_bound_operations, args=(100000000,))\n    t2 = Thread(target=thread_cpu_bound_operations, args=(50000000,))\n    t3 = Thread(target=thread_cpu_bound_operations, args=(20000000,))\n    t4 = Thread(target=thread_io_bound_operations, args=(1, 1))\n    t5 = Thread(target=thread_io_bound_operations, args=(1, 1))\n    t6 = Thread(target=thread_io_bound_operations, args=(1, 1))\n\n    # 활동 시작 -> run() 메서드 호출\n    t1.start()\n    t2.start()\n    t3.start()\n    t4.start()\n    t5.start()\n    t6.start()\n\n    # 호출 스레드 차단 -> 스레드가 완료되지 않은 채로 계속 실행되지 않도록 함\n    t1.join()\n    t2.join()\n    t3.join()\n    t4.join()\n    t5.join()\n    t6.join()\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-ThreadinginPython_12.png\" />\n\n## 예제 9-4 쓰레드\n\n쓰레드 1은 각각 3.5초의 두 개의 CPU-bound 작업을 순차적으로 실행합니다(막대 6 및 7). 쓰레드 2는 각각 거의 1초의 두 개의 CPU-bound 작업을 순차적으로 실행합니다(막대 4 및 5).\n\n다른 막대는 I/O-bound 작업을 나타내며 각각 1초씩 두 개가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndef thread_io_bound_operations(n: int, secs: float | int) -> None:\n    \"\"\"n개의 I/O 바운드 작업을 secs 초 동안 실행하고 결과를 shared_list에 추가합니다.\"\"\"\n    shared_list.append([io_bound_operation(secs) for _ in range(n)])\n\n\ndef thread_cpu_bound_operations(counts: int, n: int) -> None:\n    \"\"\"CPU 바운드 작업을 실행하고 결과를 shared_list에 추가합니다.\"\"\"\n    shared_list.append([cpu_bound_operation(counts) for _ in range(n)])\n\n\ndef threading_four_threads() -> None:\n    # 두 개의 쓰레드 객체를 생성하며 각 쓰레드는 다섯 개의 I/O 바운드 작업을 수행합니다.\n    t1 = Thread(target=thread_cpu_bound_operations, args=(100000000, 2))\n    t2 = Thread(target=thread_cpu_bound_operations, args=(20000000, 2))\n    t3 = Thread(target=thread_io_bound_operations, args=(2, 1))\n    t4 = Thread(target=thread_io_bound_operations, args=(2, 1))\n\n    # 활동 시작 -> run() 메소드를 호출합니다.\n    t1.start()\n    t2.start()\n    t3.start()\n    t4.start()\n\n    # 호출 쓰레드를 차단 -> 쓰레드들이 완료될 때까지 계속 실행되지 않도록 합니다.\n    t1.join()\n    t2.join()\n    t3.join()\n    t4.join()\r\n```\n\n<img src=\"/assets/img/2024-06-22-ThreadinginPython_13.png\" />\n\n쓰레드 1(막대 6과 7)가 첫 번째 3.5초 CPU 바운드 작업(막대 6)을 실행할 때, 다른 쓰레드들과 교차되어 최종적으로 실행을 완료하는 데 5초가 걸립니다. 모든 I/O 바운드 작업은 약 1초가 걸리지만, CPU 집약적 작업이 실행 중일 때 대기하고 있을 수 있습니다. 따라서 쓰레드 1은 첫 번째 작업을 완료하는 데 주로 쓰레드 2의 두 CPU 바운드 작업(막대 4와 5)으로 인해 5초가 걸립니다.\n\n쓰레드 1(막대 6과 7)가 두 번째 3.5초 CPU 바운드 작업(막대 7)을 실행하는 경우에는 프로세서의 모든 성능을 직접 활용할 수 있습니다. 따라서 약 3.5초가 걸립니다.\n\n<div class=\"content-ad\"></div>\n\n위에서는 이 개념을 명확히 설명하기 위한 몇 가지 예시였습니다. 이제 다른 더 편리한 방법들에 대해 알아봅시다!\n\n# ThreadPoolExecutor\n\nconcurrent.futures 모듈은 스레드를 만들기 위해 사용할 수 있는 ThreadPoolExecutor 객체와 multiprocessing을 위한 ProcessPoolExecutor 객체를 제공합니다.\n\n이 글에서는 스레드에 초점을 맞추기 때문에 ThreadPoolExecutor만 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 이유\n\nThreadPoolExecutor 클래스는 비동기적으로 호출을 실행하는 데 쓰이는 Executor 하위 클래스입니다.\n\nThreadPoolExecutor는 스레드나 워커 스레드의 컬렉션을 생성하고 관리하여 재사용할 수 있게 합니다. 우리가 위에서 한 것처럼 작업을 동시에 실행하고자 할 때마다 스레드를 생성하고 소멸하는 것을 피할 수 있습니다. 이렇게 하면 이러한 작업이 시간이 많이 소요되기 때문에 성능이 향상됩니다.\n\n# 작동 방식\n\n<div class=\"content-ad\"></div>\n\n## The Executor 클래스\n\n`ProcessPoolExecutor` 클래스와 마찬가지로 `ThreadPoolExecutor`도 `Executor` 클래스를 확장합니다. `Executor` 클래스는 다섯 가지 메서드만을 정의하는 추상 기본 클래스로 다음과 같습니다:\n\n- `submit()`\n- `map()`\n- `shutdown()`\n\n다른 두 메서드는 사실 `__enter__()`와 `__exit__()`로, 이는 파이썬의 매직 메서드로 컨텍스트 관리 프로토콜을 구현합니다. 이들 덕분에 `ThreadPoolExecutor`를 `with` 문에서 사용할 수 있습니다(권장). `with` 문은 `__enter__()` 메서드를 호출하며, `with` 코드 블록을 벗어날 때 `__exit__()`가 호출됩니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nclass Executor(object):\n    \"\"\"구체적인 비동기 업무 처리자들을 위한 추상 기본 클래스입니다.\"\"\"\n    ...\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.shutdown(wait=True)\n        return False\n```\n\nExecutor는 단지 추상 클래스이며 대부분의 로직은 ThreadPoolExecutor 메서드에서 구현됩니다. submit() 및 shutdown() 메서드는 ThreadPoolExecutor 클래스에서 구현되었으며 map() 메서드의 로직은 Executor 클래스에서 구현되었습니다. 내부적으로 submit()을 사용하기 때문입니다.\n\n## ThreadPoolExecutor 클래스\n\nPython의 concurrent.futures 모듈의 ThreadPoolExecutor는 작업을 관리하기 위해 내부적으로 큐를 사용합니다. 큐는 ThreadPoolExecutor의 생성자에서 생성됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n## __init__() 메서드\n\n__init__() 메서드는 새로운 ThreadPoolExecutor 인스턴스를 초기화하고 큐 및 몇 가지 더 많은 객체를 생성합니다.\n\n아래의 SimpleQueue 클래스는 간단한 비제한 FIFO(선입선출) 큐입니다. 먼저 들어간 순서대로 큐에서 항목이 처리되거나 제거되는 선입선출 원칙을 따릅니다.\n\nmax_workers 매개변수에 인자로 전달하여 사용할 수있는 스레드의 최대 수를 설정할 수 있습니다. 그렇게 하지 않으면 기본값은 머신의 프로세서 수에 4를 더한 값이 됩니다. 그 값이 32를 초과하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nclass ThreadPoolExecutor(_base.Executor):\n    ...\n\n    def __init__(self, max_workers=None, thread_name_prefix='',\n                 initializer=None, initargs=()):\n        ...\n        if max_workers is None:\n            # 두 종류의 작업에 대해 process_cpu_count + 4를 사용합니다.\n            # 그러나 많은 코어를 가진 기계에서 예상치 못하게 많은 리소스를 소비하도록 제한합니다.\n            max_workers = min(32, (os.process_cpu_count() or 1) + 4)\n        ...\n\n        self._max_workers = max_workers\n        self._work_queue = queue.SimpleQueue()\n        ...\n```\n\n우리는 쓰레드에 선택적으로 이름 접두사를 전달하고, 워커 쓰레드를 초기화하는 데 사용되는 호출 가능한 객체, 그리고 그 인수를 포함하는 튜플을 전달할 수 있습니다.\n\n## submit() 메서드\n\nsubmit() 메서드는 호출 가능한 객체를 실행할 수 있도록 예약합니다. 호출 가능한 객체는 함수 이름과 해당 인수를 전달하는 인수로 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nwith ThreadPoolExecutor(max_workers=1) as executor:\n    future = executor.submit(pow, 323, 1235)\n    print(future.result())  # blocks\n```\n\n이 작업은 호출 가능한 함수의 비동기 실행을 나타내는 Future 객체로 래핑되어 있으며, submit() 메서드에 의해 즉시 반환됩니다.\n\nFuture는 비동기 작업의 최종 결과를 나타내는 추상화로, 초기에 결과를 알 수없는 결과를 대신하는 객체입니다. 보통 결과의 계산이 아직 완료되지 않았기 때문에 결과가 아직 알려지지 않았을 때 사용됩니다.\n\nfuture.result()는 호출한 함수(pow 함수)에서 반환된 값을 반환합니다. 호출이 아직 완료되지 않았을 경우 이 메서드는 최대 timeout 초까지 대기합니다 (timeout은 result(timeout=None)의 유일한 매개변수입니다). 호출이 timeout 초 내에 완료되지 않으면 TimeoutError가 발생합니다. timeout은 int 또는 float가 될 수 있으며, 지정되지 않거나 None인 경우 대기 시간 제한이 없습니다.\n\n\n<div class=\"content-ad\"></div>\n\n아래는 ThreadPoolExecutor 클래스의 일부 소스 코드를 볼 수 있습니다. submit() 메서드가 호출되면 Future와 _WorkItem 객체가 생성됩니다. 그런 다음 _WorkItem은 _work_queue에 넣어집니다.\n\n```js\nclass ThreadPoolExecutor(_base.Executor):\n    ...\n\n    def submit(self, fn, /, *args, **kwargs):\n        with self._shutdown_lock, _global_shutdown_lock:\n            ...\n\n            f = _base.Future()\n            w = _WorkItem(f, fn, args, kwargs)\n\n            self._work_queue.put(w)\n            self._adjust_thread_count()\n            return f\n        ...\n```\n\n_WorkItem은 작업 (fn), 인수들 (args 및 kwargs) 및 미래 객체 (_base.Future())를 함께 래핑하는 데 사용되는 객체입니다. 작업이 실행되고 결과가 Future 객체에 설정되는 run() 메서드를 구현합니다.\n\n```js\nclass _WorkItem:\n    def __init__(self, future, fn, args, kwargs):\n        self.future = future\n        self.fn = fn\n        self.args = args\n        self.kwargs = kwargs\n\n    def run(self):\n        if not self.future.set_running_or_notify_cancel():\n            return\n\n        try:\n            result = self.fn(*self.args, **self.kwargs)\n        except BaseException as exc:\n            self.future.set_exception(exc)\n            # 예외 'exc'와의 참조 순환을 끊습니다\n            self = None\n        else:\n            self.future.set_result(result)\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n```\n\n<div class=\"content-ad\"></div>\n\nrun() 메서드는 작업자 스레드에서 호출됩니다. worker 모듈 함수인 _worker에 구현되어 있으며, 이 함수는 스레드에 대상으로 전달된 함수입니다.\n\n```js\ndef _worker(executor_reference, work_queue, initializer, initargs):\n    ...\n\n            if work_item is not None:\n                work_item.run()\n                # 객체에 대한 참조 삭제. GH-60488 참조\n                del work_item\n                continue\n\n            ...\n```\n\n스레드는 ThreadPoolExecutor 클래스 생성자에서 호출되는 _adjust_thread_count() 메서드에서 생성됩니다.\n\n```js\nclass ThreadPoolExecutor(_base.Executor):\n\n    def _adjust_thread_count(self):\n        ...\n        if num_threads < self._max_workers:\n            thread_name = '%s_%d' % (self._thread_name_prefix or self,\n                                     num_threads)\n            t = threading.Thread(name=thread_name, target=_worker,\n                                 args=(weakref.ref(self, weakref_cb),\n                                       self._work_queue,\n                                       self._initializer,\n                                       self._initargs))\n            t.start()\n            self._threads.add(t)\n            _threads_queues[t] = self._work_queue\n    ...\n```\n\n<div class=\"content-ad\"></div>\n\n## map() 메소드\n\nmap()은 Executor 클래스에 직접 구현되어 있으며 내부적으로 submit() 메소드를 사용합니다.\n\n```js\nclass Executor(object):\n    \"\"\"이것은 구체적인 비동기 executor를 위한 추상 기본 클래스입니다.\"\"\"\n    ...\n    def map(self, fn, *iterables, timeout=None, chunksize=1):\n        ...\n        fs = [self.submit(fn, *args) for args in zip(*iterables)]\n        ...\n```\n\nmap()은 스레드 풀에 작업을 제출하는 또 다른 방법입니다. 내장된 map(fn, *iterables) 함수와 유사하지만 fn으로 전달하는 함수는 비동기적으로 실행되며 fn에 대해 여러 호출을 동시에 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n내장 map() 함수는 지연 평가를 제공합니다. 이는 해당 함수에서 반환된 iterable의 값들이 요청될 때에만 계산되고 반환된다는 것을 의미합니다.\n\n그러나 Executor.map(fn, *iterables)를 호출할 때에는 해당 함수가 제공된 iterable의 모든 항목을 미리 가져옵니다. 이는 필요시에만 처리되는 '지연 평가' 방식과 대조적입니다.\n\n이는 우리가 작업에서 값을 가져오기 위해 순서가 지정된 iterable에서 값들을 가져올 때 반복할 수 있는 iterator를 반환합니다.\n\n만약 timeout이 지정되지 않거나 None이면 대기 시간 제한이 없습니다. 따라서 반복을 시작할 때 첫 번째 요소가 이용 가능할 때까지 두 번째 요소에는 액세스하지 않습니다. timeout이 특정 int나 float로 설정된 경우 주어진 시간 초과 후 결과를 얻을 수 없는 경우 TimeoutError가 발생합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 함수 호출이 예외를 발생시키면, 해당 예외는 반복자에서 값을 검색할 때 발생됩니다.\n\n예제를 살펴보겠습니다! 이 예제는 정말 멋지고, 제가 20번 정도 실행했어요 🤭\n\n기본적으로 ThreadPoolExecutor를 사용하여 5개의 워커 스레드로 위키피디아에서 20가지 이국적인 호주 동물을 로드합니다.\n\n특정 시점에는 하나의 스레드만 실행될 수 있지만, 5개의 스레드가 사용 가능합니다. 따라서 첫 번째 스레드가 실행을 시작하면 컨텍스트 스위치가 발생하고 두 번째 스레드가 시작할 수 있습니다. 왜냐하면 OS가 I/O 작업임을 감지하고 자원을 다른 스레드에 할당함으로써 시간 자원을 낭비하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 표를 보면 언제나 동시에 5개의 스레드만 작동 중임을 알 수 있습니다. 하나의 스레드가 작업을 완료하면 다른 작업을 시작하기 위해 재사용됩니다. 작업을 완료하는 데는 4초 미만이 소요됩니다.\n\n![image](/assets/img/2024-06-22-ThreadinginPython_14.png)\n\n반면에, 만약 우리가 20마리의 호주 동물을 동기적으로 로드한다면 거의 15초가 걸립니다! 😱\n\n![image](/assets/img/2024-06-22-ThreadinginPython_15.png)\n\n<div class=\"content-ad\"></div>\n\n위 코드에서는 20가지의 이국적인 호주 동물을 확인할 수 있어요.\n\n```js\nimport concurrent.futures\nfrom time import perf_counter, time\nimport urllib.request\nimport logging\n\nfrom concurrency.utils import get_saving_path, postprocess_times\nfrom concurrency.visualize import barh\n\n\nformat = \"%(asctime)s: %(message)s\"\nlogging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n\n\nURLS = [\n    \"https://en.wikipedia.org/wiki/Emu\",\n    \"https://en.wikipedia.org/wiki/Wombat\",\n    \"https://en.wikipedia.org/wiki/Kangaroo\",\n    \"https://en.wikipedia.org/wiki/Platypus\",\n    \"https://en.wikipedia.org/wiki/Koala\",\n    \"https://en.wikipedia.org/wiki/Tasmanian_devil\",\n    \"https://en.wikipedia.org/wiki/Echidna\",\n    \"https://en.wikipedia.org/wiki/Dingo\",\n    \"https://en.wikipedia.org/wiki/Kookaburra\",\n    \"https://en.wikipedia.org/wiki/Wallaby\",\n    \"https://en.wikipedia.org/wiki/Macrotis\",\n    \"https://en.wikipedia.org/wiki/Quokka\",\n    \"https://en.wikipedia.org/wiki/Cassowary\",\n    \"https://en.wikipedia.org/wiki/Sugar_glider\",\n    \"https://en.wikipedia.org/wiki/Laughing_kookaburra\",\n    \"https://en.wikipedia.org/wiki/Rainbow_lorikeet\",\n    \"https://en.wikipedia.org/wiki/Coastal_taipan\",\n    \"https://en.wikipedia.org/wiki/Mistletoebird\",\n    \"https://en.wikipedia.org/wiki/Thylacine\",\n    \"https://en.wikipedia.org/wiki/Quoll\",\n]\n\nanimals = {}\n\n\n# I/O-bound operation\ndef load_url(url: str) -> tuple[float]:\n    \"\"\"Retrieve a single page and return start and finish times.\"\"\"\n    start = perf_counter()\n    with urllib.request.urlopen(url) as conn:\n        animals[url] = conn.read()\n    finish = perf_counter()\n    return start, finish\n\n\ndef asynchronous_load_australian_animals() -> None:\n    start = time()\n    # Use ThreadPoolExecutor to manage concurrency\n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        # Use the map method to apply load_url to each URL\n        results = executor.map(load_url, URLS)\n\n        # Process the results and times\n        times = [time for time in results]\n        start_points, end_points = postprocess_times(times)\n    end = time()\n\n    total_time = round(end - start) + 1\n\n    barh(\n        title=\"비동기 실행, 5개 스레드, I/O 바운드 작업, 호주 동물\",\n        start_points=start_points,\n        end_points=end_points,\n        path=get_saving_path(\"thread-pool-executor/images/ThreadPoolExecutor_ex1.png\"),\n        n=len(URLS),\n        secs=total_time,\n    )\n\n\nif __name__ == \"__main__\":\n    logging.info(\"비동기 작업 초기화\")\n    asynchronous_load_australian_animals()\n    logging.info(f\"len(animals): {len(animals)}\")\n    logging.info(\"비동기 작업 완료\")\n```\n\nsubmit() 메서드를 사용하면 다음과 같이 보일 수 있지만, 실행할 때마다 많이 달라집니다.\n\n<img src=\"/assets/img/2024-06-22-ThreadinginPython_16.png\" />\n\n<div class=\"content-ad\"></div>\n\nas_completed() 함수를 사용하여 Future 인스턴스를 반복 처리해야 합니다. 그렇지 않으면 postprocess_times() 함수가 예외를 발생시킬 수 있습니다.\n\nas_completed() 함수는 결과로 제공된 Future 인스턴스에 대한 iterator를 반환하며 완료된 또는 취소된 Future를 생성합니다.\n\n```python\ndef asynchronous_load_australian_animals() -> None:\n    start = time()\n    # 동시성 관리를 위해 ThreadPoolExecutor 사용\n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        # submit 메서드를 사용하여 각 URL에 load_url을 적용\n        results = [executor.submit(load_url, url) for url in URLS]\n\n        # 결과 및 시간 처리\n        times = [result.result() for result in concurrent.futures.as_completed(results)]\n        start_points, end_points = postprocess_times(times)\n    end = time()\n\n    total_time = round(end - start) + 1\n```\n\n## shutdown() 메서드\n\n<div class=\"content-ad\"></div>\n\nThreadPoolExecutor을 with 문으로 context manager로 호출하면 shutdown() 메소드를 호출할 필요가 없습니다. 왜냐하면 shutdown() 메소드가 __exit__() 매직 메소드 내에서 호출되기 때문입니다. 그렇지 않으면 현재 대기 중인 futures가 실행을 완료한 후 사용 중인 모든 리소스를 해제해야 하는 executor에게 신호를 보내기 위해 호출해야 합니다.\n\n```js\nclass Executor(object):\n    \"\"\"이것은 구체적인 비동기 executor를 위한 추상 기본 클래스입니다.\"\"\"\n    ...\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.shutdown(wait=True)\n        return False\n```\n\n이러한 도구들로 할 수 있는 일이 많습니다. Future 객체에는 프로그램 동작을 사용자 정의하는 데 사용할 수 있는 여러 메소드가 있습니다(예: cancel(), running(), done(), 등).\n\nconcurrent.futures 모듈에는 완료를 기다리도록 허용하는 wait() 함수도 제공됩니다. return_when 매개변수를 통해 반환할 시점을 지정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위에 나열한 자료들은 이 주제를 더 잘 이해하는 데 도움이 되었어요.\n\n언제든지 어떤 소셜 네트워크로든 연락 주세요. 피드백은 언제든지 환영합니다!\n\n읽어 주셔서 감사합니다 🙂\n\njavideveloper.com\n\n<div class=\"content-ad\"></div>\n\n# 기타 자료\n\n- 동시성 및 병렬성 소개\n- threading 모듈 문서\n- threading 모듈 소스 코드. Python 3.13\n- concurrent.futures 모듈 문서\n- concurrent.futures 모듈 소스 코드\n- queue 모듈 소스 코드\n- thread pool 위키백과\n- map(fn, *iterables) 내장 함수\n- concurrency-python 저장소\n- 파이썬 스레딩: SuperFastPython의 완전 가이드\n- Python의 ThreadPoolExecutor: SuperFastPython의 완전 가이드","ogImage":{"url":"/assets/img/2024-06-22-ThreadinginPython_0.png"},"coverImage":"/assets/img/2024-06-22-ThreadinginPython_0.png","tag":["Tech"],"readingTime":29},{"title":"주니어 엔지니어를 위한 지속적이고 내재된 학습 방법","description":"","date":"2024-06-22 02:39","slug":"2024-06-22-Continuousandembeddedlearningasajuniorengineer","content":"\n\n맥쿼리 그룹의 시니어 어소시에이트인 자밀라 사바조바입니다\n\n![image](/assets/img/2024-06-22-Continuousandembeddedlearningasajuniorengineer_0.png)\n\n## 개요\n\n기술의 역동적인 영역에서 시니어 엔지니어들의 지도와 강력한 학습 문화는 저의 주니어 엔지니어로서의 여정을 형성하는 데 중추적인 역할을 하였습니다.\n\n<div class=\"content-ad\"></div>\n\n2023년 맥쿼리에 시니어 어소시에이트 엔지니어로 합류한 이후로 여러 종류의 흥미로운 시스템, 클라우드 및 온프렘 시스템에 대해 배울 수 있는 기회를 가졌습니다. C++, Python, Java, Scala, React, JSON 및 YAML과 같은 다양한 프로그래밍 언어로 많은 시스템을 다루며 주로 리눅스 환경에서 실행됩니다. 컨테이너 오케스트레이션 플랫폼 및 관리형 관계형 데이터베이스와 같은 공개 클라우드 서비스를 활용합니다. 다양한 기술 스택과 개발환경에서 일하며 문제 해결 능력을 향상시키고 지속적인 학습과 성장을 위한 튼튼한 기반을 마련할 수 있다고 믿습니다.\n\n주니어 시절의 열정적인 학습자와 경험 많은 멘토 사이의 관계는 맥쿼리에서 번영하는 엔지니어링 문화에 기여합니다. 여기서는 C++로 작성된 맞춤형 거래 및 위험 플랫폼을 개발할 기회를 가지고 있습니다.\n\n본 글에서는 맥쿼리에서 엔지니어로서 성장하는 데 필수적인 세 가지 요인에 대해 살펴보겠습니다.\n\n## 지원하는 환경\n\n<div class=\"content-ad\"></div>\n\n팀 내에서는 주니어 엔지니어로서 처음에 바로 모든 것을 올바르게 할 수는 없다는 점을 이해하고 있습니다. 시니어 엔지니어들로부터의 지도를 받으면서 또한 배운 내용을 실천함으로써 중요성을 두는 환경 속에서, 우리는 이러한 경험을 통해 시스템을 개선하고 방향을 잡을 수 있는 기회를 얻게 됩니다.\n\n예를 들어, 제 팀이 기능 브랜치에서 겹치는 작업으로 인한 Git 버전 관리의 문제에 직면했을 때, 우리는 스택 방식을 도입하도록 협력했습니다. 단일 에픽 브랜치를 마스터로 사용하고 각자의 브랜치를 일관되게 리베이스해 나가면서, 워크플로우를 최적화하고 생산성을 크게 향상시킬 수 있었습니다. 이 예는 지원적인 학습 환경이 도전을 성장의 기회로 변화시킬 수 있다는 것을 보여줍니다.\n\n맥쿼리가 전문 개발에 대한 헌신을 나타내는 또 다른 사례는 내부 교육 제공을 통한 것입니다. 이러한 노력은 외부 벤더 트레이닝 기회 및 Coursera, Udemy와 같은 온라인 자원에 대한 접근을 통해 모든 맥쿼리 엔지니어가 클라우드 개발 및 아키텍처 스킬을 갖추도록 목표로 합니다.\n\n저는 아마존 사무실에서 개최된 'AWS 아키텍처 구축' 과정을 수강할 기회를 갖게 되어 직접적인 지원을 경험해볼 수 있었습니다. 이 과정은 AWS 대표에 의해 진행되었으며, 다양한 산업 및 기술 팀에서 전문가들의 다양한 필요에 부합하도록 설계되었습니다.\n\n<div class=\"content-ad\"></div>\n\n그러한 노력들은 귀중한 것이며, 우리의 기술 능력을 확장할 뿐만 아니라 리더들로부터 받는 신뢰를 증명합니다. 학습 문화는 우리가 최신 지식과 최상의 실천 방법을 갖추고 현장의 동적인 도전에 대처할 수 있도록 보장합니다. 이는 혁신이 일반적인 상황을 유지하고, 직원들이 자신들의 경력에서 새로운 높이에 이를 수 있도록 촉진합니다.\n\n맥쿼리는 우리의 전문적 발전을 적극적으로 지원하는 다양한 학습 포럼을 제공합니다:\n\n* 내부 기술 이벤트: 맥쿼리는 전 세계의 엔지니어링 길드, 점심시간 학습 세션 및 '엔지니어링 사무실 시간' - 매일 엔지니어들이 모여 서로 소프트웨어 관련 질문을 도와주는 전용 시간을 정기적으로 만드는 등 이러한 노력은 엔지니어링 커뮤니티에서 협업과 지식 공유를 촉진하며 업계의 최신 기술 동향과 최상의 실천 방법을 제공합니다.\n\n* 워크숍: 특정 기술이나 방법론에 대해 더 깊이 파고들기 위한 실습 세션을 진행하며, 프로젝트에 직접 적용할 수 있는 실용적인 경험과 지식을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n- 맞춤형 교육: 내부 교육 프로그램은 주제 전문가에 의해 전달되며 제품 및 시스템이 어떻게 작동하는지 이해하는 데 도움이 됩니다. 다양한 엔지니어링 레벨에 맞게 설계된 구조화된 학습 경로를 통해 우리가 필요로 하는 리소스를 이해하고 발전하는 기술 스택에서 능숙해질 수 있도록 지원합니다.\n\n- 온라인 학습: 다양한 교육 도구를 제공하는 온라인 학습 플랫폼을 통해 기술 및 엔지니어링 분야에서 학습을 향상시키는 데 도움이 되는 광범위한 교육 도구 모음을 이용할 수 있습니다. 기술자들을 발전시키기 위해 특별히 디자인된 워크샵, 교육 세션 및 리소스가 제공됩니다.\n\n이러한 학습 기회는 다양한 학습 스타일을 고려하고 기술 전문가들과 상호작용할 수 있는 공간을 제공합니다. 맥쿼리 엔지니어는 이러한 포럼에서 얻은 지식을 실제 시나리오에 적용함으로써 지속적으로 기술을 향상시키고 혁신적인 프로젝트에 기여할 수 있습니다.\n\n## 다른 사람들의 경험을 활용하기\n\n<div class=\"content-ad\"></div>\n\n우리 CGM 비즈니스를 지원하는 시스템의 복잡한 코딩 풍경 속에서, 광범위한 코드베이스를 신속하게 이해하는 것은 내 전문 지식을 확장할 수 있는 기회였습니다. 한 번에, 독점 기술을 이해하는 것은 어려운 과제처럼 보였습니다. 그러나, 경험이 풍부한 개발자의 지침을 구하여 학습을 가속화시켰습니다. 팀원들이 제공한 예시는 복잡한 코드를 해석하는 데 도움이 되었을 뿐만 아니라 프로젝트에 의미 있는 기여를 할 수 있도록 했습니다. 우리 팀 내에서 경험을 활용하는 중요성을 강조했습니다.\n\n시니어 엔지니어들과 긴밀히 협업함으로써 실시간 데이터 처리와 복잡한 알고리즘 최적화와 같은 고급 기능들과 관여할 수 있는 기회가 있었습니다. 이 협업은 동시 컴퓨팅과 고장 허용 시스템과 같은 기술 스킬을 가속화시켰습니다. 이는 거래 플랫폼에서 중요한 부분입니다.\n\n우리 독점 기술을 현대화하는 과정에서, 기존 시스템과 통합 중인 최첨단 기술 모두를 배울 수 있는 독특한 기회를 가졌습니다. 내부 교육 아카데미는 엔지니어들이 이해하고 숙련하기 위한 구조적 학습 경로를 제공해줌으로써 매우 중요한 부분을 담당하고 있습니다.\n\n## 지속적인 학습과 발전\n\n<div class=\"content-ad\"></div>\n\n맥쿼리에서는 지속적인 학습에 대한 강조가 우리 문화 속에 자리 잡혀 있습니다. 클라우드 시험과 자격증을 위해 협력적으로 준비하는 스터디 그룹을 통해 우리는 집단 성장 문화를 육성하고 있습니다. 이러한 스터디 세션은 강의 자료를 탐구하고 챕터를 심층적으로 이해하며 어려움을 공동으로 해결할 수 있는 공간을 제공합니다. 이러한 역량 강화는 우리의 전문적 성장을 육성하는 데 중요한 역할을 합니다.\n\n준 엔지니어로서 우리는 최신 산업 지식과 새로운 기술을 경험할 수 있는 다양한 학습 기회를 제공받습니다. 맥쿼리는 주제 전문가들과 소통하고 배운 내용을 실제 상황에 적용할 수 있도록 지원함으로써 우리의 성장을 지원합니다.\n예를 들어, 최근 새 웹 애플리케이션을 구현하는 프로젝트에 참여할 기회를 얻었는데, 이를 통해 기술 발전에 대한 책임감을 느낄 수 있었습니다. 쿠버네티스 컨테이너와 리액트 프레임워크와 직접 작업하면서 프로젝트 전반을 이끌어 갈 수 있는 기회를 얻었습니다. 선임 엔지니어들로부터의 학습 기회와 기술적 시범이 나를 지원하고, 웹 애플리케이션의 GUI 디자인, 프론트엔드 기능과 기능성, 백엔드 데이터 관리, 테스트, 배포 등을 포함한 웹 애플리케이션을 만드는 방법에 대한 소중한 경험을 쌓을 수 있었습니다.\n\n## 결론\n\n맥콰리의 문화는 선임 전문가들이 다음 세대 엔지니어들을 키워내기 위해 시간과 경험을 투자하는 문화입니다. 내장형 학습 관행은 번창하는 엔지니어링 경력을 위한 견고한 기반을 형성합니다. 지식 공유와 지속적인 전문적 발전에 대한 확고한 헌신은 모든 수준의 엔지니어의 성장을 지원합니다.\n\n<div class=\"content-ad\"></div>\n\n학습을 촉진하고 최첨단 프로젝트에 참여할 수 있는 기회를 제공하는 데 헌신하는 태도 때문에 Macquarie가 이렇게 멋진 경력을 쌓을 수 있는 곳이 되었습니다. Macquarie와 같이 다양한 기술 스택을 다루는 것은 그저 흥미로운 기술적 연습 이상의 것입니다. 이는 경력 가능성, 리더십 역할의 기회, 기술 혁신의 최전선에 있는 기회를 제공합니다.\n\n더 알고 싶으세요? Macquarie에서의 경력이 어디로 안내해 줄 수 있는지 알아보세요.","ogImage":{"url":"/assets/img/2024-06-22-Continuousandembeddedlearningasajuniorengineer_0.png"},"coverImage":"/assets/img/2024-06-22-Continuousandembeddedlearningasajuniorengineer_0.png","tag":["Tech"],"readingTime":5},{"title":"Python Async Await  사용해본 후 배운 7가지 교훈","description":"","date":"2024-06-22 02:38","slug":"2024-06-22-PythonAsyncAwait7ThingsILearntAfterDealingWithThemForAWhile","content":"\n\n\n![이미지](/assets/img/2024-06-22-PythonAsyncAwait7ThingsILearntAfterDealingWithThemForAWhile_0.png)\n\n# 1) \"async def\"를 사용하면 비동기 함수를 작성할 수 있습니다\n\n```python\ndef hello():\n    return 'hello'\n\nprint(hello)  # <function hello at 0x100ce8e00>\n```\n\n^ 여기에 일반 함수가 있습니다\n\n\n<div class=\"content-ad\"></div>\n\n```python\nasync def hello():\n    return 'hello'\n\nprint(hello)  # <function hello at 0x102b58e00>\n```\n\n여기에 async def 키워드를 사용하여 생성된 비동기 함수가 있습니다. 출력했을 때에도 여전히 함수 형식으로 출력되는 것을 볼 수 있습니다.\n\n# 2) 비동기 함수 호출은 코루틴을 반환합니다.\n\n```python\ndef hello():\n    return 'hello'\n\nprint(hello())  # hello\n```\n\n<div class=\"content-ad\"></div>\n\n^ 일반 함수 호출 예입니다 - 'hello' 문자열을 반환하는단 뜻이에요\n\n```js\nasync def hello():\n    return 'hello'\n\nprint(hello())  \n\n# <coroutine object hello at 0x10276f320>\n\n# RuntimeWarning: coroutine 'hello' was never awaited\n```\n\n^ 일반 함수처럼 async 함수를 호출할 때 반환 값 대신 코루틴 객체가 반환됩니다.\n\n^ 또한 RuntimeWarning: coroutine 'hello' was never awaited 메시지가 표시됩니다 - 코루틴은 일반적으로 await를 사용하여 대기해야 합니다(잠시 후에 설명하겠습니다)\n\n<div class=\"content-ad\"></div>\n\n# 3) 코루틴의 의미\n\n코루틴은 일시적으로 일시 중단 및 재개될 수있는 특별한 기능인 함수입니다. 다른 작업이 실행 중일 때 일시 중단 및 재개될 수 있는 기능이기도 합니다. 또한 다른 코루틴에게 일시적으로 제어를 양도할 수도 있습니다.\n\n이를 통해 우리는 동시에 하나 이상의 작업을 동시에 실행할 수 있게 됩니다.\n\n# 4) “asyncio.run()”을 사용하여 코루틴을 직접 실행할 수 있습니다\n\n<div class=\"content-ad\"></div>\n\n```js\nasync def hello():\n    return 'hello'\n\nprint(hello())\n\n# <coroutine object hello at 0x10276f320>\n\n# RuntimeWarning: coroutine 'hello' was never awaited\n```\n\n^ 이것이 코루틴을 실행하는 방법이 아닙니다.\n\n```js\nimport asyncio\n\nasync def hello():\n    print('running hello coroutine')\n    return 'hello'\n\nasyncio.run(hello()) # running hello coroutine\n```\n\n^ 이것이 코루틴을 실행하는 방법입니다.\n\n<div class=\"content-ad\"></div>\n\n주의 — asyncio는 파이썬 표준 라이브러리의 일부이므로 Python과 함께 설치되어 있으며이 작동하도록 추가로 제3자 라이브러리를 설치할 필요가 없습니다. asyncio를 가져와서 사용할 수 있습니다.\n\n# 5) 코루틴 실행에 \"await\" 사용하기\n\nhello 코루틴과 main 코루틴이 있다고 가정해 봅시다.\n\n```python\nimport asyncio\n\nasync def hello():\n    print('hello 코루틴 실행 중')\n    return 'hello'\n\nasync def main():\n    x = await hello()\n    print(x)\n\nasyncio.run(main())    \n\n# hello 코루틴 실행 중\n# hello\n```\n\n<div class=\"content-ad\"></div>\n\n^ 다른 코루틴 메인 안에서 hello를 호출하려면 await 키워드를 사용해야 합니다.\n\nawait hello()를 \"hello()가 끝날 때까지 기다렸다가 반환 값을 x에 할당한다\"고 생각할 수 있습니다. 이것이 x를 출력할 때 hello를 얻는 이유입니다.\n\n# 6) \"await\"는 \"async def\"를 사용하여 정의된 함수에서만 사용할 수 있습니다\n\n```python\nimport asyncio\n\nasync def hello():\n    print('hello 코루틴 실행 중')\n    return 'hello'\n\nasync def test():\n    x = await hello()\n    print(x)\n\nasyncio.run(test())\n```\n\n<div class=\"content-ad\"></div>\n\n여기서는 일반 함수 테스트 안에 await를 사용하려고 시도했기 때문에 SyntaxError가 발생합니다.\n\nawait 키워드를 사용하려면 async def를 사용하여 정의된 async 함수 내에 있어야 합니다.\n\nasyncio.gather를 사용하여 둘 이상의 코루틴을 동시에 실행할 수 있습니다.\n\n```python\nimport asyncio\n\nasync def hello():\n    print('시작')\n    await asyncio.sleep(1)\n    print('끝')\n\nasync def main():\n    await asyncio.gather(hello(), hello(), hello())\n\nasyncio.run(main())\n\n# 시작\n# 시작\n# 시작\n# 끝\n# 끝\n# 끝\n```\n\n<div class=\"content-ad\"></div>\n\n- 이 스크립트를 실행할 때, 먼저 3개의 start가 출력됩니다.\n- 약 1초 지연 후, 3개의 end가 출력됩니다.\n\n무슨 일이 일어나고 있을까요?\n\n- asyncio.sleep(1)은 우리의 코루틴을 1초간 재우게 합니다.\n- asyncio.gather는 3개의 hello() 코루틴을 동시에 동시에 실행시킵니다.\n- 이것이 모든 start가 함께 출력되고, 모든 end도 함께 출력되는 이유입니다.\n\n# 만약 제작자로서 저를 지원하고 싶다면\n\n<div class=\"content-ad\"></div>\n\n- 이 이야기에 대해 50번 박수를 쳐주세요\n- 여러분의 생각을 말씀해 주세요\n- 이야기에서 가장 좋았던 부분을 강조해 주세요\n\n감사합니다! 이 작은 행동들이 큰 도움이 되고, 정말 감사드립니다!\n\nYouTube: https://www.youtube.com/@zlliu246\n\nLinkedIn: https://www.linkedin.com/in/zlliu/\n\n<div class=\"content-ad\"></div>\n\n제 Ebooks: [https://zlliu.co/ebooks](https://zlliu.co/ebooks)","ogImage":{"url":"/assets/img/2024-06-22-PythonAsyncAwait7ThingsILearntAfterDealingWithThemForAWhile_0.png"},"coverImage":"/assets/img/2024-06-22-PythonAsyncAwait7ThingsILearntAfterDealingWithThemForAWhile_0.png","tag":["Tech"],"readingTime":4},{"title":"Streamlit으로 Kaggle 같은 플랫폼을 만드는 방법 학생들을 위한 프로젝트 사례","description":"","date":"2024-06-22 02:33","slug":"2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell","content":"\n\n![How I Created a Kaggle-Like Platform for My Students Using Streamlit and How You Can Do It as Well](/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_0.png)\n\n안녕하세요! 저는 Kaggle을 좋아하고 데이터 과학과 머신 러닝을 보급하는 데 그가 한 기여가 매우 갓큼하다고 믿습니다. 비디오 게임과 게임화를 즐기는 사람으로서, Kaggle의 랭킹 및 포인트 시스템이 참가자들이 건강하고 건설적인 경쟁을 통해 모델을 개선하도록 장려하는 방법을 인정합니다. Kaggle은 매우 인기가 높아져 많은 교수들이 기계 학습을 가르치는 데 선호하는 도구 중 하나로 Kaggle을 포함시켜왔다.\n\n그러나 기계 학습 강의를 하는 비즈니스 스쿨의 교수로서, Kaggle을 사용하여 학생들의 최종 기계 학습 프로젝트를 평가하는 도구로 사용하는 것이 조금은 복잡하다는 것을 느꼈습니다. 먼저, 학생들의 제출물을 추적하는 것이 지루하고 수동적이며, 제가 가르치는 학생들(대부분 데이터 과학 및 프로그래밍 초보자임을 유의해 주세요)이 자신들의 노력의 결과가 Kaggle 랭킹의 맨 아래에 배치된 것을 보는 것이 좌절스러울 수 있다고 생각합니다. 이러한 이유로 Kaggle이 가르치는 도구로 설계된 것은 아니라는 점을 인지하는 것이 중요하다고 생각합니다.\n\n항상 제 학생들에게 맞춤화된 Kaggle의 소형 버전을 만들고 싶어했습니다. 이 플랫폼은 Kaggle의 게임화 성공을 반영하고, 수학 프로그래밍 및 조합 최적화를 포함하여 여러 주제의 템플릿으로 서비스를 제공할 수 있게 해줍니다. 처음에는 일반적인 파이썬 웹 개발 프레임워크인 Django나 Flask를 사용하여 이러한 플랫폼을 구축하는 데 필요한 노력에 despondent해졌습니다.\n\n<div class=\"content-ad\"></div>\n\nStreamlit을 최근에 알게 되어서 정말 기뻤어요! Streamlit은 Google Sheets와 상호 작용할 수 있는 능력을 갖추고 있어요. 이 글에서는 Python, Streamlit, 그리고 Google Sheets를 사용하여 Kaggle과 유사한 웹 애플리케이션을 만들어 수업을 게임으로 변화시킬 수 있는 방법을 보여드릴 거에요. 이 앱을 통해 학생들은 개별 계정으로 로그인하고, CSV 파일을 업로드하여 해결책을 제출하고, 다양한 머신 러닝 메트릭을 기반으로 해결책을 평가하고, 제출물들의 순위를 동적으로 확인할 수 있어요. 무엇보다도, 무료로 이 앱을 배포하는 방법도 설명할 거에요.\n\n손을 더럽히며 배우시기 준비되셨나요? 최종 앱 결과물을 한 눈에 확인해 볼까요...\n\n![앱 결과물](https://miro.medium.com/v2/resize:fit:1200/1*SUhDDLi4ozYwvL1hoShNGA.gif)\n\n이 글이 길 수도 있음을 참고해 주세요. 가능한 한 자세하게 설명드리려고 노력하고 있어요. 데이터 과학 전문가일 필요는 없지만 Python에 서툰 교사나 교수님들에게 많은 도움이 될 수 있다고 생각하기 때문이에요. 이미 Python 전문가라면, 이 글은 건너뛰고 아래 프로젝트의 GitHub 저장소로 바로 이동하실 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n내가 학생들과 함께 구현한 원래 프로젝트에서 앱에는 세 가지 다른 기계 학습 섹션이 포함되어 있었습니다: 회귀 문제용 하나, 이진 분류 문제용 하나, 시계열 예측 문제용 하나입니다. 이 간단한 튜토리얼에서는 이 중 하나에 초점을 맞출 것입니다: UC Irvine Machine Learning Repository의 유명한 Pima 당뇨병 데이터셋을 사용한 이진 분류 문제입니다. 이 데이터셋은 Kaggle에서도 다운로드할 수 있습니다.\n\n## 기사 색인:\n\n- Streamlit과 Google Sheets\n- 앱 디자인\n- 앱 구현 및 배포\n- [1] — 프로젝트 환경 설정\n- [2] — Google Sheets 데이터베이스 설정\n- [3] — 데이터 개인 정보 및 보안\n- [4] — Google Sheets 연결 설정\n- [5] — 라이브러리, 상태 세션 변수 및 앱 구성\n- [6] — 로그인 모듈\n- [7] — 결과 제출 모듈\n- [8] — 동적 순위 매기기 모듈\n- [9] — 제출 로그 모듈\n- [10] — 앱 배포\n- 현실 성과\n- 결론\n- 참고 문헌\n\n# Streamlit과 Google Sheets\n\n<div class=\"content-ad\"></div>\n\n2023년 1.28 버전부터 Streamlit은 사용자가 st.connection 메서드를 사용하여 Google Sheets에 연결할 수 있게 되었습니다. 이 방법을 사용하면 Google Sheets를 데이터베이스로 활용하여 CRUD (생성, 읽기, 업데이트, 삭제) 작업을 수행할 수 있습니다. 이 기능에 대해 알게 된 것은 Sven | Coding Is Fun이 만든 YouTube 비디오에서 알게 되었습니다. 시청하고 싶다면 아래 링크를 남겨 놓겠습니다.\n\n당신이 생각하는 것을 알겠어요. 하지만 겁먹지 마세요. Excel (Google Sheets)이 데이터베이스가 아님을 잘 알고 있습니다. 그리고 동의합니다. 회사가 데이터베이스로 사용하는 것에 대해 악몽을 꾸기도 합니다. 그러나 우리가 만들고자 하는 앱을 위해서는 충분히 좋습니다. 필요한 모든 작업을 수행할 수 있으며 온라인에서 사용할 수 있으며, 개인 정보를 보호해줍니다 (우리와 앱만 액세스할 수 있음 — 제 구글 계정을 해킹할 만한 능력이 있는 사람이 아니라면), 그리고 가장 중요한 것은 무료입니다. 이 부분을 개선할 수 있는 여지가 있다는 것을 인지하고 있으며, Google Sheets를 Supabase와 연결하는 가능성을 탐색하고 있습니다.\n\n![image](/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_1.png)\n\n구현에 바로 들어가기 전에, 앱이 반드시 갖춰야 할 다른 모듈들과 구현 전략을 신중하게 검토하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n# 앱 디자인\n\n저희가 만들고자 하는 앱은 여러 과정을 필요로 합니다. 먼저, 오직 우리 학생들만이 액세스할 수 있도록 보장하기 위해 로그인 시스템이 필요합니다. 사용자가 로그인하면 결과를 제출할 모듈이 필요합니다. 이에는 .csv 파일을 업로드하는 과정, 파일이 예상되는 행 수와 요청된 열과 일치하는지 확인하는 과정, 모델 예측과 실제 테스트 데이터 간의 평가 지표를 계산하는 과정이 필요합니다. 이후 학생은 강의실 순위와 결과를 동료들과 비교할 수 있는 기능이 있어야 합니다. 마지막으로 학생은 그룹 프로젝트이기 때문에 모든 제출물과 팀원들의 제출물을 볼 수 있어야 합니다. Figure 3은 앱의 사용자 플로우 다이어그램의 전반적인 개요를 보여줍니다.\n\n<img src=\"/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_2.png\" />\n\n# 앱 구현\n\n<div class=\"content-ad\"></div>\n\n이 앱에는 Visual Studio Code를 사용할 것입니다. 귀하는 귀하의 기기에 새 프로젝트 폴더를 만들고 해당 폴더에서 Visual Studio Code를 열 것을 강력히 권장합니다. 제 경우에는 폴더 이름을 project_app_medium으로 지정하기로 결정했습니다.\n\n## 프로젝트 환경 설정\n\n다른 Python 프로젝트와의 의존성 충돌을 피하기 위해 각 Streamlit 앱을 위한 가상 환경을 생성하는 것을 강력히 권장합니다. 가상 환경을 생성하고 활성화한 후 아래 라이브러리를 설치해야 합니다.\n\n```js\npandas == 1.5.3\nnumpy == 1.26.4\nmatplotlib\nstreamlit\nstreamlit_option_menu\nstreamlit-extras\nst-gsheets-connection\nscikit-learn\n```\n\n<div class=\"content-ad\"></div>\n\n라이브러리를 설치하려면 새 텍스트 파일을 만들고 requirements.txt라는 이름을 지정하십시오. 이 비어 있는 텍스트 파일 안에 위에 나열된 라이브러리들을 복사하여 저장하세요; 우리 앱을 배포할 때 이 파일이 필요합니다. 그런 다음 터미널에 다음 명령어를 입력하세요.\n\n```js\npip install -r requirements.txt\n```\n\n이 명령은 requirements.txt 파일에 나열된 모든 라이브러리를 설치합니다. 우리가 사용하는 라이브러리에 대해 설명하자면, 모든 데이터 과학 프로젝트의 \"미레푸아\"로 시작하는 것이 좋습니다: numpy, pandas 및 matplotlib이 있습니다. 또한 streamlit이 필요합니다, 이는 우리의 프레임워크를 포함하는 기본 라이브러리입니다. Streamlit의 기능을 확장하기 위해 streamlit_option_menu와 같은 커뮤니티에서 개발된 확장들을 가져올 것입니다. 이는 간단한 사이드바 메뉴를 만들 수 있는 streamlit_option_menu와 맞춤화 기능을 많이 포함하는 streamlit-extras가 있습니다. 추가로 st-gsheets-connection를 사용하여 Google Sheets와 연결하는 데 도움을 줄 것입니다. 위에 나열된 라이브러리들 외에도 데이터 보안 및 보호를 위해 hashlib를 사용할 것입니다. 데이터베이스의 세부 정보를 정의할 때 더 자세히 이야기하겠습니다.\n\n이 튜토리얼 동안 다음 폴더 구조가 사용되며 다음 구성 요소가 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n- .streamlit: 이 폴더에 Streamlit 관련 설정이 저장됩니다. 이 폴더 안에는 Google Sheets API와 연결하기 위해 필요한 인증 정보가 포함된 secrets.toml 파일이 저장됩니다.\n- app.py: 메인 Streamlit 스크립트입니다.\n- .gitignore: Git에서 무시할 파일들이름대로, 프로젝트 커밋 시 무시됩니다.\n- logo.png (선택 사항): 회사 로고가 있는 이미지로, 앱의 사이드바 메뉴 상단에 표시됩니다. 완전히 선택 사항이며, 저는 제 회사 SAVILA GAMES 로고를 표시하고 있습니다.\n- requirements.txt: 앱을 실행하는 데 필요한 Python 종속성입니다.\n- README.md: 프로젝트 설명\n\n\nproject_app_medium/\n│\n├── .streamlit/                # 일반 Streamlit 설정\n│   └── secrets.toml           # Google Sheets 연결에 필요한 인증 정보\n├── app.py                     # 앱 코드\n├── .gitignore\n├── logo.png               \n├── requirements.txt           # Python 종속성\n└── README.md \n\n\n## Google Sheets 데이터베이스 설정\n\n환경이 설정되었으니, Google Sheets 데이터베이스 구조를 만들어야 합니다. Google Sheets 앱을 열고 새 파일을 만들어주세요. 저는 프로젝트 데이터베이스라고 이름 지었습니다. 그리고 파일에 네 개의 탭을 만드세요. 첫 번째로 \"users\" 탭은 모든 사용자 로그인 자격 증명과 사용자 그룹 구성 정보를 포함할 것입니다. 이 탭의 정보를 사용하여 애플리케이션의 로그인 모듈을 만들 것입니다. 탭은 다음과 같은 열 구조여야 합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n| email                  | name    | last name | password  | group    |\n|------------------------|---------|-----------|-----------|----------|\n| john.doe@example.com   | John    | Doe       | Pass1234  | G1       |\n| jane.smith@example.com | Jane    | Smith     | SecurePwd | G2       |\n| alex.jones@example.com | Alex    | Jones     | MyPass789 | G1       |\n| emma.brown@example.com | Emma    | Brown     | Emma12345 | G3       |\n```\n\n다음 탭은 “log” 탭입니다. 이 탭은 사용자가 제출한 작업의 과거 정보를 저장할 것입니다. 또한 랭킹 및 이력 제출 모듈의 로직을 위해서도 사용될 것입니다. 이 탭은 다음과 같은 열 구조를 가져야 합니다:\n\n```js\n| user                  | group     | time             | score |\n|-----------------------|-----------|------------------|-------|\n| john                  | G1        | 2024-06-17 10:00 | 0.85  |\n| jane                  | G2        | 2024-06-17 11:00 | 0.72  |\n| alex                  | G1        | 2024-06-17 12:00 | 0.90  |\n| emma                  | G3        | 2024-06-17 13:00 | 0.65  |\n```\n\n다음 탭은 “test_data” 탭입니다. 이 탭은 학생들이 제출한 출력물의 품질을 평가하는 데 사용되는 실제 테스트 y 데이터를 포함할 것입니다. 이 튜토리얼에서는 Pima 데이터세트를 분할하고 마지막 78행을 테스트 데이터 세트로 선택할 것입니다. 이 탭은 이진 결과 데이터만 포함하며 다음과 같은 열 구조를 갖게 될 것입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n| y          |\n|------------|\n| 0          |\n| 1          |\n| 1          |\n| 0          |\n```\n\n마지막으로 만들 표는 'configuration' 탭입니다. 이 탭에는 프로젝트에 대한 사용자 정의 가능한 매개변수가 포함될 것입니다. 이러한 매개변수에는 마감일과 팀 당 하루에 허용된 시도 횟수가 포함될 수 있습니다 (참고로, Kaggle은 팀 당 하루에 다섯 번의 시도를 허용합니다). 이 탭을 통해 프로젝트 특성을 동적으로 변경하여 서로 다른 학기에 쉽게 적용할 수 있습니다. \"configuration\" 탭은 하나의 행만 있고 다음과 같은 열 구조를 가져야 합니다:\n\n```js\n| deadline              | max_per_day     |\n|-----------------------|-----------------|\n| 2024-07-01 23:59      | 5               |\n```\n\n이 튜토리얼 프로젝트에서 사용된 구글 시트의 예제를 다음 링크를 클릭하여 다운로드하고 확인할 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n## 데이터 개인 정보 보호 및 보안\n\n이것은 우리가 통제하고 우리 학생들만 이용할 수 있는 작은 프로젝트입니다. 그러나 학생들의 개인 정보에 대한 데이터 보안에 대해 주의를 기울여야 합니다. 만약 어떤 이유로인지 데이터베이스가 유출되어 사기꾼이나 악의적인 주체들의 손에 넘어간다면 상황은 어떨까요? 우리 학생들의 이름, 이메일 및 비밀번호에 접근할 수 있다면 그들을 위험에 빠트릴 수 있습니다. 그러므로, 이러한 상황이 발생해도 데이터가 이 악의적인 주체들에게는 의미가 없도록 보장해야 합니다.\n\n최소 비용으로 이러한 시스템을 구현할 수 있는지 궁금해 할 수 있습니다. 이것이 해싱과 hashlib 라이브러리가 구원해줍니다.\n\n해싱은 입력 데이터를 수학적 알고리즘을 사용하여 일정 크기의 문자열, 일반적으로 해시 코드로 변환하는 과정입니다. 데이터 무결성을 보장하고 빠른 데이터 검색을 용이하게 하며 민감한 정보를 안전하게 보관합니다. 사용 가능한 해싱 알고리즘은 무엇이 있을까요? 다행히 파이썬에는 hashlib를 포함해 여러 해싱 알고리즘을 제공하는 라이브러리가 함께 제공됩니다.\n\n<div class=\"content-ad\"></div>\n\n- MD5 (md5)\n- SHA-1 (sha1)\n- SHA-224 (sha224)\n- SHA-256 (sha256)\n- SHA-384 (sha384)\n- SHA-512 (sha512)\n- SHA-3 family (sha3_224, sha3_256, sha3_384, sha3_512)\n- BLAKE2 family (blake2b, blake2s)\n\n저희 튜토리얼에서는 hashlib에서 제공하는 사용 가능한 해싱 알고리즘 중 하나인 SHA-256을 사용하여 학생들의 이메일과 비밀번호를 해시 코드로 변환할 것입니다. 이 해시 코드는 데이터베이스에 저장될 것입니다. 이렇게 함으로써, 데이터베이스가 누출되더라도 그들의 데이터는 보호될 것입니다. 해싱의 장점은 해싱된 코드를 브루트 포스를 통해 원래 정보로 역으로 변환하는 것이 사실적으로 매우 어렵다는 사실에 있습니다. 이메일이 SHA-256을 사용하여 해싱된 경우, 이전 섹션의 이메일은 안전하고 해독 불가능한 것으로 됩니다.\n\n```js\n# 원본 이메일\n\n['john.doe@example.com',\n 'jane.smith@example.com',\n 'alex.jones@example.com',\n 'emma.brown@example.com']\n```\n\n```js\n# 해싱된 이메일\n\n['836f82db99121b3481011f16b49dfa5fbc714a0d1b1b9f784a1ebbbf5b39577f',\n 'f2d1f1c853fd1f4be1eb5060eaae93066c877d069473795e31db5e70c4880859',\n '134318bc6349ad35d7e6b95123898eecdd437ad9b0c49cc4bdd66a811afc6909',\n 'd41d9b2f5671358bc6faf79b7435b4a9805a72d012f06d4804815328f39aed1e']\n```\n\n<div class=\"content-ad\"></div>\n\n인식하기가 꽤 어려운 게 아닌가요? 아래에서 데이터프레임과 열이 주어지면 지정된 열의 해시된 항목을 반환하는 함수를 찾을 수 있습니다. 이렇게 하면 데이터베이스 정보를 해싱하고 이 값을 온라인 구글 시트 데이터베이스에 저장할 수 있습니다.\n\n```js\nimport hashlib\n\ndef hashit(df, column):\n  return_list = []\n  for data in df[column].tolist():\n    hash_object = hashlib.sha256()\n    hash_object.update(data.encode())\n    return_list.append(hash_object.hexdigest())\n\n  return return_list\n```\n\n일반적으로 생각할 수 있겠지만, 만약 유저들이 사용자 이름과 비밀번호를 알고 있다면 학생 중 한 명으로 사칭할 수 있을 거라고 생각할 수 있습니다. 그러나 이 상황은 이미 고려되었습니다. 학생들은 우리가 제공한 이메일 주소와 비밀번호로 앱에 로그인하지만, 실제 이메일과 암호를 로그인 화면에 입력합니다. 그런 다음 앱은 그들의 로그인 자겁을 가져와 SHA-256을 사용하여 이를 해싱한 후 해싱된 결과를 데이터베이스와 대조합니다.\n\n따라서 데이터베이스가 유출되고 누군가 정보를 사용하여 로그인을 시도하더라도, 그것이 작동하지 않을 것입니다. 왜냐하면 그들의 입력이 다시 해싱되어 저장된 해시와 일치하지 않기 때문입니다. 아래의 코드와 출력 예시를 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n```js\n비밀번호 = \"password\"\n해시_객체.update(비밀번호.encode())\n해싱된_비밀번호 = 해시_객체.hexdigest()\n\nprint(해싱된_비밀번호)\n```\n\n출력 결과:\n\n```js\n\"5377a16433598554e4a73a61195dbddea9d9956a22df04c3127c698b0dcdee48\"\n```\n\n이제 이미 해싱된 비밀번호를 다시 해싱하면 아래 코드와 같이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nhash_object.update(hash_password.encode())\ndouble_hash_password = hash_object.hexdigest()\nprint(double_hash_password)\n```\n\n다음과 같은 결과를 얻습니다:\n\n```js\n\"dfd4bb46c954f3802c7c2385b1a6b625b3cf0b4ce6adf59d3eec711c293994bb\"\n```\n\n이 두 암호가 일치하지 않는 것을 쉽게 확인할 수 있습니다. 이전에 해싱된 비밀번호를 다시 해싱하면 완전히 새로운 결과가 생성되는 것을 보실 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## Google Sheets 연결 설정하기\n\n연결을 설정하는 데 필요한 모든 지침은 st-gsheets-connection 패키지의 GitHub 리포지토리에서 찾을 수 있습니다. 함께 지침을 따라봅시다:\n\n- Google 개발자 콘솔로 이동하여 새 프로젝트를 만듭니다. Google Cloud 아이콘 바로 옆에는 드롭다운 메뉴가 있습니다. 클릭한 다음 \"새 프로젝트 만들기\"를 클릭합니다. 프로젝트 이름은 자유롭게 지정할 수 있습니다; 저의 경우에는 project_app_medium로 지정하겠습니다.\n\n![Alt text](https://miro.medium.com/v2/resize:fit:1200/1*cB8ePsTHYUxlcVs23UGd4w.gif)\n\n<div class=\"content-ad\"></div>\n\n- 이제 선택한 프로젝트로 두 가지 다른 API를 활성화해야 합니다: Google 드라이브 및 Google 시트. 페이지 상단의 검색 창에 \"Google 드라이브\"를 입력하고 API를 선택한 다음 \"활성화\"를 클릭하세요. Google 시트에 대해서도 동일한 단계를 반복하세요.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*qzHDsIgXl7UYbFpztJuBmg.gif)\n\n- 프로젝트 API가 활성화된 상태에서 이제 이에 접근할 수 있는 기술 사용자를 생성해야 합니다. \"자격 증명\"을 클릭한 후 \"자격 증명 생성\"을 클릭하고 \"서비스 계정\" 옵션을 선택하세요. 기술 사용자에게 이름을 할당하세요. 저의 경우에는 \"medium-project-google-sheets\"로 이름을 지었습니다. 기술 사용자에게 \"편집자\" 역할을 할당하고 마지막으로 \"완료\"를 클릭하세요.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*q-X587bUfw893wKpAygU9w.gif)\n\n<div class=\"content-ad\"></div>\n\n- 기술 사용자가 생성되었으므로, 이 사용자의 자격 증명을 생성해야 합니다. 방금 만든 사용자를 클릭한 후 \"Keys\"를 클릭하고, \"Add Key\"를 클릭한 다음 \"Create New Key\"를 선택하십시오. JSON 옵션을 선택하고 \"Done\"을 클릭하세요. 그러면 앱에서 Google Sheets를 사용하는 데 필요한 모든 자격 증명이 포함된 JSON 파일이 자동으로 다운로드됩니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*-364Zn18VbaeLdU0uXMvaA.gif)\n\n- 마지막 단계는 방금 다운로드한 자격 증명을 secrets.toml 파일에 저장하는 것입니다. 아직 수행하지 않았다면 프로젝트 폴더 안에 .streamlit이라는 새 폴더를 만드세요. 이 폴더 안에 secrets.toml이라는 새 파일을 생성하세요. 선택한 텍스트 편집기(예: VS Code)로 파일을 열고 아래 정보를 붙여넣으세요.\n\n```js\n# .streamlit/secrets.toml\n\n[connections.gsheets]\nspreadsheet = \"<스프레드시트 이름 또는 URL>\"\nworksheet = \"<워크시트 GID 또는 폴더 ID>\" # 워크시트 GID는 공개 스프레드시트 URL을 사용할 때 사용되며, service_account를 사용할 때에는 폴더 ID로 선택됩니다.\ntype = \"\" # 공개 스프레드시트 URL을 사용할 때는 비워두세요. service_account를 사용할 때 -> type = \"service_account\"\nproject_id = \"\"\nprivate_key_id = \"\"\nprivate_key = \"\"\nclient_email = \"\"\nclient_id = \"\"\nauth_uri = \"\"\ntoken_uri = \"\"\nauth_provider_x509_cert_url = \"\"\nclient_x509_cert_url = \"\"\n```\n\n<div class=\"content-ad\"></div>\n\n- secrets.toml 파일의 각 요소를 Google에서 다운로드한 JSON 자격 증명 파일의 데이터로 대체하세요. \"spreadsheet\" 필드에는 프로젝트용으로 만든 Google 스프레드 시트 데이터베이스의 URL을 복사하세요. 그다음 JSON 파일에서 \"client_email\" 데이터를 복사하고 Google 스프레드 시트 데이터베이스로 이동하세요. 스프레드 시트에서 \"공유\"를 클릭한 다음 \"client_email\"을 텍스트 입력란에 붙여넣기하고, \"편집자\" 권한이 선택되어 있는지 확인한 후 \"전송\"을 클릭하세요.\n\n이 모든 준비가 끝나면 이제 앱을 코딩할 준비가 되었습니다.\n\n## 라이브러리, 상태 세션 변수 및 앱 구성\n\n이제 앱에 필요한 라이브러리를 가져와 앱이 사용할 세션 상태 변수를 만들 것입니다. 대부분의 세션 상태 변수는 로그인 모듈과 관련이 있을 것입니다. Streamlit에서 세션 상태 변수는 세션 내의 다른 상호 작용 사이에서 정보를 저장합니다. 이 정보는 사용자 입력 또는 선택과 같은 상태를 유지하여 앱을 다시 실행할 때마다 유지하는 데 도움이 됩니다. 처음에는 이러한 변수를 빈 문자열로 설정하고 앱이 실행됨에 따라 업데이트될 것입니다. 우리의 특정 앱에서는 사용자 이름(학생 이메일을 사용), 비밀번호 및 사용자가 속한 그룹을 위한 상태 변수를 만들 것입니다. 또한 st.set_page_config() 메서드를 사용하여 페이지 제목과 페이지 아이콘(favicon)을 설정할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nfrom pathlib import Path\nimport streamlit as st\nfrom streamlit_option_menu import option_menu\nfrom streamlit_extras.add_vertical_space import add_vertical_space\nfrom streamlit_extras.stylable_container import stylable_container \nfrom streamlit_gsheets import GSheetsConnection\nfrom sklearn import metrics\nimport hashlib\n\nif 'user_name' not in st.session_state:\n    st.session_state['user_name'] = ''\n\nif 'student_name' not in st.session_state:\n    st.session_state['student_name'] = ''\n\nif 'password' not in st.session_state:\n    st.session_state['password'] = ''\n\nif 'group' not in st.session_state:\n    st.session_state['group'] = ''\n\nst.set_page_config(\n        page_title='Medium Project', \n        page_icon='📈' \n    )\n```\n\n작업이 정상적으로 진행되고 있는지 테스트하려면 다음 명령어를 터미널에서 실행하여 디렉토리 루트에서 애플리케이션을 실행할 수 있습니다. 이렇게 하면 로컬호스트에서 앱이 실행됩니다.\n\n```js\nstreamlit run app.py\n```\n\n\"Medium Project\"라는 빈 페이지와 선택한 파비콘 📈이 표시된 Figure 8이 나타납니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_3.png\" />\n\n## 로그인 모듈\n\n이제 세션 상태 변수가 생성되었으니, 앱의 첫 번째 모듈인 로그인 모듈을 코드로 작성할 수 있습니다. 이 모듈은 Figure 9에 설명된 논리를 포함할 것입니다.\n\n<img src=\"/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n먼저, Google Sheets 데이터베이스와 연결을 설정할 것입니다. 그다음, st.sidebar 방법을 사용하여 옵션_메뉴 방법과 결합하여 사이드바 내비게이션 메뉴를 만들어 앱의 다른 페이지를 쉽게 생성할 수 있게 할 것입니다. 이 설정이 완료되면 로그인 모듈의 로직을 구성할 것입니다.\n\n사용자가 로그인되어 있지 않은 경우, 다음 모듈에 액세스하는 것을 방지할 것입니다. 로그인 모듈에서 사용자 이름(이메일)과 비밀번호를 요청할 것입니다. 이 자격 증명은 데이터베이스와 일치하는지 확인됩니다. 일치하는 경우, 사용자 자격 증명은 세션 상태 변수에 저장되고 사용자는 성공적인 로그인이 확인된 메시지를 받게 됩니다. 일치하지 않는 경우 사용자는 로그인에 실패했다는 경고 메시지를 받게 됩니다. 또한 데이터베이스로부터 프로젝트 구성 데이터(프로젝트 마감일 및 하루 최대 제출 횟수)를 저장할 것입니다. 위의 논리는 아래 코드를 따라 구현됩니다.\n\n```js\n테이블 태그를 마크다운 형식으로 변경하실 수 있습니다.'''\n## 제출 결과 모듈\n```\n\n<div class=\"content-ad\"></div>\n\n지금은 로그인 모듈이 준비되어 있으므로, 제출 모듈을 만들어 나갈 수 있습니다. 이 모듈에서는 학생들이 모델의 예측 결과를 담은 CSV 파일을 업로드할 수 있습니다. 이 모듈에는 Figure 10에 설명된 로직이 포함될 것입니다.\n\n![Figure 10](/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_5.png)\n\n프로젝트 마감 기한이 지나지 않았고 해당 팀이 하루 최대 제출 횟수를 초과하지 않은 경우에만 학생들이 이 모듈에 접근할 수 있습니다 (앱 스팸 방지 및 테스트 데이터셋에 과적합되는 위험을 피하기 위함). 이 중 하나라도 만족하지 않는 경우, 학생은 해당 문제를 안내하는 경고를 받게 됩니다. 모든 조건이 충족된다면, 학생은 CSV 파일을 업로드하여 결과물을 제출할 수 있습니다. 파일의 형태가 요구 사항(테스트 데이터셋 크기와 동일한 행의 수, 그리고 \"predictions\"이라는 열이 포함)와 일치하는 경우 제출이 승인될 것입니다. 형태가 일치하지 않는 경우, 학생은 문제에 대한 자세한 내용을 담은 경고 메시지를 받게 됩니다.\n\n모든 것이 일치하는 경우, 모듈은 모델 평가 지표를 계산할 것입니다. 이 특정 케이스에서는 정확도 점수를 사용하고 있지만 원하는 경우 F1 점수를 사용할 수도 있습니다. 코드를 쉽게 수정할 수 있습니다. 이 작업을 마친 후, 모듈은 학생의 제출을 구글 시트 데이터베이스의 \"log\" 탭에 저장할 것입니다. 위의 로직은 아래의 코드를 따라 구현될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nif selected == '결과 제출':\n\n  st.markdown(\"\"\"\n      <style>\n      div[data-testid=\"stMetric\"] {\n          background-color: #EEEEEE;\n          border: 2px solid #CCCCCC;\n          padding: 5% 5% 5% 10%;\n          border-radius: 5px;\n          overflow-wrap: break-word;\n      }\n      </style>\n      \"\"\"\n      , unsafe_allow_html=True)\n\n  st.header('예측 결과 제출')\n  st.subheader(\"머신러닝 분류\")\n  st.divider()\n  st.subheader(f\" 남은 시간: {days} 일, {hours} 시간 및 {minutes} 분\")\n  st.divider()\n\n  if st.session_state['user_name'] == '':\n      st.warning('프로젝트 솔루션을 제출하려면 로그인하세요')\n  else:\n          \n      group_log_df = conn.read(worksheet=\"log\", usecols=list(range(log_df_n_cols)), ttl=1).dropna(how=\"all\")\n      group_log_df = group_log_df[group_log_df['group'] == st.session_state['group']]\n      group_log_df['time'] = pd.to_datetime(group_log_df['time'])\n\n      test_data = conn.read(worksheet=\"test_data\", usecols=list(range(1)), ttl=30).dropna(how=\"all\")\n      test_data_y = test_data['y']\n\n      n_test = len(test_data)\n\n      current_date = pd.Timestamp.today()\n\n      submissions_count = group_log_df[(group_log_df['time'].dt.date == current_date.date())].shape[0]\n\n      time_diff = deadline - current_date\n      time_diff = time_diff.dt.total_seconds().iloc[0]\n\n\n      if time_diff <= 0:\n          st.warning('죄송합니다. 이미 마감된 프로젝트 기한으로 더 이상 제출할 수 없습니다')\n      else:\n          if submissions_count >= max_per_day:\n              st.warning(f'죄송합니다. 팀이 하루에 {submissions_count}번 이미 제출하여 하루 제출 한계를 초과했습니다')\n          else:\n\n              user_file = st.file_uploader(\"예측 파일을 업로드하세요\",type=['csv'])\n              st.caption(f\"당신의 파일은 'predictions'라는 적어도 하나의 열과 {n_test}개의 행이 필요합니다\")\n\n              if user_file is not None:\n\n                  submit_pred = st.button('제출',type=\"primary\",key=\"submit_pred\")\n\n                  if submit_pred:\n\n                      pred_df = pd.read_csv(user_file) \n\n                      if 'predictions' not in pred_df.columns.to_list():\n                          st.error('죄송합니다. 파일에 \"predictions\" 열이 없습니다', icon=\"🚨\")\n                      elif len(pred_df) != n_test:\n                          st.error(f'죄송합니다. 파일의 행 수({len(pred_df)})가 예상 길이({n_test})와 일치하지 않습니다', icon=\"🚨\")\n                      else:\n                          with st.spinner('소금 구륗기 해결책 데이터베이스에 업로드 중'):\n                              user_predictions = pred_df['predictions']\n\n                              timestamp = datetime.datetime.now()\n                              timestamp = timestamp.strftime(\"%d/%m/%Y, %H:%M:%S\")\n                              st.write(f'제출일: {timestamp}')                \n                      \n                              ACC = metrics.accuracy_score(test_data_y,user_predictions)\n\n                              F1 = metrics.f1_score(test_data_y,user_predictions)\n\n                              cm = pd.DataFrame(metrics.confusion_matrix(test_data_y,user_predictions),\n                                              columns = [\"T 예측\",\"F 예측\"],index=[\"T 실제\",\"F 실제\"])\n\n                              columns_part_2 = st.columns(3)\n\n                              with columns_part_2[0]:\n                                  st.metric(\"정확도\",f\"{100*ACC:.1f} %\")\n                              with columns_part_2[1]:\n                                  st.metric(\"F1-점수\",f'{F1:.3f}')\n                              \n                              with columns_part_2[2]:\n                                  st.dataframe(cm,use_container_width=True)\n\n                              solution_dict = dict()\n                              solution_dict['user'] = st.session_state['student_name']\n                              solution_dict['group'] = st.session_state['group']\n                              solution_dict['time'] = timestamp\n                              solution_dict['score'] = ACC\n\n                              logs_df_2 = conn.read(worksheet=\"log\", usecols=list(range(log_df_n_cols)), ttl=1).dropna(how=\"all\")\n                              solution_2 = pd.DataFrame([solution_dict])\n                              updated_log_2 = pd.concat([logs_df_2,solution_2],ignore_index=True)\n                              conn.update(worksheet=\"log\",data = updated_log_2)\n                              st.success(f'당신의 솔루션이 {timestamp}에 업로드되었습니다',icon=\"✅\")\n                              st.balloons()\r\n```\n\n## 동적 랭킹 모듈\n\n로그인한 사용자가 이미 솔루션을 제출할 수 있는 앱을 보유하고 있습니다. 이제 앱의 게임화 요소를 추가하여 학생들이 다른 팀이 제출한 솔루션과 어떻게 비교되는지 보여주는 동적 랭킹을 만들어야 합니다. 이 모듈의 논리는 간단하며 다이어그램이 필요하지 않습니다. 기본적으로 구글 시트 데이터베이스의 \"log\" 탭에서 모든 데이터를 수집하고, 각 팀의 최상의 점수를 찾아내어 이 점수를 테이블로 제공해야 합니다. 최고 점수를 가진 팀이 최상위 위치에 있고, 그래로 이어지며 동일한 점수를 가진 두 개 이상의 팀이 있는 경우, 앱은 더 빨리 솔루션을 제출한 팀에 대해 더 높은 순위를 부여할 것입니다. 위의 논리는 아래 코드를 따라 구현될 것입니다.\n\n```js\nif selected == \"순위\":\n    st.header('순위')\n    \n    if st.session_state['user_name'] == '':\n        st.warning('랭킹을 확인하려면 로그인하세요')\n    else:\n        st.write('아래 테이블에 프로젝트의 순위가 표시됩니다')\n\n        rank_df = conn.read(worksheet=\"log\", usecols=list(range(log_df_n_cols)), ttl=1).dropna(how=\"all\")\n        GROUPS = list(rank_df['group'].unique())\n        default_time = pd.to_datetime('01/01/1901, 00:00:00')\n\n        st.header(\"머신러닝 분류 부분\")\n        st.divider()\n\n        ranking_list_2 = []\n        for gr in GROUPS:\n\n            mini_df_2 = rank_df[rank_df['group'] == gr]\n            if len(mini_df_2) == 0:\n                row = {'group':gr,'정확도':0,'time':default_time}\n                ranking_list_2.append(row)\n                continue\n            else:\n                best_idx_2 = np.argmax(mini_df_2['score'])\n                best_value_2 = mini_df_2.iat[best_idx_2,-1]\n                best_time_2 = pd.to_datetime(mini_df_2.iat[best_idx_2,2])\n                row = {'group':gr,'정확도':best_value_2,'time':best_time_2}\n                ranking_list_2.append(row)\n        ranking_df_2 = pd.DataFrame(ranking_list_2).sort_values(by = ['정확도','time'],ascending=[False, True])\n        ranking_df_2 = ranking_df_2.reset_index(drop=True)\n        ranking_df_2.iat[0,0] = ranking_df_2.iat[0,0] + \"   🥇\"\n        ranking_df_2.iat[1,0] = ranking_df_2.iat[1,0] + \"   🥈\"\n        ranking_df_2.iat[2,0] = ranking_df_2.iat[2,0] + \"   🥉\"\n        st.dataframe(ranking_df_2,use_container_width=True,hide_index=True)\r\n```\n\n<div class=\"content-ad\"></div>\n\n## 제출 로그 모듈\n\n마지막에 구현할 모듈은 제출 로그 모듈입니다. 이 모듈을 통해 각 학생은 프로젝트 기간 동안 자신과 팀원이 제출한 모든 제출물에 대한 이력 로그에 액세스할 수 있습니다. 이 모듈의 논리는 간단하며 다이어그램이 필요하지 않습니다. 우리는 Google Sheets 데이터베이스의 \"log\" 탭에서 모든 데이터를 수집하고, 현재 사용자 그룹에 대해 필터링한 후 정보를 테이블 형식으로 제시해야 합니다. 위의 논리는 아래 코드를 따라 구현될 것입니다.\n\n```js\nif selected == '내 그룹 제출물':\n    st.header('내 그룹 제출물')\n    \n    if st.session_state['user_name'] == '':\n        st.warning('제출 이력을 확인하려면 로그인해주세요')\n    else:\n        st.write(f'아래 테이블은 당신의 그룹인 **{st.session_state[\"group\"]}**의 제출 이력을 보여줍니다.')\n        group_log_df = conn.read(worksheet=\"log\", usecols=list(range(log_df_n_cols)), ttl=1).dropna(how=\"all\")\n        group_log_df = group_log_df[group_log_df['group'] == st.session_state['group']]\n        group_log_df = group_log_df[['user','time','score']]\n        \n       \n        st.subheader('제출 이력:')\n        st.dataframe(group_log_df,use_container_width=True,hide_index=True)    \n```\n\n마지막 모듈을 코드화하면 앱이 완료됩니다. 전체 코드는 아래 링크에서 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 앱 배포\n\n지금까지 우리 앱은 로컬에서 매끄럽게 실행되었습니다. 그러나 이를 구축한 주요 목적은 여러분의 학생들과 그들의 최종 프로젝트에 사용하기 위함입니다. 이제 배포가 중요해집니다. 이 프로젝트에서는 Streamlit Community Cloud를 사용하여 앱을 배포하기로 결정했습니다. 무료이며 쉽게 사용할 수 있습니다. 이 서비스는 GitHub을 사용하여 앱을 배포하는데, 유일한 단점은 GitHub 리포지토리가 공개되어야 한다는 것입니다. 따라서 Google에서 다운로드한 기술 사용자 자격 증명과 같은 민감한 정보를 업로드하지 않도록 주의해야 합니다. 이 정보는 Streamlit Community Cloud 서비스 내에서 직접 관리될 것이므로 걱정하지 마세요. 다른 배포 옵션을 살펴보고 싶다면 Damian Boh가 작성한 훌륭한 아래 기사를 읽어보기를 권장합니다.\n\n우리 앱을 배포하려면 다음 단계를 따라주세요:\n\n- 새로운 공개 GitHub 리포지토리를 만듭니다. README.md 파일은 리포를 만들 때 직접 생성할 수 있습니다.\n- 아래 파일을 업로드하거나 커밋하되, 반드시 secrets.toml 파일은 업로드하지 않습니다:\n\n\n<div class=\"content-ad\"></div>\n\n- app.py\n- requirements.txt\n- logo.png (옵션, 대학 로고 또는 회사 로고로 앱을 사용자 정의하고 싶을 때)\n\n당신의 저장소는 Figure 11과 유사해야 합니다.\n\n![Figure 11](/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_6.png)\n\n3. Streamlit Community Cloud 사이트 https://streamlit.io/cloud 에 가서 로그인합니다. 계정이 없다면 계정을 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n4. 한 번 사인 인했으면 사이트 오른쪽 상단에 있는 \"앱 생성\" 버튼을 클릭하고, 그런 다음 GitHub에서 앱 코드를 가져올 옵션을 선택합니다.\n\n5. 앱 파일을 포함하는 레포의 이름을 입력하고, 기본 브랜치를 선택하고, Python 파일의 이름을 입력하세요 (우리의 경우 app.py). 또한 앱 URL 이름을 사용자 정의할 수 있습니다; medium-kaggle-like-app을 선택했습니다. 폼은 Figure 12와 비슷해야 합니다.\n\n![Figure 12](/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_7.png)\n\n6. \"배포\" 버튼을 클릭하세요. 몇 분 정도 걸릴 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n7. 앱을 배포하면 즉시 Streamlit에서 오류 메시지를 받게 됩니다. 그림 13에서 보는 것처럼요. 이것은 완전히 정상적인 현상입니다. 배포된 앱은 기술 사용자 자격 증명에 액세스할 수 없기 때문에 GitHub에 secrets.toml 파일을 업로드하지 않았거든요. 하지만 걱정하지 마세요. 다음 단계에서 이 오류를 해결할 거예요.\n\n![이미지](/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_8.png)\n\n8. 페이지 오른쪽 아래 구석에 \"앱 관리\"라는 메뉴가 있습니다. 클릭하고, 샌드위치 메뉴를 열어 \"설정\"을 선택한 다음 \"Secrets\"를 선택하고 secrets.toml 파일의 내용을 그곳에 복사해주세요. 앱이 자동으로 다시 시작되고 정상적으로 작동해야 할 거예요.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1200/1*IAqyQoKrSUpH1a4C7nG8RQ.gif)\n\n<div class=\"content-ad\"></div>\n\n아래 링크를 클릭하여 최종 배포된 앱을 확인할 수 있어요. john.doe@example.com을 사용하여 계정에 로그인하고, 패스워드는 pass1234입니다. 이 프로젝트의 GitHub 저장소에는 결과물로 제출할 수 있는 .csv 파일이 포함되어 있어요.\n\n# 실제 적용 결과\n\n이 튜토리얼의 프레임워크를 활용하여, 저는 제가 석사생들에게 가르치는 파이썬 금융 수업의 최종 프로젝트를 게임화했어요. 솔직히 말해서, 이 프로젝트에 대한 제 기대는 저조했어요. 각 팀이 프로젝트 기한 내에 플랫폼과 최소한 두 번 이상 상호 작용할 것을 기대했기 때문에, 7개팀과 3개 프로젝트 섹션 전체에서 50-60회의 상호 작용이 이루어지면 성공으로 간주될 것이었죠.\n\n그러나 학생들은 제게 인도자로서 받을 수 있는 최고의 선물 중 하나를 주었어요. 한 달 후에 앱은 690회 이상의 제출을 받았는데, 제 초기 기대의 거의 12배에 달하는 수치였죠. 이 수준의 참여는 저에게 있어서 전례가 없었어요. 각 그룹은 프로젝트 섹션 당 평균 30회 이상의 제출을 제출했는데, 이는 섹션 당 거의 하루에 한 번의 제출에 해당했어요. 첫 제출과 각 섹션 및 팀의 최고 제출을 비교했을 때, 평균적으로 21%의 개선이 있었고, 일부 팀은 제출물을 60% 이상 개선했어요. 일반적인 버전의 프로젝트를 게임화된 버전 대신에 구현했다면, 이 수준의 개선은 실현되지 않았을 것으로 생각돼요. 이는 게임화의 힘을 입증하는 것이에요. 이제 이 앱을 교실에서 쉽게 구현할 수 있고, 모든 것이 무료에요. 멋지죠, 그렇지 않나요?\n\n<div class=\"content-ad\"></div>\n\n게임화에 대해 더 많이 알고 싶다면, 내 이전 글을 꼭 읽어보세요. 그 글에서는 게임화의 논리와 참여 및 학습을 촉진하는 방법에 대해 논의했습니다.\n\n# 결론\n\n이 글은 Streamlit을 사용하여 Google Sheets와 통합하여 CRUD 앱을 만드는 전체 과정을 안내했습니다. 이 앱은 학생들을 위한 기계 학습 프로젝트에 게임 요소를 도입하는 데 사용할 수 있습니다. 또한 Streamlit Community Cloud 서비스를 사용하여 앱을 배포하는 방법도 보여드렸습니다. 이 코드는 매우 유연하며 기계 학습 프로젝트에만 국한되지 않습니다. 저는 이를 제 프로젝트 예약 수업 중 하나에 적용하고 탁월한 결과를 얻었습니다.\n\n아래 게시글에서 Bruno Scalia C. F. Leite는 Streamlit을 사용하여 물류 앱을 배포하는 방법에 대해 소개합니다. Streamlit을 사용하여 운영 연구 애플리케이션을 만드는 방법에 대해 알고 싶다면 아래 링크를 통해 그의 글을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n이 글이 유익하고 즐거웠기를 진심으로 바랍니다. 그렇다면 귀하의 생각을 듣고 싶어요! 댓글을 남기거나 👏로 감사를 표현해 주시면 감사하겠습니다. 최신 기사 업데이트를 받고 싶다면 저를 Medium에서 팔로우해 주세요. 여러분의 지원과 피드백이 제게 지속적인 탐구와 공유를 이끄는 원동력이 되어요. 읽어 주셔서 감사합니다. 다음 글에서 더 많은 통찰을 기대해 주세요!\n\n# 참고 자료\n\n- Python 및 Streamlit으로 Google Sheets 데이터 입력 폼 만들기 | 빠르고 쉬운 튜토리얼 : https://www.youtube.com/watch?v=_G5f7og_Dpo&t=66s\n- 학생 그룹 프로젝트를 개선하는 방법: 게이미피케이션 기법으로 학습 향상 : https://medium.com/@luisfernandopa1212/transforming-group-projects-enhancing-learning-with-gamification-techniques-81e2ba2e02ff\n- 운영 연구 솔루션 설계: Streamlit을 활용한 사용자 친화적 라우팅 애플리케이션 : https://medium.com/towards-data-science/designing-operations-research-solutions-a-user-friendly-routing-application-with-streamlit-17212553861d\n- Streamlit 웹 앱 온라인으로 쉽게 배포하는 방법 3가지 : https://towardsdatascience.com/3-easy-ways-to-deploy-your-streamlit-web-app-online-7c88bb1024b1\n- Penard, Wouter; van Werkhoven, Tim. “보안 해시 알고리즘 패밀리에 대해”. staff.science.uu.nl. 2016–03–30.\n- 연방 등록 호시즈 02–21599, FIPS 게시물 180–2 승인 공지.","ogImage":{"url":"/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_0.png"},"coverImage":"/assets/img/2024-06-22-HowICreatedaKaggle-LikePlatformforMyStudentsUsingStreamlitandHowYouCanDoItasWell_0.png","tag":["Tech"],"readingTime":28},{"title":"결정 트리Decision Trees를 사용한 탐색적 데이터 분석 방법","description":"","date":"2024-06-22 02:31","slug":"2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis","content":"\n\n\n![Decision Tree](/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_0.png)\n\n# 소개\n\n의사 결정 트리(DT)는 가장 직관적인 머신러닝 알고리즘입니다.\n\n내 의견이죠. 하지만 데이터 과학 분야에서도 흔히 느껴지는 감정이라고 확신합니다.\n\n\n<div class=\"content-ad\"></div>\n\n운영 연구와 데이터 과학 분야에서 매우 활용되는 DT(의사 결정 트리)의 성공 요인은 인간의 의사 결정 과정과 유사한 프로세스를 따라가기 때문입니다. 이 과정은 각 노드가 주어진 변수에 대해 간단한 이진 결정을 갖는 플로 차트에 기반하며, 최종 결정에 이르기까지 계속됩니다.\n\n간단한 예를 들어, 티셔츠 구매. 저는 셔츠를 사려고 할 때 가격, 브랜드, 사이즈, 색상과 같은 몇 가지 변수를 고려할 수 있습니다. 따라서 저는 결정 프로세스를 예산에서 시작합니다:\n\n- 가격이 $30 이상이면 구매하지 않을 것입니다. 그렇지 않은 경우에는 구매할 것입니다.\n- $30 미만으로 무언가를 찾으면 좋아하는 브랜드의 제품이어야 합니다. 그렇다면 결정 과정을 계속합니다.\n- 이제, 제 사이즈에 맞는지 확인해보죠. 맞다면 계속 진행합니다.\n- 마지막으로, $30 미만, 브랜드 X, 사이즈 S인 검은색 티셔츠라면 구매할 것이고, 그렇지 않다면 계속 찾거나 \"구매하지 않을 것\"으로 결정 프로세스를 마칠 수 있습니다.\n\n![의사 결정 트리 샘플](/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_1.png)\n\n<div class=\"content-ad\"></div>\n\n이 프로세스는 매우 논리적이고 간단하여 모든 종류의 데이터에 적용할 수 있습니다. 이 알고리즘의 단점은 데이터 세트의 변화에 매우 민감하여 특히 데이터가 작을 때 민감하다는 것입니다. 따라서 데이터의 작은 변동성을 쉽게 학습하여 기계 학습 모델을 과적합시킬 수 있습니다.\n\n이러한 결정 트리(DT)의 이러한 특성은 예측에 위협이 될 수 있지만 탐색적 데이터 분석 과정 중에 이를 활용하고자 하는 것입니다.\n\n이 게시물에서는 데이터에서 더 나은 통찰력을 추출하기 위해 DT의 힘을 어떻게 활용하는지 배워보겠습니다. 계속 진행합시다.\n\n# EDA란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n탐색적 데이터 분석 또는 EDA는 데이터 과학 프로젝트의 단계 중 하나로, 데이터 세트를 가져와 변수를 탐색하여 대상 변수에 가장 큰 영향을 미치는 요소를 최대한 파악하려는 과정입니다.\n\n이 단계에서 데이터 과학자는 데이터를 이해하고 분포가 어떤지, 오류나 누락된 데이터가 있는지, 데이터의 첫 인사이트를 추출하고 설명 변수가 대상 변수에 어떻게 영향을 미치는지 시각화하여 학습하고자 합니다.\n\n# 결정 트리 사용하기\n\nDT가 데이터의 가장 작은 변동을 포착할 수 있는 능력 때문에, 변수 간 관계를 이해하는 데 도움이 됩니다. 여기서는 데이터를 탐색 중이므로 데이터 분할이나 알고리즘 세밀 조정에 대해 신중할 필요가 없습니다. 우리는 그저 최상의 통찰을 얻기 위해 DT를 실행하기만 하면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n그것을 어떻게 하는지 봅시다.\n\n## 데이터셋\n\n이 연습에 사용될 데이터셋은 Paulo Cortez가 작성한 UCI Repository의 학생 성적 데이터입니다. 이 데이터셋은 크리에이티브 커먼즈 저작자표시 4.0 국제 라이선스(CC BY 4.0) 하에 배포됩니다.\n\n```python\n# 라이브러리 불러오기\nimport pandas as pd\nimport seaborn as sns\nsns.set_style()\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.tree import plot_tree\n\n# 데이터셋 불러오기\nfrom ucimlrepo import fetch_ucirepo\n\n# 데이터셋 가져오기\nstudent_performance = fetch_ucirepo(id=320)\n\n# 데이터 (판다스 데이터프레임 형식)\nX = student_performance.data.features\ny = student_performance.data.targets\n\n# 시각화를 위해 X와 Y 모으기\ndf = pd.concat([X,y], axis=1)\n\ndf.head(3)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_2.png)\n\n이 데이터에서 어떤 변수가 최종 성적 G3에 더 큰 영향을 미치는지 결정하려고 합니다.\n\n## 회귀 DT로 탐색하기\n\n이제 실패, 결석 및 공부 시간이 G3에 미치는 영향을 확인하기 위해 DT를 생성해보겠습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n# 탐색할 컬럼\ncols = ['failures', 'absences', 'studytime']\n\n# X & Y 분리\nX = df[cols]\ny = df.G3\n\n# 의사결정트리 학습\ndt = DecisionTreeRegressor().fit(X, y)\n\n# 의사결정트리 그리기\nplt.figure(figsize=(20,10))\nplot_tree(dt, filled=True, feature_names=X.columns, max_depth=3, fontsize=8);\n```\n\n이것이 생성된 의사결정트리입니다.\n\n<img src=\"/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_3.png\" />\n\n이제 우리는 나열한 변수들 간의 관계를 이해하기 위한 좋은 시각화가 있습니다. 이 트리에서 얻을 수 있는 인사이트는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- 각 상자 안의 첫 번째 줄의 조건에 따라 왼쪽이 \"예\"를, 오른쪽이 \"아니오\"를 의미한다는 것을 알고 계셔야 합니다.\n- 실패 횟수가 적은 학생들(0.5 또는 0이라고 말해야 할 정도)이 더 높은 성적을 받습니다. 왼쪽 상자의 값이 오른쪽보다 높은 것을 관찰할 수 있습니다.\n- 실패한 학생들 중 공부 시간이 2.5 미만인 학생들이 더 높은 성적을 받습니다. 값이 거의 1점 더 높습니다.\n- 실패 횟수와 공부 시간이 1.5 미만, 그리고 결석이 22회 미만인 학생들은 공부 시간이 적고 결석률이 높은 학생들보다 더 높은 최종 성적을 받습니다.\n\n## 여유 시간과 외출\n\n여가 시간의 양과 외출 빈도에 기반하여 더 높은 성적을 받는 학생들을 알아보고 싶다면 여기에 있는 코드입니다.\n```js\n# 탐색할 열\ncols = ['여가시간', '외출']\n\n# X와 Y 분리\nX = df[cols]\ny = df.G3\n\n# 의사결정 트리 피팅\ndt = DecisionTreeRegressor().fit(X,y)\n\n# DT 플롯\nplt.figure(figsize=(20,10))\nplot_tree(dt, filled=True, feature_names=X.columns, max_depth=3, fontsize=10);\n```\n\n<div class=\"content-ad\"></div>\n\n![Decision Trees for Exploratory Data Analysis](/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_4.png)\n\ngoout와 freetime 변수는 1= 매우 낮음부터 5= 매우 높음까지의 척도로 조정되어 있습니다. 자주 외출하지 않는 사람들(1.5)과 여가 시간이 없는 사람들(1.5)은 많이 외출하고 어느 정도 여가 시간이 있는 사람들과 마찬가지로 낮은 성적을 받을 수 있음을 주목해주세요. 가장 높은 성적을 받는 사람들은 외출과 여가 시간이 균형을 이루고 있습니다(외출 1.5, 여가 시간 1.5에서 2.5 사이).\n\n## Classification DT로 탐색하기\n\n동일한 연습을 Classification Tree 알고리즘을 사용해 할 수 있습니다. 논리와 코딩은 동일하지만, 결과 값은 이제 값이 아닌 예측된 클래스를 보여줍니다. Seaborn 패키지(3-Clause BSD License)에서 가져온 뉴욕 시티의 택시 운행 데이터셋을 사용한 간단한 예제를 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n만약 우리가 런 총액과 결제 방법 간의 관계를 탐구하고 싶다면, 다음 코드를 확인해보세요.\n\n```js\n# 데이터셋 로드\ndf = sns.load_dataset('taxis').dropna()\n\n# 탐색할 열\ncols = ['total']\n\n# X & Y 분리\nX = df[cols]\ny = df['payment']\n\n# 의사결정 트리 적합\ndt = DecisionTreeClassifier().fit(X,y)\n\n# 트리 시각화\nplt.figure(figsize=(21,10))\nplot_tree(dt, filled=True, feature_names=X.columns, max_depth=3, \n          fontsize=10, class_names=['cash', 'credit_card']);\n```\n\n<img src=\"/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_5.png\" />\n\n결과 트리를 눈으로 확인해본 결과, 총액이 낮은 경우 현금으로 결제하는 가능성이 훨씬 높다는 것을 알 수 있습니다. $9.32 미만의 총액은 일반적으로 현금으로 결제됩니다.\n\n<div class=\"content-ad\"></div>\n\n좋죠, 그렇죠?\n\n# 이제 가기 전에\n\n이 튜토리얼에서는 데이터셋 내 변수들 간의 관계를 탐색하는 빠른 방법인 결정 트리를 사용하는 방법에 대해 배웠습니다.\n\n이 알고리즘은 처음에 쉽게 찾아지지 않는 패턴을 빠르게 포착할 수 있습니다. 우리는 데이터의 그 절삭을 찾기 위해 결정 트리의 힘을 활용하여 거기서 훌륭한 통찰을 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n빠른 코드에 대한 노트하나: plot_tree() 함수에서 max_depth 기능을 사용하여 원하는 수준을 설정할 수 있습니다. 또한 sklearn의 DT 인스턴스에서 해당 하이퍼파라미터를 설정할 수도 있습니다. 선택은 당신의 몫입니다. plot_tree에서 사용하는 장점은 모델을 다시 훈련시킬 필요 없이 다양한 깊이를 빠르게 테스트할 수 있다는 것입니다.\n\n```js\nplot_tree(dt, filled=True, feature_names=X.columns, max_depth=3);\n```\n\n만약 이 내용을 좋아하신다면, 더 많은 내용을 위해 저를 팔로우해주세요.\n\nLinkedIn에서 저를 찾아서 연결해요. 함께해요!\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n제가 소개하고 싶은 좋은 참고 자료가 있어요. 이 기술은 멋진 브라질 데이터 과학자 Teo Calvo로부터 배웠어요. 그는 Teo Me Why 채널에서 매일 생방송으로 멋진 프로그램을 제공하고 계세요. 포르투갈어를 구사하신다면, 그의 작품에 대해 더 알아보세요.","ogImage":{"url":"/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_0.png"},"coverImage":"/assets/img/2024-06-22-UsingDecisionTreesforExploratoryDataAnalysis_0.png","tag":["Tech"],"readingTime":6}],"page":"24","totalPageCount":151,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}