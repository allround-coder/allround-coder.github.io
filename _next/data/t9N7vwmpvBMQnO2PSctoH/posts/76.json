{"pageProps":{"posts":[{"title":"위협 인텔리전스를 멋지다고 생각하시나요","description":"","date":"2024-05-16 17:43","slug":"2024-05-16-DoyouthinkThreatIntelligenceiscool","content":"\n\n\n![이미지](/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_0.png)\n\n# 위협 인텔리전스란 무엇인가요?\n\n요약하자면, 위협 인텔은 다양한 위협 행위자와 캠페인에 대한 정보를 수집할 수 있는 활동의 집합입니다.\n\n## 위협 인텔리전스 활동\n\n\n<div class=\"content-ad\"></div>\n\n많은 사람들(아마도 당신도)이 생각하는 것:\n\n위협 인텔리전스 = 침해 표시\n\n이는 그 이상입니다!\n\n위협 인텔리전스 활동의 다른 예시를 소개해 드릴게요:\n\n<div class=\"content-ad\"></div>\n\n- 위협 모델링: 조직에 특정한 위협을 매핑합니다. 예를 들어, 은행은 TV 온라인 채널보다 다른 위협을 대상으로 합니다.\n\n![이미지](/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_1.png)\n\n그럼 이게 왜 중요할까요?\n\n두 가지 예를 들어볼게요:\n\n<div class=\"content-ad\"></div>\n\n- 비즈니스 결정: 귀하의 조직에 특화된 위협 모델을 정의하면 비즈니스는 귀하의 요구에 맞게 특정 투자 결정을 내릴 수 있습니다. 제 나라에 곰이 없다면 왜 스스로를 곰으로부터 방어해야 하는 걸까요? 같은 개념이 여기에도 적용됩니다.\n- 레드팀: 위협 모델을 사용하여 레드팀은 귀하의 조직을 대상으로 하는 위협 그룹들이 사용하는 기술, 전술 및 절차(Techniques, Tactics, and Procedures, TTPs)를 모의할 수 있습니다.\n\n- OSINT: \"오픈소스 인텔리전스\"의 약자입니다. 기본적으로 인터넷을 활용하여 위협 가해자에 대한 정보를 수집할 수 있습니다. 서로 다른 플랫폼을 사용하여 캠페인이나 위협 그룹 뒤에 숨은 사람들의 실제 신원을 파악할 수 있습니다.\n\n이미지:\n<img src=\"/assets/img/2024년-05월-16일-도유희감위협인텔리전스잘하는거시아이쿨_2.png\" />\n\n또한, OSINT를 통해 사이버범죄 포럼에서 정보를 수집할 수 있습니다. 아래에서 확인할 수 있는 몇 가지 예시를 찾아보세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nxss[.]in\nExploit[.]in\nCracked[.]io\nNulled[.]to\nInfinity[.]ink\nBreachForums[.]st\nWww-club[.]link\n```\n\n- **HUMINT**: 사람 지능을 나타내는 HUMINT는 정보를 사람의 소스로부터 수집하는 과정을 나타냅니다. 위협 인텔리전스에서 HUMINT는 사람을 통해 위협 그룹과 내부 연결을 설정하는 데 사용될 수 있습니다. 예를 들어, 해킹 그룹의 미래 목표에 대한 소중한 정보를 얻고 이를 사용하여 고객/희생자의 보안을 강화할 수 있습니다.\n- **Compromise Monitoring**: 포럼, 해킹 커뮤니티 및 텔레그램 채널을 적극적으로 모니터링하여 귀하나 귀하의 고객과 관련된 게시물을 확인하는 스크레이핑 도구를 만드는 것을 권장합니다.\n\n<img src=\"/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_3.png\" />\n\n- **Puppet Master**: 네, 정말로 그렇습니다! 당신은 꼭두각시 주인이 될 수 있습니다... 하지만 위협 인텔리전스 분야에서요.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_4.png)\n\n위협 인텔리전스의 퍼펫 마스터는 해킹 커뮤니티 내에서 자신의 페르소나의 신뢰도를 구축하고 보호하고 있어요.\n\nHUMINT와 퍼펫 마스터링의 차이는 무엇인가요? — HUMINT는 외부 소스를 다루는 반면, 퍼펫 마스터는 자체 가짜 \"위협 요소\"를 보유하고 사이버 범죄 집단에 침투하여 정보를 수집해요.\n\n이 주제에 관한 정말 좋은 책인 “Securing Online Personas” by deadlock (온라인에서 찾을 수 있어요)를 공부할 수 있어요.\n\n\n<div class=\"content-ad\"></div>\n\n그리고 위에 나열된 것 외에도 수행할 수 있는 많은 위협 인텔 활동이 있습니다.\n\n## 데이터, 정보 및 인텔리전스\n\n이 세 용어의 차이점을 알고 계신가요?\n\n\"This is an IP address\" - `데이터`\n\n<div class=\"content-ad\"></div>\n\n\"이 IP 주소는 명령 및 제어에 사용됩니다\" -` 정보\n\n\"이 IP 주소는 우리의 인프라를 대상으로하여 민감한 문서를 추출하기 위한 경제 스파이 활동을 목적으로 하는 명령 및 제어에 사용된 주소입니다\" -` 인텔리전스\n\n## 용어 및 참조\n\n위협 인텔리전스 산업에서 사용되는 용어 예시 중 일부를 아래에서 찾을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n소스 = 지능을 제공하는 모든 제공자 또는 채널을 가리킵니다. 보고서에서 사람들은 정보를 어떻게 또는 누구로부터 얻었는지 언급하지 않습니다. 대신, 매우 자주 \"우리의 소스가 우리에게 XYZ를 알려주었다\"고 언급됩니다.\n\n페르소나 = 주로 위협 요소 또는 그룹과 관련된 생성된 신원입니다. 일반적으로 사용자 이름에 의해 정의됩니다.\n\n작전 보안 (OPSEC) = 실제 신원을 추적으로부터 활동 및 페르소나를 보호하는 데 적용하는 방법입니다.\n\n침해 지표 (IoC) = 시스템이 침입되었거나 침해당했다는 것을 시사하는 증거 또는 유물입니다. 이는 악성 파일 해시, IP 주소, 도메인 이름, URL, 이메일 주소, 레지스트리 키, 파일 경로, 사용자 에이전트 문자열 등을 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n**기술, 전술 및 절차(TTPs)** = 위협 행위자가 목표를 달성하는 데 사용하는 구체적인 방법 또는 접근법입니다. MITRE ATT&CK 페이지에서 포괄적인 목록을 찾을 수 있습니다.\n\n![이미지](/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_5.png)\n\n**YARA 규칙** = \"Yet Another Recursive Acronym\"의 약자로 악의적 파일을 식별하는 데 사용할 수 있는 규칙을 생성하고 공유하기 위해 주로 설계된 오픈 소스 도구입니다. 잠재적으로 위험한 파일의 정적 분석에 기반을 두고 있습니다. 참조: [https://github.com/VirusTotal/yara](https://github.com/VirusTotal/yara)\n\n![이미지](/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_6.png)\n\n<div class=\"content-ad\"></div>\n\n적 정보 = 위협 주체의 신원과 온라인 존재에 관한 정보\n\n작업 정보 = 위협 주체와 관련된 캠페인 및 TTP(Tactics, Techniques, Procedures)에 대한 정보\n\n초기 접근 중개자 (IAB) = 초기 접근을 판매하는 위협 주체 (침해된 VPN, RDP, SSH 또는 다른 원격 액세스 계정)\n\n위협 모형 = 조직의 잠재적 위협을 평가하기 위한 구조화된 접근 방식\n\n<div class=\"content-ad\"></div>\n\nMISP는 \"악성 소프트웨어 정보 공유 플랫폼 및 위협 공유\"로, 조직이 보안 위협에 대한 정보를 수집, 공유 및 분석할 수 있게 해주는 오픈 소스 위협 인텔리전스 플랫폼입니다.\n\n![Do you think Threat Intelligence is cool](/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_7.png)\n\n# 정보의 품질을 어떻게 평가할 수 있을까요?\n\n얻은 위협 인텔리전스는 NATO 해군 시스템을 사용하여 분류할 수 있으며, 이를 사용하여 정보의 신뢰성 및 유효성을 분류할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n`<img src=\"/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_8.png\" />`\n\n`<img src=\"/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_9.png\" />`\n\n# Is it dangerous? OPSec Considerations\n\nDefinitely YES!\n\n<div class=\"content-ad\"></div>\n\n많은 갱단, 깡패, 범죄 조직 및 테러 조직이 사이버 위협 가해자들과 관련되어 있음을 고려해야 합니다. 그들은 돈, 무기 및 영향력을 갖고 있습니다.\n\n그래서 좋은 OPSec을 갖추는 것이 매우 중요합니다. 몇 가지 조언은 다음과 같습니다:\n\n- VPN 및 프록시 사용\n- 포런 및 기타 플랫폼에 계정을 등록할 때 Proton Email 또는 YOPMail 사용\n- 브라우저의 사용자 에이전트를 소유하지 않는 기기를 모방하도록 설정\n- 브라우저에서 쿠키 추적 비활성화\n- 기기의 로컬 시간을 다른 시간대로 변경\n- 수집한 정보에 “필요한 정보만” 원칙 적용\n- 개인성 사용자 이름을 실제 신원의 사용자 이름과 연결하지 마세요\n- 포럼 계정 및 개성에 무작위 시간에 로그인하여 자신의 가능한 시간대를 드러내지 않도록 함\n- 거짓말하는 법을 배우세요! 예를 들어, 리소스와 날씨에 대한 토론 중에 만난 경우, 반대의 것에 대해 불평하세요 (그리고 전하는 거짓말을 추적하세요). 현실: 귀하는 국가에서 눈이 옵니다. 당신이 할 말: “아이고 여기 너무 덥네요”. 실생활 취미와는 1%도 관련이 없는 개성에 대한 취미를 개발하세요. 이해하셨으면 좋겠네요 ;)\n\n매우 유심히 읽어주셔서 감사하며 건강하세요!","ogImage":{"url":"/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_0.png"},"coverImage":"/assets/img/2024-05-16-DoyouthinkThreatIntelligenceiscool_0.png","tag":["Tech"],"readingTime":5},{"title":"선명한 오리진 스토리","description":"","date":"2024-05-16 17:41","slug":"2024-05-16-VIVIDOriginStory","content":"\n\n![VIVID Gallery](/assets/img/2024-05-16-VIVIDOriginStory_0.png)\n\n안녕하세요! VIVID 갤러리가 재디자인한 웹사이트를 공개하게 되어 매우 기쁩니다. 우리는 예술가와 수집가에 대한 약속을 가슴 깊이 간직하고 있으며, 최신 업데이트에서는 예술가와 수집가 모두의 경험을 향상시키기 위한 여정을 강화하고자 합니다. VIVID의 정체성을 조금이나마 소개하며, 당신과 함께 하는 미션과 목표를 나누는 것에 흥분합니다. 오늘의 우리를 이끄는 기원 이야기를 다시 들려주면서 새로운 미학을 축하하고자 합니다. 당신과 함께 손잡고 계속해서 성장해 나가는 것을 기대합니다.\n\nVIVID는 플랫폼 이상의 것을 구축하려 애썼습니다; 창작자와 예술 애호가 모두에게 필요한 완벽한 경험을 조성하기 위한 목표를 세웠습니다. 우리의 야망은 예술가들에게 견고한 기술적, 예술적 지원을 제공하여 그들의 창의적 꿈을 현실로 이끌어 주는 것이었습니다. 동시에 우리는 디지털 아트의 열렬한 후원자인, 열정을 가진 수집가들의 단단한 공동체를 짜기를 희망했습니다.\n\n그 초기 시절에는 비트코인으로 창작, 기록, 수집하는 것은 중요한 인프라가 없어 지각이 된 큰 과업처럼 보였습니다.\n\n<div class=\"content-ad\"></div>\n\n이 안전하고 탈중앙화된, 변경 불가능한 생태계는 모든 디지털 작품이 원래의 블록체인처럼 영원히 존속할 수 있는 안식처를 제공했습니다.\n\n수집가들의 여정을 높이기 위해 우리는 이례적이고 프리미엄한 경험을 상상했습니다 — 기존의 오더널이나 인기 있는 NFT 마켓플레이스에서 만나볼 수 있는 비인간적인 거래와는 대조적인 경험입니다. 우리의 목표는 전통적이고 개인적인 미술 수집 경험을 재도입하여 진지한 미술 애호가들이 예술가와 VIVID의 창립자들과 직접 소통하며 디지털 시대에서 종종 잃어버린 개인적 연결을 유도하는 것이었습니다.\n\n2023년의 곰 시장에서 데뷔한 VIVID는 Ordinals의 매력에 대해 거의 무감각한 세상을 뚫고 나아갔습니다. 우리의 첫 번째 컬렉션인 \"Rising Echoes\"는 2023년 7월 빛을 보았습니다. 판매를 매듭짓는 길은 Magic Eden 런치패드에서 출시 후 몇 일 동안의 도전 끝에 이르렀습니다 — NFT 세계의 순발력 지배 문화 속에서의 인내심 대결로, 디지털 미술 세계에서 기대하는 즉각적인 만족감과의 시험이기도 했습니다. 마침내, Rising Echoes는 완판되었고 이 경로를 계속해서 나아갈 수 있도록 우리에게 격려를 주었습니다.\n\n급변하는 환경에서 선구자로 남아, 우리는 예술과 기술의 교차로에 서 있으며, 가상 경계를 초월하는 연결을 구축하고 있습니다. 지루한 미래의 아름다움과 블록체인 기술의 무한한 잠재력을 믿음으로, VIVID는 새로운 영역을 개척하며 모든 화소와 대화로 새로운 길을 개척하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n전통 예술계를 모태로 한 VIVID는 예술가들이 번성하고 수집가들이 고품질 디지털 예술을 발견하고 투자할 수 있는 육성 환경을 제공합니다. 하르토의 Floraforms와 같은 혁신적인 프로젝트의 도입은 VIVID의 혁신적 접근과 기술적 능력을 강조하며 Ordinals 공간 내 생성적 예술에 대한 새로운 가능성을 열어줍니다.\n\n![이미지](/assets/img/2024-05-16-VIVIDOriginStory_1.png)\n\n2024년 발렌타인 데이에 빈의 명문 Belvedere 박물관은 VIP들을 위해 호화로운 파티를 개최하여 FloraForms 콜렉션을 론칭하고 발행하는 독특한 기회를 제공했습니다. 이 이벤트는 즉시 FloraForms Ordinals 콜렉션의 매진을 보았으며, 역사적인 예술 기관과 혁신적인 예술가의 협업으로 전세계적으로 전복적인 생성적 예술 계획으로 자리 잡았습니다. 이 협력은 박물관의 깊은 유산과 구스타프 클림트 시대를 존중함과 함께 예술가, 철학자, 사상가들의 혁신적이고 변혁적인 기여를 찬양하는 시기를 기념하였습니다.\n\n![이미지](/assets/img/2024-05-16-VIVIDOriginStory_2.png)\n\n<div class=\"content-ad\"></div>\n\n벨베데레 미술관과 협력하여 Gustav Klimt의 상징인 작품에 경의를 표하며 FloraForms를 통해 전통적인 예술과 디지털 예술 영역 사이의 간극을 좁히는 VIVID의 야망을 보여줍니다. 이는 과거와 현재 예술적 움직임 사이의 대화를 시작함으로써 디지털 예술 컬렉션의 문화적 의미를 풍부하게 하고 예술 커뮤니티 내 참여와 소유의 독특한 기회를 제공합니다.\n\nFloraForms의 성공적인 출시 이후, VIVID는 CyberSea의 Deus Ex Machina, Moodsoup의 Mitosis, 그리고 Medusa의 Artifakt와 같은 명망 높은 아티스트들과의 협업을 공개했습니다. 각 컬렉션은 상당한 사전 판매 관심을 보이며 출시 즉시 완판되었습니다.\n\n![이미지1](/assets/img/2024-05-16-VIVIDOriginStory_3.png)\n\n![이미지2](/assets/img/2024-05-16-VIVIDOriginStory_4.png)\n\n<div class=\"content-ad\"></div>\n\n\n![VIVIDOriginStory_5](/assets/img/2024-05-16-VIVIDOriginStory_5.png)\n\n아티스트와 그들의 컬렉터 사이의 동적인 연결을 유지하기 위해 Discord를 통해 제공하며, 컬렉터들에게 아티스트에 대한 이전에 없던 접근권한을 제공합니다. 이러한 가까운 상호작용은 컬렉터들이 아티스트와 깊게 상호작용할 수 있도록 하여 VIVID 컬렉션 경험의 중요한 요소로 남아 있습니다.\n\nDiscord에서 아티스트와 컬렉터들의 커뮤니티에 가입하세요.\n\n앞으로도 Shaderism, Manuel Larino 및 Dario Lanza와 같은 비전있는 아티스트들과의 미래 프로젝트를 발표하는 것에 흥분하며, 이들은 모두 VIVID의 탁월함에 대한 약속을 담고 있으며, VIVID 애호가들의 컬렉션을 풍부하게 하는 훌륭한 작품을 제공할 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n창립자들의 공유된 비전과 커뮤니티의 적극적인 참여는 미술의 풍부한 유산을 존중하면서 혁신적인 플랫폼을 형성하는 데 중요한 역할을 합니다. VIVID는 고품질 생성 미술 작품의 창작과 획득의 무한한 잠재력을 발견할 수 있는 예술가와 수집가들이 함께 모여 커뮤니티를 구축하며 발전할 것입니다.","ogImage":{"url":"/assets/img/2024-05-16-VIVIDOriginStory_0.png"},"coverImage":"/assets/img/2024-05-16-VIVIDOriginStory_0.png","tag":["Tech"],"readingTime":4},{"title":"점들로부터 평면까지 Vision-Language Subspace Prompting","description":"","date":"2024-05-16 17:39","slug":"2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting","content":"\n\n\n![lecture image](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png)\n\n최근에 고급 컴퓨터 비전 수업인 Dr. 이 제가 이끄는 Dr. 송 이제의 흥미로운 게스트 강연을 듣게 되었습니다. Dr. Li는 삼성 AI의 연구 과학자로, 도메인 일반화, 연합 학습 및 PEFT 방법과 같은 인공 지능의 중요 주제를 다루었습니다. 특히, 그의 최신 연구인 \"Vision-Language Sub Space Prompting\"에 초점을 맞춘 프레젠테이션은 정보 전달 뿐만 아니라 유사성 검색과 같은 작업에서 진화하는 방법론을 구체화하는 데 있어서 흥미롭고 사유를 제공했습니다. 그의 통찰력에 영감을 받아, 저는 그의 최신 연구 작업에 대한 직관에 대한 생각을 이 블로그를 통해 공유할 필요성을 느꼈습니다.\n\nSub-space prompting의 복잡성에 뛰어들기 전에, 대규모 언어 모델(LLMs)과 시각-언어 모델(VLMs)의 영역에서 우리를 이 시점으로 이끌어 온 혁신의 역사를 잠깐 돌아봅시다. 여러분, 정말 대단한 여정이었습니다!\n\nLLMs의 초기 발전은 Word2Vec 및 GloVe와 같은 모델의 등장으로 시작되었고, 이 모델은 단어의 밀도 있는 벡터 표현을 생성했습니다. 이 임베딩은 단어 간 의미와 관계를 포착해, 더 정교한 모델을 위한 기초를 마련했습니다. 이는 예전 페이스북에서 누군가가 우리를 \"찌르다\"라는 말만 들어도 정말 즐거웠던 소셜 미디어 초기 시절과 닮았네요.\n\n\n<div class=\"content-ad\"></div>\n\n그 다음에는 트랜스포머의 시대가 도래했습니다. 특히 2017년 Vaswani 등이 소개한 트랜스포머 아키텍처는 해당 분야를 완전히 혁신했습니다. 트랜스포머는 셀프 어텐션 메커니즘을 활용하여 모델이 문장이나 문서 전체를 병렬로 처리하고 맥락적 관계를 더 효과적으로 포착할 수 있게 했습니다.\n\n이어서 BERT(Bidirectional Encoder Representations from Transformers)와 GPT(Generative Pre-trained Transformer)가 등장했습니다. BERT는 모델이 양방향으로 컨텍스트를 학습하는 이중향 교육 개념을 소개했고, GPT는 비지도 학습과 자기회귀 언어 생성의 힘을 보여줌으로써 언어 모델링 분야에서 새로운 기준을 세웠습니다. GPT가 마치 AI 세계를 황홀한 롤러코스터 여행에 초대하며 기록을 깨고 모두를 감탄시켰다고 상상해보세요.\n\n한편 VLM(Visual Language Models)도 주목을 받고 있었습니다. VisualBERT와 ViLBERT와 같은 모델은 시각적 및 텍스트 데이터를 결합하여 이미지 캡션 달기, 시각적 질문 응답 등 양쪽 모드를 이해해야 하는 작업을 가능하게 했습니다. AI의 동적인 듀오로 생각해보세요. 함께 세계를 보고 설명하며 복잡한 퍼즐을 해결합니다.\n\n이어서 OpenAI의 CLIP(Contrastive Language–Image Pre-training)이 등장합니다. CLIP은 대규모 이미지-텍스트 쌍 데이터셋에서 훈련하여 이미지와 텍스트를 연관시키는 방법을 학습함으로써 제로샷 분류 작업에서 놀라운 성능을 달성했습니다. CLIP은 마치 AI 파티에 등장해 파티 분위기를 살려 이미지와 텍스트를 완벽하게 이해하는 능력으로 모든 이들을 감탄시켰다고 생각해보세요.\n\n<div class=\"content-ad\"></div>\n\n서브스페이스 프롬프팅의 구체적인 내용을 살펴보면, 기술의 진화를 이해하는 것이 매우 중요합니다. 각 혁신은 이전 성공과 교훈을 기반으로 구축되어 오늘날 우리가 보는 정교한 모델과 방법론으로 이어졌습니다.\n\n# 비전-언어 모델 이해: CLIP 사례\n\n![이미지](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_1.png)\n\n비전-언어 모델(VLM)에 대해 생각할 때, 그것들을 AI 세계의 스위스 아미 나이프로 상상해보세요. 다재다능하고 소형이면서 예상치 못하게 강력합니다. 이 분야에서 빛나는 별 한 개는 OpenAI가 개발한 CLIP입니다. 수백만 개의 세심하게 레이블이 붙은 이미지를 사탕 가게에서 사탕을 먹는 아이처럼 먹이는 전통적 모델들이 있는 세계에서, CLIP은 자가 감독 학습 방식으로 젠의 접근을 취합니다. 이 방법은 대조 손실 함수를 사용하여 멋지기만 한 것이 아니라 놀랍도록 효과적으로 작동하여 CLIP이 많은 지도 학습 모델들을 앞설 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\nCLIP은 그냥 어떤 모델이 아닙니다. 그것은 다양한 작업을 동시에 수행하는 마에스트로입니다. 이 모델은 우수한 특성 표현을 만들어내는데, 이는 단순히 좋을 뿐만 아니라 수많은 하위 작업들을 해결하는 금빛 열쇠 같습니다. 예를 들어, CLIP에서 임베딩을 사용하면 순식간에 이미지 또는 텍스트 분류 시스템을 만들 수 있습니다. 또는 이 임베딩을 사용하여 텍스트와 이미지를 결합한 가장 가까운 이웃 검색을 수행할 수 있습니다. 마치 화가가 캔버스에 색을 섞는 것처럼요.\n\n# 프롬프트 엔지니어링의 진화\n\n훌륭한 임베딩이면 큰 책임도 따라옵니다. 즉, 그것들을 효과적으로 활용하는 방법을 알아내야 합니다. 이것이바로 프롬프트 엔지니어링, 모델로부터 최상의 답변을 도출하기 위해 완벽한 질문을 만드는 예술입니다. 이는 데이터를 위한 매치메이킹과 같습니다. 이를 통해 질문과 모델이 완벽히 맞는지를 확인합니다. 연구자들은 구문을 다듬어 순수한 요청인 \"'CLASS'의 사진.\"과 같은 것을 \"새 종류 중 'CLASS'의 사진.\"과 같이 더 화려하게 변환하여 결과물을 향상시키고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 수동으로 이러한 프롬프트를 제작하는 것은 설명서 없이 가구를 조립하는 것만큼 지루합니다. 이에 프롬프트 학습 분야가 등장해 밝은 기사갑옷을 입은 기사처럼 나타났습니다. 이는 강한, 수동으로 만들어진 프롬프트를 더 유연한 것으로 대체합니다 — Coop 및 CoCoOp과 같은 작품에서 나타나는 학습 가능한 임베딩, 즉 컨텍스트 최적화(즉, 소프트 프롬프팅이나 소프트 프롬프터라고도 함). 이러한 동적 임베딩은 특정 작업의 맛을 향상시킬 수 있도록 맞춤형 양념 조합과 같습니다. 예를 들어, 정의된 일련의 범주를 분류하는 것과 같은 특정 작업에 더욱 특화된 작업에 특화된 작업에 대한 임베딩입니다.\n\n# Vision-Language Sub Space Prompting\n\n<img src=\"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_3.png\" />\n\n그들의 논문인 “Vision-Language Sub Space Prompting”에서 컨텍스트 최적화의 일반적인 문제점들이 지적됩니다. 일반적으로 학습 가능한 소프트 프롬프트는 너무 열정적인 학생들처럼 작용합니다 — 익숙한 주제에서 뛰어나지만 새로운 자료에 직면할 때 성능이 떨어지며 새로운 클래스에 마주했을 때 성능이 떨어집니다. 전통적으로 해결책은 수동으로 만들어진 프롬프트 조미료를 약간 뿌리는 것이었지만, 이는 모델이 초기에 학습한 작업에 대해 빛나던 빛깔을 둔화시키는 경우가 많았습니다. 이는 주로 여기서 특징 공간에서 2개의 점 간 거리가 계산되고 점 단독이 항상 수업의 충분한 의미를 포착할 수 없기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n이를 해결하기 위해 SuPr (Subspace Prompting) 개념이 제안되었습니다. 클래스를 하나의 공간 점으로 나타내는 대신 전체 하위 공간으로 나타내는 새로운 차원으로의 건너뛰기와 같다고 생각해보세요. 일반적인 소프트 프롬프트 세트를 더 짧은 길이로 분할하여 파라미터 수를 증가시키지 않고도 앙상블을 유지할 수 있습니다. 각 세트는 토큰 시퀀스를 생성하여 텍스트 인코더에 공급하여 임베딩의 꽃다발을 만들어내는 하위 공간을 생성합니다 - 그들은 이를 하위 공간의 지원 지점이라고 부릅니다.\n\n![image alt text](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_4.png)\n\n기존의 유클리드 메트릭을 뒤바꾸는 포인트-서브스페이스 거리를 사용하여 (누가 평범한 유클리드를 좋아할까요?). 이 방법은 단순히 새로운 레이어를 추가하는 것이 아니라 하위 공간이 VLM(비전-언어 모델)과 상호 작용하는 방식을 변경하여 시각적 의미의 더 풍부한 스펙트럼을 포괄할 수 있는 지평을 넓힙니다.\n\n![image alt text](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_5.png)\n\n<div class=\"content-ad\"></div>\n\n이 방법론을 통해 다른 다중 모달에 적용할 수 있는 가능성이 있으며, 여러 기본 모델에 기반을 둔 차별적인 방식으로 공동 내재 표현이 학습될 것으로 예상됩니다.\n\n참고문헌","ogImage":{"url":"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png"},"coverImage":"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png","tag":["Tech"],"readingTime":5},{"title":"RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM","description":"","date":"2024-05-16 17:38","slug":"2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM","content":"\n\nRWKV 신경 구조는 주의를 사용하지 않습니다. 이는 시퀀스 길이에 대해 제곱으로 증가하는 어텐션 계산 비용을 갖는 트랜스포머 아키텍처보다 추론에서 훨씬 더 효율적입니다. 이 글에서 RWKV를 설명하고 사용하는 방법을 보여드렸어요:\n\nRWKV를 개발한 팀은 주기적으로 아키텍처를 개선하고 새로운 모델을 출시합니다. 현재 RWKV의 여섯 번째 버전이 출시되었으며 Hugging Face Hub에서 7B RWKV-6이 공개되었습니다:\n\n- BlinkDL/rwkv-6-world (Apache 2.0 라이선스)\n\n이 모델은 100개 이상의 언어를 지원하며 2.5T 토큰에 대해 사전 훈련되었습니다. 이 사이즈의 LLM 중 비영어권 언어에 대해 최고의 성능을 보여준다고 하네요. 저희 자체 평가에 따르면요:\n\n<div class=\"content-ad\"></div>\n\n![2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png](/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png)\n\nLlama 3 8B보다 더 좋아 보입니다. 또한 RWKV-5보다 현저히 더 좋습니다. 그러나 영어 작업에 대해서는 Mistral 7B와 Llama 3 8B 대부분의 벤치마크에서 성과가 낮습니다. 아마도 아키텍처 때문이 아니라 단순히 영어 토큰을 훨씬 적게 학습했기 때문일 것입니다.\n\n제 작업을 지원하려면 AI의 최근 발전에 대한 더 많은 기사/튜토리얼을 제공하는 뉴스레터를 구독해 주시기 바랍니다.","ogImage":{"url":"/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png"},"coverImage":"/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png","tag":["Tech"],"readingTime":1},{"title":"GPT-4o가 정말 엔드투엔드 멀티모달 모델인가요 공포, 당황, 암울에 빠진 당신을 떤들어드릴게요","description":"","date":"2024-05-16 17:36","slug":"2024-05-16-TerrifiedMortifiedPetrifiedIsGPT-4oTrulyanEnd-to-EndMulti-ModalModel","content":"\n\n\n![GPT-4 Omni](/assets/img/2024-05-16-TerrifiedMortifiedPetrifiedIsGPT-4oTrulyanEnd-to-EndMulti-ModalModel_0.png)\n\nOpenAI가 강력하고 다재다능한 AI 모델인 GPT-4 옴니를 소개했습니다. 다양한 입력 및 출력 형식을 처리하는 능력이 탁월하지만, 기술 전문가와 연구자들이 GPT-4 옴니가 엔드 투 엔드 다중 모달 모델인지 아닌지를 이해하는 것은 매우 중요합니다. 이 구별은 다양한 영역에서의 응용에 대한 적절한 기대 설정 및 기능을 올바르게 활용하는 데 중요합니다.\n\n다중 모달 모델 이해하기: 다중 모달 모델은 텍스트, 이미지, 오디오, 비디오 등 여러 유형의 데이터를 처리하고 생성하는 AI 시스템입니다. 이러한 모델은 다양한 입력의 종합적인 이해가 필요한 작업을 수행하기 위해 여러 모달리티의 정보를 통합합니다. 특히 엔드 투 엔드 다중 모달 모델은 각 모달리티에 대해 별도의 모델을 사용하지 않고 원시 데이터 입력(예: 음성, 이미지)을 직접 처리하여 출력(예: 텍스트, 생성된 이미지)을 생성합니다.\n\n엔드 투 엔드 모델 이해하기: 머신 러닝에서 \"엔드 투 엔드\"란 작업을 분리되지 않고 단일하고 일관된 프로세스에서 시작부터 끝까지 수행하는 모델을 가리킵니다. 그러나 GPT-4 옴니는 전달 방식을 사용하여 각각 다른 유형의 데이터 및 작업을 독립적으로 처리하는 전문 컴포넌트를 포함합니다.\n\n\n<div class=\"content-ad\"></div>\n\n지켜보면서도 놀라운 발전을 지켜봤어요. 태스크에 특화된 NLP 모델에서부터 BERT와 GPT-3 같은 조기 단계의 사전 훈련된 LLM까지, 마침내 일반적이고 생산적인 ChatGPT로 이어졌습니다. 이러한 모델들은 세상의 지식, 상식적 지식, 심지어 도메인 특정 지식과 추론을 표현하는 데 놀라운 능력을 보여주었답니다. 이러한 발전에도 불구하고 연구자들은 여전히 이러한 신생 능력을 설명할 수 있는 탄탄한 이론을 개발하는 데 어려움을 겪고 있습니다.\n\n텍스트 전용 modalities에서의 종단 간 훈련에 대한 확장 법칙은 어느 정도 이해하기 쉽습니다. 그러나 다중 모달 데이터에 대해 단일 신경망을 최적화하는 종단 간 방식은 정말 놀라운 것이죠. 많은 질문이 떠오르는데요:\n\n<div class=\"content-ad\"></div>\n\n- 서로 다른 데이터 모달리티를 어떻게 정렬할까요? 각 모달리티(텍스트, 비전, 오디오)는 고유한 특성과 표현 요구사항이 있습니다. 이를 하나의 모델에 정렬하는 것은 복잡한 도전입니다.\n- 최적의 학습 매개변수: 각 모달리티가 자체 최적의 학습률, 손실 함수 및 훈련 일정이 필요할까요? 이러한 매개변수를 한 모델에서 균형있게 맞추는 것은 매우 어려울 수 있습니다.\n- 계산 리소스: 이러한 모델을 훈련하는 데 얼마나 많은 계산 리소스가 필요할까요? 다중 모달 데이터의 규모와 복잡성은 계산 파워와 시간에 대한 수요를 크게 증가시킵니다.\n\nEnd-to-End인가 아닌가? 다중 모달 데이터에서 GPT-4o를 훈련하는 것에 대한 탐구 OpenAI의 GPT-4와는 달리 GPT-4 Omni에 대한 포괄적인 기술적 문서가 제공되지 않습니다. 내부 문서가 GPT-4o의 신원을 인식하는 데 도움이 되었을 수 있다고 믿어, 직접 GPT-4o로부터 답을 찾기로 결정했습니다.\n\n![이미지1](/assets/img/2024-05-16-TerrifiedMortifiedPetrifiedIsGPT-4oTrulyanEnd-to-EndMulti-ModalModel_1.png)\n\n![이미지2](/assets/img/2024-05-16-TerrifiedMortifiedPetrifiedIsGPT-4oTrulyanEnd-to-EndMulti-ModalModel_2.png)\n\n<div class=\"content-ad\"></div>\n\n📝 참고: GPT-4o에서 자체적으로 엔드 투 엔드 훈련을 거부하고 다단계 파이프라인을 주장한 후 질문을 한 것으로, 제 개인적인 편견을 피하기 위해 정확한 정보를 확인하려고 노력했습니다.\n\n# 가능한 다단계 파이프라인은 무엇인가요?\n\nGPT-4o와의 대화를 통해 \"모듈식\" 및 \"분리된\" 구성 요소에 대한 세부 정보에 대해 요약했습니다. (이것은 FAKE NEWS일 수도 있습니다)\n\nGPT-4 Omni은 다양한 유형의 입력과 출력을 수용하고 생성할 수 있는 능력을 가지지만, 엔드 투 엔드 구조 대신 모듈식 접근 방식을 따릅니다.\n\n<div class=\"content-ad\"></div>\n\n모든 비텍스트 입력은 처리되기 전에 먼저 텍스트로 변환되어야 합니다: GPT-4 Omni의 핵심은 GPT-4 언어 모델이며, 이 모델은 텍스트 입력을 처리하고 텍스트 응답을 생성합니다. 이 모델은 방대한 텍스트 데이터 코퍼스에서 훈련되어 제공된 문맥을 기반으로 인간과 유사한 텍스트를 이해하고 생성할 수 있습니다.\n\n다음은 이 작업 흐름의 자세한 내용입니다:\n\n- 음성인식 (음성 입력): GPT-4 Omni는 음성 인식 시스템을 사용하여 말로 된 언어를 텍스트로 변환합니다. 이 시스템은 일반적으로 Google의 Speech-to-Text와 같은 고급 모델을 기반으로 합니다. 음성 인식 구성 요소는 음성 단어를 정확하게 전사하기 위해 오디오 데이터와 해당 텍스트 전사본의 방대한 데이터 세트에서 독립적으로 훈련됩니다.\n- 이미지 인식 및 처리: 이미지 입력의 경우 GPT-4 Omni는 외부 이미지 인식 및 처리 시스템에 의존하여 시각 데이터를 언어 모델이 해석할 수 있는 형식으로 변환합니다. 즉, 밀집 벡터(임베딩) 또는 문구 텍스트입니다. 이 두 형식은 모델이 해석할 수 있지만 각각 다른 목적과 영향을 가지고 있습니다. 밀집 벡터 또는 임베딩은 시각 데이터의 숫자 표현입니다. 임베딩은 이미지 분류, 유사성 검색, 특정 유형의 이미지 기반 질문에 대한 답변과 같이 이미지의 추상적인 특징을 이해하고 응답 생성하는 작업에 유용합니다. 이 문구화 과정은 이미지 내용을 자연어 설명으로 번역하여 언어 모델이 직접 해석하고 응답할 수 있도록 합니다. 이 기술은 이미지 내용의 자연어 이해가 필요한 작업에 유용하며, 캡션 생성, 상세한 설명 제공, 이미지의 특정 측면에 대한 질문에 대답하는 등의 작업에 활용됩니다.\n- 텍스트 음성 변환: 출력이 발화형태로 전달되어야 할 경우 GPT-4 Omni는 텍스트-음성(TTS) 시스템을 사용합니다. 이 시스템은 생성된 텍스트 응답을 다시 음성으로 변환합니다. TTS 구성 요소는 텍스트와 해당 발화 형태의 쌍으로 훈련되어 자연스러운 발화 합성을 보장합니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\nGPT-4o의 기술 세부 정보가 공개되길 열심히 기다리고 있습니다. 멀티모달 모델에 대해 전문가가 아니기 때문에 더 알아보기 위해 계획 중입니다. 저와 같이 인간과 비슷한 지능을 모방하려는 연구자들에게 이 정보가 큰 영향을 줄 것으로 믿습니다.","ogImage":{"url":"/assets/img/2024-05-16-TerrifiedMortifiedPetrifiedIsGPT-4oTrulyanEnd-to-EndMulti-ModalModel_0.png"},"coverImage":"/assets/img/2024-05-16-TerrifiedMortifiedPetrifiedIsGPT-4oTrulyanEnd-to-EndMulti-ModalModel_0.png","tag":["Tech"],"readingTime":4},{"title":"Google Colab에서 GPT-4o로 시작하기 단계별 안내","description":"","date":"2024-05-16 17:34","slug":"2024-05-16-GettingStartedwithGPT-4oonGoogleColabAStep-by-StepGuide","content":"\n\n<img src=\"/assets/img/2024-05-16-GettingStartedwithGPT-4oonGoogleColabAStep-by-StepGuide_0.png\" />\n\n2024년 5월 13일에 OpenAI는 최신 AI 모델인 GPT-4o를 선보였습니다. 이 모델은 텍스트와 이미지 모두와의 고급 음성 상호작용 능력을 보여주었습니다. 인공지능(AI)은 다양한 산업을 변화시키고 있으며, OpenAI의 GPT-4o 모델은 이 혁명의 선두주자입니다.\n\n<img src=\"/assets/img/2024-05-16-GettingStartedwithGPT-4oonGoogleColabAStep-by-StepGuide_1.png\" />\n\n본 안내서는 Google Colab에서 GPT-4o를 설정하고 사용하는 방법을 안내해 드립니다. 간단한 수학 문제를 해결하거나 복잡한 텍스트를 생성하거나 이미지를 분석하려는 경우, 이 튜토리얼을 통해 시작할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![image](https://miro.medium.com/v2/resize:fit:1400/0*Ob6Fa-AKD9ZTpBsW.gif)\n\n# 1. 설정\n\n코딩을 시작하기 전에 Google Colab 계정과 OpenAI의 GPT-4o API에 액세스할 수 있는지 확인해주세요. GPT-4o의 환경 설정 및 몇 가지 흥미로운 기능을 탐색하는 단계별 안내서가 여기 있습니다.\n\n## 단계 1: OpenAI Python 패키지 설치\n\n<div class=\"content-ad\"></div>\n\n먼저 OpenAI Python 패키지를 설치해야 합니다. 새로운 Colab 노트북을 열고 다음 명령을 실행하세요:\n\n```js\n!pip install --upgrade openai --quiet\n```\n\n## 단계 2: 라이브러리 가져오기 및 API 키 설정\n\n그 다음으로 필요한 라이브러리를 가져오고 API 키를 설정하세요. Google Colab의 userdata 모듈을 사용하여 API 키를 안전하게 저장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport json\nfrom openai import OpenAI\nimport os\nfrom google.colab import userdata\n\nMODEL = \"gpt-4o\"\n\nclient = OpenAI(api_key=userdata.get('openai'))\n```\n\n## 단계 3: 첫 번째 완성물 생성\n\nGPT-4o가 어떻게 작동하는지 감을 잡기 위해 간단한 완성물을 만들어 봅시다. 모델에 간단한 수학 문제를 해결하도록 요청하겠습니다.\n\n```js\ncompletion = client.chat.completions.create(\n  model=MODEL,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Help me with my math homework!\"},\n    {\"role\": \"user\", \"content\": \"Hello! Could you solve 4+5?\"}\n  ]\n)\n\nprint(\"Assistant: \" + completion.choices[0].message.content)\n```\n\n<div class=\"content-ad\"></div>\n\n## 단계 4: 더 복잡한 질문하기\n\nGPT-4o에게 더 복잡한 질문을 하여 기능을 더 잘 이해할 수 있습니다. 예를 들어, 모델의 기원 및 훈련 세부 정보에 대해 질문할 수 있습니다.\n\n```js\ncompletion = client.chat.completions.create(\n  model=MODEL,\n  messages=[\n    {\"role\": \"user\", \"content\": \"당신의 이름은 무엇이며 누가 만들었나요? 훈련의 종료일은 언제인가요?\"}\n  ]\n)\n\nprint(\"Assistant: \" + completion.choices[0].message.content)\n```\n\n## 2. 함수 호출을 위한 JSON 모드\n\n<div class=\"content-ad\"></div>\n\nGPT-4는 JSON 응답을 생성할 수 있어서 구조화된 데이터와 함수 호출에 유용합니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/0*TyQQvSdTlOenI5YY.gif)\n\n## 단계 1: JSON 응답 생성\n\n주간 운동 루틴을 생성하기 위해 JSON 응답을 만들어 봅시다.\n\n<div class=\"content-ad\"></div>\n\n```js\ncompletion = client.chat.completions.create(\n  model=MODEL,\n  response_format={\"type\": \"json_object\"},\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a trainer who always responds in JSON\"},\n    {\"role\": \"user\", \"content\": \"Create a weekly workout routine for me\"}\n  ]\n)\n\nprint(completion.choices[0].message)\njson.loads(completion.choices[0].message.content)\n```\n\n# 3. Image Understanding\n\nGPT-4o can also understand and process images. We’ll explore how to work with images by encoding them in base64.\n\n![Image](https://miro.medium.com/v2/resize:fit:1080/0*pYZ8nvgtWiL55IhN.gif)\n\n<div class=\"content-ad\"></div>\n\n## 단계 1: 이미지 인코딩\n\n먼저 이미지를 base64로 인코딩하세요.\n\n```js\nfrom IPython.display import Image, display\nimport base64\n\nIMAGE_PATH = \"/content/IMG-20240118-WA0023.jpg\"\n\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nbase64_image = encode_image(IMAGE_PATH)\ndisplay(Image(IMAGE_PATH))\n```\n\n## 단계 2: 이미지 분석\n\n<div class=\"content-ad\"></div>\n\n```js\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"당신은 친절하고 마크다운으로 응답하는 도우미입니다. 수학 숙제를 도와주세요!\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"꽃의 색깔은 무엇인가요?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n        ]}\n    ],\n    temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n## 단계 3: URL 이미지 분석\n\nURL에서 직접 이미지를 분석할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"마크다운으로 응답하는 도움이 되는 도우미에요. 수학 숙제를 도와줄게요!\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"꽃의 색깔은 무엇인가요?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Ranunculus_repens_1_%28cropped%29.JPG/192px-Ranunculus_repens_1_%28cropped%29.JPG\"}\n        ]}\n    ],\n    temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n# 4. Function Calling\n\nGPT-4o는 사용자 입력에 기반하여 미리 정의된 함수를 호출할 수 있어요. 외부 데이터 소스나 서비스를 통합하는 데 특히 유용합니다.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1200/0*YKVvRxnSDMQEWPqb.gif\" />\n\n\n<div class=\"content-ad\"></div>\n\n## 단계 1: 함수 정의\n\nNBA 경기의 현재 점수를 가져오는 함수를 정의하세요.\n\n```js\ndef get_nba_game_score(team):\n    if \"lakers\" in team.lower():\n        return json.dumps({\"team\": \"레이커스\", \"score\": \"102\", \"opponent\": \"워리어스\", \"opponent_score\": \"98\"})\n    elif \"bulls\" in team.lower():\n        return json.dumps({\"team\": \"불스\", \"score\": \"89\", \"opponent\": \"셀틱스\", \"opponent_score\": \"95\"})\n    else:\n        return json.dumps({\"team\": team, \"score\": \"N/A\", \"opponent\": \"N/A\", \"opponent_score\": \"N/A\"})\n```\n\n## 단계 2: 대화 초기화 및 함수 호출\n\n<div class=\"content-ad\"></div>\n\n모델이 이 함수를 호출할 수 있는 대화를 만들어보세요.\n\n```js\ndef function_calling():\n    messages = [{\"role\": \"user\", \"content\": \"레이커스 게임 점수가 어떻게 되나요?\"}]\n\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_nba_game_score\",\n                \"description\": \"주어진 팀의 NBA 게임의 현재 점수를 가져옵니다.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"team\": {\"type\": \"string\", \"description\": \"NBA 팀의 이름, 예: 레이커스, 불스\"},\n                    },\n                    \"required\": [\"team\"],\n                },\n            },\n        }\n    ]\n\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\",\n    )\n\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n\n    if tool_calls:\n        available_functions = {\"get_nba_game_score\": get_nba_game_score}\n        messages.append(response_message)\n\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            function_to_call = available_functions[function_name]\n            function_args = json.loads(tool_call.function.arguments)\n\n            function_response = function_to_call(team=function_args.get(\"team\"))\n\n            messages.append(\n                {\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": function_response}\n            )\n\n        second_response = client.chat.completions.create(\n            model=MODEL,\n            messages=messages,\n        )\n\n        return second_response\n\nprint(function_calling())\n```\n\n축하합니다!\n\n이제 Google Colab에서 GPT-4o를 설정하고 사용하는 방법을 배웠습니다. 이 가이드는 기본 텍스트 완성, JSON 응답, 이미지 처리 및 함수 호출 내용을 다루었습니다. 이러한 기능을 확장하여 다양한 분야의 정교한 AI 응용프로그램을 구축할 수 있습니다. 즐거운 코딩하세요!\n\n<div class=\"content-ad\"></div>\n\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/0*e35njv6_nLGt-8QY.gif)\n","ogImage":{"url":"/assets/img/2024-05-16-GettingStartedwithGPT-4oonGoogleColabAStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-05-16-GettingStartedwithGPT-4oonGoogleColabAStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":8},{"title":"전문 블로그 포스트 스타일로 해석하면 다음과 같습니다 최고의 전투  LLama 3, Claude 3, GPT4 옴니, Gemini 15 Pro-Light 등","description":"","date":"2024-05-16 17:33","slug":"2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore","content":"\n\n모든 것이 적어도 다음 순간까지 조용해 보였으니, 최신 TOP 모델들을 다중 모달리티, 능력, 맥락, 성능 및 가격을 고려하여 비교해봅시다.\n\n## 다중 모달리티\n\n![다중 모달리티 이미지](/assets/img/2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore_0.png)\n\n지금은 이미지가 상용 모델에서 흔하지만 LLama 3를 제외한 모든 모델이 이미지를 가지고 있습니다 (META는 올해 후반에 다중 모달 런치를 약속했습니다).\n\n<div class=\"content-ad\"></div>\n\n금니 1.5 버전과 GPT 4 Omni는 오디오 및 비디오(어떤 면에서는 그 스냅샷)를 처리할 수 있는 기능으로 눈에 띕니다.\n\n그리고 지금은 GPT 4 Omni만이 이러한 모달리티의 모든 기능을 갖췄지만, api에서는 오늘날 사용할 수 없으며 올해 후반에 가능할 예정입니다.\n\n# 문맥 길이\n\n![이미지](/assets/img/2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore_1.png)\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 베타 테스팅을 위해 2M 토큰을 갖춘 Gemini와 나중에 전체 릴리즈를 위한 큰 컨텍스트 창을 갖추었습니다. Claude 3는 1M으로 따르며, GPT 4는 128K로 진행했고, LLama는 8K로 증가했습니다.\n\n실제 사용은 우리가 한계에 도달했을 때 이 컨텍스트 중 얼마나 사용되는지를 알려주는 논문 RULER에 기반한 점수를 보여줍니다. Llama 3이 더 작은 창을 갖고 있지만 매우 최적화된 사용을 보여주는 것이 흥미로운데요.\n\n# 벤치마크\n\n## 텍스트\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore_2.png\" />\n\n모든 점수를 분석해 보면 모두 동일한 수준에 있다는 것을 알 수 있습니다. 이제 GPT4o와 Gemini 1.5 Flash가 모든 추가 기능을 가지고 있음에도 불구하고 매우 빠르게 응답한다는 것은 상당히 인상적입니다. 이는 다른 제품들의 응답 속도에 지연이 발생하는 반면에 잘 작동합니다.\n\n물론, LLama 3도 크기와 비교했을 때 매우 인상적하며 매우 양호한 점수를 얻고 있습니다.\n\n## 비전\n\n<div class=\"content-ad\"></div>\n\n아래는 테이블로 변환된 내용입니다.\n\n\n![](/assets/img/2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore_3.png)\n\n여전히 모두 같은 범위에 있는 것 같아요.\n\n# 가격\n\n![](/assets/img/2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n이 목록은 이러한 모델들의 가격을 나타냅니다. 오늘날 GPT4 Omni는 다른 모델들과 비교하여 독특한 능력을 갖고 있지 않지만 GPT 4 Turbo보다 빠른 버전으로, 시각에 대해서는 Gemini 1.5 및 Claude 3의 대안이 있고 텍스트에 대해서는 해당하는 모든 것이 있습니다.\n\nGPT 4 Omni는 아직 매우 비싸다는 것을 알 수 있습니다. 단지 Claude 3 Opus에 밀리는 것 뿐입니다.\n\n마지막으로 LLama 3, Claude 3 Haiku 및 Gemini 1.5 Flash는 가장 좋은 성능 대비 비용을 가지고 있으며, 중간/간단한 작업을 대부분 수행할 수 있기 때문입니다.\n\n이 데이터 컴필레이션이 솔루션에 대해 더 나은 성능 대비 비용 모델을 선택하는 데 도움이 되기를 바랍니다.\n\n<div class=\"content-ad\"></div>\n\n최근 오픈 소스 모델의 벤치마크를 비교할 계획입니다.\n\n참고: GPT4 Omni의 기술 보고서에 대한 접근이 아직 없어 오디오 및 비디오의 벤치마크를 수행하지 않았습니다. 해당 보고서가 공개되면 분석할 예정입니다.","ogImage":{"url":"/assets/img/2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore_0.png"},"coverImage":"/assets/img/2024-05-16-BattleoftheTOPLLama3Claude3GPT4OmniGemini15Pro-Lightandmore_0.png","tag":["Tech"],"readingTime":3},{"title":"AI의 잠재력을 발휘하다 자동화를 넘어서는 여정","description":"","date":"2024-05-16 17:32","slug":"2024-05-16-UnlockingthePotentialofAIAJourneyBeyondAutomation","content":"\n\n![AI Image](/assets/img/2024-05-16-UnlockingthePotentialofAIAJourneyBeyondAutomation_0.png)\n\n인공지능(AI)은 수년 동안 핫한 키워드로 떠오르며 산업을 혁신하고 일상생활을 변화시킬 것으로 약속되어 왔습니다. 그러나 많은 사람들에게는 이 개념이 흐릿하게 느껴지며 종종 공상과학 서술과 혼동되거나 자동화의 또 다른 파동으로 오해되곤 합니다. 그러나 AI는 인간의 작업을 기계로 대체하는 데 그치지 않습니다. 인간의 잠재력을 증대시키고 혁신을 이끄는 그 새로운 가능성들을 열어주는 것입니다. AI가 어떻게 우리의 세계를 재정립하고 있는지 살펴보고, 그 잠재력을 이해하는 것이 미래를 받아들이기에 왜 중요한지 알아봅시다.\n\n# AI의 진화: 무엇을 보던 이상\n\nAI의 기원은 20세기 중반으로 거슬러 올라가며 앨런 터링(Alan Turing)과 존 맥카시(John McCarthy)와 같은 선구자들이 현재 우리가 보고 있는 것의 기초를 닦았습니다. 초기의 AI 시스템은 한정적이었으며 종종 간단한 규칙 기반 작업에만 제한되어 있었습니다. 현재로 거슬러 올라가보면, AI는 기계 학습과 신경망부터 자연어 처리와 컴퓨터 비전에 이르기까지 다양한 기술 스펙트럼을 포괄하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n현대 AI의 가장 큰 특징은 학습과 적응 능력입니다. 예를 들어, 기계 학습 알고리즘이 시스템이 명시적으로 프로그래밍되지 않은 데이터를 기반으로 성능을 향상시킬 수 있도록 합니다. 이 능력은 정적 문제 해결에서 동적 문제 해결로의 전환으로, AI의 혁신적인 힘의 기초가 됩니다.\n\n# AI의 실용: 현실 세계 응용\n\nAI의 영향은 광범위하며 다양한 분야에 깊은 영향을 미칩니다:\n\n의료: AI는 진단과 치료를 혁신하고 있습니다. 기계 학습 모델은 의학 이미지를 이전에 없던 정확도로 분석할 수 있어 암과 같은 질병의 조기 발견에 도움을 줍니다. 예측 분석은 환자 치료를 관리하고 개인 맞춤 치료 계획을 개선하여 결과를 향상시키고 비용을 절감하는 데 도움을 줍니다.\n\n<div class=\"content-ad\"></div>\n\n**금융:** 금융 산업에서 AI 알고리즘들은 거대한 양의 데이터를 분석하여 사기 거래를 탐지하고, 신용 위험을 평가하며, 거래 전략을 최적화합니다. 로보 어드바이저들은 개인 맞춤형 투자 조언을 제공하여 금융 계획을 대중에게 보다 접근 가능하게 만들어 줍니다.\n\n**교통:** 자율 주행 차량이 교통 분야에서 AI의 가장 두드러진 형태일지도 모릅니다. Tesla와 Waymo와 같은 기업들이 자율 주행 기술을 개척하며 보다 안전하고 효율적인 도로를 약속합니다. AI는 물류 및 공급망을 최적화하여 적시 배송을 보장하고 환경 영향을 줄입니다.\n\n**고객 서비스:** AI 기반 챗봇과 가상 어시스턴트는 24시간 내내 즉각적인 지원을 제공하여 고객 서비스를 향상시킵니다. 이러한 시스템은 루틴 문의를 처리하며, 복잡한 문제에 직면한 인간 요원들이 더 복잡한 문제에 집중할 수 있도록 돕습니다.\n\n# **윤리적 요구사항: 도전에 부딪히다**\n\n<div class=\"content-ad\"></div>\n\n위대한 힘에는 큰 책임이 따르죠. AI의 급속한 발전으로 윤리적 고려사항이 중요시됩니다. 데이터 프라이버시, 알고리즘 편향, 직업 이탈 가능성과 같은 문제들은 선제적으로 다루어져야 합니다.\n\nAI 시스템이 투명하고 책임 있는지 확인하는 것이 중요합니다. 이는 AI 의사 결정 과정을 모니터링하는 강력한 프레임워크를 개발하고 혁신을 억압하지 않으면서 개인 권리를 보호하는 규제를 시행하는 것을 의미합니다. 게다가 AI 개발 팀의 다양성 증진은 편향을 완화하고 AI 솔루션이 공정하고 포용적이 되도록 도와줄 수 있습니다.\n\n## 미래 수용하기: 협력과 교육\n\nAI 주도된 미래로 나아가는 여정은 모두가 함께 하는 것입니다. 정부, 산업, 학계 및 시민 사이의 협력이 AI의 잠재력을 최대한 발휘하기 위해 중요합니다. 혁신을 촉진하고 사회가 가치를 지키는 정책이 지속 가능한 발전의 핵심이 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n교육 또한 중요한 역할을 합니다. AI가 계속 발전함에 따라, 학습과 기술 개발에 대한 우리의 접근도 변해야 합니다. STEM 교육을 촉진하고 스킬 재교육 프로그램에 투자하며 평생 학습 문화를 육성하는 것은 인공지능이 보조된 환경에 대비하여 노동력을 준비하는 데 도움이 될 것입니다.\n\n# 결론: AI의 약속\n\nAI는 기술적 발전 이상이며, 우리에게 생활, 일과 세상과의 상호작용을 다시 생각하게 하는 패러다임 변화입니다. AI를 신중하고 윤리적으로 수용함으로써 우리는 전례없는 기회를 발견하고 인류가 직면한 가장 시급한 과제 중 일부를 해결할 수 있습니다. AI의 미래는 기계가 인간을 대체하는 것이 아니라, 우리의 능력을 향상시키고 함께 어떤 것을 달성해 나갈 수 있는지의 범위를 확장하는 데 있습니다.\n\n이 여정에서 각 개인은 자신의 역할이 있습니다. 기술인, 정책제정자, 교육자 또는 호기심 많은 관찰자이든, AI를 이해하고 참여하는 것이 필수적입니다. 혁신, 책임성, 그리고 더 나은 미래를 형성하기 위한 약속이 공유된 비전으로 나아가도록 합시다.","ogImage":{"url":"/assets/img/2024-05-16-UnlockingthePotentialofAIAJourneyBeyondAutomation_0.png"},"coverImage":"/assets/img/2024-05-16-UnlockingthePotentialofAIAJourneyBeyondAutomation_0.png","tag":["Tech"],"readingTime":3},{"title":"라즈베리 파이에서 도커 이미지를 실행하는 방법","description":"","date":"2024-05-16 17:30","slug":"2024-05-16-HowtorundockerimagesonRaspberryPi","content":"\n\n<img src=\"/assets/img/2024-05-16-HowtorundockerimagesonRaspberryPi_0.png\" />\n\n여기에 긴 소개 단락을 만들려고 했지만, 만약 이 링크를 클릭했다면 도커와 라즈베리 파이에 대해 이미 알고 있을 것 같네요, 맞죠? 그러면 바로 이 기사의 \"어떻게\" 부분을 확인해 볼까요?\n\n간편하게 하기 위해 파이용 기본 운영 체제 인 \"Raspberry Pi OS\" (이전 명칭은 Raspbian)를 사용할 것입니다. 또한 Raspberry Pi 4를 사용할 것입니다. Raspberry Pi 4는 이전 모델보다 성능이 향상되었으며, RAM (1GB, 2GB 또는 4GB) 및 더 빠른 처리 능력의 옵션이 있습니다.\n\n# SD 카드 준비하기\n\n<div class=\"content-ad\"></div>\n\n라즈베리 파이 OS의 최신 버전을 다운로드하려면 다음 단계를 따라주세요:\n\n1. 라즈베리 파이 웹사이트 방문: https://www.raspberrypi.org/ 에서 라즈베리 파이 공식 웹사이트를 방문해주세요.\n\n2. 다운로드 섹션으로 이동: 웹사이트에서 \"다운로드\" 섹션을 찾아주세요. 보통 상단 메뉴나 홈페이지에 있습니다.\n\n3. 라즈베리 파이 OS 선택: 다운로드 섹션 안에서 사용 가능한 운영 체제 목록을 보실 수 있습니다. \"Raspberry Pi OS\" 또는 해당 변형(최소 버전의 경우 \"Raspberry Pi OS Lite\"와 같은)을 찾아주세요.\n\n<div class=\"content-ad\"></div>\n\n4. 에디션 선택: 사용 가능한 다양한 에디션이 있을 수 있습니다. 전체 데스크톱 버전이나 Lite 버전과 같이 선택할 수 있습니다. 당신의 요구에 가장 잘 맞는 것을 선택하세요. 데스크톱 버전에는 그래픽 사용자 인터페이스가 포함되어 있고 Lite 버전은 명령줄만 지원합니다. 저는 대부분의 경우 Lite 버전을 선호합니다.\n\n5. 이미지 다운로드: 원하는 에디션의 다운로드 링크를 클릭하세요. 이렇게 하면 다운로드 프로세스가 시작됩니다.\n\n6. 다운로드 확인 (옵션): Raspberry Pi 웹 사이트에서 제공하는 체크섬을 사용하여 다운로드한 이미지의 무결성을 확인하는 것이 좋은 습관입니다. 이는 다운로드 프로세스 중에 이미지가 손상되지 않았음을 보장합니다.\n\n7. SD 카드 준비: 다운로드가 완료되면, 다운로드한 이미지를 SD 카드에 작성해야 합니다. 이 작업에는 Etcher(https://www.balena.io/etcher/)와 같은 소프트웨어를 사용할 수 있습니다. 소프트웨어가 제공하는 지침에 따라 이미지를 SD 카드에 작성하세요.\n\n<div class=\"content-ad\"></div>\n\nSD 카드를 추출하기 전에 해당 카드에서 SSH를 활성화하세요. 라즈베리파이 재단은 라즈베리파이 전역의 보안을 향상시키고 해킹 위험을 줄이기 위해 예방 조치로 기본 이미지에서 SSH(Secure Shell) 액세스를 비활성화했습니다. 대신 사용자는 \"boot/\" 디렉토리에 \"ssh\"라는 이름의 텍스트 파일을 만듦으로써 SSH를 활성화할 수 있습니다. 이 파일은 비어 있어도 되고 원하는 텍스트를 포함할 수도 있습니다.\n\n![이미지](/assets/img/2024-05-16-HowtorundockerimagesonRaspberryPi_1.png)\n\n8. 라즈베리파이에 SD 카드를 삽입하세요: 이미지를 SD 카드에 기록한 후, 해당 SD 카드를 라즈베리파이의 SD 카드 슬롯에 삽입합니다.\n\n# Docker 설치\n\n<div class=\"content-ad\"></div>\n\n라즈베리 파이를 부팅하면 본주어(avahi) 서비스를 통해 네트워크에서 해당 장치를 찾을 수 있습니다.\n\n## SSH로 연결하기\n\n```shell\n$ ssh pi@raspberrypi.local\n```\n\n비밀번호는 raspberry입니다.\n\n<div class=\"content-ad\"></div>\n\n보안 상의 이유로 passwd 명령어를 사용하여 사용자 pi의 암호를 변경하는 것이 좋습니다.\n\n## Docker 설치 프로그램 시작\n\n![이미지](/assets/img/2024-05-16-HowtorundockerimagesonRaspberryPi_2.png)\n\nDocker 프로젝트에서 유지보수하는 스크립트는 systemd 서비스 파일의 생성을 자동화하고 관련 Docker 이진 파일을 /usr/bin/ 디렉토리로 복사합니다.\n\n<div class=\"content-ad\"></div>\n\n스크립트를 실행하려면 다음 명령을 실행하세요:\n\n```js\ncurl -sSL https://get.docker.com | sh\n```\n\n이전에는 Raspberry Pi에 Docker를 설치하는 것이 수동 프로세스를 필요로 했는데, 종종 처리 능력이 제한된 장치에서 Docker를 처음부터 빌드하는 작업이 필요했고, 이는 몇 시간이 걸릴 수 있었습니다. Hypriot과 같은 ARM 애호가들의 노력 덕분에 Docker의 지속적인 통합(CI) 프로세스에는 이제 .deb 패키지가 지원 옵션으로 포함되어 있습니다.\n\n더 최신 버전을 테스트하고 싶다면 get.docker.com을 test.docker.com으로 교체하여 테스트 버전 사용으로 전환할 수 있습니다. 그러나 이러한 버전에는 아직 해결되지 않은 문제가 일부 남아 있을 수 있음을 감안해주시기 바랍니다.\n\n<div class=\"content-ad\"></div>\n\n# 도커 구성\n\n최상의 경험을 얻기 위해 수행해야 하는 몇 가지 수동 단계가 있습니다.\n\n## 도커 자동 시작 설정\n\n```js\n$ sudo systemctl enable docker\n```\n\n<div class=\"content-ad\"></div>\n\n라즈베리 파이를 다시 시작하거나 다음 명령을 사용하여 도커 데몬을 시작할 수 있습니다:\n\n```js\n$ sudo systemctl start docker\n```\n\n## 도커 클라이언트 활성화\n\n도커 클라이언트를 사용하려면 관리자 권한이 있거나 docker 그룹에 속해 있어야 합니다. \"pi\" 사용자 또는 그와 동일한 권한을 가진 사용자가 docker 그룹에 추가되었는지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n$ sudo usermod -aG docker pi\n```\n\n이 변경 사항을 적용한 후에는 로그 아웃하고 ssh를 통해 다시 연결하십시오.\n\n## ARM 아키텍처 이미지 실행\n\nARM 아키텍처를 사용하는 Raspberry Pi 3 및 Raspberry Pi 4와 같은 모델을 위한 이미지는 대부분 latest로 표시되어 있어 ARM 아키텍처를 지원하도록 제작되어있어 잘 작동해야 합니다. 그러나 ARMv6 아키텍처를 사용하는 이전 모델 (예: Raspberry Pi 1 및 Raspberry Pi Zero)의 경우 호환성이 제한 될 수 있으며 ARMv6를 대상으로한 이미지를 별도로 지정하거나 직접 빌드해야 할 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n도커를 실행할 준비가 되었습니다. 라즈베리 파이를 사용할 수 있습니다. 이제 라즈베리 파이에서 새 이미지를 실행해 보세요. 제 웹 서비스를 라즈베리 파이에 배포하고 집에서 애플리케이션 백엔드를 실행할 예정입니다.\n\n다음 \"어떻게\" 글을 기대해 주세요!\n","ogImage":{"url":"/assets/img/2024-05-16-HowtorundockerimagesonRaspberryPi_0.png"},"coverImage":"/assets/img/2024-05-16-HowtorundockerimagesonRaspberryPi_0.png","tag":["Tech"],"readingTime":4},{"title":"빵을 만들었어요 My First Loaf of Bread을 구웠어요","description":"","date":"2024-05-16 17:29","slug":"2024-05-16-Bread-yorNotHereICrumbIBakedMyFirstLoafofBread","content":"\n\n## DIY\n\n요리를 하는 것을 항상 좋아했어요. 그런데 빵을 처음부터 만드는 건 별로 좋아하지 않았어요. 하지만 십대 아들과 빵 만들기를 통해 더 가까워졌죠. 그 후로 계속 함께 배우고 있어요.\n\n지난 주말, 저는 요리하던 음식과 함께 집에서 빵을 굽고 싶었어요.\n\n그냥 나 혼자 생각해보다가요.\n\n<div class=\"content-ad\"></div>\n\n나는 레시피를 찾아보았는데, 밀가루, 액티브 드라이 이스트, 소금, 설탕, 따뜻한 물, 올리브 오일만 있으면 된다는 걸 보고 너무 흥분했어.\n\n어째서인지, 내 마음 속에선 이 모든 해가 빵을 만드는 데 정말 많은 재료가 필요하다고 생각하고 있었는데, 아니면 몇 년 동안 가게에서 산 빵 봉지 뒷면의 재료 목록을 너무 많이 읽어서 그런 거랬던 걸까.\n\n우리가 가진 재료들을 손에 쥐고 시작했어. 저희에겐 믹서기나 첨단 기기들은 없어 — 그냥 기본적인 그릇, 계량컵, 우드 스푼뿐이야. 아직 특정한 주방 가전제품을 사기 귀찮은지, 아니면 오래된 방법으로 잘 따라가며 대부분을 손으로 하는 게 나을지 고민 중이야.\n\n이 빵을 굽는 일은 정말 시간이 많이 드는 작업이었어. 반죽이 오븐에 넣기 전에 두 번씩 반죽이 부풀어올라야 했어.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-16-Bread-yorNotHereICrumbIBakedMyFirstLoafofBread_0.png)\n\n들고 있는 오븐 안에 신선하게 구워진 빵의 매력적인 향기가 부드럽게 집 안을 스며들었어요. 마치 디즈니 영화 속 장면으로 옮겨간 것 같은 기분이었죠.\n\n따뜻하고 편안한 효모와 껍질의 향기로 가득한 공기 속에서, 제 돼지새 친구 시트린도 즐거운 지저귀며 반죽이 부풀고 구워지는 마법으로 우리 집을 가득 채웠어요.\n\n빵이 구워지는 유혹적인 향기에 우리 엄마도 방에서 나왔답니다.\n\n<div class=\"content-ad\"></div>\n\n\"와우! 여기 너무 좋은 냄새 나요!\" 그녀가 웃으며 말했습니다.\n\n우리는 빵이 구워지는 동안 함께 농담을 주고 웃었습니다.\n\n빵이 완성되자 제가 꺼내어 식히고 나서 잘라 보기 전에 조금 식혔어요.\n\n우리는 모두 버터와 잼이 바른 빵 한 조각을 맛보았고, 우리가 처음으로 구워 본 것으로는 꽤 맛있게 나왔다고 결론내렸어요.\"\n\n<div class=\"content-ad\"></div>\n\n우리는 새로운 시도를 해서 우리의 기대를 뛰어넘는 결과를 얻어 자랑스러웠어요.\n\n이제 제가 마침내 제대로 된 파스타를 만들어보려고 해요.\n\n한편, 만일 집에서 직접 빵을 만들어보고 싶다면 이 레시피를 사용해보세요.\n\n재료:\n\n<div class=\"content-ad\"></div>\n\n- 중력분 4컵 (테이블에 묻힐 때 사용할 추가용도)\n\n- 활성 드라이 이스트 1팩 (7g)\n\n- 소금 1 tsp\n\n- 설탕 1 tbsp\n\n<div class=\"content-ad\"></div>\n\n- 300ml 따뜻한 물\n\n- 올리브 오일 2큰 술 (동그랗게 발라 줄 때 사용할 추가 오일)\n\n조리 방법:\n\n1. 이스트 활성화: 작은 그릇에 설탕을 물에 녹여주세요. 이스트를 물 위에 뿌리고 거품이 날 때까지 5~10분 정도 기다려주세요.\n\n<div class=\"content-ad\"></div>\n\n2. 건조 재료 섞기: 큰 볼에 밀가루와 소금을 섞어주세요. 가운데에 구멍을 만들고 이스트 혼합물과 올리브 오일을 부어주세요.\n\n3. 반죽하기: 나무 숟가락이나 손을 사용하여 재료를 섞어 거친 반죽이 형성될 때까지 섞어주세요. 그런 다음 반죽을 가볍게 풀린 표면에 놓고 탄력이 생길 때까지 약 8~10분 동안 반죽해주세요.\n\n4. 첫 번째 발효: 반죽을 가벼운 오일로 바른 볼에 넣고 깨끗한 부엌 수건으로 덮어 따뜻하고 환기가 잘 되는 곳에서 약 1~1.5시간 동안 2배로 부풀어 오르게 해주세요.\n\n5. 반죽 모양 만들기: 반죽이 부풀어 오르면 공기 벼토를 제거하기 위해 눌러주세요. 풀린 표면에 반죽을 내려놓고 빵 모양이나 로롤을 만들기 위해 작게 나눠주세요.\n\n<div class=\"content-ad\"></div>\n\n6. 두 번째 발효: 모양을 잡은 반죽을 기름칠된 베이킹 시트나 기름칠된 빵 모양 팬에 옮기세요. 부엌 수건으로 덮고 다시 30~45분 정도 더 발효시킵니다. 반죽이 다시 부풀어올 때까지 기다립니다.\n\n7. 오븐 예열: 반죽이 발효되는 동안 오븐을 200°C (400°F)로 예열해주세요.\n\n8. 구워내기: 반죽이 두 번째로 부풀어온 후에 날카로운 칼로 빵 윗면을 베어주세요. 그리고 예열된 오븐에 넣어주세요. 오븐에서 25~30분 동안 (빵 크기에 따라 다름) 구워주세요. 빵이 황금색으로 익으면서 아래를 두드렸을 때 텅 빈 소리가 날 때까지.\n\n9. 식혀서 즐기기: 빵을 오븐에서 꺼내서 철제 랙에 올려 식히고, 적어도 30분 동안 식힌 후 슬라이스하여 제공하세요.","ogImage":{"url":"/assets/img/2024-05-16-Bread-yorNotHereICrumbIBakedMyFirstLoafofBread_0.png"},"coverImage":"/assets/img/2024-05-16-Bread-yorNotHereICrumbIBakedMyFirstLoafofBread_0.png","tag":["Tech"],"readingTime":3}],"page":"76","totalPageCount":151,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}