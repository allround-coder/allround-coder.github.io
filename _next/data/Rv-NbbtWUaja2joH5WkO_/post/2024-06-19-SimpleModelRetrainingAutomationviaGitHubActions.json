{"pageProps":{"post":{"title":"GitHub Actions를 통한 간단한 모델 재학습 자동화","description":"","date":"2024-06-19 23:41","slug":"2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions","content":"\n\n\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_0.png)\n\n비즈니스에 엄청난 가치를 창출할 수 있는 것은 기계 학습 모델입니다. 그러나 이를 개발하는 것은 일회성 활동이 아닙니다. 대신 모델이 계속 가치를 제공할 수 있도록 지속적인 프로세스여야 합니다. 이것이 MLOps가 나오게 된 이유입니다.\n\nCI/CD 원칙과 기계 학습 개발을 결합한 것을 MLOps라고 합니다. 이를 통해 모델이 지속적인 가치를 제공할 수 있도록 합니다.\n\n기계 학습 모델이 지속적인 이점을 제공하는 한 가지 방법은 필요할 때 재학습하는 것입니다. 예를 들어, 데이터 드리프트가 감지될 경우 모델을 재학습하는 것입니다. 모델 재학습 자동화를 위해 재학습 트리거의 환경을 설정하여 수행할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\nGitHub Actions는 GitHub에서 제공하는 기능으로, CI/CD 플랫폼에 사용되며 GitHub 저장소에서 소프트웨어 개발 프로세스를 자동화하는 데 사용됩니다.\n\n이 기사에서는 GitHub Actions를 사용하여 모델 재학습을 자동화하는 방법을 가르쳐 드립니다. 그 방법을 알아볼까요?\n\n# 준비\n\n이 프로젝트에서는 모델 개발 및 자동화 데모를 수행할 것입니다. 전체 프로젝트 구조는 아래 차트와 같을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_1.png)\n\n일단 GitHub Actions을 이 리포지토리에서 사용할 수 있도록 GitHub 리포지토리를 준비하는 것부터 시작해봅시다. 선호하는 이름으로 빈 리포지토리를 만들 수 있습니다. 저는 이 리포지토리를 만들었어요.\n\n추가로, Docker를 사용하여 모델을 배포하는 것을 시뮬레이션해볼 거예요. 이를 위해 Docker Desktop을 설치해봅시다. 그리고 Dockerhub에 가입하지 않은 경우에는 가입하세요.\n\n그런 다음, Repo 및 Workflow 범위를 지닌 GitHub 개인 액세스 토큰(PAT)을 생성해봅시다. 토큰을 어딘가에 보관하고, 방금 만든 빈 리포지토리로 돌아가봅시다. 설정으로 이동하여 \"비밀 값 및 변수\"를 선택합니다. 그런 다음, PAT, Docker 사용자 이름 및 Docker 비밀번호를 포함하는 리포지토리 비밀값을 생성하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_2.png)\n\nGitHub 저장소를 로컬 또는 작업하는 플랫폼에 복제하세요. 준비가 되었으면 튜토리얼 전체 구조를 준비해 봅시다. 좋아하는 IDE에서 다음과 같이 폴더를 생성하세요.\n\n```js\ndiabetes-project/\n├── data/\n├── notebooks/\n├── scripts/\n├── models/\n├── .github/\n│   └── workflows/\n``` \n\n폴더가 갖춰지면 가상 환경을 설정합니다. 고립된 환경을 원하기 때문에 이는 좋은 관행입니다. 루트 폴더로 이동하여 다음 CLI 코드를 사용하세요.\n\n\n<div class=\"content-ad\"></div>\n\n```js\npython -m venv your_environment_name\n```\n\n가상 환경을 활성화하려면 아래 코드를 실행하세요.\n\n```js\nyour_environment_name\\Scripts\\activate\n```\n\n가상 환경을 활성화한 후, 튜토리얼을 위해 필요한 모든 패키지를 설치할 것입니다. 루트 폴더에 requirements.txt 파일을 생성하고 아래 패키지를 채워넣어주세요.\n\n<div class=\"content-ad\"></div>\n\n```js\nfastapi\nuvicorn\npandas\nscikit-learn\nmatplotlib\nseaborn\nevidently\n```\n\n요구 사항이 준비되면 가상 환경에 패키지를 설치할 것입니다.\n\n```js\npip install -r requirements.txt\n```\n\n모든 준비가 완료되었으므로, 이제 모델을 개발하고 모델 재학습 자동화를 시작할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 모델 개발\n\n이 튜토리얼에서는 공개 도메인 데이터 세트인 Open-Source Diabetes 데이터 세트를 사용할 것입니다. 데이터 세트를 다운로드하여 Data 폴더에 넣어주세요. 저는 데이터 세트를 data.csv로 이름을 변경했지만, 원하시는 이름으로 변경하셔도 됩니다.\n\n우리는 주피터 노트북에서 초기 모델 개발을 진행할 것입니다. 노트북을 만들어서 notebooks 폴더에 넣으세요. 그런 다음, 데이터 세트를 읽어오는 것부터 시작해보겠습니다.\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata_path = '..//data//data.csv'\n\ndf = pd.read_csv(data_path)\n```\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 GitHub Actions 능력을 자동으로 재교육하는 데 초점을 맞추기로 했어요. 데이터 탐색 이외의 것에 초점을 맞출 거예요. 노트북에서 데이터 탐색 부분을 포함했으니, 확인하고 싶다면 방문해주세요.\n\n이제 데이터 전처리와 파이프라인 시작으로 넘어갈게요. 데이터 파이프라인을 사용하여 표준 개발 프로세스를 모방할 거예요.\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nnumeric_features = X.columns\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features)\n    ])\n```\n\n파이프라인이 준비되면 머신 러닝 모델로 랜덤 포레스트 알고리즘을 사용할 거예요. 다른 목적에 맞는 다른 모델을 선택할 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\npipeline.fit(X_train, y_train)\r\n```\n\n모델을 평가하고 성능을 확인해봐요.\n\n```js\r\nfrom sklearn.metrics import classification_report\n\ny_pred = pipeline.predict(X_test)\n\n# 모델 평가\nreport = classification_report(y_test, y_pred)\nprint(report)\r\n```\n\n<img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n전반적으로 성능은 만족스러운 수준입니다. 더 나아질 여지는 있지만, 현재 모델을 유지하고 이를 모델 폴더에 저장하겠습니다.\n\n```js\nimport pickle\n\nwith open('..//models//pipeline.pkl', 'wb') as f:\n    pickle.dump(pipeline, f)\n```\n\n모델이 완성되면 우리는 프로덕션 환경에 배포할 것입니다. 이를 API로 배포하고 모델을 컨테이너화하기 위해 Docker를 사용할 것입니다.\n\n모델을 API로 배포하기 위해 app.py라는 파일을 생성하여 스크립트 폴더에 저장해 봅시다. 파일 내부에 다음 코드를 사용하여 모델을 API로 만들 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport pickle\nimport pandas as pd\n\napp = FastAPI()\n\ncolumns = ['임신횟수', '글루코스', '혈압', \n'피하지방', '인슐린', 'BMI', '당뇨위계보이지DNA', '나이']\n\ndict_res = {0: '당뇨가 아님', 1: '당뇨'}\n\npipeline_path = 'models/pipeline.pkl'\nwith open(pipeline_path, 'rb') as pipeline_file:\n    pipeline = pickle.load(pipeline_file)\n\nclass DataInput(BaseModel):\n    data: list\n\n@app.post(\"/predict\")\nasync def predict(input_data: DataInput):\n    try:\n        df = pd.DataFrame(input_data.data, columns=columns)\n        predictions = pipeline.predict(df)\n        results = [dict_res[pred] for pred in predictions]\n    \n        return {\"예측결과\": results}\n    \n    except Exception as e:\n        print(\"에러:\", str(e))\n        raise HTTPException(status_code=400, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n모델 API에 액세스할 수 있는지 테스트해 봅시다. 먼저, 응용 프로그램을 시작하려면 CLI에서 다음 코드를 실행합니다.\n\n```js\nuvicorn scripts.app:app --host 0.0.0.0 --port 8000\n```\n\n그런 다음, Jupyter Notebook에서 다음 코드를 실행하여 API를 테스트합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport requests\n\nurl = \"http://localhost:8000/predict\"\n\ndata = {\n    \"data\": [\n        [1, 85, 66, 29, 0, 26.6, 0.351, 31]\n    ]\n}\n\nresponse = requests.post(url, json=data)\nprint(response.json())\n```\n\nAPI에 전달하는 데이터의 위치가 훈련 데이터와 동일한지 확인해주세요. API가 잘 작동하면 Docker 이미지를 빌드하고 허브에 푸시할 것입니다.\n\n먼저 루트 폴더에 dockerfile을 생성해주세요. 해당 파일에 다음 코드를 채워넣어주세요.\n\n```js\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY scripts/app.py app.py\nCOPY models models\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n<div class=\"content-ad\"></div>\n\n위 코드에서는 Python 환경을 설정하고 API를 실행하는 데 필요한 파일을 컨테이너로 복사하여 포트 8000에서 수신 대기하는 방법을 안내합니다.\n\nDockerfile로 이미지를 빌드하는 방법은 준비가 되면 다음 코드를 사용하면 됩니다.\n\n```js\ndocker build -t username/image_name -f Dockerfile .\ndocker login -u username\ndocker push username/image_name:latest\n```\n\n위 코드에서 username을 귀하의 Dockerhub 사용자명으로, image_name을 선호하는 응용 프로그램 이름으로 변경해주세요. 성공한다면, 내 것과 같이 Dockerhub에 귀하의 이미지가 표시될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그래서 우리가 모델 API를 Docker에 넣고 Dockerhub에 푸시한 이유는 무엇인가요? 이것은 응용 프로그램을 실행할 모든 환경에서 일관성을 보장하기 때문입니다.\n\n또한 GitHub Actions가 다음 섹션에서 모델을 다시 학습하고 다시 이 컨테이너로 푸시하는 데 얼마나 강력한지를 보여줍니다. 따라서 우리는 모델을 배포하기 위해 이미지를 가져오기만 하면 됩니다.\n\n아래 코드를 실행하여 Dockerhub에서 이미지를 가져와 로컬 환경에서 실행해보세요.\n\n```js\ndocker login -u username\ndocker pull username/image_name:latest\ndocker run -d -p 8000:8000 username/image_name:latest\n```\n\n<div class=\"content-ad\"></div>\n\n지금까지 제품 모델이 운영 중입니다. 다음 부분에서는 GitHub Actions를 사용하여 모델을 특정 트리거로 다시 학습하는 방법을 살펴보겠습니다.\n\n# GitHub Actions를 활용한 모델 재학습\n\n제가 언급했듯이, 머신 러닝 모델은 지속적인 프로젝트입니다. 이를 통해 어떤 가치를 제공하려면 중요한데, 왜냐하면 모델이 항상 동일한 품질을 유지할 것으로 기대하기 어렵기 때문입니다. 특히, 드리프트가 발생하면 더욱 그렇습니다.\n\n이 튜토리얼에서는 운영 데이터셋에서 데이터 드리프트가 감지될 때 자동으로 모델 재학습을 수행하는 방법을 배우겠습니다. 먼저, 데이터셋에서 드리프트를 감지하는 방법을 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 코드를 사용하여 데이터셋에서 drift를 시뮬레이션해 보겠습니다.\n\n```js\nimport numpy as np\n\ndef introduce_drift(data, drift_features, drift_amount=0.1, random_seed=42):\n    np.random.seed(random_seed)\n    drifted_data = data.copy()\n    \n    for feature in drift_features:\n        if feature in data.columns:\n            drifted_data[feature] += np.random.normal(loc=0, scale=drift_amount, size=data.shape[0])\n    \n    return drifted_data\n    \nfeatures_to_drift = ['Glucose', 'BloodPressure', 'SkinThickness', 'Pregnancies']\n\ndrifted_data = introduce_drift(X_test, features_to_drift, drift_amount=50)\ndrifted_data = drifted_data.reset_index(drop=True)\n```\n\n위 코드에서는 Test 데이터의 일부 열을 drift했습니다. drift_amount를 조절하여 데이터가 얼마나 변하는지 제어할 수 있습니다.\n\n튜토리얼에는 학습 데이터(참조)와 drift 데이터(신규)가 필요합니다. 나중에 다시 학습할 때 사용할 타겟 열도 저장해두는 것이 좋습니다.\n\n<div class=\"content-ad\"></div>\n\n\nreference_data['Outcome'] = y_train.reset_index(drop=True)\r\ndrifted_data['Outcome'] = y_test.reset_index(drop=True)\r\n\r\ndrifted_data.to_csv('..//data//new_data.csv', index=False)\r\nreference_data.to_csv('..//data//reference_data.csv', index=False)\r\n\n\r\nEvidently(제가 Evidently와 어떤 제휴도 없습니다)를 사용하여 제품 데이터가 참조 데이터에 비해 드리프트했는지 확인할 수 있습니다. 다음 코드로 확인할 수 있습니다.\r\n\r\n```python\r\nfrom evidently.metric_preset import DataDriftPreset\r\nfrom evidently.report import Report\r\n\r\ndata_drift_report = Report(metrics=[DataDriftPreset()])\r\n\r\ndata_drift_report.run(current_data=drifted_data.drop('Outcome', axis=1), \r\nreference_data=reference_data.drop('Outcome', axis=1), column_mapping=None)\r\nreport_json = data_drift_report.as_dict()\r\ndrift_detected = report_json['metrics'][0]['result']['dataset_drift']\r\n```\r\n\r\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_4.png)\r\n\n\n<div class=\"content-ad\"></div>\n\n결과에는 나중에 재훈련 자동화를 위해 사용할 드리프트 데이터 세트가 나타납니다.\n\n드리프트 감지를 시뮬레이션하기 위해 drift_detection.py라는 파일을 만들어서 스크립트 폴더에 저장해보겠습니다. 아래 코드를 파일에 채워 넣어주세요.\n\n```js\nimport pandas as pd\nfrom evidently.metric_preset import DataDriftPreset\nfrom evidently.report import Report\n\nreference_data = pd.read_csv('data/reference_data.csv')\nnew_data = pd.read_csv('data/new_data.csv')\n\ndata_drift_report = Report(metrics=[\n    DataDriftPreset()\n])\n\ndata_drift_report.run(reference_data=reference_data.drop('Outcome', axis=1), \n                      current_data=new_data.drop('Outcome', axis=1), column_mapping=None)\n\nreport_json = data_drift_report.as_dict()\ndrift_detected = report_json['metrics'][0]['result']['dataset_drift']\n\nif drift_detected:\n    print(\"데이터 드리프트가 감지되었습니다. 모델을 재훈련합니다.\")\n    with open('drift_detected.txt', 'w') as f:\n        f.write('drift_detected')\nelse:\n    print(\"데이터 드리프트를 감지하지 못했습니다.\")\n    with open('drift_detected.txt', 'w') as f:\n        f.write('no_drift')\n```\n\n위 코드에서 우리는 드리프트 감지 여부를 drift_detected.txt 파일에 저장하고, 드리프트가 감지되었는지 여부에 따라 정보를 출력합니다. 드리프트가 감지된 경우, 모델을 재훈련하고 싶습니다. 이에 대비하여 훈련 스크립트를 준비해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n스크립트 폴더에 train_model.py라는 파일을 만들고 다음 코드로 채워주세요.\n\n```python\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport pickle\n\nreference_data = pd.read_csv('data/reference_data.csv')\nnew_data = pd.read_csv('data/new_data.csv')\n\ndf= pd.concat([reference_data, new_data], ignore_index=True)\n\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nnumeric_features = X.columns\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features)\n    ])\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\npipeline.fit(X, y)\n\nwith open('models/pipeline.pkl', 'wb') as f:\n    pickle.dump(pipeline, f)\n```\n\n위의 코드는 학습 및 드리프트 데이터를 새로운 학습 모델로 결합하고, 이를 사용하여 새 모델을 학습하는 것입니다. 이는 간단한 접근 방식일 뿐이며, 실제 세계의 학습 데이터는 더 많은 준비가 필요하고, 새 모델은 적절한 평가가 필요합니다.\n\n그러나 모든 스크립트가 준비되면, GitHub Actions를 통해 드리프트가 감지될 때 모델을 재학습할 수 있도록 준비할 것입니다. 재학습에 필요한 모든 구성을 포함하는 YAML 파일을 준비해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n그래서, .github\\workflows 폴더에 mlops_pipeline.yml 파일을 생성해봅시다. 폴더 이름이 제대로 되었는지 확인하세요; GitHub Actions은 적절한 이름이 필요합니다. 아래의 코드로 mlops_pipeline.yml을 채워넣어주세요.\n\n```js\nname: Diabetes Retraining Pipeline with Data Drift Detection\n\non:\n  push:\n    paths:\n      - 'data/new_data.csv'\npermissions:\n  contents: write\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v2 \n      with:\n        python-version: 3.9\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run data drift detection\n      run: |\n        python scripts/drift_detection.py\n      continue-on-error: true \n\n    - name: Check for data drift\n      id: check_drift\n      run: |\n        if grep -q 'drift_detected' drift_detected.txt; then\n          echo \"Data drift detected.\"\n          echo \"drift=true\" >> $GITHUB_ENV\n        else\n          echo \"No data drift detected.\"\n          echo \"drift=false\" >> $GITHUB_ENV\n        fi\n      shell: bash\n\n    - name: Model Retraining if Data Drift detected\n      if: env.drift == 'true'\n      run: |\n        python scripts/train_model.py\n\n    - name: Commit and push updated model\n      if: env.drift == 'true'\n      env:\n        GIT_COMMITTER_NAME: github-actions\n        GIT_COMMITTER_EMAIL: github-actions@github.com\n      run: |\n        git config --global user.name \"github-actions\"\n        git config --global user.email \"github-actions@github.com\"\n        git remote set-url origin https://x-access-token:${ secrets.ACTIONS_PAT }@github.com/username/image_name.git\n        git add models/pipeline.pkl\n        git commit -m \"Update model after retraining on $(date -u +'%Y-%m-%d %H:%M:%S UTC')\"\n        git push\n\n    - name: Build Docker image\n      if: env.drift == 'true'\n      run: |\n        docker build -t username/image_name -f dockerfile .\n\n    - name: Log in to Docker Hub\n      if: env.drift == 'true'\n      run: echo \"${ secrets.DOCKER_PASSWORD }\" | docker login -u \"${ secrets.DOCKER_USERNAME }\" --password-stdin\n\n    - name: Push Docker image to Docker Hub\n      if: env.drift == 'true'\n      run: |\n        docker push username/image_name:latest\n\n    - name: Notify about the process\n      run: |\n        if [[ \"$GITHUB_ENV\" == *\"drift=false\"* ]]; then\n          echo \"No data drift detected. No retraining necessary.\"\n        else\n          echo \"Data drift detected. Model retrained and deployed.\"\n        fi\n      shell: bash\n```\n\n위의 YAML에서 수행한 전체 설정 구조는 아래 이미지에 나와 있습니다.\n\n![Simple Model Retraining Automation via GitHub Actions](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_5.png)\n\n\n<div class=\"content-ad\"></div>\n\nGitHub Actions에서 사용하는 트리거는 data 폴더 안에 new_data.csv 파일이 푸시될 때입니다. 그러나 모델 재학습은 drift가 감지될 때만 실행됩니다. 모델을 다시 훈련한 후, 해당 모델을 GitHub 저장소와 Docker Hub에 다시 푸시할 것입니다.\n\n사용자명/image_name Docker 식별자를 꼭 자신의 것으로 변경해주세요. 동일한 식별자를 사용하는 경우 Repository Secrets를 생성할 수도 있습니다.\n\n모든 파일이 준비되면 GitHub 저장소에 푸시해야 합니다. 그런 다음 새로운 drift 데이터를 생성하여 new_data.csv로 저장하고, 해당 데이터를 저장소에 다시 푸시해보세요.\n\nGitHub 저장소의 Actions 탭으로 이동해주세요. 성공적으로 실행되었다면, 'Success' 상태를 가진 'build'라는 작업이 하나 표시될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_6.png)\n\n작업을 클릭하여 프로세스의 모든 세부 정보를 확인할 수 있습니다. 각 단계의 정보를 확인하여 프로세스를 이해하거나 실행에 실패했는지 확인할 수 있습니다.\n\n![image](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_7.png)\n\n저장소의 모델로 이동하면 모델이 업데이트되었는지 확인할 수 있습니다. 모델을 다시 학습했을 때 커밋 메시지를 사용하여 알림을 받습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_8.png)\n\n도커 허브 저장소를 확인해서 이미지가 업데이트되었는지도 확인할 수 있어요.\n\n여기까지면 GitHub Actions를 사용해서 모델 재교육 프로세스를 간단히 할 수 있어요. 여러분이 원하는대로 스크립트를 조정할 수 있어요. 예를 들어 트리거, 재교육 조건, 데이터셋 등을 말이에요.\n\n이 글에서 사용한 코드들이 필요하면, 해당 레포지토리에 푸시해 놓았어요.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 글에서는 GitHub Actions를 사용하여 모델 재학습 프로세스를 자동화하는 방법을 배웠습니다. YAML 파일을 통해 구성을 설정하고 트리거를 결정함으로써 GitHub Actions를 사용하여 필요한 모든 프로세스를 간편하게 처리할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_0.png"},"coverImage":"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_0.png","tag":["Tech"],"readingTime":16},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_0.png\" alt=\"이미지\"></p>\n<p>비즈니스에 엄청난 가치를 창출할 수 있는 것은 기계 학습 모델입니다. 그러나 이를 개발하는 것은 일회성 활동이 아닙니다. 대신 모델이 계속 가치를 제공할 수 있도록 지속적인 프로세스여야 합니다. 이것이 MLOps가 나오게 된 이유입니다.</p>\n<p>CI/CD 원칙과 기계 학습 개발을 결합한 것을 MLOps라고 합니다. 이를 통해 모델이 지속적인 가치를 제공할 수 있도록 합니다.</p>\n<p>기계 학습 모델이 지속적인 이점을 제공하는 한 가지 방법은 필요할 때 재학습하는 것입니다. 예를 들어, 데이터 드리프트가 감지될 경우 모델을 재학습하는 것입니다. 모델 재학습 자동화를 위해 재학습 트리거의 환경을 설정하여 수행할 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<p>GitHub Actions는 GitHub에서 제공하는 기능으로, CI/CD 플랫폼에 사용되며 GitHub 저장소에서 소프트웨어 개발 프로세스를 자동화하는 데 사용됩니다.</p>\n<p>이 기사에서는 GitHub Actions를 사용하여 모델 재학습을 자동화하는 방법을 가르쳐 드립니다. 그 방법을 알아볼까요?</p>\n<h1>준비</h1>\n<p>이 프로젝트에서는 모델 개발 및 자동화 데모를 수행할 것입니다. 전체 프로젝트 구조는 아래 차트와 같을 것입니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_1.png\" alt=\"이미지\"></p>\n<p>일단 GitHub Actions을 이 리포지토리에서 사용할 수 있도록 GitHub 리포지토리를 준비하는 것부터 시작해봅시다. 선호하는 이름으로 빈 리포지토리를 만들 수 있습니다. 저는 이 리포지토리를 만들었어요.</p>\n<p>추가로, Docker를 사용하여 모델을 배포하는 것을 시뮬레이션해볼 거예요. 이를 위해 Docker Desktop을 설치해봅시다. 그리고 Dockerhub에 가입하지 않은 경우에는 가입하세요.</p>\n<p>그런 다음, Repo 및 Workflow 범위를 지닌 GitHub 개인 액세스 토큰(PAT)을 생성해봅시다. 토큰을 어딘가에 보관하고, 방금 만든 빈 리포지토리로 돌아가봅시다. 설정으로 이동하여 \"비밀 값 및 변수\"를 선택합니다. 그런 다음, PAT, Docker 사용자 이름 및 Docker 비밀번호를 포함하는 리포지토리 비밀값을 생성하세요.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_2.png\" alt=\"이미지\"></p>\n<p>GitHub 저장소를 로컬 또는 작업하는 플랫폼에 복제하세요. 준비가 되었으면 튜토리얼 전체 구조를 준비해 봅시다. 좋아하는 IDE에서 다음과 같이 폴더를 생성하세요.</p>\n<pre><code class=\"hljs language-js\">diabetes-project/\n├── data/\n├── notebooks/\n├── scripts/\n├── models/\n├── .<span class=\"hljs-property\">github</span>/\n│   └── workflows/\n</code></pre>\n<p>폴더가 갖춰지면 가상 환경을 설정합니다. 고립된 환경을 원하기 때문에 이는 좋은 관행입니다. 루트 폴더로 이동하여 다음 CLI 코드를 사용하세요.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">python -m venv your_environment_name\n</code></pre>\n<p>가상 환경을 활성화하려면 아래 코드를 실행하세요.</p>\n<pre><code class=\"hljs language-js\">your_environment_name\\<span class=\"hljs-title class_\">Scripts</span>\\activate\n</code></pre>\n<p>가상 환경을 활성화한 후, 튜토리얼을 위해 필요한 모든 패키지를 설치할 것입니다. 루트 폴더에 requirements.txt 파일을 생성하고 아래 패키지를 채워넣어주세요.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">fastapi\nuvicorn\npandas\nscikit-learn\nmatplotlib\nseaborn\nevidently\n</code></pre>\n<p>요구 사항이 준비되면 가상 환경에 패키지를 설치할 것입니다.</p>\n<pre><code class=\"hljs language-js\">pip install -r requirements.<span class=\"hljs-property\">txt</span>\n</code></pre>\n<p>모든 준비가 완료되었으므로, 이제 모델을 개발하고 모델 재학습 자동화를 시작할 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<h1>모델 개발</h1>\n<p>이 튜토리얼에서는 공개 도메인 데이터 세트인 Open-Source Diabetes 데이터 세트를 사용할 것입니다. 데이터 세트를 다운로드하여 Data 폴더에 넣어주세요. 저는 데이터 세트를 data.csv로 이름을 변경했지만, 원하시는 이름으로 변경하셔도 됩니다.</p>\n<p>우리는 주피터 노트북에서 초기 모델 개발을 진행할 것입니다. 노트북을 만들어서 notebooks 폴더에 넣으세요. 그런 다음, 데이터 세트를 읽어오는 것부터 시작해보겠습니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> seaborn <span class=\"hljs-keyword\">as</span> sns\n<span class=\"hljs-keyword\">import</span> matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt\n\ndata_path = <span class=\"hljs-string\">'..//data//data.csv'</span>\n\ndf = pd.read_csv(data_path)\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>이 기사에서는 GitHub Actions 능력을 자동으로 재교육하는 데 초점을 맞추기로 했어요. 데이터 탐색 이외의 것에 초점을 맞출 거예요. 노트북에서 데이터 탐색 부분을 포함했으니, 확인하고 싶다면 방문해주세요.</p>\n<p>이제 데이터 전처리와 파이프라인 시작으로 넘어갈게요. 데이터 파이프라인을 사용하여 표준 개발 프로세스를 모방할 거예요.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> sklearn.model_selection <span class=\"hljs-keyword\">import</span> train_test_split\n<span class=\"hljs-keyword\">from</span> sklearn.pipeline <span class=\"hljs-keyword\">import</span> Pipeline\n<span class=\"hljs-keyword\">from</span> sklearn.compose <span class=\"hljs-keyword\">import</span> ColumnTransformer\n<span class=\"hljs-keyword\">from</span> sklearn.impute <span class=\"hljs-keyword\">import</span> SimpleImputer\n<span class=\"hljs-keyword\">from</span> sklearn.preprocessing <span class=\"hljs-keyword\">import</span> StandardScaler\n\nX = df.drop(<span class=\"hljs-string\">'Outcome'</span>, axis=<span class=\"hljs-number\">1</span>)\ny = df[<span class=\"hljs-string\">'Outcome'</span>]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"hljs-number\">0.2</span>, random_state=<span class=\"hljs-number\">42</span>)\n\nnumeric_features = X.columns\nnumeric_transformer = Pipeline(steps=[\n    (<span class=\"hljs-string\">'imputer'</span>, SimpleImputer(strategy=<span class=\"hljs-string\">'mean'</span>)),\n    (<span class=\"hljs-string\">'scaler'</span>, StandardScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (<span class=\"hljs-string\">'num'</span>, numeric_transformer, numeric_features)\n    ])\n</code></pre>\n<p>파이프라인이 준비되면 머신 러닝 모델로 랜덤 포레스트 알고리즘을 사용할 거예요. 다른 목적에 맞는 다른 모델을 선택할 수도 있어요.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> sklearn.<span class=\"hljs-property\">ensemble</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">RandomForestClassifier</span>\n\npipeline = <span class=\"hljs-title class_\">Pipeline</span>(steps=[\n    (<span class=\"hljs-string\">'preprocessor'</span>, preprocessor),\n    (<span class=\"hljs-string\">'classifier'</span>, <span class=\"hljs-title class_\">RandomForestClassifier</span>(random_state=<span class=\"hljs-number\">42</span>))\n])\n\npipeline.<span class=\"hljs-title function_\">fit</span>(X_train, y_train)\n</code></pre>\n<p>모델을 평가하고 성능을 확인해봐요.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> sklearn.<span class=\"hljs-property\">metrics</span> <span class=\"hljs-keyword\">import</span> classification_report\n\ny_pred = pipeline.<span class=\"hljs-title function_\">predict</span>(X_test)\n\n# 모델 평가\nreport = <span class=\"hljs-title function_\">classification_report</span>(y_test, y_pred)\n<span class=\"hljs-title function_\">print</span>(report)\n</code></pre>\n<img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_3.png\">\n<div class=\"content-ad\"></div>\n<p>전반적으로 성능은 만족스러운 수준입니다. 더 나아질 여지는 있지만, 현재 모델을 유지하고 이를 모델 폴더에 저장하겠습니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> pickle\n\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-title function_\">open</span>(<span class=\"hljs-string\">'..//models//pipeline.pkl'</span>, <span class=\"hljs-string\">'wb'</span>) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">f</span>:\n    pickle.<span class=\"hljs-title function_\">dump</span>(pipeline, f)\n</code></pre>\n<p>모델이 완성되면 우리는 프로덕션 환경에 배포할 것입니다. 이를 API로 배포하고 모델을 컨테이너화하기 위해 Docker를 사용할 것입니다.</p>\n<p>모델을 API로 배포하기 위해 app.py라는 파일을 생성하여 스크립트 폴더에 저장해 봅시다. 파일 내부에 다음 코드를 사용하여 모델을 API로 만들 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> fastapi <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">FastAPI</span>, <span class=\"hljs-title class_\">HTTPException</span>\n<span class=\"hljs-keyword\">from</span> pydantic <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">BaseModel</span>\n<span class=\"hljs-keyword\">import</span> pickle\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n\napp = <span class=\"hljs-title class_\">FastAPI</span>()\n\ncolumns = [<span class=\"hljs-string\">'임신횟수'</span>, <span class=\"hljs-string\">'글루코스'</span>, <span class=\"hljs-string\">'혈압'</span>, \n<span class=\"hljs-string\">'피하지방'</span>, <span class=\"hljs-string\">'인슐린'</span>, <span class=\"hljs-string\">'BMI'</span>, <span class=\"hljs-string\">'당뇨위계보이지DNA'</span>, <span class=\"hljs-string\">'나이'</span>]\n\ndict_res = {<span class=\"hljs-number\">0</span>: <span class=\"hljs-string\">'당뇨가 아님'</span>, <span class=\"hljs-number\">1</span>: <span class=\"hljs-string\">'당뇨'</span>}\n\npipeline_path = <span class=\"hljs-string\">'models/pipeline.pkl'</span>\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-title function_\">open</span>(pipeline_path, <span class=\"hljs-string\">'rb'</span>) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">pipeline_file</span>:\n    pipeline = pickle.<span class=\"hljs-title function_\">load</span>(pipeline_file)\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">DataInput</span>(<span class=\"hljs-title class_\">BaseModel</span>):\n    <span class=\"hljs-attr\">data</span>: list\n\n@app.<span class=\"hljs-title function_\">post</span>(<span class=\"hljs-string\">\"/predict\"</span>)\n<span class=\"hljs-keyword\">async</span> def <span class=\"hljs-title function_\">predict</span>(<span class=\"hljs-attr\">input_data</span>: <span class=\"hljs-title class_\">DataInput</span>):\n    <span class=\"hljs-attr\">try</span>:\n        df = pd.<span class=\"hljs-title class_\">DataFrame</span>(input_data.<span class=\"hljs-property\">data</span>, columns=columns)\n        predictions = pipeline.<span class=\"hljs-title function_\">predict</span>(df)\n        results = [dict_res[pred] <span class=\"hljs-keyword\">for</span> pred <span class=\"hljs-keyword\">in</span> predictions]\n    \n        <span class=\"hljs-keyword\">return</span> {<span class=\"hljs-string\">\"예측결과\"</span>: results}\n    \n    except <span class=\"hljs-title class_\">Exception</span> <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">e</span>:\n        <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"에러:\"</span>, <span class=\"hljs-title function_\">str</span>(e))\n        raise <span class=\"hljs-title class_\">HTTPException</span>(status_code=<span class=\"hljs-number\">400</span>, detail=<span class=\"hljs-title function_\">str</span>(e))\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    <span class=\"hljs-keyword\">import</span> uvicorn\n    uvicorn.<span class=\"hljs-title function_\">run</span>(app, host=<span class=\"hljs-string\">\"0.0.0.0\"</span>, port=<span class=\"hljs-number\">8000</span>)\n</code></pre>\n<p>모델 API에 액세스할 수 있는지 테스트해 봅시다. 먼저, 응용 프로그램을 시작하려면 CLI에서 다음 코드를 실행합니다.</p>\n<pre><code class=\"hljs language-js\">uvicorn scripts.<span class=\"hljs-property\">app</span>:app --host <span class=\"hljs-number\">0.0</span><span class=\"hljs-number\">.0</span><span class=\"hljs-number\">.0</span> --port <span class=\"hljs-number\">8000</span>\n</code></pre>\n<p>그런 다음, Jupyter Notebook에서 다음 코드를 실행하여 API를 테스트합니다.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> requests\n\nurl = <span class=\"hljs-string\">\"http://localhost:8000/predict\"</span>\n\ndata = {\n    <span class=\"hljs-string\">\"data\"</span>: [\n        [<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">85</span>, <span class=\"hljs-number\">66</span>, <span class=\"hljs-number\">29</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">26.6</span>, <span class=\"hljs-number\">0.351</span>, <span class=\"hljs-number\">31</span>]\n    ]\n}\n\nresponse = requests.<span class=\"hljs-title function_\">post</span>(url, json=data)\n<span class=\"hljs-title function_\">print</span>(response.<span class=\"hljs-title function_\">json</span>())\n</code></pre>\n<p>API에 전달하는 데이터의 위치가 훈련 데이터와 동일한지 확인해주세요. API가 잘 작동하면 Docker 이미지를 빌드하고 허브에 푸시할 것입니다.</p>\n<p>먼저 루트 폴더에 dockerfile을 생성해주세요. 해당 파일에 다음 코드를 채워넣어주세요.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">FROM</span> <span class=\"hljs-attr\">python</span>:<span class=\"hljs-number\">3.9</span>-slim\n\n<span class=\"hljs-variable constant_\">WORKDIR</span> /app\n\n<span class=\"hljs-variable constant_\">COPY</span> requirements.<span class=\"hljs-property\">txt</span> requirements.<span class=\"hljs-property\">txt</span>\n<span class=\"hljs-variable constant_\">RUN</span> pip install -r requirements.<span class=\"hljs-property\">txt</span>\n\n<span class=\"hljs-variable constant_\">COPY</span> scripts/app.<span class=\"hljs-property\">py</span> app.<span class=\"hljs-property\">py</span>\n<span class=\"hljs-variable constant_\">COPY</span> models models\n\n<span class=\"hljs-variable constant_\">EXPOSE</span> <span class=\"hljs-number\">8000</span>\n\n<span class=\"hljs-variable constant_\">CMD</span> [<span class=\"hljs-string\">\"uvicorn\"</span>, <span class=\"hljs-string\">\"app:app\"</span>, <span class=\"hljs-string\">\"--host\"</span>, <span class=\"hljs-string\">\"0.0.0.0\"</span>, <span class=\"hljs-string\">\"--port\"</span>, <span class=\"hljs-string\">\"8000\"</span>]\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>위 코드에서는 Python 환경을 설정하고 API를 실행하는 데 필요한 파일을 컨테이너로 복사하여 포트 8000에서 수신 대기하는 방법을 안내합니다.</p>\n<p>Dockerfile로 이미지를 빌드하는 방법은 준비가 되면 다음 코드를 사용하면 됩니다.</p>\n<pre><code class=\"hljs language-js\">docker build -t username/image_name -f <span class=\"hljs-title class_\">Dockerfile</span> .\ndocker login -u username\ndocker push username/<span class=\"hljs-attr\">image_name</span>:latest\n</code></pre>\n<p>위 코드에서 username을 귀하의 Dockerhub 사용자명으로, image_name을 선호하는 응용 프로그램 이름으로 변경해주세요. 성공한다면, 내 것과 같이 Dockerhub에 귀하의 이미지가 표시될 것입니다.</p>\n<div class=\"content-ad\"></div>\n<p>그래서 우리가 모델 API를 Docker에 넣고 Dockerhub에 푸시한 이유는 무엇인가요? 이것은 응용 프로그램을 실행할 모든 환경에서 일관성을 보장하기 때문입니다.</p>\n<p>또한 GitHub Actions가 다음 섹션에서 모델을 다시 학습하고 다시 이 컨테이너로 푸시하는 데 얼마나 강력한지를 보여줍니다. 따라서 우리는 모델을 배포하기 위해 이미지를 가져오기만 하면 됩니다.</p>\n<p>아래 코드를 실행하여 Dockerhub에서 이미지를 가져와 로컬 환경에서 실행해보세요.</p>\n<pre><code class=\"hljs language-js\">docker login -u username\ndocker pull username/<span class=\"hljs-attr\">image_name</span>:latest\ndocker run -d -p <span class=\"hljs-number\">8000</span>:<span class=\"hljs-number\">8000</span> username/<span class=\"hljs-attr\">image_name</span>:latest\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>지금까지 제품 모델이 운영 중입니다. 다음 부분에서는 GitHub Actions를 사용하여 모델을 특정 트리거로 다시 학습하는 방법을 살펴보겠습니다.</p>\n<h1>GitHub Actions를 활용한 모델 재학습</h1>\n<p>제가 언급했듯이, 머신 러닝 모델은 지속적인 프로젝트입니다. 이를 통해 어떤 가치를 제공하려면 중요한데, 왜냐하면 모델이 항상 동일한 품질을 유지할 것으로 기대하기 어렵기 때문입니다. 특히, 드리프트가 발생하면 더욱 그렇습니다.</p>\n<p>이 튜토리얼에서는 운영 데이터셋에서 데이터 드리프트가 감지될 때 자동으로 모델 재학습을 수행하는 방법을 배우겠습니다. 먼저, 데이터셋에서 드리프트를 감지하는 방법을 살펴보겠습니다.</p>\n<div class=\"content-ad\"></div>\n<p>아래 코드를 사용하여 데이터셋에서 drift를 시뮬레이션해 보겠습니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n\ndef <span class=\"hljs-title function_\">introduce_drift</span>(data, drift_features, drift_amount=<span class=\"hljs-number\">0.1</span>, random_seed=<span class=\"hljs-number\">42</span>):\n    np.<span class=\"hljs-property\">random</span>.<span class=\"hljs-title function_\">seed</span>(random_seed)\n    drifted_data = data.<span class=\"hljs-title function_\">copy</span>()\n    \n    <span class=\"hljs-keyword\">for</span> feature <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">drift_features</span>:\n        <span class=\"hljs-keyword\">if</span> feature <span class=\"hljs-keyword\">in</span> data.<span class=\"hljs-property\">columns</span>:\n            drifted_data[feature] += np.<span class=\"hljs-property\">random</span>.<span class=\"hljs-title function_\">normal</span>(loc=<span class=\"hljs-number\">0</span>, scale=drift_amount, size=data.<span class=\"hljs-property\">shape</span>[<span class=\"hljs-number\">0</span>])\n    \n    <span class=\"hljs-keyword\">return</span> drifted_data\n    \nfeatures_to_drift = [<span class=\"hljs-string\">'Glucose'</span>, <span class=\"hljs-string\">'BloodPressure'</span>, <span class=\"hljs-string\">'SkinThickness'</span>, <span class=\"hljs-string\">'Pregnancies'</span>]\n\ndrifted_data = <span class=\"hljs-title function_\">introduce_drift</span>(X_test, features_to_drift, drift_amount=<span class=\"hljs-number\">50</span>)\ndrifted_data = drifted_data.<span class=\"hljs-title function_\">reset_index</span>(drop=<span class=\"hljs-title class_\">True</span>)\n</code></pre>\n<p>위 코드에서는 Test 데이터의 일부 열을 drift했습니다. drift_amount를 조절하여 데이터가 얼마나 변하는지 제어할 수 있습니다.</p>\n<p>튜토리얼에는 학습 데이터(참조)와 drift 데이터(신규)가 필요합니다. 나중에 다시 학습할 때 사용할 타겟 열도 저장해두는 것이 좋습니다.</p>\n<div class=\"content-ad\"></div>\n<p>reference_data['Outcome'] = y_train.reset_index(drop=True)\r\ndrifted_data['Outcome'] = y_test.reset_index(drop=True)</p>\n<p>drifted_data.to_csv('..//data//new_data.csv', index=False)\r\nreference_data.to_csv('..//data//reference_data.csv', index=False)</p>\n<p>Evidently(제가 Evidently와 어떤 제휴도 없습니다)를 사용하여 제품 데이터가 참조 데이터에 비해 드리프트했는지 확인할 수 있습니다. 다음 코드로 확인할 수 있습니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> evidently.metric_preset <span class=\"hljs-keyword\">import</span> DataDriftPreset\r\n<span class=\"hljs-keyword\">from</span> evidently.report <span class=\"hljs-keyword\">import</span> Report\r\n\r\ndata_drift_report = Report(metrics=[DataDriftPreset()])\r\n\r\ndata_drift_report.run(current_data=drifted_data.drop(<span class=\"hljs-string\">'Outcome'</span>, axis=<span class=\"hljs-number\">1</span>), \r\nreference_data=reference_data.drop(<span class=\"hljs-string\">'Outcome'</span>, axis=<span class=\"hljs-number\">1</span>), column_mapping=<span class=\"hljs-literal\">None</span>)\r\nreport_json = data_drift_report.as_dict()\r\ndrift_detected = report_json[<span class=\"hljs-string\">'metrics'</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'result'</span>][<span class=\"hljs-string\">'dataset_drift'</span>]\n</code></pre>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_4.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>결과에는 나중에 재훈련 자동화를 위해 사용할 드리프트 데이터 세트가 나타납니다.</p>\n<p>드리프트 감지를 시뮬레이션하기 위해 drift_detection.py라는 파일을 만들어서 스크립트 폴더에 저장해보겠습니다. 아래 코드를 파일에 채워 넣어주세요.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> evidently.<span class=\"hljs-property\">metric_preset</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">DataDriftPreset</span>\n<span class=\"hljs-keyword\">from</span> evidently.<span class=\"hljs-property\">report</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Report</span>\n\nreference_data = pd.<span class=\"hljs-title function_\">read_csv</span>(<span class=\"hljs-string\">'data/reference_data.csv'</span>)\nnew_data = pd.<span class=\"hljs-title function_\">read_csv</span>(<span class=\"hljs-string\">'data/new_data.csv'</span>)\n\ndata_drift_report = <span class=\"hljs-title class_\">Report</span>(metrics=[\n    <span class=\"hljs-title class_\">DataDriftPreset</span>()\n])\n\ndata_drift_report.<span class=\"hljs-title function_\">run</span>(reference_data=reference_data.<span class=\"hljs-title function_\">drop</span>(<span class=\"hljs-string\">'Outcome'</span>, axis=<span class=\"hljs-number\">1</span>), \n                      current_data=new_data.<span class=\"hljs-title function_\">drop</span>(<span class=\"hljs-string\">'Outcome'</span>, axis=<span class=\"hljs-number\">1</span>), column_mapping=<span class=\"hljs-title class_\">None</span>)\n\nreport_json = data_drift_report.<span class=\"hljs-title function_\">as_dict</span>()\ndrift_detected = report_json[<span class=\"hljs-string\">'metrics'</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'result'</span>][<span class=\"hljs-string\">'dataset_drift'</span>]\n\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-attr\">drift_detected</span>:\n    <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"데이터 드리프트가 감지되었습니다. 모델을 재훈련합니다.\"</span>)\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-title function_\">open</span>(<span class=\"hljs-string\">'drift_detected.txt'</span>, <span class=\"hljs-string\">'w'</span>) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">f</span>:\n        f.<span class=\"hljs-title function_\">write</span>(<span class=\"hljs-string\">'drift_detected'</span>)\n<span class=\"hljs-attr\">else</span>:\n    <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"데이터 드리프트를 감지하지 못했습니다.\"</span>)\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-title function_\">open</span>(<span class=\"hljs-string\">'drift_detected.txt'</span>, <span class=\"hljs-string\">'w'</span>) <span class=\"hljs-keyword\">as</span> <span class=\"hljs-attr\">f</span>:\n        f.<span class=\"hljs-title function_\">write</span>(<span class=\"hljs-string\">'no_drift'</span>)\n</code></pre>\n<p>위 코드에서 우리는 드리프트 감지 여부를 drift_detected.txt 파일에 저장하고, 드리프트가 감지되었는지 여부에 따라 정보를 출력합니다. 드리프트가 감지된 경우, 모델을 재훈련하고 싶습니다. 이에 대비하여 훈련 스크립트를 준비해야 합니다.</p>\n<div class=\"content-ad\"></div>\n<p>스크립트 폴더에 train_model.py라는 파일을 만들고 다음 코드로 채워주세요.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">from</span> sklearn.pipeline <span class=\"hljs-keyword\">import</span> Pipeline\n<span class=\"hljs-keyword\">from</span> sklearn.compose <span class=\"hljs-keyword\">import</span> ColumnTransformer\n<span class=\"hljs-keyword\">from</span> sklearn.impute <span class=\"hljs-keyword\">import</span> SimpleImputer\n<span class=\"hljs-keyword\">from</span> sklearn.preprocessing <span class=\"hljs-keyword\">import</span> StandardScaler\n<span class=\"hljs-keyword\">from</span> sklearn.ensemble <span class=\"hljs-keyword\">import</span> RandomForestClassifier\n<span class=\"hljs-keyword\">import</span> pickle\n\nreference_data = pd.read_csv(<span class=\"hljs-string\">'data/reference_data.csv'</span>)\nnew_data = pd.read_csv(<span class=\"hljs-string\">'data/new_data.csv'</span>)\n\ndf= pd.concat([reference_data, new_data], ignore_index=<span class=\"hljs-literal\">True</span>)\n\nX = df.drop(<span class=\"hljs-string\">'Outcome'</span>, axis=<span class=\"hljs-number\">1</span>)\ny = df[<span class=\"hljs-string\">'Outcome'</span>]\n\nnumeric_features = X.columns\nnumeric_transformer = Pipeline(steps=[\n    (<span class=\"hljs-string\">'imputer'</span>, SimpleImputer(strategy=<span class=\"hljs-string\">'mean'</span>)),\n    (<span class=\"hljs-string\">'scaler'</span>, StandardScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (<span class=\"hljs-string\">'num'</span>, numeric_transformer, numeric_features)\n    ])\n\npipeline = Pipeline(steps=[\n    (<span class=\"hljs-string\">'preprocessor'</span>, preprocessor),\n    (<span class=\"hljs-string\">'classifier'</span>, RandomForestClassifier(random_state=<span class=\"hljs-number\">42</span>))\n])\n\npipeline.fit(X, y)\n\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'models/pipeline.pkl'</span>, <span class=\"hljs-string\">'wb'</span>) <span class=\"hljs-keyword\">as</span> f:\n    pickle.dump(pipeline, f)\n</code></pre>\n<p>위의 코드는 학습 및 드리프트 데이터를 새로운 학습 모델로 결합하고, 이를 사용하여 새 모델을 학습하는 것입니다. 이는 간단한 접근 방식일 뿐이며, 실제 세계의 학습 데이터는 더 많은 준비가 필요하고, 새 모델은 적절한 평가가 필요합니다.</p>\n<p>그러나 모든 스크립트가 준비되면, GitHub Actions를 통해 드리프트가 감지될 때 모델을 재학습할 수 있도록 준비할 것입니다. 재학습에 필요한 모든 구성을 포함하는 YAML 파일을 준비해야 합니다.</p>\n<div class=\"content-ad\"></div>\n<p>그래서, .github\\workflows 폴더에 mlops_pipeline.yml 파일을 생성해봅시다. 폴더 이름이 제대로 되었는지 확인하세요; GitHub Actions은 적절한 이름이 필요합니다. 아래의 코드로 mlops_pipeline.yml을 채워넣어주세요.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Diabetes</span> <span class=\"hljs-title class_\">Retraining</span> <span class=\"hljs-title class_\">Pipeline</span> <span class=\"hljs-keyword\">with</span> <span class=\"hljs-title class_\">Data</span> <span class=\"hljs-title class_\">Drift</span> <span class=\"hljs-title class_\">Detection</span>\n\n<span class=\"hljs-attr\">on</span>:\n  <span class=\"hljs-attr\">push</span>:\n    <span class=\"hljs-attr\">paths</span>:\n      - <span class=\"hljs-string\">'data/new_data.csv'</span>\n<span class=\"hljs-attr\">permissions</span>:\n  <span class=\"hljs-attr\">contents</span>: write\n<span class=\"hljs-attr\">jobs</span>:\n  <span class=\"hljs-attr\">build</span>:\n    runs-<span class=\"hljs-attr\">on</span>: ubuntu-latest\n\n    <span class=\"hljs-attr\">steps</span>:\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Checkout</span> code\n      <span class=\"hljs-attr\">uses</span>: actions/checkout@v2\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Set</span> up <span class=\"hljs-title class_\">Python</span>\n      <span class=\"hljs-attr\">uses</span>: actions/setup-python@v2 \n      <span class=\"hljs-attr\">with</span>:\n        python-<span class=\"hljs-attr\">version</span>: <span class=\"hljs-number\">3.9</span>\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Install</span> dependencies\n      <span class=\"hljs-attr\">run</span>: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.<span class=\"hljs-property\">txt</span>\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Run</span> data drift detection\n      <span class=\"hljs-attr\">run</span>: |\n        python scripts/drift_detection.<span class=\"hljs-property\">py</span>\n      <span class=\"hljs-keyword\">continue</span>-on-<span class=\"hljs-attr\">error</span>: <span class=\"hljs-literal\">true</span> \n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Check</span> <span class=\"hljs-keyword\">for</span> data drift\n      <span class=\"hljs-attr\">id</span>: check_drift\n      <span class=\"hljs-attr\">run</span>: |\n        <span class=\"hljs-keyword\">if</span> grep -q <span class=\"hljs-string\">'drift_detected'</span> drift_detected.<span class=\"hljs-property\">txt</span>; then\n          echo <span class=\"hljs-string\">\"Data drift detected.\"</span>\n          echo <span class=\"hljs-string\">\"drift=true\"</span> >> $GITHUB_ENV\n        <span class=\"hljs-keyword\">else</span>\n          echo <span class=\"hljs-string\">\"No data drift detected.\"</span>\n          echo <span class=\"hljs-string\">\"drift=false\"</span> >> $GITHUB_ENV\n        fi\n      <span class=\"hljs-attr\">shell</span>: bash\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Model</span> <span class=\"hljs-title class_\">Retraining</span> <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title class_\">Data</span> <span class=\"hljs-title class_\">Drift</span> detected\n      <span class=\"hljs-attr\">if</span>: env.<span class=\"hljs-property\">drift</span> == <span class=\"hljs-string\">'true'</span>\n      <span class=\"hljs-attr\">run</span>: |\n        python scripts/train_model.<span class=\"hljs-property\">py</span>\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Commit</span> and push updated model\n      <span class=\"hljs-attr\">if</span>: env.<span class=\"hljs-property\">drift</span> == <span class=\"hljs-string\">'true'</span>\n      <span class=\"hljs-attr\">env</span>:\n        <span class=\"hljs-attr\">GIT_COMMITTER_NAME</span>: github-actions\n        <span class=\"hljs-attr\">GIT_COMMITTER_EMAIL</span>: github-actions@github.<span class=\"hljs-property\">com</span>\n      <span class=\"hljs-attr\">run</span>: |\n        git config --<span class=\"hljs-variable language_\">global</span> user.<span class=\"hljs-property\">name</span> <span class=\"hljs-string\">\"github-actions\"</span>\n        git config --<span class=\"hljs-variable language_\">global</span> user.<span class=\"hljs-property\">email</span> <span class=\"hljs-string\">\"github-actions@github.com\"</span>\n        git remote set-url origin <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//x-access-token:${ secrets.ACTIONS_PAT }@github.com/username/image_name.git</span>\n        git add models/pipeline.<span class=\"hljs-property\">pkl</span>\n        git commit -m <span class=\"hljs-string\">\"Update model after retraining on $(date -u +'%Y-%m-%d %H:%M:%S UTC')\"</span>\n        git push\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Build</span> <span class=\"hljs-title class_\">Docker</span> image\n      <span class=\"hljs-attr\">if</span>: env.<span class=\"hljs-property\">drift</span> == <span class=\"hljs-string\">'true'</span>\n      <span class=\"hljs-attr\">run</span>: |\n        docker build -t username/image_name -f dockerfile .\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Log</span> <span class=\"hljs-keyword\">in</span> to <span class=\"hljs-title class_\">Docker</span> <span class=\"hljs-title class_\">Hub</span>\n      <span class=\"hljs-attr\">if</span>: env.<span class=\"hljs-property\">drift</span> == <span class=\"hljs-string\">'true'</span>\n      <span class=\"hljs-attr\">run</span>: echo <span class=\"hljs-string\">\"${ secrets.DOCKER_PASSWORD }\"</span> | docker login -u <span class=\"hljs-string\">\"${ secrets.DOCKER_USERNAME }\"</span> --password-stdin\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Push</span> <span class=\"hljs-title class_\">Docker</span> image to <span class=\"hljs-title class_\">Docker</span> <span class=\"hljs-title class_\">Hub</span>\n      <span class=\"hljs-attr\">if</span>: env.<span class=\"hljs-property\">drift</span> == <span class=\"hljs-string\">'true'</span>\n      <span class=\"hljs-attr\">run</span>: |\n        docker push username/<span class=\"hljs-attr\">image_name</span>:latest\n\n    - <span class=\"hljs-attr\">name</span>: <span class=\"hljs-title class_\">Notify</span> about the process\n      <span class=\"hljs-attr\">run</span>: |\n        <span class=\"hljs-keyword\">if</span> [[ <span class=\"hljs-string\">\"$GITHUB_ENV\"</span> == *<span class=\"hljs-string\">\"drift=false\"</span>* ]]; then\n          echo <span class=\"hljs-string\">\"No data drift detected. No retraining necessary.\"</span>\n        <span class=\"hljs-keyword\">else</span>\n          echo <span class=\"hljs-string\">\"Data drift detected. Model retrained and deployed.\"</span>\n        fi\n      <span class=\"hljs-attr\">shell</span>: bash\n</code></pre>\n<p>위의 YAML에서 수행한 전체 설정 구조는 아래 이미지에 나와 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_5.png\" alt=\"Simple Model Retraining Automation via GitHub Actions\"></p>\n<div class=\"content-ad\"></div>\n<p>GitHub Actions에서 사용하는 트리거는 data 폴더 안에 new_data.csv 파일이 푸시될 때입니다. 그러나 모델 재학습은 drift가 감지될 때만 실행됩니다. 모델을 다시 훈련한 후, 해당 모델을 GitHub 저장소와 Docker Hub에 다시 푸시할 것입니다.</p>\n<p>사용자명/image_name Docker 식별자를 꼭 자신의 것으로 변경해주세요. 동일한 식별자를 사용하는 경우 Repository Secrets를 생성할 수도 있습니다.</p>\n<p>모든 파일이 준비되면 GitHub 저장소에 푸시해야 합니다. 그런 다음 새로운 drift 데이터를 생성하여 new_data.csv로 저장하고, 해당 데이터를 저장소에 다시 푸시해보세요.</p>\n<p>GitHub 저장소의 Actions 탭으로 이동해주세요. 성공적으로 실행되었다면, 'Success' 상태를 가진 'build'라는 작업이 하나 표시될 것입니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_6.png\" alt=\"image\"></p>\n<p>작업을 클릭하여 프로세스의 모든 세부 정보를 확인할 수 있습니다. 각 단계의 정보를 확인하여 프로세스를 이해하거나 실행에 실패했는지 확인할 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_7.png\" alt=\"image\"></p>\n<p>저장소의 모델로 이동하면 모델이 업데이트되었는지 확인할 수 있습니다. 모델을 다시 학습했을 때 커밋 메시지를 사용하여 알림을 받습니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_8.png\" alt=\"image\"></p>\n<p>도커 허브 저장소를 확인해서 이미지가 업데이트되었는지도 확인할 수 있어요.</p>\n<p>여기까지면 GitHub Actions를 사용해서 모델 재교육 프로세스를 간단히 할 수 있어요. 여러분이 원하는대로 스크립트를 조정할 수 있어요. 예를 들어 트리거, 재교육 조건, 데이터셋 등을 말이에요.</p>\n<p>이 글에서 사용한 코드들이 필요하면, 해당 레포지토리에 푸시해 놓았어요.</p>\n<div class=\"content-ad\"></div>\n<h1>결론</h1>\n<p>이 글에서는 GitHub Actions를 사용하여 모델 재학습 프로세스를 자동화하는 방법을 배웠습니다. YAML 파일을 통해 구성을 설정하고 트리거를 결정함으로써 GitHub Actions를 사용하여 필요한 모든 프로세스를 간편하게 처리할 수 있습니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}