{"pageProps":{"post":{"title":"AWS Glue로 다수의 CSV 파일을 처리하는 ETL 단계별 팁","description":"","date":"2024-05-17 20:37","slug":"2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3","content":"\n\n이건 훌륭한 이미지입니다! 이 디지털 시대에 데이터는 기업에게 귀중한 자산이 되었습니다. 데이터를 효과적으로 처리하고 분석하는 것이 유용한 통찰력을 얻고 스마트한 의사결정을 하는 데 중요합니다. AWS Glue는 데이터를 쉽고 효율적으로 관리하고 분석하는 데 도움이 되는 포괄적인 솔루션이 됩니다.\n\n이 세션에서는 AWS Glue, 데이터 카탈로그, 및 크롤러가 하나의 버킷에 있는 여러 CSV 파일을 단일 데이터 세트로 읽는 방법에 대해 논의할 것입니다. 아래는 아키텍처 요약입니다:\n\n![아키텍처 이미지](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_1.png)\n\n<div class=\"content-ad\"></div>\n\n# S3 버킷 준비하기\n\n처리하려는 모든 CSV 파일이 아마존 S3의 단일 버킷에 저장되어 있는지 확인하세요.\n\n![이미지](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_2.png)\n\n# AWS Glue 데이터 카탈로그에서 데이터베이스 생성하기\n\n<div class=\"content-ad\"></div>\n\nAWS Management Console에서 AWS Glue를 열고, \"데이터베이스\" 섹션으로 이동하여 메타데이터를 저장할 새 데이터베이스를 생성하세요.\n\n![이미지](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_3.png)\n\n# 크롤러 생성\n\n이전에 생성한 데이터베이스를 선택하여 크롤러에 의해 생성된 테이블을 저장하세요. 크롤러를 사용하여 테이블 추가를 선택하세요. 크롤러에 이름을 지정하고 CSV 파일을 포함하는 S3 버킷 위치를 선택하여 데이터 원본을 지정하세요. S3의 데이터 원본에 액세스할 수 있는 IAM 역할을 지정하고 Glue 데이터 카탈로그에 항목을 생성할 수 있는 권한이 있는 IAM 역할을 지정하세요. 필요에 따라 크롤러 옵션을 구성하세요. 크롤러를 주기적으로 실행하려면 빈도를 설정하세요. 구성을 완료한 후 크롤러를 실행하세요. 크롤러는 지정된 버킷의 모든 CSV 파일을 읽고 Glue 데이터 카탈로그에 하나 이상의 테이블을 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_4.png)\n\n![Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_5.png)\n\nOnce the crawler status is complete you can preview the table data that has been created using Athena\n\n![Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n# AWS Glue에서 ETL 작업 만들기\n\n![ETL image](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_7.png)\n\nAWS Glue은 데이터 변환의 핵심 프로세스인 ETL(추출, 변환, 로드)을 수행하는 다양한 방법을 제공합니다. 시각적 ETL, 주피터, 또는 스크립팅을 통해 가장 적합한 방법을 선택할 수 있습니다.\n\n## 시각적 ETL\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_8.png\" />\n\n기술적 배경이 없는 분들에게 시각적 ETL은 이상적인 선택지입니다. 직관적인 드래그 앤 드롭 인터페이스를 통해 코드 작성 없이도 ETL 워크플로를 구축할 수 있습니다. 다양한 데이터 원본을 쉽게 연결하고 데이터 변환을 적용하며 처리된 데이터를 원하는 대상에로 로드할 수 있습니다.\n\n여기 AWS Glue로 Data Catalog에서 S3로 시각적 ETL을 구축하는 단계별 안내서가 있습니다.\n\n- AWS Glue Studio에 액세스\n- 새 워크플로 생성\n- 데이터 원본 선택\n- 변환 추가\n- 데이터 대상 선택\n- 작업 구성\n- 작업 검토 및 실행\n\n<div class=\"content-ad\"></div>\n\n## Jupyter Notebook\n\n![Image](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_9.png)\n\n보다 경험 많은 데이터 전문가들에게 Jupyter는 더 많은 유연성과 파워를 제공합니다. Jupyter 노트북을 사용하면 Python 코드와 텍스트, 시각화를 결합하여 복잡한 데이터 분석을 수행할 수 있습니다.\n\n다음은 AWS Glue를 사용하여 Jupyter Notebook을 사용하는 단계입니다. Data Catalog에서 S3로 콘솔에서 사용하세요.\n\n<div class=\"content-ad\"></div>\n\n- AWS Glue Studio를 열어주세요.\n- 새로운 주피터 노트북을 생성해주세요.\n- 주피터 노트북에 파이썬 코드를 작성해주세요.\n- 작성한 파이썬 코드를 실행해주세요.\n- 주피터 노트북을 저장하고 공유해주세요.\n\n## 스크립팅\n\nETL 프로세스를 완전히 제어하고 싶은 경우, AWS Glue를 사용하여 Python 및 Scala와 같은 다양한 프로그래밍 언어로 스크립트를 작성할 수 있습니다. 이러한 스크립트는 귀하의 특정 요구에 맞게 설계된 복잡한 데이터 변환을 수행하는 데 사용될 수 있습니다.\n\n아래는 데이터 카탈로그부터 S3까지 콘솔에서 AWS Glue를 스크립팅과 함께 사용하는 단계입니다.\n\n<div class=\"content-ad\"></div>\n\n- AWS Glue 콘솔을 열어주세요.\n- 좌측 탐색 패널에서 Glue를 선택합니다.\n- 메인 패널 상단에서 Jobs를 선택합니다.\n- 'Create job'을 클릭합니다.\n- 작업의 이름을 입력해주세요. 예를 들어 \"TransferDataFromCatalogToS3\"와 같이 지정합니다.\n- Script location 섹션에서 Glue 스크립트를 선택합니다.\n- Glue 스크립트 상자에 다음과 같은 Python 스크립트를 입력하세요. 이는 예시입니다.\n\n```js\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# Read\ndyf = glueContext.create_dynamic_frame.from_catalog(database='db-s3-glue ', \n                                                    table_name='1_source'\n                                                   )\n\n# Store\noutput_dyf = glueContext.write_dynamic_frame.from_options(frame=dyf, \n                                                          connection_type=\"s3\", \n                                                          format=\"glueparquet\", \n                                                          connection_options={\"path\": \"s3://s3-glue/2-target/\", \"partitionKeys\": []}, \n                                                          format_options={\"compression\": \"uncompressed\"}\n                                                         )\n\njob.commit()\n```\n\n# 다음은 무엇이 있을까요?\n\n- MySQL, SQL Server, Aurora와 같은 RDBMS 소스 탐색하기.\n- Redshift와 같은 데이터 웨어하우스로의 대상 데이터 탐색하기.\n- Workflows(오케스트레이션)를 사용하여 작업 자동화하기.\n- 스트림 처리.\n\n<div class=\"content-ad\"></div>\n\n## 최선의 인사\n\n린탕 길랑","ogImage":{"url":"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_0.png"},"coverImage":"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_0.png","tag":["Tech"],"readingTime":5},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>이건 훌륭한 이미지입니다! 이 디지털 시대에 데이터는 기업에게 귀중한 자산이 되었습니다. 데이터를 효과적으로 처리하고 분석하는 것이 유용한 통찰력을 얻고 스마트한 의사결정을 하는 데 중요합니다. AWS Glue는 데이터를 쉽고 효율적으로 관리하고 분석하는 데 도움이 되는 포괄적인 솔루션이 됩니다.</p>\n<p>이 세션에서는 AWS Glue, 데이터 카탈로그, 및 크롤러가 하나의 버킷에 있는 여러 CSV 파일을 단일 데이터 세트로 읽는 방법에 대해 논의할 것입니다. 아래는 아키텍처 요약입니다:</p>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_1.png\" alt=\"아키텍처 이미지\"></p>\n<div class=\"content-ad\"></div>\n<h1>S3 버킷 준비하기</h1>\n<p>처리하려는 모든 CSV 파일이 아마존 S3의 단일 버킷에 저장되어 있는지 확인하세요.</p>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_2.png\" alt=\"이미지\"></p>\n<h1>AWS Glue 데이터 카탈로그에서 데이터베이스 생성하기</h1>\n<div class=\"content-ad\"></div>\n<p>AWS Management Console에서 AWS Glue를 열고, \"데이터베이스\" 섹션으로 이동하여 메타데이터를 저장할 새 데이터베이스를 생성하세요.</p>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_3.png\" alt=\"이미지\"></p>\n<h1>크롤러 생성</h1>\n<p>이전에 생성한 데이터베이스를 선택하여 크롤러에 의해 생성된 테이블을 저장하세요. 크롤러를 사용하여 테이블 추가를 선택하세요. 크롤러에 이름을 지정하고 CSV 파일을 포함하는 S3 버킷 위치를 선택하여 데이터 원본을 지정하세요. S3의 데이터 원본에 액세스할 수 있는 IAM 역할을 지정하고 Glue 데이터 카탈로그에 항목을 생성할 수 있는 권한이 있는 IAM 역할을 지정하세요. 필요에 따라 크롤러 옵션을 구성하세요. 크롤러를 주기적으로 실행하려면 빈도를 설정하세요. 구성을 완료한 후 크롤러를 실행하세요. 크롤러는 지정된 버킷의 모든 CSV 파일을 읽고 Glue 데이터 카탈로그에 하나 이상의 테이블을 생성합니다.</p>\n<div class=\"content-ad\"></div>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_4.png\" alt=\"Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3\"></p>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_5.png\" alt=\"Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3\"></p>\n<p>Once the crawler status is complete you can preview the table data that has been created using Athena</p>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_6.png\" alt=\"Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3\"></p>\n<div class=\"content-ad\"></div>\n<h1>AWS Glue에서 ETL 작업 만들기</h1>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_7.png\" alt=\"ETL image\"></p>\n<p>AWS Glue은 데이터 변환의 핵심 프로세스인 ETL(추출, 변환, 로드)을 수행하는 다양한 방법을 제공합니다. 시각적 ETL, 주피터, 또는 스크립팅을 통해 가장 적합한 방법을 선택할 수 있습니다.</p>\n<h2>시각적 ETL</h2>\n<div class=\"content-ad\"></div>\n<img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_8.png\">\n<p>기술적 배경이 없는 분들에게 시각적 ETL은 이상적인 선택지입니다. 직관적인 드래그 앤 드롭 인터페이스를 통해 코드 작성 없이도 ETL 워크플로를 구축할 수 있습니다. 다양한 데이터 원본을 쉽게 연결하고 데이터 변환을 적용하며 처리된 데이터를 원하는 대상에로 로드할 수 있습니다.</p>\n<p>여기 AWS Glue로 Data Catalog에서 S3로 시각적 ETL을 구축하는 단계별 안내서가 있습니다.</p>\n<ul>\n<li>AWS Glue Studio에 액세스</li>\n<li>새 워크플로 생성</li>\n<li>데이터 원본 선택</li>\n<li>변환 추가</li>\n<li>데이터 대상 선택</li>\n<li>작업 구성</li>\n<li>작업 검토 및 실행</li>\n</ul>\n<div class=\"content-ad\"></div>\n<h2>Jupyter Notebook</h2>\n<p><img src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_9.png\" alt=\"Image\"></p>\n<p>보다 경험 많은 데이터 전문가들에게 Jupyter는 더 많은 유연성과 파워를 제공합니다. Jupyter 노트북을 사용하면 Python 코드와 텍스트, 시각화를 결합하여 복잡한 데이터 분석을 수행할 수 있습니다.</p>\n<p>다음은 AWS Glue를 사용하여 Jupyter Notebook을 사용하는 단계입니다. Data Catalog에서 S3로 콘솔에서 사용하세요.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>AWS Glue Studio를 열어주세요.</li>\n<li>새로운 주피터 노트북을 생성해주세요.</li>\n<li>주피터 노트북에 파이썬 코드를 작성해주세요.</li>\n<li>작성한 파이썬 코드를 실행해주세요.</li>\n<li>주피터 노트북을 저장하고 공유해주세요.</li>\n</ul>\n<h2>스크립팅</h2>\n<p>ETL 프로세스를 완전히 제어하고 싶은 경우, AWS Glue를 사용하여 Python 및 Scala와 같은 다양한 프로그래밍 언어로 스크립트를 작성할 수 있습니다. 이러한 스크립트는 귀하의 특정 요구에 맞게 설계된 복잡한 데이터 변환을 수행하는 데 사용될 수 있습니다.</p>\n<p>아래는 데이터 카탈로그부터 S3까지 콘솔에서 AWS Glue를 스크립팅과 함께 사용하는 단계입니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>AWS Glue 콘솔을 열어주세요.</li>\n<li>좌측 탐색 패널에서 Glue를 선택합니다.</li>\n<li>메인 패널 상단에서 Jobs를 선택합니다.</li>\n<li>'Create job'을 클릭합니다.</li>\n<li>작업의 이름을 입력해주세요. 예를 들어 \"TransferDataFromCatalogToS3\"와 같이 지정합니다.</li>\n<li>Script location 섹션에서 Glue 스크립트를 선택합니다.</li>\n<li>Glue 스크립트 상자에 다음과 같은 Python 스크립트를 입력하세요. 이는 예시입니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> sys\n<span class=\"hljs-keyword\">from</span> awsglue.<span class=\"hljs-property\">transforms</span> <span class=\"hljs-keyword\">import</span> *\n<span class=\"hljs-keyword\">from</span> awsglue.<span class=\"hljs-property\">utils</span> <span class=\"hljs-keyword\">import</span> getResolvedOptions\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">context</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkContext</span>\n<span class=\"hljs-keyword\">from</span> awsglue.<span class=\"hljs-property\">context</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">GlueContext</span>\n<span class=\"hljs-keyword\">from</span> awsglue.<span class=\"hljs-property\">job</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Job</span>\n  \nsc = <span class=\"hljs-title class_\">SparkContext</span>.<span class=\"hljs-title function_\">getOrCreate</span>()\nglueContext = <span class=\"hljs-title class_\">GlueContext</span>(sc)\nspark = glueContext.<span class=\"hljs-property\">spark_session</span>\njob = <span class=\"hljs-title class_\">Job</span>(glueContext)\n\n# <span class=\"hljs-title class_\">Read</span>\ndyf = glueContext.<span class=\"hljs-property\">create_dynamic_frame</span>.<span class=\"hljs-title function_\">from_catalog</span>(database=<span class=\"hljs-string\">'db-s3-glue '</span>, \n                                                    table_name=<span class=\"hljs-string\">'1_source'</span>\n                                                   )\n\n# <span class=\"hljs-title class_\">Store</span>\noutput_dyf = glueContext.<span class=\"hljs-property\">write_dynamic_frame</span>.<span class=\"hljs-title function_\">from_options</span>(frame=dyf, \n                                                          connection_type=<span class=\"hljs-string\">\"s3\"</span>, \n                                                          format=<span class=\"hljs-string\">\"glueparquet\"</span>, \n                                                          connection_options={<span class=\"hljs-string\">\"path\"</span>: <span class=\"hljs-string\">\"s3://s3-glue/2-target/\"</span>, <span class=\"hljs-string\">\"partitionKeys\"</span>: []}, \n                                                          format_options={<span class=\"hljs-string\">\"compression\"</span>: <span class=\"hljs-string\">\"uncompressed\"</span>}\n                                                         )\n\njob.<span class=\"hljs-title function_\">commit</span>()\n</code></pre>\n<h1>다음은 무엇이 있을까요?</h1>\n<ul>\n<li>MySQL, SQL Server, Aurora와 같은 RDBMS 소스 탐색하기.</li>\n<li>Redshift와 같은 데이터 웨어하우스로의 대상 데이터 탐색하기.</li>\n<li>Workflows(오케스트레이션)를 사용하여 작업 자동화하기.</li>\n<li>스트림 처리.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<h2>최선의 인사</h2>\n<p>린탕 길랑</p>\n</body>\n</html>\n"},"__N_SSG":true}