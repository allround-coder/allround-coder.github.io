{"pageProps":{"posts":[{"title":"백엔드에서 Server-Sent EventsSSE로 실시간 알림을 전달하는 방법","description":"","date":"2024-05-20 22:16","slug":"2024-05-20-HowWeUsedServer-SentEventsSSEtoDeliverReal-TimeNotificationsonOurBackend","content":"\n\n판매자 성장팀으로서, 우리의 업무에는 판매자들의 시스템과의 상호작용을 향상시키기 위해 고안된 작업들로 구성된 다양한 도전 과제를 지정, 추적 및 관리하는 것이 포함되어 있습니다.\n\n우리의 최신 목표는 새로운 작업 지정 또는 완료시에 즉각적인 알림을 판매자들에게 제공하는 것입니다. 이를 달성하기 위해 HTTP 폴링, Server-Sent Events (SSE), 그리고 웹 소켓이라는 세 가지 잠재적인 옵션을 탐색했습니다.\n\n이 글에서는 알림 시스템으로 SSE (Server-Sent Events)를 사용하기로 결정한 이유에 대해 논의할 것입니다. 우리가 고려한 다른 옵션들보다 SSE의 장점을 알아보고, NestJS 프레임워크를 사용한 TypeScript 코드의 명확한 예시를 제공할 것입니다.\n\n![이미지](/assets/img/2024-05-20-HowWeUsedServer-SentEventsSSEtoDeliverReal-TimeNotificationsonOurBackend_0.png)\n\n<div class=\"content-ad\"></div>\n\n# 사용한 기술 소개\n\n실시간 알림 시스템을 구현하기 위해 다음을 사용했습니다:\n\n- Server-Sent Events (SSE): 실시간 및 일방향 채널을 서버와 클라이언트 간에 수립하는 데 SSE를 사용합니다. 이를 통해 알림을 발생 즉시 전달할 수 있습니다. SSE는 가벼우면서도 직관적인 솔루션이며, 우리의 요구에 맞는 알림을 클라이언트로 푸시할 때 사용합니다.\n- Redis Pub/Sub: Redis pub/sub을 사용하여 다중 발행자가 다중 구독자에게 메시지를 보낼 수 있는 메시징 시스템으로 활용합니다. Redis pub/sub을 사용하여 알림을 다중 파드로 분산 배포함으로써 모든 판매자가 제때 알림을 받을 수 있도록 보장했습니다.\n- Notification Write API: 다른 응용 프로그램에서 알림을 수집하기 위해 알림 쓰기 API를 개발했습니다. 이 API는 알림을 Redis로 전송합니다.\n- Notification Read API: Redis를 구독하고 모든 알림을 수신하는 알림 읽기 API도 개발했습니다. 알림을 수신한 후, 읽기 API는 SSE를 통해 연결된 클라이언트에게 알림을 전송합니다.\n\n## 왜 SSE를 선택했는가\n\n<div class=\"content-ad\"></div>\n\n신중한 고려 끝에 SSE가 알림 시스템에 최적인 옵션이라고 결정했습니다. 이 결정의 주요 이유 중 하나는 HTTP 폴링이 가능한 옵션이지만 즉각적인 알림 전달을 제공하지 않는다는 것입니다. 폴링은 서버가 새 이벤트를 확인하기 위해 일정 간격마다 터치되어야 하며 많은 사용자가 알림에 구독되어있는 경우에는 자원을 많이 사용하고 비효율적일 수 있습니다.\n\n그에 비해 SSE는 서버와 클라이언트 사이의 실시간 단방향 채널을 제공하여 발생하는 즉시 알림을 즉시 전달할 수 있습니다. 이는 판매자가 즉시 알림을 받을 수 있도록 해야 하는 우리의 사용 사례에 있어서 중요합니다.\n\n웹 소켓이 실시간 통신에 대한 인기 있는 선택지이긴 하지만, SSE에 비해 구현이 더 복잡합니다. 웹 소켓은 서버와 클라이언트 간의 양방향 통신을 제공하여 전체 이중 통신이 가능합니다. 하지만 우리는 클라이언트로 알림을 푸시하는 것만 필요하기 때문에 SSE를 더 간단하고 효율적인 솔루션으로 선택했습니다.\n\n요약하면, 시간당 실시간 알림을 제공하고 웹 소켓에 비해 더 간단하고 가벼운 솔루션인 SSE를 사용하기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\n## Redis Pub/Sub\n\n여러 응용 프로그램 팟에서 알림을 보내기 위해 Redis pub/sub을 사용했습니다. Redis pub/sub은 여러 발행자가 여러 구독자에게 메시지를 보낼 수 있는 메시징 시스템입니다. Redis pub/sub을 사용하여 알림을 여러 팟에 분산하여, 판매자가 모든 알림을 제때 받을 수 있도록 했습니다.\n\n```js\nimport { Redis } from 'ioredis';\n\n@Injectable()\nexport class RedisService implements OnModuleInit {\n\n  ...\n\n  async onModuleInit() {\n    this.subscriber = new Redis({\n      sentinels: this.redisSentinelConfig.addresses,\n      name: this.redisSentinelConfig.masterName,\n      password: this.redisSentinelConfig.password,\n    });\n    \n    await this.subscriber.subscribe(this.redisSentinelConfig.channelName, async (err, count) => {\n      if (err) {\n        this.logger.error(`Failed to subscribe: ${err.message}`);\n        return;\n      }\n      this.logger.log(`Subscribed successfully! This client is currently subscribed to ${count} channels`);\n    });\n    \n    this.subscriber.on('message', async (channel, message: string) => {\n      const liveNotification: LiveNotification = JSON.parse(message);\n      await this.liveNotificationService.emit(liveNotification);\n    });\n  }\n}\n```\n\n# 시스템 디자인\n\n<div class=\"content-ad\"></div>\n\n다른 애플리케이션으로부터 알림을 수집하기 위해 알림 작성 API를 개발했습니다. 이 API는 알림을 Redis로 전송합니다. 알림 읽기 API는 Redis를 구독하고 모든 알림을 수신합니다. 알림을 수신한 후 읽기 API는 SSE를 통해 연결된 클라이언트에게 알림을 전송합니다.\n\n다양한 읽기 API가 서로 다른 애플리케이션 팟에서 실행될 수 있기 때문에 각 읽기 API는 Redis에서 모든 알림을 수신합니다. 그러나 읽기 API는 현재 팟에 연결된 클라이언트에 따라 알림을 필터링합니다. 이렇게 함으로써 연결된 각 클라이언트에게는 관련 알림만 전송되어 네트워크로 전송되는 불필요한 데이터 양을 최소화합니다.\n\n판매자를 위해 Redis pub/sub를 통해 발행된 알림이 모든 읽기 API 인스턴스에 수신됩니다. 그러나 모든 알림이 모든 연결된 클라이언트에게 관련이 있는 것은 아닙니다. 연결된 각 클라이언트에게는 관련 알림만 전송되도록 하기 위해 읽기 API는 클라이언트의 구독에 따라 알림을 필터링합니다.\n\n[2024-05-20-HowWeUsedServer-SentEventsSSEtoDeliverReal-TimeNotificationsonOurBackend_1.png] 를 참고하세요.\n\n<div class=\"content-ad\"></div>\n\ntable 태그를 Markdown 형식으로 변경하십시오.\n\n```js\nimport { AuthGuard } from '@nestjs/passport';\nimport { Controller, Sse, UseGuards } from '@nestjs/common';\n\n@Controller('/live-notification')\nexport class LiveNotificationController {\n  constructor(private readonly liveNotificationService: LiveNotificationService) {}\n\n  @Sse()\n  @ApiBearerAuth()\n  @UseGuards(AuthGuard('jwt'))\n  public getEventsBySeller(@Tracers() tracers: ITracers, @SellerId() sellerId: number) {\n    return this.liveNotificationService.subscribeForSeller(sellerId);\n  }\n}\n```\n\n```js\nimport { EventEmitter } from 'events';\nimport { filter, fromEvent } from 'rxjs';\n\n@Injectable()\nexport class LiveNotificationService implements OnModuleInit {\n  \n  private readonly emitter = new EventEmitter();\n  \n  ...\n  \n  public async emit(data: LiveNotification) {\n    this.emitter.emit('liveNotification', { data });\n  }\n  \n  public subscribeForSeller(sellerId: number) {\n    const source = fromEvent(this.emitter, 'liveNotification');\n    return source.pipe(\n      filter(({ data: liveNotification }) => \n        liveNotification?.content == 'heartbeat' || \n        liveNotification?.sellerId == sellerId)\n    );\n  }\n}\n```\n\n# 문제점\n\n## 수직 확장 문제\n\n<div class=\"content-ad\"></div>\n\n우리의 SSE와 Redis 기반 알림 시스템은 연결된 클라이언트에 신속하고 효율적으로 알림을 전달합니다. 그러나 알림이 증가함에 따라 모든 읽기 API가 모든 알림을 구독하기 때문에 시스템에서 잠재적인 수직 확장 문제가 예상됩니다. 이는 시스템에서 잠재적 병목 현상을 일으킬 수 있습니다. 판매자를 위해 Redis pub/sub를 통해 알림이 발행되면 읽기 API의 모든 인스턴스에서 받게 되지만, 모든 알림이 모든 연결된 클라이언트에게 관련이 있는 것은 아닙니다. 각 연결된 클라이언트에게는 관련 있는 알림만 전송되도록 하기 위해 읽기 API에서 고객의 구독에 따라 알림을 필터링합니다. 알림의 수가 많아지면 이 필터링 과정이 느려져 알림 전송이 지연될 수 있습니다.\n\n## 하트비트 메시지\n\n우리 프론트엔드 개발자들은 권한 부여 헤더를 보내기 위해 이벤트 소스 라이브러리를 사용했습니다. 그러나 서버와 브라우저 간 연결이 1분 후에 끊어진다고 보고했습니다. 문제를 조사한 후 라이브러리가 끊김을 감지하기 위해 하트비트 메시지를 보내도록 요구함을 깨닫게 되었습니다. 문제를 해결하기 위해 30초마다 하트비트 메시지를 보내는 기능을 구현하여 문제를 해결했습니다.\n\n```js\nonModuleInit(): any {\n  setInterval(() => {\n    const emitterListenerCount = this.emitter.listenerCount('liveNotification');\n    this.logger.log(`활성 에미터 리스너 수를 가진 SSE 클라이언트로 하트비트 메시지를 보냈습니다: ${emitterListenerCount}`);\n    this.emitter.emit('liveNotification', { data: { content: 'heartbeat' } });\n  }, 30000);\n}\n```\n\n<div class=\"content-ad\"></div>\n\n# 성능\n\n현재 알림 시스템은 90,000개의 동시 연결을 처리하고 있으며, 우리는 15개의 팟을 운영 중이며 부하를 처리하고 있습니다. 각 Kubernetes 팟은 약 800MB의 메모리와 300Mi의 CPU 리소스를 소비합니다.\n\n# 요약\n\n서버-센트 이벤트(SSE)는 실시간 알림 전달 기능을 제공하면서 웹 소켓과 비교해 더 간단하고 가벼운 솔루션으로 입증되었습니다. Redis pub/sub를 사용하여 알림을 여러 팟에 분산하여 모든 클라이언트가 제때 알림을 수신하도록 보장했습니다. SSE 기반의 알림 시스템을 통해 우리는 클라이언트에게 알림을 효과적으로 전달하는 안정적이고 효율적인 솔루션을 성공적으로 구축했습니다.\n\n<div class=\"content-ad\"></div>\n\n프론트엔드 세부 정보와 구현에 대해 좀 더 알고 싶다면, 제 동료가 쓴 기사를 읽어보시기를 추천합니다.","ogImage":{"url":"/assets/img/2024-05-20-HowWeUsedServer-SentEventsSSEtoDeliverReal-TimeNotificationsonOurBackend_0.png"},"coverImage":"/assets/img/2024-05-20-HowWeUsedServer-SentEventsSSEtoDeliverReal-TimeNotificationsonOurBackend_0.png","tag":["Tech"],"readingTime":7},{"title":"앵귤러에서 let 구문을 사용해 타입 축소하는 방법","description":"","date":"2024-05-20 22:15","slug":"2024-05-20-EleganttypenarrowingwithletsyntaxinAngular","content":"\n\n![이미지](/assets/img/2024-05-20-EleganttypenarrowingwithletsyntaxinAngular_0.png)\n\nAngular 18은 아직 릴리즈되지 않았지만, 18.1 버전에 이미 새로운 강력한 추가 기능이 기다리고 있습니다. 바로 템플릿 로컬 변수 또는 @let 구문이라고도 알려진 기능입니다.\n\n간단히 말해서, 이제 템플릿에서 JavaScript 파일과 동일한 방식으로 보조 로컬 변수를 정의할 수 있게 됩니다.\n\n가능한 응용 중 하나는 AsyncPipe를 사용하여 구독하는 스트림에서 값 unwrap하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n현재 솔루션\n\n\n```js\n<ng-container *ngIf=\"data$ | async as data\">\n  <p>{ data }</p>\n</ng-container>\n```\n\n\n```js\n<ng-container *ngIf=\"{ data: data$ | async } as vm\">\n  <p>{ vm.data }</p>\n</ng-container>\n```\n\n새로운 솔루션\n\n<div class=\"content-ad\"></div>\n\n```js\n@let data = data$ | async ;\n\n<p>{ data }</p>\n```\n\n이것은 '동적' 뷰 모델 속성을 생성할 수도 있게 해줍니다. 특히, 사용자 선택에 따라 변경되는 isActive와 같은 컬렉션을 다룰 때 특히 유용합니다.\n\nEnea Jahollari의 훌륭한 기사에서 새로운 구문의 다양한 사용법에 대해 더 읽어볼 수 있습니다.\n\n언급할 가치가 있는 점은 신호의 값을 템플릿 로컬 변수에 저장할 수 있는 능력입니다. 이것은 타입 좁힘이 매우 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 해당 컴포넌트를 고려해봅시다:\n\n```js\ntype AnalysisState =\n  | { status: 'pending' }\n  | { status: 'completed'; result: string };\n  \n@Component({\n  selector: 'app-analysis-card',\n  standalone: true,\n  templateUrl: './analysis-card.component.html',\n  styleUrl: './analysis-card.component.scss',\n})\nexport class AnalysisCardComponent {\n  public analysisState: AnalysisState = { status: 'pending' };\n\n  constructor() {\n    // mock change simulation\n    setTimeout(() => {\n      this.analysisState = { status: 'completed', result: 'xyz' };\n    }, 5000);\n  }\n}\n```\n\n```js\n@if (analysisState.status === \"pending\") {\n  <p>Analysis is pending</p>\n} @else {\n  <p>Analysis completed with result: { analysisState.result }</p>\n}\n```\n\n새로운 제어 흐름 구문을 통해 이전의 *ngIf 디렉티브로는 불가능했던 적절한 타입 축소가 가능해졌습니다. 상세 내용은 제 이전 글 중 하나에서 확인하실 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n그러나, 컴포넌트 상태를 처리하기 위해 시그널을 사용하는 경우:\n\n```js\n@Component({\n  selector: 'app-analysis-card',\n  standalone: true,\n  templateUrl: './analysis-card.component.html',\n  styleUrl: './analysis-card.component.scss',\n})\nexport class AnalysisCardComponent {\n  public analysisState: WritableSignal<AnalysisState> = signal({\n    status: 'pending',\n  });\n\n  constructor() {\n    // 모의 변경 시뮬레이션\n    setTimeout(() => {\n      this.analysisState.set({ status: 'completed', result: 'xyz' });\n    }, 5000);\n  }\n}\n```\n\n타입 추론이 작동하지 않습니다:\n\n```js\n@if (analysisState().status === \"pending\") {\n  <p>분석 대기 중</p>\n} @else {\n  <!-- 'result' 속성이 'AnalysisState' 타입에 존재하지 않습니다. -->\n  <p>분석이 완료되었습니다. 결과: { analysisState().result }</p>\n}\n```\n\n<div class=\"content-ad\"></div>\n\n두 가지 해결 방법이 있습니다:\n\n- 시그널 값과 함께 게터를 사용하세요\n\n```js\n@Component({\n  selector: 'app-analysis-card',\n  standalone: true,\n  templateUrl: './analysis-card.component.html',\n  styleUrl: './analysis-card.component.scss',\n})\nexport class AnalysisCardComponent {\n  public _analysisState: WritableSignal<AnalysisState> = signal({\n    status: 'pending',\n  });\n\n  get analysisState() {\n    return this._analysisState();\n  }\n\n  constructor() {\n    // 모의 변경 시뮬레이션\n    setTimeout(() => {\n      this._analysisState.set({ status: 'completed', result: 'xyz' });\n    }, 5000);\n  }\n}\n```\n\n```js\n@if (analysisState.status === \"pending\") {\n  <p>분석 대기 중</p>\n} @else {\n  <p>분석 완료 및 결과: { analysisState.result }</p>\n}\n```\n\n<div class=\"content-ad\"></div>\n\n- 보조 if 블록으로 신호의 값 언래핑하기\n\n```js\n@Component({\n  selector: 'app-analysis-card',\n  standalone: true,\n  templateUrl: './analysis-card.component.html',\n  styleUrl: './analysis-card.component.scss',\n})\nexport class AnalysisCardComponent {\n  public analysisState: WritableSignal<AnalysisState> = signal({\n    status: 'pending',\n  });\n\n  constructor() {\n    // mock change simulation\n    setTimeout(() => {\n      this.analysisState.set({ status: 'completed', result: 'xyz' });\n    }, 5000);\n  }\n}\n```\n\n```js\n@if (analysisState(); as analysisState) {\n  @if (analysisState.status === \"pending\") {\n    <p>Analysis is pending</p>\n  } @else {\n    <p>Analysis completed with result: { analysisState.result }</p>\n  }\n}\n```\n\n새로운 방법으로 템플릿 지역 변수를 만드는 방법을 소개했으므로 목표를 달성하는 더 우아한 방법이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nanalysisState = analysisState();\n\nif (analysisState.status === \"pending\") {\n  console.log(\"Analysis is pending\");\n} else {\n  console.log(`Analysis completed with result: ${analysisState.result}`);\n}\n```\n\nThe `analysisState` variable now holds the result of the `analysisState()` function call. If the status is \"pending,\" a message saying \"Analysis is pending\" will be logged. If the status is anything else, a message saying \"Analysis completed with result: \" followed by the actual result will be logged.\n\nThank you, and have a great day! 😊\n","ogImage":{"url":"/assets/img/2024-05-20-EleganttypenarrowingwithletsyntaxinAngular_0.png"},"coverImage":"/assets/img/2024-05-20-EleganttypenarrowingwithletsyntaxinAngular_0.png","tag":["Tech"],"readingTime":5},{"title":"리액트 컴파일러, 리액트의 새로운 방식","description":"","date":"2024-05-20 22:13","slug":"2024-05-20-ReactCompileranewwaytoReact","content":"\n\n최근 Meta사에서 React를 위한 새로운 컴파일러를 발표했어요. 이 소식은 생각보다 더 큰 뜻을 가지고 있어요. 이 블로그에서는 React의 새로운 컴파일러에 대한 기본적인 내용을 이해해보려 합니다.\n\n![ReactCompileranewwaytoReact](/assets/img/2024-05-20-ReactCompileranewwaytoReact_0.png)\n\nUI 컴파일러에 대해 간단히 살펴보자면:\nSvelte, Angular, Solid과 같은 UI 프레임워크들은 이미 내장된 컴파일러를 가지고 있어요. 이러한 컴파일러들은 코드를 최적화된 JavaScript로 변환하여 성능을 향상시키고 런타임 오버헤드를 줄여줘요. 따라서 React에 컴파일러를 도입하는 것은 오랜 기간이 지나야 한다고 생각되었던 일이에요. 이러한 조치는 React를 이러한 현대적인 프레임워크들과 동일선상으로 끌어올리는 데 도움이 되며, 코드베이스가 커짐에 따라 발생하는 성능 문제를 해결하는 데 도움이 될 거예요.\n\n컴파일러가 왜 필요한가요?\n여러 이유가 있어요. 대부분의 React 개발자들은 이미 알고 있지만, React 렌더링은 코드베이스의 크기가 커지면 성능이 저하될 수 있는 공격적인 특성을 가지고 있어요. React는 메모이제이션 기술을 제공하지만, 효과적으로 학습하고 구현하기 어려울 수 있어요.\n\n<div class=\"content-ad\"></div>\n\nReact 컴파일러 작동 방식에 대해 알아보겠습니다.\n컴파일러가 일관된 최적화를 달성하는 데 사용하는 여러 기술이 있습니다. 이 중 몇 가지를 아래에서 언급해보겠습니다:\n\n자동 의존성 분석: React 컴파일러는 자동 의존성 검출 및 최적화를 도입하여 성능을 향상시킵니다. 변경된 컴포넌트만 재렌더링함으로써 불필요한 렌더링을 최소화하고 애플리케이션 실행 속도를 높입니다. 컴포넌트 의존성을 분석하고 캐싱 메커니즘을 활용하여 React는 불필요한 렌더링을 최소화하고 응용 프로그램을 가속화합니다. 이는 수동 의존성 명시의 필요성을 줄이고 보일러플레이트 코드를 낮춥니다. 컴파일러는 코드베이스 전체에서 일관된 최적화를 보장하며 업데이트를 일괄처리하고 중복 렌더링을 건너뛰는 최적화된 렌더링 전략을 구현합니다.\n\n코드 변환: React 컴파일러는 빌드 시간에 코드를 변환합니다. 클 때 필요한 곳에 메모이제이션 논리를 삽입하여 비용이 많이 드는 계산이나 큰 객체가 변경된 경우에만 다시 계산되거나 재생성되도록 보장합니다. 이 변환은 개발자가 컴포넌트를 최적화하는 데 필요한 수동 노력을 줄입니다.\n\n향상된 Hooks: React 컴파일러는 useMemo와 useCallback과 같은 기존 훅을 개선할 수 있습니다. 적절한 곳에 자동으로 이러한 훅을 삽입하여 컴파일러는 개발자가 수동으로 의존성을 지정하지 않고도 메모이제이션의 이점을 얻을 수 있도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n과거 트랜스파일된 코드와 새로운 컴파일된 코드 비교\n코드로 들어가서 컴파일러의 유무에 따라 동일한 코드가 어떻게 다른지 살펴보겠습니다:\n\n간단한 코드 스니펫을 살펴보죠. Hello World 텍스트가 있는 간단한 `div`입니다.\n\n```js\nexport default function Hello() {\n  return <div className=\"foo\">Hi There</div>;\n}\n```\n\n현재 컴파일 없이 트랜스파일된 코드는 이렇게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\n// transpiled code (without compiler)\nexport default function Hello() {\n  return __jsx(\"div\", {\n    className: \"foo\"\n  }, \"Hi There\");\n}\n```\n\n```js\n// compiled code (with React compiler)\nimport { c as _c } from \"react/compiler-runtime\";\nexport default function Hello() {\n  const $ = _c(2);\n  if ($[0] !== \"8b8c470796627445ffbcfa7127db0cfba267736e0e4708dfa79d32043c5e5a7c\") {\n    for (let $i = 0; $i < 2; $i += 1) {\n      $[$i] = Symbol.for(\"react.memo_cache_sentinel\");\n    }\n    $[0] = \"8b8c470796627445ffbcfa7127db0cfba267736e0e4708dfa79d32043c5e5a7c\";\n  }\n  let t0;\n  // cached component\n  if ($[1] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = __jsx(\"div\", {\n      className: \"foo\"\n    }, \"Hi There\");\n    $[1] = t0;\n  } else {\n    t0 = $[1];\n  }\n  return t0;\n}\n```\n\nNow there is a lot of compiled code; let's look at the code block by block.\nThe first block is responsible for loading the memo cache, which caches the components.\nThe second `if` statement is where the compiler saves our `div` in a cache. If there is a re-render and nothing in the component has changed, we get the cached component. This is how static content is handled by the compiler.\n\nLet’s look at a more advanced code snippet to see how the compiler manages states in the code.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nimport { useState } from \"react\";\n\nexport default function Hello() {\n  const [name, setName] = useState(\"Jack\");\n  return (\n    <div>\n      <p>Hi: {name}</p>\n      <strong>Static Content</strong>\n    </div>\n  );\n}\n```\n\n여기에는 코드에 정의된 상태 \"name\"이 있지만 컴포넌트 내에서 상태가 전혀 변경되지 않습니다. 이것이 React 컴파일러의 강점을 볼 때입니다. 변환된 코드와 컴파일된 코드를 비교해 봅시다.\n\n```js\n// 변환된 코드 (컴파일러 미사용)\nimport { useState } from \"react\";\nexport default function Hello() {\n  const [name, setName] = useState(\"Jack\");\n  return __jsx(\"div\", null, __jsx(\"p\", null, \"Hi: \", name), __jsx(\"strong\", null, \"Static Content\"));\n}\n```\n\n지금 변환된 코드를 가져왔습니다. 상태는 최적화 없이 변환됩니다. 이제 컴파일된 코드를 살펴봅시다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\nimport { c as _c } from \"react/compiler-runtime\";\nimport { useState } from \"react\";\nexport default function Hello() {\n  const $ = _c(6);\n  if ($[0] !== \"ff7f138520311a2041bfadf8c5306ca9ddda64020c5c7c91ce7bfd217639da89\") {\n    for (let $i = 0; $i < 6; $i += 1) {\n      $[$i] = Symbol.for(\"react.memo_cache_sentinel\");\n    }\n    $[0] = \"ff7f138520311a2041bfadf8c5306ca9ddda64020c5c7c91ce7bfd217639da89\";\n  }\n  const [name] = useState(\"Jack\");\n  let t0;\n// name is cached\n  if ($[1] !== name) {\n    t0 = __jsx(\"p\", null, \"Hi: \", name);\n    $[1] = name;\n    $[2] = t0;\n  } else {\n    t0 = $[2];\n  }\n  let t1;\n\n  if ($[3] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t1 = __jsx(\"strong\", null, \"Static Content\");\n    $[3] = t1;\n  } else {\n    t1 = $[3];\n  }\n  let t2;\n  if ($[4] !== t0) {\n    t2 = __jsx(\"div\", null, t0, t1);\n    $[4] = t0;\n    $[5] = t2;\n  } else {\n    t2 = $[5];\n  }\n  return t2;\n}\n```\n\n여기에서 컴파일러에 의해 상태가 자동으로 캐시됩니다. 따라서 다음 렌더링에서는 값이 캐시에서 반환됩니다. 이름의 값을 변경할 때만 if 블록으로 이동하여 향후 렌더링을 위해 캐시에 저장합니다.\n\n결론:\nReact에 컴파일러를 추가하면 프로그래머들이 더 쉽게 최적화된 코드를 작성할 수 있습니다. 이 블로그에서는 React의 컴파일러가 출시되면 기대할 수 있는 기능을 분석하고 이해하기 위한 기본 사용 사례를 탐색했습니다.\nReact 컴파일러의 고급 개념 및 사용 사례에 대해 계속 알아보십시오. 정적 분석, 최적화된 렌더링 및 useMemo와 같은 고급 훅의 사용과 같은 추가 기능을 탐색할 예정입니다.","ogImage":{"url":"/assets/img/2024-05-20-ReactCompileranewwaytoReact_0.png"},"coverImage":"/assets/img/2024-05-20-ReactCompileranewwaytoReact_0.png","tag":["Tech"],"readingTime":5},{"title":"리액트 컨퍼런스 2024 내용 정리","description":"","date":"2024-05-20 22:12","slug":"2024-05-20-ReactConf2024Highlights","content":"\n\n이번년도의 React Conf에서는 React 및 React Native 팀 및 커뮤니티로부터의 흥미로운 발표와 업데이트가 가득했습니다. 컨퍼런스가 종료되면서 React 개발의 미래를 위한 중요한 하이라이트를 살펴보겠습니다.\n\n# React 19이 릴리스 후보로 이동\n\n![ReactConf2024Highlights_0.png](/assets/img/2024-05-20-ReactConf2024Highlights_0.png)\n\n![ReactConf2024Highlights_1.png](/assets/img/2024-05-20-ReactConf2024Highlights_1.png)\n\n<div class=\"content-ad\"></div>\n\nReact 19가 공식적으로 릴리스 후보 단계에 진입했습니다. 안정적인 릴리스는 다가오는 몇 주 안에 예정되어 있습니다. 새 버전은 향상된 기능, 성능 향상, 그리고 현대적인 개발 관행과 더 잘 호환되는 기능을 약속하고 있습니다. 더 많은 정보를 알고 싶다면 Alexander Savelyev의 기사를 방문해주세요.\n\n# React 컴파일러 오픈 소스화\n\n![이미지](/assets/img/2024-05-20-ReactConf2024Highlights_2.png)\n\n중요한 움직임으로 React 팀이 React 컴파일러를 오픈 소스로 공개했습니다. 이 도구는 React 애플리케이션을 최적화하기 위해 React 코드를 매우 효율적인 JavaScript로 변환하여 애플리케이션의 로드 시간을 줄이고 런타임 성능을 향상시키는 것을 목표로 하고 있습니다. 더 많은 정보를 알고 싶다면 React 컴파일러 워킹 그룹 리포지토리를 방문해주세요.\n\n<div class=\"content-ad\"></div>\n\n# React Native의 새로운 아키텍처가 베타 버전으로 출시되었습니다\n\n![ReactConf2024Highlights_3](/assets/img/2024-05-20-ReactConf2024Highlights_3.png)\n\nReact Native는 새로운 아키텍처를 소개했으며 이제 베타 버전으로 사용할 수 있습니다. 이 아키텍처는 React Native를 더 견고하고 확장 가능한 프레임워크로 강화하여 iOS 및 Android 플랫폼의 모바일 응용 프로그램 성능을 향상시키는 것을 약속합니다. 더 많이 알아보려면 React Native의 새로운 아키텍처 리포지토리를 방문하세요.\n\n# 새로운 React Native 앱을 위해 추천하는 Expo\n\n<div class=\"content-ad\"></div>\n\n\n![](/assets/img/2024-05-20-ReactConf2024Highlights_4.png)\n\nThe React Native team officially recommends using Expo for all new React Native applications. Expo provides a comprehensive set of tools and services that streamline the development process, making it easier to build, deploy, and quickly iterate on React Native apps. Explore the React Native website to learn more.\n\n# Universal React Server Components\n\n![](/assets/img/2024-05-20-ReactConf2024Highlights_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n이러한 구성 요소를 사용하면 서버 및 클라이언트 측에서 UI 구성 요소를 렌더링하여 다양한 플랫폼에서 성능 및 리소스 활용을 최적화할 수 있습니다. 자세한 내용은 Universal React Server Components에 대한 내 기사를 방문해 보세요.\n\n# Remix와 React Router 통합\n\n![Image](/assets/img/2024-05-20-ReactConf2024Highlights_6.png)\n\n흥미로운 소식! Remix와 React Router가 통합되었습니다. 이 통합은 React 애플리케이션의 라우팅을 통합하여 React 프로젝트에서 라우트를 효율적이고 간소화된 방법으로 관리하는 것을 목표로 합니다. 더 많은 정보를 원하시면 Shopify의 기사를 방문해 보세요.\n\n<div class=\"content-ad\"></div>\n\n# RedwoodJS가 이제 서버 컴포넌트를 지원합니다\n\n![이미지](/assets/img/2024-05-20-ReactConf2024Highlights_7.png)\n\nRedwoodJS는 React 기반의 의견이 강한 풀 스택 및 서버리스 웹 애플리케이션 프레임워크로 유명한데, 이제 React 서버 컴포넌트를 지원합니다. 이 통합은 동적이고 서버 렌더링된 애플리케이션을 쉽게 구축할 수 있는 기능을 더욱 향상시켰습니다. 더 많은 정보를 알고 싶다면 RedwoodJS의 기사를 방문해주세요.\n\n# Pigment CSS\n\n<div class=\"content-ad\"></div>\n\n![ReactConf2024Highlights_8](/assets/img/2024-05-20-ReactConf2024Highlights_8.png)\n\nPigment CSS는 React 애플리케이션의 스타일링에 대한 새로운 표준을 설정하고 있습니다. 이 혁신적인 CSS-in-JS 라이브러리는 React 생태계와 완벽하게 통합되도록 설계되었으며 성능 및 사용 편의성에 중점을 둔 향상된 스타일링 기능을 제공합니다. 더 많은 정보를 얻으려면 MUI 블로그를 방문해주세요.\n\n# React Native을 이용한 TV, AR 및 VR\n\nReact Native는 TV 애플리케이션 및 공간 컴퓨팅에 대한 향상된 지원으로 거실 및 그 이상으로 그 영역을 확장하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n빌드 엑스포 앱들을 TV용으로 만들어 보세요 \n\n리액트 네이티브 visionOS\n\n리액트 네이티브-tvos\n\n# 보너스 발표\n\n<div class=\"content-ad\"></div>\n\n\n![ReactConf2024Highlights_9](/assets/img/2024-05-20-ReactConf2024Highlights_9.png)\n\n공식 React Conf 앱은 새 React 컴파일러를 활용하여 Google Play 및 App Store에서 사용할 수 있습니다.\n\n![ReactConf2024Highlights_10](/assets/img/2024-05-20-ReactConf2024Highlights_10.png)\n\nReact Native를 위한 새 홈페이지가 론칭되었습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![ReactConf 2024 Highlights](/assets/img/2024-05-20-ReactConf2024Highlights_11.png)\n\n마이크로소프트는 Windows 시작 메뉴와 오피스 워드 데스크톱을 포함한 주요 제품들에서 React Native를 광범위하게 활용하고 있습니다. 이를 통해 크로스 플랫폼 기능을 활용하여 사용자 경험을 향상시키고 소프트웨어 스위트 전반에 걸쳐 개발을 간소화하고 있습니다.\n\n요약하면, React Conf 2024은 다양한 업데이트와 새로운 기술로 React 및 React Native의 미래에 동적인 방향을 제시했습니다. 생방송 이벤트를 놓친 분들이나 세션을 다시 보고 싶은 분들은 녹화된 컨퍼런스를 시청하여 이 영향력 있는 이벤트에서 공유된 본질과 통찰력을 완전히 경험해보세요.\n\n제 LinkedIn에서 언제든지 연락주시기 바랍니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 친절한 한국어 번역 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n- 작가를 클랩하고 팔로우해 주세요 👏\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- 알고리즘 콘텐츠와 안 통하는 블로깅 플랫폼에 지친 적이 있나요? Differ를 사용해보세요\n- PlainEnglish.io에서 더 많은 콘텐츠 확인하기","ogImage":{"url":"/assets/img/2024-05-20-ReactConf2024Highlights_0.png"},"coverImage":"/assets/img/2024-05-20-ReactConf2024Highlights_0.png","tag":["Tech"],"readingTime":5},{"title":"윈도우 IIS 서버에 NextJs 애플리케이션을 배포하는 방법","description":"","date":"2024-05-20 22:10","slug":"2024-05-20-DeployingNextJsApplicationonWindowsIISServer","content":"\n\n\n![Deploying Next.js Application on Windows IIS Server](/assets/img/2024-05-20-DeployingNextJsApplicationonWindowsIISServer_0.png)\n\nNextJS는 단일 페이지 애플리케이션을 만들기 위한 서버 측 렌더링된 React 기반 프레임워크입니다. 서버 측 렌더링 기능으로 인해 매우 인기가 높습니다. NextJS를 사용하여 다음 애플리케이션을 작성하는 것은 매우 간단합니다. NextJS를 사용하기 위해 명시적으로 구성할 필요가 없습니다. npm run dev를 실행하고 애플리케이션을 구축하기 시작하면 됩니다.\n\n# 서버 측 렌더링이란?\n\n서버 측 렌더링(SSR)은 프런트엔드 프레임워크가 웹페이지를 준비하기 위해 사용자별 데이터를 서버 측에서 가져와 사용자 화면으로 보내는 능력입니다. 사용자에게 표시하기 위해 클라이언트 측에서 웹페이지를 준비하는 대신에 서버 측에서 웹페이지를 준비합니다.\n \n\n<div class=\"content-ad\"></div>\n\nNextJS에서는 서버에서 페이지를 렌더링하거나 일반적인 create-react-app SPA처럼 클라이언트 측에서 렌더링할 수 있는 옵션이 있습니다.\n\n서버 측 렌더링의 장점은 분명히 있습니다. 그래서 NextJS 프레임워크가 인기를 얻은 이유이기도 합니다. SSR 덕분에 초기 페이지 로딩이 빨라져 최종 사용자에게 더 나은 상호작용을 제공합니다. 또한 검색 엔진이 사이트를 크롤링하여 더 나은 검색 엔진 최적화를 제공하는데 도움이 됩니다.\n\n# 만약 NextJS가 프론트엔드 프레임워크라면, 이 프레임워크는 어떻게 서버에서 웹 페이지를 준비하는 것일까요?\n\nNextJS 프레임워크는 NodeJS 기반으로 만들어졌습니다. 아시다시피, NodeJS는 서버 측에서 페이지를 준비하기 위해 사용되는 NextJS 애플리케이션에 강력함을 제공하는 백엔드 JavaScript 런타임 환경입니다.\n\n<div class=\"content-ad\"></div>\n\n# NextJS 애플리케이션 배포하기\n\nWindows IIS에서 Next.js 애플리케이션을 호스팅하는 방법을 여러 곳에서 찾아보았어요. 괜찮은 튜토리얼을 몇 개 찾았지만 제 문제를 해결해 주지는 못했어요. Vercel과 Netlify, Heroku 등과 같은 유사한 플랫폼에 호스팅하는 것이 이상적일 것 같아요. 하지만 제 애플리케이션을 로컬에서 호스팅하고 싶어요. 왜냐하면 내 애플리케이션에 대중이 접근하는 것을 원하지 않기 때문이에요. 그건 제 개인적인 용도로 사용할 거거든요. 그래서 여러분이 구글에서 시간을 낭비하지 않고 필요한 정보를 찾을 수 있도록, 간단한 가이드를 제공해 드리겠어요.\n\n하지만 그 전에, 사용하려는 것에 대해 간단히 설명해볼게요.\n\n저희는 iisnode을 사용하려고 해요. iisnode은 C++로 작성된 오픈 소스 네이티브 IIS 모듈로, Node.js를 Windows IIS 내에서 실행할 수 있게 해줘요. 저희는 IIS 서버 배포 방식으로 Node.js를 사용할 거에요. 그러려면 Server.js와 web.config 두 파일이 필요하답니다.\n\n<div class=\"content-ad\"></div>\n\n지금 application의 진입점인 server.js를 만들어보겠습니다:\n\n```js\nconst { createServer } = require(\"http\");\nconst { parse } = require(\"url\");\nconst next = require(\"next\");\n\nconst dev = process.env.NODE_ENV !== \"production\";\n\nconst port = process.env.PORT || 3000; // 포트를 IIS가 실행 중인 포트로 변경하세요. 기본값은 80이고 개발 중이면 3000입니다.\nconst hostname = \"localhost\";\nconst app = next({ dev, hostname, port });\nconst handle = app.getRequestHandler();\n\napp.prepare().then(() => {\n  createServer(async (req, res) => {\n    try {\n      const parsedUrl = parse(req.url, true);\n      const { pathname, query } = parsedUrl;\n\n      if (pathname === \"/a\") {\n        await app.render(req, res, \"/a\", query);\n      } else if (pathname === \"/b\") {\n        await app.render(req, res, \"/b\", query);\n      } else {\n        await handle(req, res, parsedUrl);\n      }\n    } catch (err) {\n      console.error(\"Error occurred handling\", req.url, err);\n      res.statusCode = 500;\n      res.end(\"internal server error\");\n    }\n  })\n    .once(\"error\", (err) => {\n      console.error(err);\n      process.exit(1);\n    })\n    .listen(port, async () => {\n      console.log(`> Ready on http://localhost:${port}`);\n    });\n});\n```\n\n웹 구성 파일인 web.config은 IIS 및 ASP.NET Core Module이 응용 프로그램을 구성하는 데 사용하는 파일입니다. 그래서 여기에 우리의 web.config 파일이 있습니다:\n\n```js\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<!--\n     이 구성 파일은 iisnode를 사용하여 IIS 또는 IIS Express에서 노드 프로세스를 실행하는 경우 필요합니다.\n     자세한 내용은 다음을 참조하십시오:\n\n     https://github.com/tjanczuk/iisnode/blob/master/src/samples/configuration/web.config\n-->\n\n<configuration>\n  <system.webServer>\n    <!-- WebSocket 지원에 대한 자세한 정보는 http://blogs.msdn.com/b/windowsazure/archive/2013/11/14/introduction-to-websockets-on-windows-azure-web-sites.aspx에서 확인할 수 있습니다 -->\n    <webSocket enabled=\"false\" />\n    <handlers>\n      <!-- server.js 파일이 iisnode 모듈에 의해 처리되는 node.js 사이트임을 나타냄 -->\n      <add name=\"iisnode\" path=\"server.js\" verb=\"*\" modules=\"iisnode\"/>\n    </handlers>\n    <rewrite>\n      <rules>\n        <!-- node-inspector 디버깅을 위한 요청 방해하지 않음 -->\n        <rule name=\"NodeInspector\" patternSyntax=\"ECMAScript\" stopProcessing=\"true\">\n          <match url=\"^server.js\\/debug[\\/]?\" />\n        </rule>\n\n        <!-- 먼저 /public 폴더의 물리적 파일과 일치하는 들어오는 URL을 고려 -->\n        <rule name=\"StaticContent\">\n          <action type=\"Rewrite\" url=\"public{REQUEST_URI}\"/>\n        </rule>\n\n        <!-- 모든 다른 URL은 node.js 사이트 진입점으로 매핑됨 -->\n        <rule name=\"DynamicContent\">\n          <conditions>\n            <add input=\"{REQUEST_FILENAME}\" matchType=\"IsFile\" negate=\"True\"/>\n          </conditions>\n          <action type=\"Rewrite\" url=\"server.js\"/>\n        </rule>\n      </rules>\n    </rewrite>\n    \n    <!-- 'bin' 디렉토리는 node.js에서 특별한 의미가 없으며, 앱을 그 안에 배치할 수 있음 -->\n    <security>\n      <requestFiltering>\n        <hiddenSegments>\n          <add segment=\"node_modules\"/>\n        </hiddenSegments>\n      </requestFiltering>\n    </security>\n\n    <!-- 오류 응답을 변경하지 않도록 함 -->\n    <httpErrors existingResponse=\"PassThrough\" />\n    <iisnode node_env=\"production\"/>\n\n    <!--\n      다음 옵션을 사용하여 IIS 내에서 Node를 호스트하는 방법을 제어할 수 있습니다:\n        * watchedFiles: 변경 사항을 감지하여 서버를 다시 시작할 파일 목록\n        * node_env: NODE_ENV 환경 변수로 전달될 값\n        * debuggingEnabled - 기본 디버거가 활성화되는지 여부\n\n      모든 옵션 목록은 https://github.com/tjanczuk/iisnode/blob/master/src/samples/configuration/web.config에서 확인 가능합니다\n    -->\n    <!--<iisnode watchedFiles=\"web.config;*.js\"/>-->\n  </system.webServer>\n</configuration>\n```\n\n<div class=\"content-ad\"></div>\n\n웹 구성 파일을 추가한 후에는 package.json을 약간 변경해야 합니다. npm run start가 서버를 시작하도록 설정되어야 하며 next start를 사용하지 않아야 합니다. 다음은 package.json 스크립트가 보이는 방식입니다:\n\n```js\n  \"scripts\": {\n    \"dev\": \"node server.js\",\n    \"build\": \"next build\",\n    \"start\": \"node server.js\",\n    \"lint\": \"next lint\"\n  },\n```\n\nserver.js를 추가하고 web.config를 추가한 후 npm run build를 실행하면 애플리케이션의 루트 폴더가 다음과 같이 보입니다.\n\n![애플리케이션 루트 폴더](/assets/img/2024-05-20-DeployingNextJsApplicationonWindowsIISServer_1.png)\n\n<div class=\"content-ad\"></div>\n\n# IIS에서 NextJS 애플리케이션을 배포하는 방법\n\n배포를 설정하기 전에 IISNode와 URLRewrite를 설치해야 합니다. 그 후, IIS에 새 웹 사이트를 생성하고 물리적 경로를 .next, node_modules, server.js 및 web.config가 포함된 폴더로 지정하세요. 권한 문제나 다른 가능한 문제를 피하려면 해당 폴더를 Windows Server의 루트 사용자 폴더 안에 배치하세요.\n\nIIS에서 웹 서버를 시작하면 자동으로 iisnode이라는 새 폴더가 생성됩니다.\n\n만약 배포 폴더가 C:/ 드라이브의 wwwroot에 있다면 전체 폴더에 대해 IIS_Users에게 읽기/쓰기 권한을 명시적으로 부여해야 할 수 있습니다. 다른 예상치 못한 오류가 발생하는 경우, 댓글을 남기거나 내 Discord @pallepadehat을 추가해주세요. 도와드릴게요!\n\n<div class=\"content-ad\"></div>\n\n이 Github Repo에서 소스 코드를 확인할 수 있어요.","ogImage":{"url":"/assets/img/2024-05-20-DeployingNextJsApplicationonWindowsIISServer_0.png"},"coverImage":"/assets/img/2024-05-20-DeployingNextJsApplicationonWindowsIISServer_0.png","tag":["Tech"],"readingTime":7},{"title":"리액트, Framer 연동해서 사용하기","description":"","date":"2024-05-20 22:09","slug":"2024-05-20-ReactFramersequences","content":"\n\n웹 애플리케이션용 애니메이션 코드 학습 경험을 공유하려 합니다. 웹 애플리케이션을 위해 애니메이션을 코드로 작성하는 데 사용한 몇 가지 간단하고 빠른 기술을 소개하겠습니다.\n\n## 내용\n\n5분 이내에 만들 수 있는 3가지 기본사항을 살펴보겠습니다.\n\n- 각기 다른 변형 및 staggarChildren으로 간단한 연속 효과\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](https://miro.medium.com/v2/resize:fit:1400/1*RDCcMOuV4CbcB0dhPd-sgQ.gif)\n\n2. Sequencing with useAnimate()\n\n![Image 2](https://miro.medium.com/v2/resize:fit:1400/1*-vqhUfXmPoehZSW8hRd7Lg.gif)\n\n3. Sequence sequences 😄\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*fYmW-ckGVVVdBn1hR23jzA.gif\" />\n\n## 설정\n\nReact 애플리케이션에 framer-motion을 설치하기만 하면 됩니다.\n\n```js\nnpm install framer-motion\n```\n\n<div class=\"content-ad\"></div>\n\n## 이 기사에 대한 링크\n\nFigma 링크, CodeSandbox: 링크 1, 링크 2, 링크 3\n\n# 변형 및 staggerChildren을 사용한 간단한 순차\n\n이것은 간단한 시각적 상태 변경이 있는 요소들에서 순차를 만드는 가장 쉬운 방법입니다. 이에는 두 가지 부분만이 필요합니다:\n\n<div class=\"content-ad\"></div>\n\n- 변형을 사용하여 애니메이션 설정하기\n- staggerChildren으로 조정하기\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*RDCcMOuV4CbcB0dhPd-sgQ.gif)\n\n## 단계\n\n- 애니메이션 디자인하기. 글귀가 약간 세로 방향으로 움직이면서 일련의 카드를 흐려지게 만들고 카드를 가로로 가로질러 나누는 선을 그렸습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-20-ReactFramersequences_0.png\" />\n\n2. 컴포넌트와 스타일을 생성합니다.\n\n```js\ntype Props = {\n  label: string;\n  text: string;\n};\n\nexport const Card = ({ label, text }: Props) => {\n  return (\n    <div className={styles.cardContainer}>\n      <div className={styles.stepLabel}>{label}</div>\n      <div className={styles.divider}></div>\n      <div className={styles.text}>{text}</div>\n    </div>\n  );\n};\n```\n\n3. 이제 우리는 컴포넌트 내에서 재생하고 싶은 애니메이션을 위해 framer variants를 연결할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*dJC-pf0US95Bjk_16mz6vw.gif\" />\n\n변형 요소는 애니메이션이 시작하고 끝나는 스타일을 정의합니다. 여기서 텍스트에 대한 fadeIn 애니메이션과 함께 (텍스트가 수직으로도 5px 이동하는) 'start' 및 'end' 속성을 정의하고, 구분선에 대한 drawIn 애니메이션에 대해서도 두 가지 변형을 사용했습니다.\n\n```js\n...\n  const fadeIn = {\n    start: { opacity: \"0%\", y: \"0px\" },\n    end: {\n      opacity: \"100%\",\n      y: \"-5px\",\n      transition: {\n        duration: 1.2,\n        ease: \"easeOut\",\n        delay: 2,\n      },\n    },\n  };\n\n  const drawIn = {\n    start: { width: \"0\" },\n    end: {\n      width: \"100%\",\n      transition: {\n        duration: 1.2,\n        ease: \"easeOut\",\n        delay: 2,\n      },\n    },\n  };\n...\n```\n\n4. 부모 구성 요소에서 일련의 순서를 조율합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*RDCcMOuV4CbcB0dhPd-sgQ.gif)\n\n부모 'CardContainer'에서 몇 줄의 코드로 자식 카드에 staggerChildren을 사용하여 지연을 둔 애니메이션을 시작할 수 있습니다.\n\n```js\n...\nexport const CardContainer = () => {\n  const stagger = {\n    start: {},\n    end: {\n      transition: {\n        staggerChildren: 1,\n      },\n    },\n  };\n...\n```\n\n이 샌드박스에서 전체 코드를 확인하세요.\n\n\n<div class=\"content-ad\"></div>\n\n# useAnimate()을 사용한 순차 처리\n\n이 방법은 조금 더 노력이 필요하지만 더 많은 제어를 제공합니다. 각각의 요소를 '선택'하고 서로의 속성 및 타이밍에 관계하여 변경하는 개념입니다.\n\n- 대상으로 지정할 요소들에 대한 범위 추가\n- 요소들을 대상으로 선택하고 애니메이션을 추가합니다\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*-vqhUfXmPoehZSW8hRd7Lg.gif)\n\n<div class=\"content-ad\"></div>\n\n## 단계\n\n1. 애니메이션을 디자인하세요.\n\n![image](/assets/img/2024-05-20-ReactFramersequences_1.png)\n\n2. 컴포넌트를 생성하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n...\nexport const Card = () => {\n  return (\n    <div className={styles.cardContainer}>\n      <div className={styles.textBox}>\n        <div className={styles.i}>I</div>\n        <div className={styles.heartTrack}>\n          <div className={styles.heart}>\n            <img src=\"/images/heart.svg\" alt=\"heart\" />\n          </div>\n        </div>\n        <div className={styles.framer}>Framer</div>\n      </div>\n    </div>\n  );\n};\n...\n```\n\n![React Framer sequences](/assets/img/2024-05-20-ReactFramersequences_2.png)\n\n3. 사용할 요소에 애니메이션을 정의하여 순차적인 애니메이션을 활성화합니다. 이 경우에는 카드 컴포넌트의 바깥 요소에 범위를 설정하여 '나무 구조 안에서 애니메이션화할 요소를 선택할 수 있습니다.\n\n```js\n...\n\nexport const Card = () => {\n  const [scope, animate] = useAnimate();\n...\n\n  return (\n    <div className={styles.cardContainer} ref={scope}>\n   ...\n```\n\n<div class=\"content-ad\"></div>\n\n4. 순서를 생성합니다. 하트가 텍스트로 내려오면서 약간 회전하고, 내 텍스트가 만나도록 슬라이드되기를 원합니다. 대상 요소를 참조하는 방법은 여러 가지가 있습니다. 클래스이름을 사용하고, 각 속성에 대한 모든 값을 제공하고 있습니다.\n\n노트할 사항은 제 텍스트 애니메이션에 설정된 ‘`’으로, 연속성을 사용하여 애니메이션이 재생되는 시점을 정의할 수 있습니다. 타임라인에서 요소를 동시에 시작할 수 있는 유용한 방법 중 하나입니다.\n\n```js\nanimate(\n[\n  [\n    `.${styles.heart}`,\n    { rotate: [0, 10, -10, 0], top: [\"-20%\", \"40%\"] },\n    { duration: 3 },\n  ],\n\n  [\n    `.${styles.textBox}`,\n    { gap: [\"250px\", \"24px\"] },\n    { duration: 2, at: \"<\" },\n  ],\n],\n{ delay: 1 }\n);\n}, []);\n```\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*-vqhUfXmPoehZSW8hRd7Lg.gif\" />\n\n<div class=\"content-ad\"></div>\n\n해당 샌드박스에서 전체 코드를 확인하세요.\n\n# 시퀀스 하는 시퀀스 😄\n\n마침내, 일련의 시퀀스를 조율하는 방법을 찾고 있었는데, 이것이 문서에서 찾을 수 있는 가장 간단한 방법이었습니다. 만약보다 견고한 해결책이 있다면 공유해주세요.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*fYmW-ckGVVVdBn1hR23jzA.gif)\n\n<div class=\"content-ad\"></div>\n\n## 단계\n\n- 시퀀스에 지연 추가\n\n```js\n  useEffect(() => {\n    animate(\n      [\n        [\n          scope.current,\n          { backgroundColor: [\"#ED2733\", \"#FF4D98\"] },\n          { duration: 1 },\n        ],\n        [\n          `.${styles.heart}`,\n          { rotate: [0, 10, -10, 0], top: [\"-20%\", \"40%\"] },\n          { duration: 3 },\n        ],\n        [\n          `.${styles.textBox}`,\n          { gap: [\"250px\", \"24px\"] },\n          { duration: 2, at: \"<\" },\n        ],\n      ],\n      { delay: delay }\n    );\n  }, []);\n```\n\n2. Cards에 delay 속성 전달\n\n<div class=\"content-ad\"></div>\n\n위의 코드를 번역하면 다음과 같다.\n\n```javascript\n...\n<Card delay={1} />\n<Card delay={2} />\n...\n```\n\n이 샌드박스에서 전체 코드를 확인할 수 있습니다.\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-05-20-ReactFramersequences_0.png"},"coverImage":"/assets/img/2024-05-20-ReactFramersequences_0.png","tag":["Tech"],"readingTime":6},{"title":"리액트 시작하는 개발자가 봐야하는 글","description":"","date":"2024-05-20 22:08","slug":"2024-05-20-GettingStartedwithReactYourFunandEasyGuide","content":"\n\n- HTML: 웹페이지의 기본 구조를 이해합니다.\n- CSS: 스타일을 적용하여 웹페이지를 멋지게 만듭니다.\n- JavaScript: 웹페이지를 인터랙티브하게 만드는 코딩 언어를 배웁니다.\n- DOM 조작: 웹페이지를 동적으로 변경하는 방법에 익숙해집니다.\n\n- TODO 앱 만들기: 간단한 할 일 목록을 만들면서 배운 것을 연습해보세요!\n\n```js\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <title>나의 TODO 앱</title>\n  <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n  <div class=\"todo-container\">\n    <input type=\"text\" id=\"todo-input\" placeholder=\"새로운 할 일 추가\">\n    <button id=\"add-btn\">추가</button>\n    <ul id=\"todo-list\"></ul>\n  </div>\n  <script src=\"script.js\"></script>\n</body>\n</html>\n```\n\n```css\n/* styles.css */\n.todo-container {\n  max-width: 400px;\n  margin: 0 auto;\n}\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n// script.js\nconst todoInput = document.getElementById('todo-input');\nconst addBtn = document.getElementById('add-btn');\nconst todoList = document.getElementById('todo-list');\n\naddBtn.addEventListener('click', () => {\n  const todoText = todoInput.value.trim();\n  if (todoText !== '') {\n    const todoItem = document.createElement('li');\n    todoItem.textContent = todoText;\n    todoList.appendChild(todoItem);\n    todoInput.value = '';\n  }\n});\n```\n\n- Components: 요소들을 웹페이지 구성의 빌딩 블록으로 생각해보세요.\n- Props: 이것들은 요소들이 서로 대화할 때 사용하는 메시지와 같습니다.\n- UseState: 웹페이지에서 무슨 일이 일어나고 있는지 추적하는 데 도움을 줍니다.\n- UseEffect: 이것은 웹페이지가 서로 다른 액션에 대응하는 방법을 관리합니다.\n\n- Create Components: 할 일 목록을 재사용할 수 있는 작은 부분들로 나누세요.\n- Use Props: 할 일 목록의 다른 부분들 사이에 정보를 전달하세요.\n- Manage State: useState로 할 일 목록에서 무슨 일이 일어나고 있는지 추적하세요.\n- Handle Side Effects: useEffect를 사용하여 새로운 할 일 항목을 가져오거나 무언가 변경될 때 페이지를 업데이트하는 등의 작업을 수행하세요.\n\n```js\n// script.js\nconst TodoApp = () => {\n  const [todos, setTodos] = React.useState([]);\n  const [todoText, setTodoText] = React.useState('');\n\n  const handleAddTodo = () => {\n    if (todoText.trim() !== '') {\n      setTodos([...todos, todoText]);\n      setTodoText('');\n    }\n  };\n\n  return (\n    <div className=\"todo-container\">\n      <input\n        type=\"text\"\n        value={todoText}\n        onChange={(e) => setTodoText(e.target.value)}\n        placeholder=\"Add a new todo\"\n      />\n      <button onClick={handleAddTodo}>Add</button>\n      <ul>\n        {todos.map((todo, index) => (\n          <li key={index}>{todo}</li>\n        ))}\n      </ul>\n    </div>\n  );\n};\n\nReactDOM.render(<TodoApp />, document.getElementById('root'));\n```\n\n<div class=\"content-ad\"></div>\n\n## 이제 React에 익숙해졌으니 창의력을 발휘해보세요! 무엇이든 시작해보고 어떤 놀라운 것들을 만들어낼 수 있는지 확인해보세요. 기억하세요, 모든 위대한 프로젝트는 간단한 아이디어로 시작됩니다. 여러분은 할 수 있어요!","ogImage":{"url":"/assets/img/2024-05-20-GettingStartedwithReactYourFunandEasyGuide_0.png"},"coverImage":"/assets/img/2024-05-20-GettingStartedwithReactYourFunandEasyGuide_0.png","tag":["Tech"],"readingTime":3},{"title":"문서에서 LLM 세부 조정을 위한 지시 생성 자동화","description":"","date":"2024-05-20 22:04","slug":"2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning","content":"\n\n\n![Automating Instruction Generation](/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_0.png)\n\n큰 언어 모델 (LLM)은 뛰어난 생성 능력으로 다양한 제품에 진입하고 있으며, 우리는 새로운 응용 분야가 버섯처럼 생겨나고 있다는 것을 발견하고 있습니다. 이러한 모델들은 일반적인 도구이며 종종 도메인 특화 지식이 부족하여 그 영향력이 다소 줄어들 수 있습니다. 이러한 유용한 도메인 지식은 분산된 기업 리포지토리에 숨겨져 있을 수 있습니다.\n\n귀하의 도메인 데이터로 사용자 정의 LLM을 세밀 조정하면 이 간극을 좁히는 데 도움이 될 수 있습니다. 이 과정으로 나아가는 데 중요한 단계 중 하나가 데이터 준비입니다. 이는 데이터의 품질이 세밀 조정된 모델의 성능에 중대한 영향을 미칠 것이기 때문에 중요한 단계입니다. 이러한 데이터 세트를 수동으로 정비하려고 하면 매우 비용이 많이 들고 시간이 많이 소요되는 작업일 수 있습니다.\n\n이 기사에서는 Mistral 7B Instruct 모델을 사용하여 내부 문서에서 지침 및 교육 데이터 세트를 자동으로 생성하는 비용 효율적인 대안을 탐색할 것입니다. 우리는 귀하의 도메인을 포괄적으로 다룰 수 있는 지침 생성의 새로운 접근법을 취할 것입니다. Mistral 7B는 또한 학습 데이터 세트 생성을 위해 검색 보조 생성 (RAG) 설정에서 사용됩니다. 한번 훈련 데이터 세트를 확보하면 이 데이터 세트를 사용하여 Mistral 7B를 실제로 세밀히 조정하여 지역 도메인 지식으로보갰습니다.MLX 프레임워크 라이브러리를 호출합니다.\n\n\n<div class=\"content-ad\"></div>\n\n지시 생성부터 모델 세밀 조정까지의 종단 간 워크플로우를 탐색할 예정이에요. 여기에서 다뤄야 할 내용이 많아요. 시작해 볼까요!\n\n## 목차\n\n1.0 주요 활성화 기술 개요\n2.0 설계 및 구현\n2.1 지시 생성\n2.2 훈련 데이터셋 생성\n2.3 Function main\n3.0 지시 생성 실행\n4.0 훈련 데이터셋 생성 실행\n5.0 MLX를 사용한 세밀 조정\n6.0 모델 유효성 검사\n7.0 최종 생각들\n\n# 1.0 주요 활성화 기술 개요\n\n<div class=\"content-ad\"></div>\n\n이 작업은 RAM이 8GB인 MacBook Air M1에서 진행될 예정입니다. 상대적으로 제한된 컴퓨팅 및 메모리 리소스 때문에 Mistral 7B Instruct v0.1 모델의 4비트 양자화 버전을 채택하고 있습니다. GGUF 형식의 이러한 양자화된 모델을 로드하기 위해 llama-cpp-python 라이브러리를 사용할 것입니다. 이 라이브러리는 llama.cpp 라이브러리의 파이썬 바인딩입니다.\n\nfaiss-cpu는 CPU를 사용하여 밀집 벡터의 효율적인 유사성 검색 및 클러스터링을 위한 라이브러리입니다. 교육 데이터 생성을 위해 RAG 기술을 채택할 것입니다. RAG 애플리케이션에는 FAISS 벡터스토어에서 관련 문서 스니펫을 검색하는 리트리버 시스템과 검색된 스니펫을 컨텍스트로 사용하여 응답을 생성하는 LLM이 포함됩니다. 이전 연구에서 앙상블 리트리버가 적합하다는 것을 보여준 바 있습니다. 그 결과물을 근거로 선택한 리트리버 목록에서 Reciprocal Rank Fusion 알고리즘을 사용하여 결과를 앙상블하고 재정렬합니다. 우리는 앙상블을 위해 BM25 리트리버와 FAISS 리트리버를 0.3:0.7의 비율로 결합할 것입니다.\n\n마지막으로 중요한 기술 부분은 세밀한 조정과 관련이 있습니다. llama.cpp 및 MLX 프레임워크 라이브러리는 세밀한 조정을 지원하기 위한 도구를 제공합니다. 후자는 Apple 실리콘을 활용하여 하드웨어 가속을 제공하여 맥에서 세밀한 조정이 매우 간편해지도록 하는 것입니다. 따라서 우리는 여기서 MLX를 채택할 것입니다.\n\n이제 개발 환경을 준비할 준비가 되었습니다. 이 프로젝트를 관리하기 위해 가상 환경을 생성합시다. 환경을 생성하고 활성화하려면 다음을 실행합시다:\n\n<div class=\"content-ad\"></div>\n\n\npython3.10 -m venv llm_tuning\nsource llm_tuning/bin/activate\n\n\n다음으로 필요한 모든 라이브러리를 설치합니다:\n\n\npip install langchain faiss-cpu sentence-transformers flask-sqlalchemy psutil unstructured pdf2image unstructured_inference pillow_heif opencv-python pikepdf pypdf\npip install mlx\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n\n\n위 마지막 줄은 M1 프로세서에서 하드웨어 가속을 사용하여 Mistral 7B를 양자화한 llama-cpp-python 라이브러리를 설치하는 과정을 포함합니다. Metal을 사용하면 계산이 GPU에서 실행됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n환경이 준비되었으니, 시스템 설계와 구현을 살펴봅시다.\n\n# 2.0 설계 및 구현\n\n그림 1에 설명된대로 데이터셋 생성 시스템에는 두 개의 모듈이 있습니다.\n\n<img src=\"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_1.png\" />\n\n<div class=\"content-ad\"></div>\n\nLoadVectorize 모듈은 최근에 출시된 (2023 년 12 월) 440 페이지의 IT 벤더 배포 가이드를 로드하는 작업을 포함합니다. 또한 문서 분할 및 벡터화를 처리하며, BM25 검색기의 인스턴스화도 처리합니다. 이 모듈은 이전 작업에서 소개되었고 여기서 그대로 사용되었습니다 [1].\n\n두 번째 모듈에는 두 가지 주요 기능이 포함되어 있습니다. 첫 번째 기능은 지시 생성을 다룹니다. 이는 QA 체인을 사용하여 문서 청크 목록의 맥락에서 지시 생성을 수행하는 작업입니다. 두 번째 기능은 앙상블 검색기의 인스턴스화를 수행한 다음 앙상블 검색기의 맥락에서 지시 목록을 대상으로 QA 체인을 생성하는 작업을 합니다.\n\n이제 두 번째 모듈을 깊이 있는 살펴보겠습니다.\n\n## 2.1 지시 생성\n\n<div class=\"content-ad\"></div>\n\n이 종단 간 워크플로우에서는 Riverbed SteelHead에 대한 샘플 400페이지 이상의 PDF 문서를 도메인 지식으로 사용하고 있습니다. Riverbed SteelHead는 응용 프로그램 가속 솔루션입니다. 첫 번째 단계로 Mistral 7B를 언어 모델로 사용하여 이 문서와 관련된 지침(또는 프롬프트)를 생성할 것입니다.\n\n여기서 주요 설계 과제는 LLM이 아직 익숙하지 않은 영역에 대해 어떤 지시를 생성해야 하는지를 어떻게 판단할 것인가입니다. 이는 모든 내부 문서에 일반적으로 적용될 수 있는 과제입니다. 벡터화 단계의 일환으로, FAISS vectorstore는 문서 청크에 대한 참조를 갖고 있습니다. 이 청크들은 총체적으로 도메인 지식을 형성합니다. 이 지시 생성 함수의 주요 아이디어는 각 청크를 개별적인 컨텍스트로 사용하여 LLM이 지시를 생성하게 하는 것입니다. 각 청크가 가진 지식을 포괄적으로 다룰 수 있는 지침을 제공하기 위해 모든 문서 청크에 대해 이상적으로는 지침을 생성해야 합니다. 생성된 지시의 수는 채택된 문서 청크 크기에 비례해야 합니다. 시간과 플랫폼 리소스 제한으로 인해 이번 데모에서는 100개의 임의의 문서 청크에 대해 두 가지 질문을 생성할 것입니다.\n\n문서 청크에 액세스하려면 FAISS 객체에서 docstore 객체를 가져와야 하며, 모든 문서 청크를 나타내는 docstore_id 목록을 가져오십시오. 각 반복에서 관련 문서 청크를 찾아 이를 질의 체인을 위한 컨텍스트로 사용합니다.\n\n이 지시 생성을 위한 프롬프트는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n선택된 각 문서 청크를 반복하면서 해당 청크를 컨텍스트로 하고 위 프롬프트를 사용하여 QA 체인을 호출합니다. 생성된 지시 사항은 진행 상황을 나타내며, 소요된 시간과 함께 콘솔에 표시됩니다. 생성된 지시 사항은 instructions.txt 파일에 저장됩니다. 생성 진행 상황을 나타내기 위해 각 반복마다 현재 질문 번호와 소요된 시간이 표시됩니다. 이해를 돕기 위해 다음 목록은 generate_instructions 함수의 코드를 보여줍니다.\n\n```js\ndef generate_instructions(db,QA_PROMPT,llm) -> None:\n    output_parser = StrOutputParser()\n    # Custom QA Chain\n    chain = (\n        {\"context\": RunnablePassthrough() , \"question\": RunnablePassthrough()}\n        | QA_PROMPT\n        | llm\n        | output_parser\n        )\n\n    # access docstore and docstore id for 100 random chunks\n    vs = db.__dict__.get(\"docstore\")\n    docstore_id_list = list(db.__dict__.get(\"index_to_docstore_id\").values())\n    rand_doc_id_list = random.choices(docstore_id_list, k=200)\n\n    query = '''\n    제공된 컨텍스트를 기반으로 SteelHead에 대한 두 가지 질문을 생성하세요. 질문은 SteelHead WAN 가속 및 관련 개념에 관한 것이어야 합니다. 질문은 다음 중 하나로 시작해야 합니다: \"What\", \"How', \"Is there a\", \"What are the\", \"How do I\", \"When is it\", \"Does SteelHead have\", \"How to\", \"What is the difference\", \"Which\", \"List\". 각 질문에 대한 답변이나 범주를 제공할 필요는 없습니다.\n    '''\n    qfile = open(\"instructions.txt\", \"w\")\n    start_gen = timeit.default_timer()\n    for i,doc_id in enumerate(rand_doc_id_list):\n        start = timeit.default_timer()\n        a_doc = vs.search(doc_id)\n        result = chain.invoke({\"question\": query, \"context\": a_doc.page_content})\n        resp_time = timeit.default_timer() - start # seconds\n        print(f'{\"-\"*50}\\nQ #{i}: {result}\\nTime: {resp_time}\\n{\"-\"*50}\\n')\n        qfile.write(result)\n    qfile.close()\n    # total time for generation\n    gen_time = timeit.default_timer() - start_gen # seconds\n    print(f'Total generation time => {timedelta(seconds=gen_time)}')\n```\n\n이제 이 모듈의 두 번째 주요 함수를 살펴봅시다.\n\n## 2.2 Training Dataset Generation\n\n<div class=\"content-ad\"></div>\n\n지시 사항이 준비되었으면 이제 훈련 데이터 세트 생성을 진행할 수 있습니다. 이전과 마찬가지로 Mistral 7B를 LLM으로 사용하며, 이번에는 RAG 설정을 사용합니다. 우리는 FAISS 최대 여유도(MMR) 및 BM25 검색기의 EnsembleRetriever를 사용할 것입니다. 이전에 언급한 바와 같이 이러한 검색기 목록에 대해 0.3:0.7 비율이 최상의 정확도 성능을 달성했음을 보여주었습니다.\n\n지시 사항을 반복하면 LLM에 대한 쿼리를 실행하여 해당 답변을 사용하여 다음 형식의 JSON 문자열을 생성합니다:\n\n`s`[INST] 'instruction'[/INST] 'answer'`/s`\n\n훈련 데이터 세트를 준비하면 이 데이터 세트의 80%가 훈련에 사용되어 train.jsonl에 저장됩니다. 남은 20%의 데이터 세트는 검증에 사용되어 valid.jsonl로 저장됩니다. 아래 목록은 위 절차를 generate_training 함수로 캡처한 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```python\ndef generate_training(db, bm25_r, QA_PROMPT, llm) -> None:\n    # retriever 생성\n    faiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 3}, max_tokens_limit=1000)\n    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_r, faiss_retriever], weights=[0.3, 0.7])\n    output_parser = StrOutputParser()\n    # 사용자 지정 QA Chain\n    chain = (\n        {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n        | QA_PROMPT\n        | llm\n        | output_parser\n    )\n    with open('instructions.txt') as tfile:\n        instructions = tfile.readlines()\n    start_t_gen = timeit.default_timer()\n    train_lines = list()\n    for i, instruction in enumerate(instructions, start=1):\n        print(f\"처리 중 ({i}/{len(instructions)}):\")\n        start = timeit.default_timer()\n        try:\n            answer = chain.invoke(instruction)\n        except Exception as e:\n            # LLM으로 답변할 수 없는 질문 건너뛰기\n            print(f'답변 실패 => {e}')\n            continue\n        resp_time = timeit.default_timer() - start # 초\n        print(f'{\"-\"*50}\\n질문 #{i}: {instruction}\\n답변: {answer}\\n소요 시간: {resp_time}\\n{\"-\"*50}\\n')\n        result = json.dumps({\n            'text': f'<s>[INST] {instruction}[/INST] {answer}</s>'\n        }) + \"\\n\"\n        # 임시 파일에 작성\n        with open('train_valid.jsonl', 'a') as file:\n            file.write(result)\n        train_lines.append(result)\n    gen_time = timeit.default_timer() - start_t_gen # 초\n    with open('train.jsonl', 'w') as file:\n        file.writelines(train_lines[:int(len(train_lines) * 0.2)])\n    with open('valid.jsonl', 'w') as file:\n        file.writelines(train_lines[int(len(train_lines) * 0.2):])\n    print(f'총 학습 생성 시간 => {timedelta(seconds=gen_time)}')\n```\n\n위의 주요 함수 중 하나를 호출하려면 다음에 설명된 대로 main 함수를 사용합니다.\n\n## 2.3 main 함수\n\n이 함수에서 두 함수에서 사용하는 여러 개의 공통 개체가 인스턴스화됩니다. 먼저 프롬프트 템플릿이 정의됩니다. 그런 다음 LlamaCpp를 사용하여 4비트 Mistral 7B Instruct 모델을 GGUF 형식으로 로드합니다. 그런 다음 pdf 문서를 벡터화하고 해당 FAISS 객체에 대한 참조 및 BM25 검색기를 얻습니다.\n\n<div class=\"content-ad\"></div>\n\n두 가지 생성 함수 중 어느 것이든 쉽게 호출할 수 있도록 명령줄 옵션을 사용해 보겠습니다. 'main' 함수는 최대 두 개의 부울 인수를 받아들이게 됩니다. 이는 제공된 명령줄 옵션에 의해 제어될 것입니다. 명령줄 옵션을 통해 발표나 훈련 데이터셋 생성 작업 중 어떤 것을 실행할지 결정하기 위해 라이브러리 argparse를 활용하겠습니다.\n\n아래 코드는 이러한 명령줄 옵션 처리 및 'main' 함수를 포함하고 있습니다.\n\n```python\ndef main(is_gen_instruct=False, is_gen_training=False):\n    # 프롬프트 템플릿\n    qa_template = \"\"\"<s>[INST] 이 신종은 도움이 되는 조수입니다.\n    아래 컨텍스트를 사용하여 이하의 질문에 정확하고 간결하게 답하세요:\n    {context}\n    [/INST] </s>{question}\n    \"\"\"\n\n    # 프롬프트 인스턴스 생성\n    QA_PROMPT = PromptTemplate.from_template(qa_template)\n\n    llm = LlamaCpp(\n        model_path=\"./models/mistral_7b_gguf/mistral-7b-instruct-v0.1.Q2_K.gguf\",\n        temperature=0.01,\n        max_tokens=2000,\n        top_p=1,\n        verbose=False,\n        n_ctx=3000\n    )\n    db, bm25_r = LoadVectorize.load_db()\n    if is_gen_instruct:\n        generate_instructions(db, QA_PROMPT, llm) \n    elif is_gen_training:\n        generate_training(db, bm25_r, QA_PROMPT, llm) \n\nif __name__ == \"__main__\":\n    # 파서 초기화\n    parser = argparse.ArgumentParser(\"LLM 미세 조정을 위한 명령어 생성 스크립트\")\n    group = parser.add_mutually_exclusive_group()\n\n    # 선택적 상호배제 인수 추가\n    group.add_argument(\"-i\", \"--instructions\", action='store_true', help = \"지시사항 생성\")\n    group.add_argument(\"-t\", \"--training\", action='store_true', help = \"훈련 및 검증 데이터 생성\")\n\n    # 명령줄에서 인수 읽기\n    args = parser.parse_args()\n    if args.instructions:\n        main(is_gen_instruct=args.instructions)\n    elif args.training:\n        main(is_gen_training=args.training)  \n```\n\n이로써 데이터 생성 시스템 구현이 완료되었습니다. 이 시스템에 대한 전체 코드는 다음 GitHub 저장소에서 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n자, 이제 한 번 시도해 봅시다!\n\n# 3.0 지시 생성 실행\n\n이 연습에서는 코드를 -i 명령행 옵션과 함께 실행하여 지시 생성 프로세스를 시작할 것입니다. 다음 콘솔 출력 추출은 200개의 질문을 생성하기 위한 실행을 나타냅니다. 이 과정 전체는 제 맥에서 2시간 넘게 소요되었습니다.\n\n```js\n$ python main.py -i\n\n--------------------------------------------------\nQ #0: \n1. SteelHead에서 QoS 설정을 어디서 찾을 수 있을까요?\n2. MX-TCP와 TCP 간에 패킷 손실 처리 측면에서 차이가 있나요?\n시간: 57.88847145799991\n--------------------------------------------------\n\n--------------------------------------------------\nQ #1: \n1. SteelHead에서 SSL 구성 정보를 어디서 찾을 수 있을까요?\n2. SSL 구성을 위해 클라이언트 가속기 간에 신뢰 관계가 필요한가요?\n시간: 47.30005858300001\n--------------------------------------------------\n\n--------------------------------------------------\nQ #2: \n1. 클러스터 내의 SteelHead 간 연결 전달을 활성화하는 구성은 어디에 있나요?\n2. 동일 SteelHead에서 다중 인터페이스를 사용하는 것과 ITD 고가용성 배포를 위해 여러 SteelHead를 사용하는 것 사이에 차이가 있나요?\n시간: 70.70811329100025\n--------------------------------------------------\n\n--------------------------------------------------\nQ #3: \n1. PBR 배포에 사용되는 SteelHead에서 CDP를 어디서 활성화할까요?\n2. SteelHead에서 CDP를 활성화하기 위해 사용해야 하는 특정 명령이 있나요?\n시간: 68.81058954199989\n--------------------------------------------------\n...\n\nQ #99: \n1. SteelHead WAN 가속화의 정확한 주소 할당은 무엇인가요?\n2. 정확한 주소 할당은 SteelHead에서 연결 풀 가속화를 어떻게 가능하게 할까요?\n시간: 63.51242004099913\n--------------------------------------------------\n\n총 생성 시간 => 2:06:10.565294\n```\n\n<div class=\"content-ad\"></div>\n\n생성된 지침을 검토한 결과, 많은 좋은 질문이 나왔어요. 그런데 \"어디서 찾을 수 있나요\"와 같은 질문들이 많았는데, 이는 도메인 지식을 얻는 데 도움이 되지 않는다고 생각해서 목록에서 제외했어요. 또한, 일부 문서 청크에 대한 질문들이 거의 동일한 경우가 많았고, 이러한 중복들은 제거했어요. 마지막으로, 부정확하거나 의미 없는 질문들이 몇 개 있었어요. 이 모든 정제 작업을 거친 뒤에 좋은 질문이 150개 남았어요. 또한, 질문들이 번호 매겨지고 예상치 못한 서식이 있어서 조정해야 했어요. 이것은 다음 작업을 위해 데이터 품질을 보장하기 위한 인간의 개입이 필요함을 명확히 보여줍니다.\n\n지침이 준비되었으니, 이제 훈련 데이터 집합 생성을 진행합시다.\n\n# 4.0 훈련 데이터 집합 생성 실행\n\n이제 동일한 스크립트를 -t 옵션을 사용하여 실행하여 훈련 및 검증 데이터 집합 생성을 시작합니다. 제 리소스가 제한된 기기에서는 시간이 많이 걸렸어요. 다행히 콘솔 출력을 통해 진행 상황을 잘 파악할 수 있었어요. 이 실행 중에 발생하는 열의 양 때문에 Mac을 일부러 공중에 두어 냉각 효과를 향상시켰어요. 아래는 이 실행의 콘솔 출력 일부입니다:\n\n<div class=\"content-ad\"></div>\n\n\n```js\n$ python main.py -t\n\n처리 중 (1/150):\n--------------------------------------------------\nQ #1: MX-TCP와 TCP 간의 데이터 손실 처리 방식에는 차이가 있나요?\nA:\n네, MX-TCP와 TCP 간에는 데이터 손실 처리에 차이가 있습니다. MX-TCP는 쓰루풋 감소 없이 데이터 손실을 처리하기 위해 설계되었으며, TCP는 일반적으로 데이터 손실 시 쓰루풋이 감소합니다. MX-TCP는 WAN을 통해 전방 오류 수정을 통해 데이터 손실을 효과적으로 처리합니다.\n시간: 152.08327770899996\n\n처리 중 (2/150):\n--------------------------------------------------\nQ #2: 클러스터 내의 SteelHeads 간 연결 전달을 활성화하기 위한 구성은 어디에 있나요?\nA:\n클러스터 내의 SteelHeads 간 연결 전달을 활성화하려면 각 SteelHead의 CLI에서 두 SteelHeads의 in-path0_0 IP 주소를 이웃으로 구성해야 합니다. 그런 다음 다음 명령을 각 SteelHead의 CLI에서 입력할 수 있습니다:\n\nenable\nconfigure terminal\nSteelHead communication enable\nSteelHead communication multi-interface enable\nSteelHead name <SteelHead name> main-ip <SteelHead IP address>\n\n연결 전달을 활성화한 후, ITD 배포에서 더 큰 탄력성과 중복성을 제공하기 위해 fail-to-block 및 allow-failure 명령을 구성할 수 있습니다.\n시간: 215.70585895799923\n\n처리 중 (3/150):\n--------------------------------------------------\nQ #3: 동일한 SteelHead에서 여러 인터페이스를 사용하는 것과 ITD 고가용성 배포를 위해 여러 SteelHeads를 사용하는 것 사이에는 차이가 있나요?\nA:\n네, 동일한 SteelHead에서 여러 인터페이스를 사용하는 것과 ITD 고가용성 배포를 위해 여러 SteelHeads를 사용하는 것 사이에 차이가 있습니다. 동일한 SteelHead에서 여러 인터페이스를 사용하면 하나의 SteelHead 이상의 가속 쓰루풋 용량을 제공할 수 있지만, 여러 SteelHeads를 사용하는 것보다 동일한 수준의 중복성이나 탄력성을 제공하지 않을 수 있습니다. 반면, 여러 SteelHeads를 사용하면 더 큰 중복성과 탄력성을 제공할 수 있지만, 동일한 SteelHead에서 여러 인터페이스를 사용하는 것만큼의 가속 쓰루풋 용량을 제공하지 않을 수 있습니다.\n시간: 179.73986179200074\n\n...\n\n처리 중 (150/150):\n--------------------------------------------------\nQ #150: SteelHead에서 올바른 주소 지정은 연결 풀 가속을 어떻게 가능하게 합니까?\n\nA:\n\n올바른 주소 지정을 통해 SteelHead에서 연결 풀 가속을 가능하게 함으로써 미리 서로간에 다수의 TCP 연결을 생성할 수 있습니다. 이는 올바른 주소 지정이 TCP/IP 패킷 헤더의 특정 값을 사용하므로 SteelHeads가 필요한 클라이언트 및 서버 IP 주소 및 포트 유형을 감지할 수 있기 때문입니다. 투명 주소 지정이 활성화된 경우 SteelHeads는 클라이언트 및 서버 IP 주소 및 포트 유형을 감지할 수 없기 때문에 미리 TCP 연결을 생성할 수 없습니다. 가속하려는 연결 수가 SteelHead 모델의 한계를 초과하는 경우, 초과된 연결은 SteelHead에 의해 가속되지 않고 통과됩니다.\n시간: 159.28865737500018\n--------------------------------------------------\n\n\n총 교육 세대 생성 시간 => 9:20:06.321521\n```\n\n이 교육 데이터세트 생성에는 9시간 이상이 소요되었습니다! 문서의 일부에 대해 언어 모델이 답변을 생성하지 못한 경우도 몇 가지 발생했습니다. 1,200개 이상의 청크가 포함된 선택된 문서를 종합적으로 다루려면 2,000개 이상의 지시어가 필요할 수 있으며, 이는 실행 기간 동안 내 Mac이 지속적인 고온을 견딜 수 있을 때에만 가능할 것입니다! 그래도 전체적인 프로세스를 보여주기 위해 우리는 다음 제한된 교육 데이터세트로 세밀한 조정을 진행할 것입니다.\n\n# 5.0 MLX를 사용한 세밀 조정\n\nMLX는 Apple 실리콘 기반의 머신 러닝 연구를 위한 배열 프레임워크입니다 [2]. Llama, Mistral 및 TinyLlama와 같은 LLM에 대한 텍스트 생성 및 세밀 조정에 사용될 수 있습니다. 세밀 조정을 위해 모델은 MLX에서 인식하는 형식이어야 하므로 이전에 사용했던 GGUF 버전을 사용할 수 없습니다. MLX는 mlx-examples Github 저장소의 스크립트를 제공하여 전체 워크플로우를 지원합니다. 아래와 같이 생성 시스템의 디렉터리 내에서 MLX 예제 저장소를 클론해 보겠습니다:\n\n\n<div class=\"content-ad\"></div>\n\n```bash\n$ git clone https://github.com/ml-explore/mlx-examples.git\n```\n\nHuggingFace에서 Mistral 7B를 다운로드하고 4비트 모델로 양자화하려면 convert.py 스크립트를 사용할 수 있습니다. 이 스크립트는 기본적으로 입력으로 HuggingFace repo를 취하고 결과를 디렉토리 mlx_model에 출력합니다. 다음은 샘플 실행 출력입니다:\n\n```bash \n$ python mlx-examples/lora/convert.py --hf-path mistralai/Mistral-7B-Instruct-v0.1 -q\n[INFO] Loading\nmodel-00003-of-00003.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54G/4.54G [33:52<00:00, 2.23MB/s]\nmodel-00001-of-00003.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.94G/4.94G [36:15<00:00, 2.27MB/s]\nmodel-00002-of-00003.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████\n```\n\n<div class=\"content-ad\"></div>\n\n이전 섹션에서의 훈련 데이터셋으로 모델을 세밀하게 조정할 준비가 되었습니다. MLX는 파라미터 효율적 세밀조정(PEFT)을 LoRA를 통해 지원합니다. LoRA는 모델의 일부 파라미터를 업데이트하는 데 중점을 둡니다. 종종 특정 레이어나 모델의 일부를 동결하는 것을 포함합니다. 이 방법을 사용하면 세밀조정이 빨라집니다. 또한 MLX는 양자화된 모델에서 QLoRA를 사용합니다.\n\n이제 lora.py 스크립트가 도움이 될 것입니다. 모델이 양자화되었음을 감지하면 이 스크립트는 자동으로 QLoRA를 사용하도록 전환합니다. --data 옵션은 훈련 및 검증 데이터셋이 포함된 디렉터리를 지정하는 데 사용됩니다. --lora-layers 옵션은 세밀 조정할 레이어 수를 설정하는 데 사용됩니다. 그리고 --iters 옵션은 훈련 반복 횟수를 지정합니다. 학습률 및 샘플링 온도와 같은 다른 선택적 설정도 있으며, 세밀 조정을 제어하는 데 사용할 수 있습니다. 모든 도움말 목록을 보려면 단순히 -h 옵션을 사용하여 스크립트를 실행하십시오.\n\n다음 콘솔 출력은 저희 모델에 대한 세밀 조정 실행을 보여주며, 저의 Mac에서 약 40분이 소요되었습니다:\n\n```js\n$ python mlx-examples/lora/lora.py \\\n  --train \\\n  --model ./mlx_model \\\n  --data ./ \\\n  --batch-size 1 \\\n  --lora-layers 2 \\\n  --iters 1000\n미리 훈련된 모델 로딩 중\n총 파라미터 1242.550M\n조정 가능한 파라미터 0.213M\n데이터셋 로딩 중\n훈련 중\n반복 1: 검증 손실 3.565, 소요 시간 32.649초\n반복 10: 훈련 손실 3.008, Iter/sec 0.401, 토큰/sec 80.419\n...\n반복 1000: 훈련 손실 1.511, Iter/sec 0.361, 토큰/sec 74.861\n반복 1000: 검증 손실 1.777, 소요 시간 31.679초\n반복 1000: adapter 가중치를 adapters.npz에 저장했습니다.\n```\n\n<div class=\"content-ad\"></div>\n\n트레이닝 시작 시 3.008의 손실이 있었고, 마지막 반복 중에는 1.511까지 떨어졌어요. 기본적으로 모델은 매 100번의 반복마다 저장됩니다. 제 컴퓨터의 자원 한정 때문에 lora 레이어를 두 개 이상 사용하면 시스템이 메모리 부족으로 작동을 멈춥니다. 그러나 귀하의 컴퓨터에 더 많은 RAM이 있다면, 레이어 수를 더 많이 실험해보세요.\n\n미세 조정이 완료된 후, 결과 모델은 현재 디렉토리에 adapters.npz로 저장됩니다. 변경 사항을 기본 모델에 병합하려면 MLX 스크립트 fuse.py를 사용할 수 있습니다. 결과로 얻은 결합된 모델을 로컬 디스크에 저장하거나 선택한 HuggingFace 저장소에 푸시할 수 있습니다. 아래 fuse.py 실행은 모델을 로컬 디렉토리 ./models/mistral7b에 저장할 겁니다:\n\n```js\n$ python mlx-examples/lora/fuse.py --model ./mlx_model  --adapter-file ./adapters.npz --save-path ./models/mistral7b\n미세 조정된 모델 로딩 중\n$\n```\n\n저희의 트레이닝 데이터셋이 상당히 한정적이므로, 병합을 포기하기로 결정했어요. 이제 모델 검증을 준비할 차례입니다.\n\n<div class=\"content-ad\"></div>\n\n# 6.0 모델 유효성 검사\n\n모델의 생성 능력을 테스트하려면 여전히 스크립트 lora.py를 사용할 수 있습니다. 아래는 생성을 위한 기본 사용 방법입니다:\n\n```js\n$ python mlx-examples/lora/lora.py --model ./mlx_model \\\n    --max-tokens 1000 \\\n    --prompt '스틸헤드 경로 선택의 목적은 무엇입니까?'\n```\n\nFine-tuning이 LLM에 도움이 되었는지 확인하기 위해 기본 및 fine-tuned 모델의 생성 테스트를 실행할 수 있습니다. 아래는 두 모델의 생성을 보여줍니다:\n\n<div class=\"content-ad\"></div>\n\n\n### 기본 모델 테스트\n\n$ python mlx-examples/lora/lora.py --model ./mlx_model --max-tokens 1000 --prompt 'SteelHead 경로 선택의 목적은 무엇입니까?'   \n사전 훈련된 모델 로드 중\n총 매개변수 1244.041M\n훈련 가능한 매개변수 1.704M\n데이터셋 로드 중\n생성 중\nSteelHead 경로 선택의 목적은 무엇입니까?\n\nWAN 최적화 솔루션의 경로 선택 기능은 네트워크를 통해 전송되는 데이터 양을 줄이고 데이터 소스와 클라이언트 사이의 왕복 횟수를 최소화하여 단일 왕복이 소요되는 데이터 전송을 최소화하여 시간 소요와 추가적인 네트워크 트래픽을 줄이는 것입니다...\n==========\n\n### 세부 조정된 모델\n\n$ python mlx-examples/lora/lora.py --model ./mlx_model --adapter-file ./adapters.npz --max-tokens 1000 --prompt 'SteelHead 경로 선택의 목적은 무엇입니까?'\n사전 훈련된 모델 로드 중\n총 매개변수 1244.041M\n훈련 가능한 매개변수 1.704M\n데이터셋 로드 중\n생성 중\nSteelHead 경로 선택의 목적은 무엇입니까?\n\n네트워크 경로 선택 기능의 목적은 SaaS 응용 프로그램 트래픽을 기반으로 두 위치 사이에서 가장 효율적인 경로를 선택하는 것입니다. SteelHead 네트워크 경로 선택은 네트워크 트래픽을 특정 응용 프로그램에 액세스하기 위해 특정 경로(네트워크 경로 또는 WAN 경로 연결)를 사용하고 네트워크 성능을 보장하기 위해 트래픽을 우선순위로 처리할 수 있습니다. 이 기능은 여러 경로(다중 WAN 링크, MPLS 연결 및 LAN 등)이 있는 환경에서 특히 유용합니다.\n\n\n세부 조정된 모델 테스트는 lora.py를 --adapter-file 옵션과 어댑터 파일명을 함께 사용하여 실행하는 것을 포함합니다. 그 이외에는 모든 것이 동일합니다.\n\n비교를 쉽게하기 위해 기본 및 세부 조정된 모델 간의 두 실행 결과를 비교한 결과가 Table 1에 나타납니다. 두 쿼리에 대한 기본 모델의 답변은 모두 잘못되었습니다. 세부 조정된 모델의 답변은 거의 정확하지만 일부 오류가 있습니다. 그럼에도 불구하고 비교적 작은 교육 데이터 집합을 사용하여 세세하게 조정한 모델은 여전히 비교적 잘 학습할 수 있습니다.\n\n<img src=\"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_2.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n# 7.0 최종 소견\n\nLLM은 공공 도메인의 정보가 풍부한 영역에서 그들의 생성 능력으로 뛰어납니다. 그러나 이산 기관 저장소에 숨겨진 많은 도메인 지식이 탭되기를 기다리고 있습니다.\n\n본문에서는 문서 조각마다 Mistral 7B를 사용하여 일정 수의 지침을 생성하여 문서의 종합적인 커버리지를 보장하는 지침 생성 접근 방법을 소개했습니다. 교육 데이터 셋 생성을 위해 우리는 다시 앙상블 검색기를 활용한 Mistral 7B를 RAG 설정에서 사용했습니다. 그런 다음 이 데이터 셋을 사용하여 Apple 실리콘에 최적화된 라이브러리 MLX와 QLoRA 기술을 사용하여 파인튜닝을 수행했습니다. 상대적으로 작은 교육 데이터 셋으로도 이 지식을 파악하는 능력이 향상된 것을 보는 것은 유망한 것입니다.\n\n읽어 주셔서 감사합니다!\n\n<div class=\"content-ad\"></div>\n\n참고 문헌\n- Querying Internal Documents using Mistral 7B with Context from an Ensemble Retriever\n- [GitHub 링크](https://github.com/ml-explore/mlx)","ogImage":{"url":"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_0.png"},"coverImage":"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_0.png","tag":["Tech"],"readingTime":20},{"title":"파이썬만 활용해 경험 없이 아름다운 웹 앱 만드는 방법","description":"","date":"2024-05-20 22:02","slug":"2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience","content":"\n\n## FastAPI, Jinja2 및 DaisyUI 사용하기.\n\n또 다른 주말, 또 다른 가려운 부분이 생겼네요. 개인적으로 저는 하드웨어든 소프트웨어든 심지어 기계 작업 프로젝트든 매체에 상관없이 무언가를 만드는 것을 좋아합니다. 저와 같은 사람들에게는 한 가지나 한 가지 기술에 집중하는 것이 어려운 도전이죠. 무언가를 만드는 욕구가 만만치 않아 빨리 만들고 싶어해요 (자랑할 만한 것은 아닐지도 모르죠). 내 생각에는 웹 앱을 만드는 것은 상당한 인내심과 지속적인 적응과 학습 의지가 필요한 일입니다. 제가 가장 좋아하는 일은 아닙니다. 아마 당신도 공감할 수 있을 겁니다.\n\n지난 블로그 글에서 읽었겠지만, 매주 새로운 기술을 탐험하기 위해 떠나는 나의 여정 중 하나로, 파이썬 웹 프레임워크인 FastAPI에 몰두해보기로 했습니다. 또 다른 신기술에 뛰어드는 것 대신에, FastAPI를 사용하여 순수하게 Python만으로 (JavaScript 사용 안 함) 기능적인 웹 앱을 개발하는 데 초점을 맞추기로 했습니다. 이 글을 쓰기 전에 Vue.js에도 조금 손을 대봤는데, 흥미로웠지만 복잡성을 더하고, 제 JavaScript 능력은 그리 높지 않다는 것을 깨달았습니다. 처음부터 다시 시작했죠.\n\n이 글에서는 주변 청크를 검색하는 기존 FastAPI 프로젝트를 확장해 나갈 것입니다. 더 많은 내용을 알고 싶으시면 이전 글을 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n기술 탐구 내역:\n\n- FastAPI — 파이썬 웹 프레임워크\n- Jinja2 — 템플릿 엔진\n- DaisyUI — Tailwind CSS용 구성 요소 라이브러리\n\n경험이 풍부한 웹 앱 개발자나 프로그래밍 전문가라면, 본 문서가 새로운 통찰을 제공하지는 않을지도 모릅니다. 그러나 Python으로 웹 앱을 구축하는 세부 사항에 대해 궁금해하는 분이나 Streamlit과 같은 것보다 더 많은 제어권을 제공하는 Python 웹 개발자 채용 공고에 대해 궁금해하는 분이라면, 여기가 바로 당신이 찾던 곳입니다. 함께 과정을 탐험해보겠습니다.\n\n솔직히 말씀드리자면, FastAPI를 사용해 프론트엔드를 구축하는 것은 쉬웠지만, 앱과 tailwindcss (DaisyUI)를 통합하는 것이 저에게 가장 시간이 많이 걸렸습니다. 그 부분을 건너뛸 수도 있었지만, 웹 앱을 구축하는 유일한 이유는 나에게 창의력을 펼칠 수 있는 자유가 있다는 점이라는 사실을 인정해야 합니다. 누가 멋진 사용자 인터페이스를 좋아하지 않겠습니까? 그러니 더 이상 말이 필요 없으니, 함께 확인해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n# DaisyUI란 무엇인가요?\n\n**모든 사람들이 멋진 사용자 인터페이스를 좋아합니다**, 하지만 말하는 것이 맞다고 여겨지며, CSS 스타일을 작성하는 일은 종종 사람들이 웹 앱의 시각적으로 매력적인 프론트엔드를 만드는 것을 꺼리게 합니다. CSS에 대해서 말할 때 **마치 녹음된 음반이라도 듣는 것 같은데**, 제가 CSS에 관해서는 좀게 놀기 싫은 타입이라서 대개 제 할 일 목록의 제일 뒷부분에 두게 되죠. 학교에서 미술 수업? 그냥 턱걸이로 벗어난다고 말해야 할 것 같아요. CSS의 픽셀 완벽한 스타일링 세부사항에 뛰어드는 것이 나를 즐겁게 하진 않습니다.\n\n저와 같은 사람들을 위해(그렇습니다, Tailwind는 프로덕션 급 앱에서도 사용됩니다), Tailwind는 생명을 구원해줍니다. **HTML 컴포넌트를 위한 미리 정의된 스타일과 템플릿을 제공하여**, 특정 클래스를 추가함으로써 쉽게 커스터마이즈할 수 있습니다.\n\nTailwind는 엄청난 인기를 얻었으며, 이제는 다양한 분야의 애플리케이션에서 상용품으로 자리 잡았습니다. 만약 잘 안다면, **앱이 순정 Tailwind CSS를 사용하고 있는지 종종 알아낼 수 있습니다**. Tailwind 클래스를 세심하게 조정할 수도 있고, 아니면 Tailwind에 맞게 제작된 컴포넌트 라이브러리를 사용하는 손쉬운 방법을 선택할 수도 있습니다. 마치 Tailwind라는 Tailwind를 사용하는 것과 같은 이치인 거죠, ㅋㅋ. 여러 가지 컴포넌트 라이브러리가 있고, **DaisyUI가 그중 하나입니다**.\n\n<div class=\"content-ad\"></div>\n\nDaisyUI를 사용하는 것에는 수많은 기능과 이점이 있습니다. 더 자세히 알아보려면 그들의 웹사이트를 방문하시면 됩니다. 제가 제일 좋아하는 기능 중 하나는 테마 선택 기능입니다. 다양한 테마 중에서 선택할 수 있어서 완벽한 색상 구성을 찾는 데 어려움을 겪지 않을 것입니다.\n\n# 웹 앱의 파일 구조\n\n```js\nroot\n|-app\n  |- chroma_db\n  |- functions.py\n  |- main.py\n  |- models.py\n  |- chroma_db\n  |- static\n    |- css\n      |- app.css\n  |- styles\n    |- app.css\n  |- templates\n    |- index.html\n  |- files\n    |- samples.pdf\n```\n\nchroma_db 폴더, models.py, functions.py는 이전에 만든 프로젝트에서 이어지게 될 것입니다. 이전에 FastAPI에 익숙하지 않으셨다면, 이 기사를 읽을 것을 권해드립니다.\n\n<div class=\"content-ad\"></div>\n\n# 사용자 인터페이스\n\n모든 기술적인 내용에 지루해하기 전에, 여기 사용자 인터페이스의 일부를 엿볼 수 있는 이미지입니다. 힘내세요!\n\n![Interface Image 0](/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_0.png)\n\n![Interface Image 1](/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_1.png)\n\n<div class=\"content-ad\"></div>\n\n비록 Behance에서 자랑할 만한 것은 아니지만, 정말 멋지게 보이지 않나요? 개인적으로 나는 기본 Times New Roman, 흑백 테마보다 훨씬 좋아하는 편이에요. 그 테마는 정말 웹 개발을 싫어하게 만들어요.\n\n만약 이 작업이 많은 노력을 필요로 할 것이라고 생각한다면, 안심하세요. 나는 오랜만에 CSS에 손 대지 않았고, 아주 빠르게 할 수 있었어요. 당신도 할 수 있다는 걸 함께 확인해봐요!\n\n# 앱을 위한 Tailwind 설정\n\nDaisyUI를 위한 CDN을 사용하거나 Tailwind 플러그인으로 설치할 수 있어요. 프로덕션 환경에서는 권장되지 않지만, 과정을 훨씬 간단하게 만드는 CDN을 사용할 수 있어요. 하지만 권장하는 방법으로, Tailwind 플러그인으로 설치할 거에요. 이 방법은 약간 더 어려울 수 있고 일부 설정 문제가 있을 수 있지만, 누가 좋은 도전을 싫어하겠어요?\n\n<div class=\"content-ad\"></div>\n\nTailwind를 설치하려면 JavaScript의 패키지 관리자 인 NPM이 설치되어 있는지 확인하십시오. NPM이 설치되어 있지 않은 경우 문서를 참조하십시오.\n\n프로젝트의 루트 디렉토리 터미널에서 다음 명령을 실행해야 합니다:\n\n```js\nnpm install -D tailwindcss\nnpx tailwindcss init\nnpm i -D daisyui@latest\n```\n\n성공적으로 실행되면 프로젝트의 루트 디렉토리에 tailwind.config.js라는 구성 파일이 생성됩니다.\n\n<div class=\"content-ad\"></div>\n\ntailwind.config.js 파일에서는 DaisyUI를 플러그인으로 추가해야 합니다. 파일은 다음과 같이 보일 것입니다:\n\n```js\nconst { default: daisyui } = require('daisyui');\n\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  content: [\"./app/templates/*.html\"],\n  theme: {\n    extend: {},\n  },\n  plugins: [\n    require(\"daisyui\")\n  ],\n  daisyui: {\n    themes: [\"light\", \"dim\", \"acid\"],\n  },\n}\n```\n\n스타일이 적용되지 않는 이유를 찾느라 시간을 많이 소비했습니다. 30분 이상을 쓴 뒤에야 HTML 파일 경로가 content 섹션에 잘못 지정되어 있어서 그랬다는 것을 발견했습니다. 비슷한 문제를 겪는다면 먼저 이 부분을 확인해 보세요. DaisyUI 테마도 여기서 설정할 수 있습니다.\n\n이제 절차를 모두 마쳤으므로 tailwind CSS 파일을 생성할 시간입니다. 주로 두 가지 파일이 생성될 것입니다: styles/app.css (입력 파일)와 static/css/app.css (클래스별 스타일이 적용된 생성된 CSS 파일).\n\n<div class=\"content-ad\"></div>\n\nstyles/app.css 파일에서 다음 tailwind 지시문을 정의하세요:\n\n```js\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\ntailwind CSS 파일을 생성하려면, 다음 명령을 실행하세요:\n\n```js\nnpx tailwindcss -i ./styles/app.css -o ./static/css/app.css --watch\n```  \n\n<div class=\"content-ad\"></div>\n\n그만입니다. Tailwind와 DaisyUI를 성공적으로 설치했습니다.\n\n# Jinja2를 사용하여 템플릿 생성하기\n\nJinja2는 Python용 템플릿 엔진으로, 애플리케이션에서 모듈식이고 동적인 HTML 콘텐츠를 만드는 데 사용됩니다. 이는 우리의 웹 애플리케이션에서 전혀 JavaScript를 사용하지 않게 될 것이므로 매우 도움이 됩니다. API에서 가져온 정보를 HTML 템플릿에 매핑하고, 그런 다음 Jinja2를 사용하여 렌더링할 것입니다. Jinja2를 사용하면 HTML 내부에 반복문, 조건문, 필터, 변수 등을 직접 사용할 수 있는 표현식을 사용할 수 있습니다. 이러한 표현식에 대해 더 알아보려면 Jinja2 문서의 관련 문서를 읽어보세요.\n\n```js\n<div>\n    <!-- 결과가 존재하는지 확인 -->\n    { if not results }\n    <h2>결과 없음</h2>\n    { endif }\n    { if results }\n    <h2>가장 가까운 이웃</h2>\n    <div id=\"results\">\n        <!-- 가장 가까운 이웃이 여기에 표시됩니다 -->\n        { for result in results }\n        <div>\n            <input type=\"checkbox\" />\n            <div>\n                <p>{ result.page_content[:25]|safe }...</p>\n            </div>\n            <div>\n                <p>{ result.page_content }</p>\n            </div>\n        </div>\n        { endfor }\n    </div>\n    { endif }\n</div>\n```\n\n<div class=\"content-ad\"></div>\n\n이 템플릿은 결과의 존재를 확인하기 위해 조건식을 사용합니다. 결과가 발견되면 각각에 대한 모달을 생성하기 위해 반복합니다. Python 프로그래밍에 익숙하다면, 이를 이해하는 데 어렵지 않을 것입니다.\n\n## HTML에 Tailwind CSS 파일 링크하기\n\nTailwind 스타일을 활성화하려면 다음 코드 줄을 HTML의 `head` 태그에 추가하여 링크해야 합니다.\n\n```js\n<link rel=\"stylesheet\" href=\"{url_for('static',path='css/app.css')}\">\n```\n\n<div class=\"content-ad\"></div>\n\n## 템플릿에 Tailwind 클래스 추가하기\n\n이 글을 길게 만들기보다는 Tailwind 클래스에 너무 깊게 파고들지 않을 거에요. 여기에서 다양한 유틸리티 클래스에 대해 읽을 수 있어요. Tailwind 클래스를 사용하면 위의 HTML이 다음과 같이 보일 거에요:\n\n```js\n<div class=\"max-w-md mx-auto\">\n    <!-- 결과가 있는지 확인하기 -->\n    { if not results }\n    <h2 class=\"text-lg font-semibold mb-2 text-info\">결과를 찾을 수 없습니다</h2>\n    { endif }\n    { if results }\n    <h2 class=\"text-lg font-semibold mb-2 text-info\">가장 가까운 이웃들</h2>\n    <div id=\"results\">\n        <!-- 가장 가까운 이웃들이 여기에 표시됩니다 -->\n        { for result in results }\n        <div class=\"collapse bg-base-200 mb-4\">\n            <input type=\"checkbox\" />\n            <div class=\"collapse-title text-xl font-medium text-primary\">\n                <p>{ result.page_content[:25]|safe }...</p>\n            </div>\n            <div class=\"collapse-content\">\n                <p>{ result.page_content }</p>\n            </div>\n        </div>\n        { endfor }\n    </div>\n    { endif }\n</div>\n```\n\n요약하자면, 일부 Tailwind 유틸리티 클래스는 다음을 나타냅니다:\n\n<div class=\"content-ad\"></div>\n\n- text-lg: 큰 글꼴 크기\n- mb-2: 2단계의 하단 여백\n- bg-base-200: 베이스 클래스의 배경 색상. 이것은 선택한 테마나 static/css/app.css에서 설정한 스타일에 따라 달라집니다.\n\n전체 HTML 또는 특정 섹션에 테마를 선택하려면 data-theme 속성을 사용하세요.\n\n예를 들어.\n\n```js\n<html data-theme=\"cupcake\"></html>\n```\n\n<div class=\"content-ad\"></div>\n\nOR\n\n```js\n<html data-theme=\"dark\">\n  <div data-theme=\"light\">\n    이 div는 항상 밝은 테마를 사용합니다.\n    <span data-theme=\"retro\">이 span은 항상 레트로 테마를 사용합니다!</span>\n  </div>\n</html>\n```\n\n# API를 사용하여 데이터 가져오기\n\nAPI가 모두 설정되었으므로, 이전 튜토리얼에서 생성한 대로, 이제 HTML에 데이터를 가져와야 합니다. 이를 위해 main.py 파일을 약간 수정해야 하고, 그럼 준비됩니다. 그러나 그에 앞서, 데이터를 가져오는 트리거 역할을 할 HTML 폼을 만들어 봅시다.\n\n<div class=\"content-ad\"></div>\n\n## HTML 폼\n\n```js\n<div class=\"max-w-md p-8 mx-auto mb-8 rounded-md shadow-md bg-neutral\">\n        <form id=\"query-form\" method=\"post\" action=\"/neighbours/\" class=\"flex flex-col mb-6\">\n            <div class=\"mb-4\">\n                <input type=\"text\" placeholder=\"Query\" id=\"query\" name=\"query\" required\n                    class=\"input input-ghost w-full max-w-xs\" />\n            </div>\n            <div class=\"mb-4\">\n                <input type=\"range\" min=\"1\" max=\"5\" value=\"3\" class=\"range\" step=\"1\" name=\"neighbours\" id=\"neighbours\" />\n                <div class=\"w-full flex justify-between text-xs px-2\">\n                    <span>|</span>\n                    <span>|</span>\n                    <span>|</span>\n                    <span>|</span>\n                    <span>|</span>\n                </div>\n            </div>\n            <button type=\"submit\" class=\"btn btn-accent\">제출</button>\n        </form>\n  </div>\n```\n\n대부분이 DaisyUI 구성 요소로 구성되어 있습니다. 중요한 점은 form의 action 속성이 우리 API의 /neighbours 엔드포인트를 가리키고 POST 메소드를 사용해야 한다는 것입니다.\n\n## FastAPI 앱\n\n<div class=\"content-ad\"></div>\n\nJinja2 템플릿을 렌더링하려면 fastapi.templating 모듈을 사용해야 합니다.\n\n```js\nfrom fastapi.templating import Jinja2Templates\n\ntemplates = Jinja2Templates(directory=\"templates\")\n```\n\nHTML 템플릿이 동적으로 렌더링되어야 하므로 CSS 스타일 또는 다른 정적 리소스를 포함하는 폴더를 마운트해야 합니다. 이를 통해 템플릿 내에서 폴더에 액세스할 수 있습니다.\n\n```js\nfrom fastapi.staticfiles import StaticFiles\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n```\n\n<div class=\"content-ad\"></div>\n\n엔드포인트 함수에 필요한 유일한 조정은 응답 클래스를 HTMLResponse로 변경하는 것뿐입니다. JSON 객체 대신 웹페이지를 렌더링할 것이기 때문입니다.\n\n```js\n@app.get(\"/\", response_class=HTMLResponse)\nasync def main(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n# 이웃 가져오기\n@app.post(\"/neighbours/\", response_class=HTMLResponse)\nasync def fetch_item(request: Request, query: str=Form(...), neighbours: int=Form(...)):\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n    results = db.similarity_search(query, k=neighbours)\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"results\": results})\n```\n\nFastAPI 폼은 Pydantic 모델을 지원하지 않습니다. 대신 Form 메서드를 사용하여 폼에서 게시된 데이터를 구문 분석합니다. 함수 매개변수에서 query: str = Form(...)는 엔드포인트가 문자열인 query이름의 폼 필드를 예상한다는 것을 나타냅니다. 함수가 호출될 때 TemplateResponse를 반환하고 결과를 템플릿에 전달합니다.\n\n다 됐어요! 완료했습니다. 이것은 Streamlit 없이 Python으로 웹 앱을 만드는 가장 쉬운 방법 중 하나였어요. Tailwind나 DaisyUI CSS 없이 기본적인 'Hello World' 애플리케이션을 선택했더라도 더 쉽게할 수 있었겠지만, 그렇게 한 것에 재미가 어디 있나요?\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_2.png\" />\n\n# 결론\n\n나는 가능한 한 철저하게 노력하여 파이썬을 사용하여 FastAPI를 이용해 웹 앱을 구축하는 데 필요한 모든 중요한 측면을 자세히 설명했다. 프론트엔드를 구축하는 동안 문제가 발생하면 언제든지 ChatGPT에 의지하고, 그것에게 프론트엔드를 만들도록 요청할 수 있다. 유용한 팁 중 하나는 Tailwind 문서에서 샘플 코드를 제공하고 앱에 통합하도록 요청하는 것이다. 튼튼한 보일러플레이트가 있으면 이를 역공학하고 원하는 대로 사용자 정의할 수 있다. 이 방식은 프로세스를 간단하게 만들며 문서 전체를 읽는 것보다 더 매력적으로 만든다.\n\n다음 주말에는 완전한 RAG 애플리케이션을 Llama3을 사용하여 구축하거나 이것에 파일 업로드 기능을 추가할 예정이다. 나는 처음부터 파일 업로드를 구현하고 처리하는 데 경험이 없기 때문에 그것이 흥미로울 것이다.\n\n\n<div class=\"content-ad\"></div>\n\n저희 튜토리얼을 즐기셨기를 바랍니다. 경험이 부족한 사람이 작성하여 이해하기 쉬울 거예요. 안녕히 계세요!\n\n## Github Repo","ogImage":{"url":"/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_0.png"},"coverImage":"/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_0.png","tag":["Tech"],"readingTime":11},{"title":"AI 에이전트들을 해킹했어요 이제 모두 무료로 이용할 수 있어요","description":"","date":"2024-05-20 21:59","slug":"2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree","content":"\n\n![image](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_0.png)\n\n몇 주 전, 무료로 Gradio API 호출을 사용할 수 있는 비밀 핵을 발견했습니다(여기 및 여기에서 더 읽을 수 있어요). Ben Auffarth가 지은 멋진 책인 랭체인에 관한 연구를 완료했는데요... 그리고 영감을 받았어요.\n\nLangchain과 함께 Gradio API를 사용하여 무한한 AI 에이전트의 가능성을 무료로 테스트할 수 있는 방법이 있는지 궁금했습니다. 당신도 당신을 위해 무료로 작동하는 에이전트의 기초를 마련할 준비가 되어 있나요?\n\n자리에 튼튼히 잡히세요. 이 기사에서도 여러분이 같은 일을 할 수 있는 방법을 설명하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 왜 AI 에이전트?\n\n내게 있어서 AI 에이전트에 관한 이야기는 지금까지 배경 속에 있던 것이라고 말해야겠어요. 첫째로, 나는 오픈 소스 모델만을 사용하여 작업하고 싶다는 생각을 가지고 있어서입니다. 둘째로, AI 에이전트 분야로 이동하기 위해 작고 정확한 모델을 찾고 있었기 때문입니다. GPU가 없기 때문에 모델의 선택과 크기가 항상 우선 순위입니다.\n\n## 그런데... 이 에이전트들이 뭐죠?\n\n저는 공정 제어 산업 자동화 엔지니어이기 때문에 에이전트가 무엇인지 설명하는 것은 쉽게 이해되어요: 에이전트들은 의사 결정 과정의 주도 역할을 합니다.\n\n<div class=\"content-ad\"></div>\n\n- 환경과 상호작용하며 선택을 내리고 특정 목표를 달성하기 위해 설계된 컴퓨터 프로그램 또는 시스템입니다.\n- 인간에 의해 직접적으로 제어되지 않고, 자율적인 개체로 독립적으로 작동하여 유연한 문제 해결 능력을 발휘합니다.\n\n에이전트는 반응적(Reactive)이거나 적극적(Proactive)인 성격, 환경의 안정성(고정 또는 동적), 그리고 다중 에이전트 시스템에 참여하는 정도와 같은 독특한 특성에 따라 분류될 수 있습니다.\n\n- 반응적 에이전트는 환경 자극에 신속히 반응하고 이러한 입력에 기반하여 행동을 취합니다.\n- 적극적 에이전트는 목표를 달성하기 위해 적극적으로 계획을 세우고 행동합니다.\n\n여러 에이전트가 협력할 때, 그들은 다중 에이전트 시스템을 형성하며 각각이 공통 목표에 기여합니다. 효과적인 조정과 소통을 보장하기 위해 이러한 에이전트들은 행동을 동기화하고 서로 상호작용해야 합니다.\n\n<div class=\"content-ad\"></div>\n\nLangchain은 내장된 기능을 갖춘 강력한 프레임워크로, 모든 종류의 AI 에이전트를 조직화하고 조정하는 데 사용할 수 있습니다. 다음 글에서 그에 대해 자세히 배워보겠습니다.\n\n여기서는 에이전트 애플리케이션을 위한 기본 도구를 만들어보겠습니다.\n\n# 문서는 퍼즐입니다\n\n<div class=\"content-ad\"></div>\n\n아직 예제가 없습니다. Gradio와 Langchain의 문서는 꽤 좋지만 주로 OpenAI 예제에 초점을 맞추고 있습니다. 오픈 소스 도구와 AI를 사용할 때마다 가장 큰 숙제가 바로 이겁니다.\n\n그래서 저는 스스로 만들기로 결심했습니다. 1주일간의 고군분투 끝에 가능하다는 것을 깨달았어요.\n\n동시에 좋은 문서는 해결책의 원천입니다. 우리는 해결해야 할 문제를 알고 있습니다: Gradio API 호출을 Langchain의 LLM 인스턴스로 결합시키기입니다.\n\n두 프레임워크의 문서를 훑어 보면서 몇 가지 영감을 얻었습니다:\n\n<div class=\"content-ad\"></div>\n\nGradio Python client: Gradio Python client을 사용하면 어떤 Gradio 앱이든 API로 쉽게 사용할 수 있습니다. 예를 들어, 마이크로부터 녹음된 오디오 파일을 전사하는 Hugging Face Space를 고려해보세요. 아래에 예시가 있습니다.\n\n![Gradio Example](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_2.png)\n\ngradio_client 라이브러리를 사용하면 프로그래밍 방식으로 오디오 파일을 전사하는 API로 Gradio를 쉽게 사용할 수 있습니다. 작동 방식을 이해하려면 이전 글 \"Chatbot Cheat Code: Build Your AI Assistant Running A HUGE LLM Without Spending A Penny — Part 1/Part 2\"를 참조해주세요.\n\nLangchain Gradio component: Hugging Face Spaces에는 수천 개의 Gradio 앱이 있습니다. 이 라이브러리는 이러한 앱들을 LLM(Large Language Model)의 손끝에 두는 데 도움이 됩니다. 구체적으로, gradio-tools는 Gradio 앱을 도구로 변환하는 Python 라이브러리로, 이를 이용해 큰 언어 모델(LLM) 기반 에이전트가 작업을 완료하는 데 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, LLM은 온라인에서 찾은 음성 녹음을 전사하고 그 내용을 요약하는 Gradio 도구를 사용할 수 있습니다. 또는 Google 드라이브의 문서에 OCR을 적용한 다음 해당 내용에 대한 질문에 답변하는 다른 Gradio 도구를 사용할 수도 있습니다.\n\nLangchain의 블로그에 따르면, 사전에 구축된 도구 중 하나가 아닌 공간을 사용하려면 쉽게 자체 도구를 만들 수 있습니다. 본 기사를 통해 프로세스가 실제로 쉬운지 여부를 직접 판단하게 될 것입니다...\n\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_3.png)\n\n# 사용자 지정 래퍼를 만들어야 합니다\n\n\n<div class=\"content-ad\"></div>\n\nLangchain은 거대한 통합 모음을 보유하고 있어요: 기본적으로 언어 모델, 문서 로더, 데이터베이스 등을 모듈식이고 쉽게 연결할 수 있어요.\n\n그들은 우리 모두의 도구 세트와 함께 사용될 수 있는 사용자 정의 LLM 클래스를 생성할 수 있는 가능성을 열어 두었어요. \n\n여기서는 LangChain에서 Llama-3-8b에 연결하는 방법을 배웠어요. 하지만 이 과정은 다른 툴을 사용하고 싶거나 LangChain에서 지원하는 것과 다른 래퍼를 사용하고 싶을 때에도 동일해요. \n\n그럼 시작해봐요. 이 예제에서는 Langchain을 Llama-3-8b에 연결할 거에요. 하지만 그레디오 API와 허깅페이스 허브 데모 애플리케이션에 대해서도 (작은 트릭들이 있긴 하지만) 동일한 프로세스가 적용돼요.\n\n<div class=\"content-ad\"></div>\n\n무료 구글 Colab 노트북을 열어보자. CPU만 있는 것으로 충분하다. Google Colab를 처음 사용하거나 무료로 얻는 방법을 모르는 경우 여기 지침을 읽어보세요:\n\n우리가 필요한 라이브러리를 먼저 설치합시다.\n\n```js\n%pip install --upgrade --quiet gradio_tools huggingface_hub langchain\n```\n\n이 노트북은 HuggingFace 토큰이 없어도 작동합니다. 그러나 강력히 권장하긴 하지만요: 여기 기사에서 지시 사항을 따라 해보세요.\n\n<div class=\"content-ad\"></div>\n\n## Gradio 클라이언트 인스턴스화\n\n이것이 첫 번째 단계입니다. 기본적으로 gradio 도구를 사용하여 Gradio 데모 애플리케이션을 호스팅하고 있는 HuggingFace Space에 API 호출을 설정합니다. 추론은 그 곳에서 이루어지며, LLM으로부터 응답을 받게 될 것입니다.\n\n```js\nfrom gradio_client import Client\n\nclient = Client(\"ysharma/Chat_with_Meta_llama3_8b\")\n\n# 이 부분은 연결을 테스트하기 위함입니다\nresult = client.predict(\n  message=\"Hello!!\",\n  request=0.95,\n  param_3=512,\n  api_name=\"/chat\"\n)\nprint(result)\n```\n\n노트북 셀을 실행하면 데모 엔드포인트와 결과에 대한 연결이 표시됩니다. HF_token을 전달하지 않으면 경고 메시지가 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n해당 부분을 수정하려면 다음과 같이 하세요:\n\n```js\nfrom gradio_client import Client\n\nyourHFtoken = \"hf_xxxxxxxxxxxxxxxxxxxx\" # 여기에 HF 토큰 입력\nclient = Client(\"ysharma/Chat_with_Meta_llama3_8b\", hf_token=yourHFtoken)\n```\n\n이제 Gradio 클라이언트가 작동하는 것을 알았습니다. Gradio와 Langchain을 연결하기 위해 Langchain에 새로운 LLM 래퍼를 생성해야 합니다.\n\n전체 노트북은 해당 GitHub 저장소에서 찾을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n## 사용자 정의 LLM 래퍼\n\n사용자 정의 LLM이 구현해야 하는 필수 사항은 두 가지뿐입니다.\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_4.png)\n\n위 문서 페이지에 따르면 새로운 클래스에서 최소한으로 _call 및 _llm_type 매개변수부터 시작합니다.\n\n<div class=\"content-ad\"></div>\n\n알림: 위의 Python 코드의 80%는 Langchain 설명서에서 직접 가져온 것입니다 😅 걱정하지 마세요, Colab 노트북 링크를 올릴 테니 그 전에 단계별로 설명해드리겠습니다.\n\n```js\nfrom typing import Any, List, Mapping, Optional\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\nclass GradioClientChat(LLM):\n    \"\"\"\n    Gradio API 호출을 기반으로 한 사용자 지정 LLM 클래스입니다.\n    \"\"\"\n    from gradio_client import Client\n    chatbot: Any = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # ChatBot 클래스를 인스턴스화합니다.\n        self.chatbot = Client(\"ysharma/Chat_with_Meta_llama3_8b\")\n\n    @property\n    def _llm_type(self) -> str:\n        return \"Gradio API client Meta_llama3_8b\"\n\n    def _call(\n            self,\n            prompt: str,\n            stop: Optional[List[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            chatbot=None,\n            request: float = 0.95,\n            param: float = 512,\n    ) -> str:\n        \"\"\"\n        지정된 프롬프트를 사용하여 Gradio API 클라이언트 Meta_llama3_8b에 API 호출을 실행하고 응답을 반환합니다.\n        \"\"\"\n        if chatbot is None:\n            chatbot = self.chatbot\n\n        if stop is not None:\n            raise ValueError(\"stop kwargs are not permitted.\")\n\n        # API에서 응답 반환\n        result = chatbot.predict(   #.submit for streaming effect / .predict for normal output\n              message=prompt,\n                request=request,\n                param_3=param,\n                api_name=\"/chat\"\n        )\n        return str(result)\n```\n\n처음 접하신 분들을 위해, 여기에서는 함수를 만드는 게 아니라 클래스를 생성하고 있습니다. 파이썬 클래스는 비슷한 객체 그룹이 공유할 수 있는 메서드(함수)와 속성(변수) 세트를 정의하는 청사진 또는 템플릿입니다. 객체를 생성하는 데 사용되는 청사진 역할을 하며, 건물의 설계와 구조를 개요로 나타내는 건축 청사진과 유사합니다. 클래스는 코드를 구조화하고 코드 재사용성을 제공하여 대규모 프로그램을 쉽게 작성하고 유지할 수 있게 해줍니다.\n\n우리의 클래스로 돌아가면: 필요한 모든 langchain 라이브러리를 가져온 후, GradioClientChat(LLM)라는 새로운 클래스를 생성합니다. 여기서 Gradio 클라이언트를 챗봇으로 사용합니다. 클래스는 LLM Langchain 클래스의 속성을 상속합니다. 이러한 이유로 _call 및 _llm_type 같은 몇 가지 속성과 메서드가 기본 사용자 정의 객체에서 필수적인 것입니다.\n\n<div class=\"content-ad\"></div>\n\n첫 번째 부분은 객체의 초기화와 _llm_type에 대한 부분이에요:\n\n```js\nclass GradioClientChat(LLM):\n    \"\"\"\n    Gradio API 호출을 기반으로 한 사용자 지정 LLM 클래스입니다.\n    \"\"\"\n    from gradio_client import Client\n    chatbot: Any = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # ChatBot 클래스의 인스턴스 생성\n        self.chatbot = Client(\"ysharma/Chat_with_Meta_llama3_8b\")\n\n    @property\n    def _llm_type(self) -> str:\n        return \"Gradio API client Meta_llama3_8b\"\n```\n\n그 다음으로, _call 메서드를 생성해요. 이 메서드는 가장 중요한 부분인데, 특정 Gradio API에 구성된 모델 매개변수(프롬프트 및 모델 매개변수)를 수락하여 추론을 실행하는 메서드입니다.\n\n```js\ndef _call(\n            self,\n            prompt: str,\n            stop: Optional[List[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            chatbot=None,\n            request: float = 0.95,\n            param: float = 512,\n    ) -> str:\n        \"\"\"\n        지정된 프롬프트를 사용하여 Gradio API client Meta_llama3_8b에 API 호출을 수행하고 응답을 반환합니다.\n        \"\"\"\n        if chatbot is None:\n            chatbot = self.chatbot\n\n        if stop is not None:\n            raise ValueError(\"stop kwargs are not permitted.\")\n\n        # API에서 응답 반환\n        result = chatbot.predict(   #.submit for streaming effect / .predict for normal output\n              message=prompt,\n                request=request,\n                param_3=param,\n                api_name=\"/chat\"\n        )\n        return str(result)\n```\n\n<div class=\"content-ad\"></div>\n\n입력 매개변수에는 변수 이름과 함께 기본값으로 request: float = 0.95, param: float = 512와 같은 값을 설정합니다. 이 값들은 구체적인 Gradio API를 반영해야 합니다. 우리의 경우 API 문서에서 ysharma/Chat_with_Meta_llama3_8b를 확인해주세요.\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_5.png)\n\n그런 다음, Gradio API 호출을 통해 추론을 실행하고 텍스트를 반환합니다.\n\n```js\n# API로부터의 응답 반환\nresult = chatbot.predict(   #.submit은 스트리밍 효과, .predict은 일반 출력용\n      message=prompt,\n      request=request,\n      param_3=param,\n      api_name=\"/chat\"\n)\nreturn str(result)\n```\n\n<div class=\"content-ad\"></div>\n\n\"새로 만든 클래스를 사용하여 셀을 실행해주세요.\n\n![image](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_6.png)\n\n# 우리의 사용자 정의 LLM을 인스턴스화하고 실행해봅시다\n\n이제 HuggingFace 데모 공간으로 둘러싸인 Langchain LLM를 실행할 준비가 되었습니다.\"\n\n<div class=\"content-ad\"></div>\n\n```js\n# llm을 인스턴스화하세요\nllm = GradioClientChat()\n\n# _call 메서드를 invoke와 함께 실행하세요\nresult = llm.invoke(\"인공 지능이란 무엇인가요?\")\nprint(result)\n```\n\n몇 초 안에 실행하면 응답을 받을 수 있어요!\n\n# 보너스 트랙: 스트리밍 효과\n\n한 번 더 단계를 추가하는 것은 문제가 되지 않겠죠? 몇 가지 추가적인 메서드를 생성할 수 있음을 보았습니다. 그 중 하나인 _stream은 생성될 때 토큰을 하나씩 반환하는 역할을 합니다. 이를 위해 이전 클래스 끝에 추가 메서드를 추가할 수 있어요.  \n\n\n<div class=\"content-ad\"></div>\n\n```python\ndef _stream(\n    self,\n    prompt: str,\n    stop: Optional[List[str]] = None,\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\n    chatbot=None,\n    request: float = 0.95,\n    param: float = 512,\n    **kwargs: Any,\n) -> Iterator[GenerationChunk]:\n    \"\"\"주어진 프롬프트에서 LLM을 스트리밍합니다. \n\n    이 메서드는 스트리밍을 지원하는 하위 클래스에 의해 재정의되어야 합니다. \n\n    구현되지 않은 경우, stream에 대한 호출의 기본 동작은 모델의 비스트리밍 버전으로 \n    대체하여 출력을 단일 청크로 반환하는 것입니다. \n\n    Args: \n        prompt: 생성할 프롬프트. \n        stop: 생성 시 사용할 정지 단어입니다. 모델 출력은 이러한 하위 문자열 중 \n            하나가 처음 발생하는 곳에서 잘립니다. \n        run_manager: 실행을 위한 콜백 매니저. \n        **kwargs: 임의의 추가 키워드 인수입니다. 일반적으로 모델 공급자 API \n            호출에 전달됩니다. \n\n    Returns: \n        GenerationChunks의 이터레이터.\n    \"\"\"\n    if chatbot is None:\n        chatbot = self.chatbot\n\n    if stop is not None:\n        raise ValueError(\"stop kwargs are not permitted.\")\n\n    # API에서 응답 반환\n    for char in chatbot.submit(   #.submit for streaming effect / .predict for normal output\n          message=prompt,\n            request=request,\n            param_3=param,\n            api_name=\"/chat\"\n            ):\n        chunk = GenerationChunk(text=char)\n        if run_manager:\n            run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n\n        yield chunk\n```\n\n첫 부분은 기본적으로 동일하다는 점을 알아두세요. 변경 사항은 2곳에서 발생합니다:\n\n- 처음부분에서 메소드의 출력이 문자열이 아닌 Iterator 객체임을 선언합니다.\n- 끝부분에서 for 루프를 시작하고, predict 대신 submit() 메서드를 호출합니다. 차이에 대해 제2부에서 설명했습니다.\n\n이제 새로운 추가와 print에서 작은 변경을 한 Class를 다시 실행할 수 있습니다. Google Colab에서도 텍스트가 생성 중에 스트리밍되는 것을 확인할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nllm = GradioClientChat()\n# 텍스트 인터페이스를 위한 코드 - Steramlit에서는 필요하지 않습니다\nfinal = ''\nfor token in llm.stream(\"what is science?\"):\n        if final == '':\n            final=token\n            print(token, end=\"\", flush=True)\n        else:\n            try:\n                print(token.replace(final,''), end=\"\", flush=True)\n                final = token\n            except:\n                pass\n```\n\nStreamlit을 사용하는 경우, 위의 코드는 필요하지 않습니다. Iterator 객체는 점진적으로 진행되기 때문에 모든 토큰을 하나씩 쌓아둡니다. 하지만 Google Colab에서는 그럴 여유가 없습니다. 단어별로 인쇄해야 하므로, 이미 생성된 내용을 뺀 새 Iterator 스트림으로 인쇄 기능을 조정해야 합니다.\n\n# 결론... 지금까지\n\n우리는 어디에서든 실행할 수 있는 AI 에이전트들의 자유 무리의 기초를 단순하게 놓았습니다. 솔직히 말해서, Gradio API 주변에 새롭게 만든 래퍼를 사용하여 Langchain 튜토리얼을 자유롭게 시도해볼 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n큰 모델인 Qwen100b와 같은 모델을 약간 조정하여 실행해 볼 수도 있어요. 성공하면 알려주세요.\n\n무엇을 기다리고 있나요?\n\n이 기사를 즐겁게 읽었기를 바라요. 이 기사에 대한 노트북도 있답니다.\n\n이 이야기가 가치 있는 정보를 제공했고 조금이라도 지원하고 싶다면, 다음을 해볼 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n- 이 이야기에 대해 많이 박수를 쳐주세요.\n- 기억하기 좀 더 유용한 부분을 강조합니다 (나중에 그것들을 더 쉽게 찾을 수 있고, 나는 더 나은 기사를 쓸 수 있습니다.)\n- 자신의 AI를 시작하는 방법을 배우세요. 이 무료 eBook을 다운로드하세요.\n- 내 링크를 사용하여 Medium 멤버십에 가입하십시오 — (무제한 Medium 이야기를 읽으려면 매월 $5)\n- Medium에서 나를 팔로우하세요.\n- 나의 최신 기사를 읽어보세요: https://medium.com/@fabio.matricardi\n\n더 많은 내용을 읽고 싶다면 여기 몇 가지 아이디어가 있습니다:\n\nYoussef Hosni의 이 기사로 직접 시도해 볼 수 있습니다.\n\n학습 자료:\n\n<div class=\"content-ad\"></div>\n\n파이썬 클래스의 예시:\n\n간단한 은행 계좌를 나타내는 프로그램을 만들고 싶다고 상상해보세요. \"BankAccount\"라는 클래스를 정의할 수 있습니다. 이 클래스에는 계좌 번호를 저장하는 \"account_number\" 및 현재 잔액을 저장하는 \"balance\"라는 두 가지 속성이 있을 수 있습니다. 또한 계좌에 돈을 입금하는 \"deposit()\" 메소드와 계좌에서 돈을 인출하는 \"withdraw()\" 메소드가 있을 것입니다.\n\n이 클래스의 인스턴스를 생성하려면 다음과 같이 클래스를 호출하는 방식으로 인스턴스화하면 됩니다:\n\n```js\naccount1 = BankAccount(\"12345\", 1000)\n```\n\n<div class=\"content-ad\"></div>\n\n지금, account1은 BankAccount 클래스의 객체이며, 계좌 번호가 \"12345\"이며 시작 잔고는 1000원입니다. 그런 다음 account1에서 deposit(500)과 같은 메서드를 호출하여 잔고에 500을 추가하거나 withdraw(200)를 호출하여 200을 빼는 등의 작업을 할 수 있습니다.\n\n클래스는 객체 지향 프로그래밍의 기본 개념으로, 관련된 데이터와 함수를 재사용 가능한 구성요소로 구성하여 복잡한 프로그램을 작성할 수 있게 해줍니다.\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_7.png)\n\n이 이야기는 Generative AI 출판물의 일환으로 발행되었습니다.\n\n<div class=\"content-ad\"></div>\n\n우리와 함께 최신 AI 이야기 속에 머무르기 위해 Substack, LinkedIn 및 Zeniteq에서 연락을 유지해보세요. 함께 AI의 미래를 만들어 봅시다!\n\n![Image](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_8.png)","ogImage":{"url":"/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_0.png"},"coverImage":"/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_0.png","tag":["Tech"],"readingTime":14}],"page":"63","totalPageCount":154,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}