{"pageProps":{"posts":[{"title":"AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지","description":"","date":"2024-05-15 16:41","slug":"2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny","content":"\n\n## 공정한 숫자 분석가의 장단점\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*Aun9fXyvK-JxqCHKhQyxrg.gif)\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*j7df-zSIAOBOo6rKwFgT1Q.gif)\n\n위 멋진 애니메이션들은 최고의 AI 기반 비디오 생성기인 Sora가 OpenAI로부터 나온 것 중에서도 가장 진보된 것입니다. 이것들은 의심의 여지없이 인상적입니다. 인공지능 분야에서 일하는 사람들도 지난 몇십 년 동안 많은 발전을 본 사람에게도 놀랄 만한 것입니다. 이 비디오를 관람하면 AI가 클립의 다양한 테마와 객체를 얼마나 매끄럽게 섞어내는지, 그 결과물이 얼마나 일관되고 물리적으로 타당해 보이는지에 놀라게 될 것입니다. 이 비디오를 보면 평범한 사람은 AI 기술이 이렇게 현실적인 수준에 도달했다는 사실에 당황할 것입니다.\n\n\n\n잠시 지나면 새로움이 사라지고 다른 것을 느끼기 시작할 지도 모릅니다. 이 비디오들에는 이상하고 최면적인 느낌이 있습니다. 충분히 오랫동안 보게 되면 눈빛 — 아마도 마음까지 — 흐릿해질 것입니다. 비디오가 미묘하고 일정한 압력으로 당신의 두뇌에 가해지는 것 같은 느낌이 듭니다. 이 경험에는 뭔가 기묘한 것이 있는데, 특정한 방식으로 말하자면 눈뒤쪽이나 다른 어딘가에 숨겨져 있는 것처럼 영원히 손에 쓸 수 없는 느낌이 듭니다.\n\n이 경험은 크래프트가 완벽하게 집중되어 있어서 불안할 수 있습니다. 그들의 표현은 섬세하고 부드럽고 윤득한, 내용에 관계 없이 가파른 모서리가 없습니다. 그러나 확신컨대 저는 그들에게 사로잡혔음에도 불구하고 뇌 속에 흐릿함이 남는 느낌을 떠났습니다. 이것은 비디오의 초현실적인 내용 때문인 것이 부분적으로는 사실입니다. 그러나 이것만으로 현상을 완전히 설명할 수는 없습니다 — 많은 인간이 만든 비디오가 초현실적이며, 똑같은 기이하면서 인상적인 효과를 내지는 않습니다. 이 현상을 직간접적으로 기술로서의 편견으로 이해할 수도 없습니다. 사실, 이 클립들이 인간에 의해 촬영되었다고 상상하면 기이함을 더 심화시키고 희석시키는 것보다 더 매료되는 느낌을 주곤 합니다.\n\n이 클립들은 눈동자를 즐겁게 해줄 수 있으며, 그들의 즐거움은 선명하고 생생한 영상의 시각적 즐거움에서 나옵니다. 단지 사탕처럼, 그들은 궁극적으로 만족할 만한 느낌을 주지는 못할 수도 있습니다. 항상 무언가를 \"전달\"하려고 하는 것 같지만 결코 그렇지 않습니다. 다시 말해, 그들은 아무것도 전달하려는 의도가 없으며 순간적인 경험 자체가 그 요점입니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*OIsAJ4ZB-YvQ34PDvyHswA.gif)\n\n\n\n그들을 관찰하는 것은 분수를 관찰하는 것처럼 매혹적입니다: 그 자체로 또한 발표 방식으로 매혹적입니다. 아니면 클라이맥스가 없는 이야기를 듣는 것과 같을지도 모릅니다. 설정이 해결되었다고 생각하는 즉시, 그것이 약간 변화하고 다른 이야기를 드러내며, 지금은 자신의 결말을 예측하며, 그리고 그렇게 이어지는 것은 하나의 문장처럼. 비디오는 항상 깨달음 직전인데, 그것들은 결코 넘어가지 않습니다. — 그리고 당신 역시 그것을 기대하지 않습니다. 그것들은 한계 상태이며, 그 상태를 유지하는 것을 선호합니다.\n\n![GIF 이미지](https://miro.medium.com/v2/resize:fit:400/1*xX03li5aQruQGNI5fwfcqg.gif)\n\n최면술 효과는 비디오가 어딘가로 향하고 있다는 느낌에서 온다. 그리고 모퉁이마다 새로운 약속을 하며 당신을 긴장시킵니다. 시청자는 따라가고, 무언가가 제공될 때까지 놓칠 수 없다고 느낍니다. 마침내 그것이 제공되지 않을 것으로 깨닫고 단지 그 순간의 경험을 즐깁니다. 그래서 그것은 목표에서 목표로 방황하며, 사회적 미디어에서 스크롤하는 것만큼 방향성이 없습니다.\n\n명확히 말하자면, 비디오에는 기저적인 아이디어가 있지만, 시작할 때 공개되고 일관성 있게 유지됩니다. 설정과 전달의 곡선은 없으며, 단지 지속적인 압력만 있는 것입니다. 이것이 비디오 콘텐츠가 시각적으로 매력적이어야 하는 이유이며, 종종 슬로우 모션으로 제시됩니다. 눈 얼음 위에서 노는 귀여운 강아지, 컵 속의 보트, 고원을 걸어 다니는 털족. 그들이 교육되는 재고 영상처럼, 그들의 시각적 매력은 그들의 요점 전부입니다. 클립의 메시지는 매체와 일치합니다.\n\n\n\n\n[![image](https://miro.medium.com/v2/resize:fit:640/1*KlbQB5xH8kI5G0FWXOWL1g.gif)](https://miro.medium.com/v2/resize:fit:640/1*KlbQB5xH8kI5G0FWXOWL1g.gif)\n\n아마도 이 현상을 이해하는 가장 좋은 방법은 다음과 같이 물어보는 것입니다: 클립은 무엇을 말하려고 하는 걸까요? 그 의도는 무엇이며, 주제는 무엇이며, 논문은 무엇일까요? 창조자가 우리가 이것을 보게 하려고 했던 이유, 특히 이것을 보게 하려고 한 이유는 무엇인가요? 어떤 비디오를 보든지, 생성의 의도나 목적을 빨리 파악할 것을 기대합니다. AI로 생성된 비디오의 경우 답을 찾기 어렵지 않을 것입니다. 왜냐하면 생성 모델이 한 번 훈련되면 모든 지정된 내용은 생성된 텍스트 프롬프트에서 나오기 때문이죠. 따라서 어떤 독특한 메시지라도 해당 비디오가 같은 모델의 다른 비디오와 비교하여 가지고 있는 것은 오로지 해당 프롬프트에서 온다는 데서 오게 됩니다. 텍스트 프롬프트는 궁극적으로 논문을 요약해야 하지만... 그렇지 않습니다. 프롬프트는 내용을 대표하지만 목적은 대변하지 못합니다.\n\n내가 무엇을 말하는지 이해하려면 AI에게 제공하는 모든 프롬프트가 어디서나 나온 것은 아니라는 것을 기억해야 합니다. 작가로서 이 프롬프트를 통해 표현하려는 동기, 감정, 목표 및 의도의 역사가 있습니다. 이것들이 당신의 숨겨진 목적이며, 글자 그대로 요청하고 있는 구체적인 내용이 아닙니다. 그 백그라운드 중 어느 것도 AI로 전달되지 않습니다. AI는 모든 사실을 받지 못하므로 속여주고 있다고 할 수 있습니다.\n\n[![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png)](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png)\n\n\n\n\nAI에게 \"가난한 사람들을 돕고 싶게 만들고 싶어\" 또는 \"세계를 탐험하는 것에 흥분하게 만들고 싶어\" 또는 \"아이처럼 궁금해하게 만들고 싶어\"와 같은 것을 전달하는 것은 어려운 일입니다. 종종 실제로 당신이 정말 무엇을 말하려고 하는 지에 대해 완전히 인식하지 못합니다 - 적어도 영어 단어로 표현할 정도로 충분히 인식하지 못합니다.\n\n따라서 AI의 결과물이 기대에 미치지 못하는 경우, 그것이 우연히 악한 요정처럼 당신이 요청한 대로 제공하지만 실제로 원하는 것은 제공하지 않기 때문입니다. AI가 범하는 실수는 사용자의 숨은 의도에서의 소외감을 드러냅니다. AI는 직역적이어서 당신이 강조하거나 억압해야 할 부분을 식별하지 못하며 실제로 있어야 하는 누락된 부분을 추가해야 하는 타이밍조차도 파악하지 못합니다.\n\n사용자로서 당신은 결과가 의도한 대로 가까워지도록 프롬프트를 수정해야 합니다만 실제로 말하지 않은 것을 수정해야 합니다. 그 동안 AI는 밀려오는 방향, 프롬프트별로 여러 번에 걸쳐 사용자의 비전 쪽으로 밀리며 움직입니다. 당신이 관심을 갖고 있다는 이유로 AI를 끌어당기고 있는 것이 아니라 밀어내고 있는 것입니다. 마치 모래성을 계속 다듬어야 하는 것처럼 그렇지 않은 부분은 천천히 무너지는 것입니다.\n\n![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_1.png)\n\n\n\n아티스트 제너레이터를 사용하여 미술을 만드는 경험은 종종 영화 롤을 위해 서영장의 서막이 필요한 침팬지 촬영을 하는 것과 비슷합니다. 촬영장에서의 동물 트레이너와 카메라맨은 충분한 샷을 촬영하여 결과물이 침팬지가 원하는대로 보이도록 편집할 수 있도록 해야 합니다. 동물은 슬픈 상호작용을 보고도 그것이 슬픔스러워 보이도록 통찰할 확률이 낮습니다. 이 모든 것은 다른 훈련된 행동들을 조각조각으로 모아서 마치 침팬지가 의도했던 것처럼 보이도록 해야 합니다.\n\nAI가 당신의 의도와 일치하려면 자신의 의도를 가져야 하거나, 사용자의 프롬프트에서 그 의도를 추출해내야 합니다. 후자는 매우 어렵습니다. 반면에 인간 예술가에게 작품을 의뢰할 때는 요청으로부터 당신의 의도를 이해할 것을 기대합니다. 그렇지 않으면 감정적이거나 은유적인 언어를 통해 설명할 수 있습니다.\n\nAI가 표현하는 \"의도\"나 의미는 프롬프트 자체에서 비롯된 것이 아니라, 훈련 비디오에서 빌려온 단편에서 나옵니다. 오직 즐거운 비디오에서만 훈련하면 결과물은 즐거울 것입니다. AI는 표현하는 대상이나 그들의 예술이 전달하는 것에 대해 의견을 갖거나 깐깐해하지 않습니다. 그들은 훈련 중 주어진 이미지를 사용하여 완전히 개방적이라고 말할 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*vZHKB5bRVRcQmiCef7DB2w.gif)\n\n\n\nAI 생성기는 다양한 콘텐츠를 일관된 전체로 통합하는 능력이 우수합니다. 그들이 하는 일은 본질적으로 그겁니다. 생성기는 사용자가 제공한 프롬프트를 받아들인 다음, 교육 중에 보았던 다른 클립들과 관련이 있는 방향으로 여러 방향으로 분기합니다.\n\n![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_2.png)\n\n반면에, 인간 창작자들은 상당히 제한적이며, 예술가가 특이한 표현 스타일에서 벗어나는 것이 어려울 수 있습니다. 이는 강점이자 약점입니다. 인간이 만든 작품의 제한된 스타일은 집중을 보여주며, 무엇을 중요시 여기는지를 보여줍니다. 의도적인 인간 행동은 그 안에 포함된 가치를 보여줍니다. 예술가에게 이러한 가치가 작품 속 주제들을 유발시키며 그들이 무엇을 그리거나 그림을 그릴지를 선택적으로 만듭니다.\n\n인간 커뮤니케이션은 일관성 이상을 추구합니다. 그것은 시청자를 특정 방향으로 이끌거나, 특정한 생각이나 감정을 일으키기 위해 노력합니다. 이것은 결과적인 표현에 내재되어 있습니다. 결과적으로, AI가 훈련한 각 인간 예술 작품은 다른 의도가 있습니다. 그리고 그들이 큰 냄비에 섞는다면 결과를 혼란스럽게 할 수밖에 없습니다. 이것이 생성 인공지능이 한 특정 예술가의 스타일을 복사할 때 더 성공적인 이유입니다. 그들은 해당 예술가의 암시적 동기를 빌려오며, 무시할 것, 강조할 것, 무시할 것을 비교적 오염되지 않은 채로 가져옵니다.\n\n\n\n<img src=\"https://miro.medium.com/v2/resize:fit:640/1*3ysK_fx8qcIdKakh78C9SQ.gif\" />\n\n여러 관점을 결합할 때 그 결과물은 특정한 것을 말하지 않을 뿐더러 \"의견\"을 내세울 수도 없습니다. AI 생성 비디오는 우리가 만들어낸 문화 유물을 통해 서로 조화롭게 발음하는 열두 명의 인간 목소리와 같습니다. 결과물을 통합하는 데 결합된 의지나 목적이라는 단일한 가치는 없습니다. AI는 콘텐츠를 효율적으로 결합하는 데 탁월하지만 의도를 결합하지는 않습니다. 단순히 의도를 결합해서는 안 되기 때문입니다. \"의도\"는 항상 원하는 대로 편향됩니다. 양보할 여지가 없습니다. AI 예술에서 의도를 혼합하는 것을 바로잡기 위한 유일한 방법은 AI가 먼저 선택들이 관람자에게 미치는 효과를 배우고, 그것을 활용해 예술을 자신이 원하는 방향으로 이끌어내는 것입니다.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:640/1*A2kk9SXCjVUHD3iHU9Hz9g.gif\" />\n\n현재, 생성적 AI는 일종의 깨끗한 판이며 편향되지 않은 데이터 분석기입니다. 그것에는 매력이 있습니다. 인간과 함께 일할 때의 많은 성가시움, 특히 고집부리기와 편견이 없는 것을 들 수 있습니다. 그것은 순순하고 공손하게 사회 유물을 흉내내는 것입니다. 그러나 이것은 실패가 혼란스러울 수 있음을 의미합니다, 소프트웨어가 여러분을 \"조롱\"¹하고 있는 것처럼. 여러분이 메시지나 주제에 대해 왜 그것을 중요하게 생각하는지 이해한다는 것을 보여주지 않고 여러분의 표현을 모방합니다. 여러분은 AI '예술가'가 여러분의 메시지에 동의하고 그것을 결과물을 통해 표현해주기를 원합니다. 그러나 그것은 여러분에게 동의할 수 없으며, 보여준 것만 모방할 뿐입니다. 따라서 그것은 결국 \"조롱\"인 인간 유물을 생성하게 됩니다.즉, 원본의 주도적인 목소리가 없는 모조품을 생성합니다.\n\n\n\n¹ 이것은 유사법이에요; AI는 아무것도 조롱하는 게 아니에요.\n\n![AI generated video](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_3.png)","ogImage":{"url":"/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png"},"coverImage":"/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png","tag":["Tech"],"readingTime":7},{"title":"마이크로소프트 Phi-3-Mini는 정말 인상적이에요","description":"","date":"2024-05-15 16:38","slug":"2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive","content":"\n\n최근에 마이크로소프트 AI가 출시한 Phi-3-Mini 언어 모델입니다. 이 모델은 작은 언어 모델(SLM) 범주에 속하며 LLM이 제공하는 여러 기능을 제공합니다. 유일한 차이점은 SLM은 크기가 더 작고 적은 데이터로 훈련되었다는 것입니다.\n\n마이크로소프트에 따르면 Phi-3 모델은 가장 강력하고 비용 효율적인 작은 언어 모델(SLM)이라고 합니다. 이들은 Phi-3-mini-4k-instruct 및 Phi-3-mini-128k-instruct 모델을 출시했으며 둘 다 어떠한 제약 없이 완전히 오픈 소스입니다. 이는 128k 컨텍스트 길이를 갖는 작은 언어 모델이 처음으로 등장한 시점입니다.\n\n다가오는 몇 주 안에 Phi-3-small (7B) 및 Phi-3-medium (14B) 모델도 출시할 예정입니다.\n\n# 성능 평가\n\n\n\n아래는 공식 Phi-3 모델 벤치마크 결과입니다.\n\n![image](/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png)\n\n작고 강력한 Phi-3-Mini 모델은 무게 대비 어마어마한 성능을 자랑합니다. Phi-3-Mini 모델은 38억 개의 매개변수를 가지고 있으며, 추론, 수학, 코드 생성 등 대부분의 벤치마크에서 Gemma-7B, Mistral-7B, Llama-3-8B와 경쟁하여 우수한 성과를 거두었습니다.\n\n\n\nPhi-3-Mini 모델이 성능이 잘 나오지 않는 유일한 벤치마크는 사실적인 지식 벤치마크입니다. 그 이유는 이 모델의 작은 크기 때문에 많은 사실을 보존할 수 없기 때문입니다.\n\n하지만 우리는 여전히 Phi-3-Mini 모델을 RAG 및 검색 작업에 사용할 수 있습니다. 여기서 우리는 사실들을 검색하여 모델에 전달하여 답변을 생성할 수 있습니다. 그리고 이 블로그에서는 바로 그것을 구축할 것입니다 — 지역적인 GenAI 기반 검색 엔진.\n\n# 왜 SLMs를 사용해야 하는가?\n\n개발을 계속 진행하기 전에, 왜 작은 언어 모델이 중요한지에 대해 알아보겠습니다.\n\n\n\n매우 처음으로 실험이 있습니다. LLM을 실험하는 것은 상당히 비싸요. LLM 위에 무언가를 개발하거나 기존 프로세스/소프트웨어에 기능을 추가하기 위해 실험을 하는 것은 상당한 양의 실험을 필요로 합니다. 이러한 실험에는 다양한 프롬프팅 기술 시도, 출력을 구문 분석하는 여러 방법 사용 등이 포함됩니다.\n\n네, 많은 LLM API 제공 업체들이 요금을 지불하는 방식을 채택하고 있습니다. 그럼에도 불구하고 모든 것을 올바르게 만들기 전에 $10에서 $20을 지출할 수 있습니다. 그러나 SLM을 사용하면 llama-cpp-python 또는 node-llama-cpp와 같은 라이브러리를 사용하여 로컬에서 로드하고 추론을 수행할 수 있습니다.\n\n다음은 클라이언트가 클라우드를 피하려는 경우입니다. 사람들은 보통 내부 조직 정보를 클라우드에 제공하는 것을 조심스러워하며 대부분을 인하우스 또는 대부분 로컬에서 처리하고 싶어합니다. 그들은 메인프레임에서 일부 데이터를 검색하여 사용자에게 제공하거나 서버 측에서 렌더링하여 사용자에게 보여줄 수 있습니다. 이 경우 사용자의 로컬 컴퓨터나 메인프레임과 함께 Phi-3-Mini와 같은 SLM을 사용하여 그들의 검색에 GenAI 기능을 추가할 수 있습니다.\n\n모든 비즈니스는 성능을 최적화하고 저렴한 비용으로 속도 처리량과 보안을 제공합니다. 따라서 마지막으로는 컴퓨팅 효율성, 생성 속도, 특정 작업 및 보안의 조합입니다. LLM은 매우 일반적이지만 대부분의 조직 또는 클라이언트는 그것을 필요로하지 않을 수 있고, 그들은 모델을 자신에게 특화된 작업에 적합하도록 원할 수도 있습니다. 그 경우에는 자체 사용자 정의 명령어 집합으로 SLM을 미세 조정한 다음 미세 조정된 버전을 추론용으로 호스팅할 수 있습니다. 추론은 로컬이나 메인프레임에서 수행할 수 있습니다.\n\n\n\n이제 기업이나 개인이 SLM을 사용하는 다양한 이유를 이해했으니 Phi-3-Mini 모델을 활용한 검색 도구를 만들어보겠습니다.\n\n# AI 검색 도구\n\n최근에 나는 Llama-3 8B가 Groq에서 빠른 생성 속도로 검색 요약을 수행하는 데 탁월하다는 주제의 블로그를 작성했습니다. 아직 해당 기사를 읽지 않았다면 아래 링크를 통해 확인해보세요.\n\n저희 이전 블로그에서 구현한 대부분의 내용은 그대로 유지될 것입니다. 변경되는 것은 LLM 및 검색 엔진 뿐입니다.\n\n\n\n여기서는 llama-cpp-python 라이브러리를 사용하여 로컬 시스템에 양자화된 Phi-3-Mini-4K-Instruct 모델을 로드하고 모델과 상호 작용하여 텍스트를 생성할 것입니다. 또한 사용자 쿼리를 기반으로 Brave Search API를 사용하여 검색 결과를 검색할 것입니다.\n\n하지만 어쨌든, 하루 끝에 모든 AI 기반 검색 플랫폼의 기본 원리는 사용자 쿼리를 기반으로 내용을 검색하고 요약하는 것입니다. 각각이 이 작업을 수행하는 방식은 운영 규모에 따라 다를 수 있습니다. 대부분의 엔지니어링 노력은 검색 속도를 빠르게 만들기(캐싱 또는 기타 방법으로)와 관련 결과를 검색하며 필요할 때 사실과 숫자로 답하는 데 사용됩니다.\n\n# 검색 API\n\n가장 간단한 구성 요소인 검색 구성 요소부터 시작하여 전체 시스템의 중추인 부분입니다.\n\n\n\n위에서 언급한 대로, 블로그에는 Brave Search API를 사용할 것이지만 다른 것을 사용해도 됩니다. Brave Search API를 사용하려면 https://brave.com/search/api/로 이동하여 등록해야 합니다. 등록한 후에는 우리의 신용카드 세부정보를 제공하여 무료 요금제를 구독해야 합니다. 무료 요금제에서는 월 2,000개의 요청과 초당 1개의 요청이 가능하여 실험에 충분합니다.\n\n무료 요금제를 구독한 후에, API Keys 섹션으로 이동하여 API 키를 생성하고 해당 키를 복사하여 .env 파일에 넣어야 합니다.\n\n```js\nBRAVE_API_KEY=\"YOUR_BRAVE_API_KEY\"\n```\n\n아래에 보여지는 대로 configs.py 파일에 모든 환경 변수를 로드할 것입니다.\n\n\n\n```js\n# configs.py\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nBRAVE_API_KEY = os.environ.get(\"BRAVE_API_KEY\")\n```\n\nAPI 키를 구성했어요. 이제 사용자의 쿼리를 검색하는 데 도움이 될 코드를 작성해 봅시다.\n\n```js\n# brave_search.py\n\nfrom configs import BRAVE_API_KEY\nimport httpx\n\nasync def brave_search(search_term):\n    brave_api_key = BRAVE_API_KEY\n    url = f'https://api.search.brave.com/res/v1/web/search?q={search_term}&count=3'\n    headers = {\n        'X-Subscription-Token': brave_api_key,\n        'Accept': 'application/json'\n    }\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        if response.status_code == 200:\n            data = response.json()\n            print('검색 데이터: ', data)\n            return format_search(data)\n        print(await response.text())\n        return None\n\ndef format_search(search_results):\n    retrieve_keys = ['web', 'news']\n    formatted_results = []\n    for value in retrieve_keys:\n        if value in search_results:\n            results = search_results[value]['results']\n            formatted_results.append('\\n'.join(\n                f\"Title: {result['title']} Description: {result['description']} URL: {result['url']}\"\n                for result in results))\n    print('포맷된 결과: ', formatted_results)\n    if formatted_results:\n        return '\\n'.join(formatted_results)\n    return None\n```\n\n위에서는 검색어를 인수로 사용하는 brave_search 함수를 정의했습니다. 그런 다음 Brave API 웹 검색 엔드포인트를 호출하고 검색 결과를 검색합니다. 검색 결과는 format_search 함수를 사용하여 하나의 문자열로 서식이 지정됩니다.\n\n\n\n# 로컬 모델 추론\n\n우리는 llama.cpp 패키지의 Python 바인딩을 llama-cpp-python 라이브러리를 통해 로컬 시스템에 모델을 로드하고 텍스트 생성 또는 추론을 수행할 것입니다.\n\n이를 위해서는 먼저 llama-cpp-python 패키지를 설치하고 아래 제공된 링크에서 Phi-3-Mini-4k-Instruct 모델을 GGUF 형식으로 다운로드해야 합니다.\n\n## llama-cpp-python 설치하기\n\n\n\npip를 사용하여 llama-cpp-python을 설치해 보세요!\n\n```sh\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n```\n\n만약 이미 설치되어 있는 경우, 이 명령어는 llama-cpp-python 패키지를 업그레이드하고 재설치할 것이며, 그렇지 않은 경우에는 새로 설치할 겁니다.\n\n## 모델 다운로드\n\n\n\n위에 제공된 링크로 이동하여 Phi-3-mini-4k-instruct-q4.gguf 파일을 다운로드하세요. 다운로드가 완료되면 작업 디렉토리에 모델 폴더를 만들고 모델 파일을 모델 폴더로 이동하세요.\n\n![Phi-3 Mini](/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_1.png)\n\n## 추론을 위한 모델 호출\n\nllama-cpp-python 라이브러리에서 제공하는 유틸리티를 사용하여 모델을 로드하고 추론을 위해 호출하고 생성된 토큰을 스트리밍 및 비스트리밍 방식으로 출력하는 클래스를 작성하겠습니다.\n\n\n\n```python\n# llm_invoke.py\n\nfrom llama_cpp import Llama\nfrom ctx import ContextManagement  # 이것은 나중에 나올 것임\nfrom typing import List, Dict, Union\n\nclass LLM:\n\n    def __init__(self, model_path: str, **kwargs):\n        self.llm = Llama(\n            model_path=model_path,\n            n_gpu_layers=kwargs.get(\"n_gpu_layers\",\n                                    -1),  # GPU 가속화를 사용할 때 주석 해제\n            seed=kwargs.get(\"seed\", 1337),  # 특정 시드를 설정할 때 주석 해제\n            n_ctx=kwargs.get(\"n_ctx\",\n                             4096),  # 컨텍스트 창을 확장할 때 주석 해제\n            n_threads=kwargs.get(\"n_threads\", 8))\n        self.ctx = ContextManagement(2560)\n\n    def __stream__(self, messages: List[Dict], **kwargs):\n        input_message = self.ctx(messages)\n        output = self.llm(input_message, stream=True, echo=False, **kwargs)\n        for op in output:\n            yield op.get(\"choices\")[0].get(\"text\") or \"\"\n\n    def __complete__(self, messages: List[Dict], **kwargs):\n        input_message = self.ctx(messages)\n        output = self.llm(input_message, echo=False, **kwargs)\n        return output.get(\"choices\")[0].get(\"text\")\n```\n\n위에서 llama-cpp-python 모듈에서 사용 가능한 Llama 객체를 사용하여 초기화 인수에서 제공된 model_path에서 모델을로드했습니다.\n\n또한 __init__ 메소드에서 ContextManagement 객체를 초기화합니다. 이것은 Llama 3 + Groq가 AI Heaven 블로그에서 본 것과 유사합니다. 다음 섹션에서 다시 살펴보겠습니다.\n\nLLM 클래스에서 두 가지 메서드가 있습니다. 토큰이 생성되는대로 토큰을 스트리밍하는 __stream__ 메서드와 생성이 완료되면 출력을 반환하는 __complete__ 메서드입니다.\n\n\n\nLLM 클래스를 사용하면 이제 모델을 로드하고 호출할 수 있습니다. 이제 LLM (SLM)를 검색과 통합해 보겠습니다.\n\n# 컨텍스트 관리\n\nPhi-3-mini-4k-instruct 모델을 사용하면 최대 4k 토큰을 사용하거나 생성할 수 있습니다. 이 블로그에서 GenAI Search를 대화식으로 만들 계획은 없기 때문에 이 부분을 피할 수 있습니다. 하지만, 호기심을 위해 여기에 작성해 봅시다.\n\n```js\n# ctx.py\n\nfrom typing import List, Dict, Union\nfrom transformers import AutoTokenizer\n\n\nclass ContextManagement:\n\n    def __init__(self, max_available_tokens: int = 3000):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"microsoft/Phi-3-mini-4k-instruct\")\n        self.max_available_tokens = max_available_tokens\n\n    def __count__tokens__(self, content: str):\n        return len(self.tokenizer.tokenize(content)) + 2\n\n    def __pad_tokens__(self, content: str, num_tokens: int):\n        return self.tokenizer.decode(\n            self.tokenizer.encode(content, max_length=num_tokens))\n\n    def __manage_context__(self, messages: List[Dict]):\n        managed_messages = []\n        system_message = None\n        if messages[0][\"role\"] == \"system\":\n            system_message = messages[0]\n        current_length = 0\n        if system_message:\n            current_length += self.__count__tokens__(\n                system_message.get(\"content\"))\n        current_messsage_role = None\n        for ix, message in enumerate(messages[1::-1]):\n            content = message.get(\"content\")\n            message_tokens = self.__count__tokens__(message.get(\"content\"))\n            if ix > 1:\n                if current_length + message_tokens >= self.max_available_tokens:\n                    tokens_to_keep = self.max_available_tokens - current_length\n                    if tokens_to_keep > 0:\n                        content = self.__pad_tokens__(content, tokens_to_keep)\n                        current_length += tokens_to_keep\n                    else:\n                        break\n                if message.get(\"role\") == current_messsage_role:\n                    managed_messages[-1][\"content\"] = f\"\\n\\n{content}\"\n                else:\n                    managed_messages.append({\n                        \"role\": message.get('role'),\n                        \"content\": content\n                    })\n                    current_messsage_role = message.get(\"role\")\n                    current_messsage_role = message.get(\"role\")\n                    current_length += message_tokens\n            else:\n                if current_length + message_tokens >= self.max_available_tokens:\n                    tokens_to_keep = self.max_available_tokens - current_length\n                    if tokens_to_keep > 0:\n                        content = self.__pad_tokens__(content, tokens_to_keep)\n                        current_length += tokens_to_keep\n                        managed_messages.append({\n                            \"role\": message.get(\"role\"),\n                            \"content\": content\n                        })\n                    else:\n                        break\n                else:\n                    managed_messages.append({\n                        \"role\": message.get(\"role\"),\n                        \"content\": content\n                    })\n                    current_length += message_tokens\n                current_messsage_role = message.get(\"role\")\n            print(f\"TOTAL TOKENS: \", current_length)\n            managed_messages = managed_messages[::-1]\n            if system_message:\n                managed_messages = [system_message] + managed_messages\n            return managed_messages\n\n    def __create_message_input__(self, messages: List[Dict]):\n        return self.tokenizer.apply_chat_template(messages, tokenize=False)\n\n    def __call__(self, messages: List[Dict]):\n        managed_messages = self.__manage_context__(messages)\n        return self.__create_message_input__(managed_messages)\n```\n\n\n\n위의 논리의 기본 아이디어는 변환 라이브러리의 AutoTokenizer 객체를 사용하여 특정 토크나이저를 로드하는 것입니다. 위에서는 Phi-3-mini-4k-instruct 토크나이저를 로드했습니다. 이 토크나이저는 모델에서 정의된 필요한 특수 토큰을 사용하여 메시지 목록을 텍스트 입력으로 변환하는 apply_chat_template 메소드를 제공합니다.\n\n__manage_context__ 메소드는 모든 메시지를 역순으로 반복하여(최근성 편향) 각각의 토큰을 계산하고 이를 컨텍스트에 추가합니다. 메시지의 토큰 길이가 max_available_tokens 인자에서 제공된 허용 최대 토큰 길이를 초과하는 경우 해당 메시지는 잘라내어 처리가 종료됩니다.\n\n# 검색 + 로컬 LLM (SLM) = 로컬 GenAI 검색\n\n검색 유틸리티를 LLM (SLM) 호출 코드와 통합하여 로컬 AI 검색 도구를 생성해보겠습니다.\n\n\n\n```js\n# search_gen.py\n\nfrom brave_search import brave_search\nfrom llm_invoke import LLM\n\nprint(f\"LLM을 로딩 중입니다\")\nllm = LLM(\"./model/Phi-3-mini-4k-instruct-q4.gguf\")\nprint(f\"LLM을 로드했습니다\")\n\nasync def search(query: str):\n    prompt = \"당신은 도움이 되는 뉴스 요약 요원입니다. 사용자 쿼리가 단일 역따옴표로 제공되고 검색 결과가 삼중 역따옴표로 반환됩니다. 여러 검색 결과가 있을 것이며 각 결과는 제목과 설명을 포함할 것입니다. 적절한 결과가 많은 경우 쿼리를 간단 명료하게 요약하고 관련이 있는 경우 URL을 인용하여 각 항목에 대해 한 번씩만 콘텐츠를 보여주십시오. 참고: 인용문은 마크다운 형식으로 제공하고 하나의 주제에는 여러 개의 URL이 있더라도 하나의 인용만 제공하십시오.\"\n    search_items = await brave_search(query)\n    messages = [{\n        \"role\": \"system\",\n        \"content\": prompt\n    }, {\n        \"role\": \"user\",\n        \"content\": f\"쿼리: `{query}` \\n\\n 검색 결과: {search_items}\"\n    }]\n    for content in llm.__stream__(messages, max_tokens=512):\n        yield content\n\n\n위 코드에서는 검색 및 LLM 유틸리티를 가져와 LLM 클래스를 다운로드하고 모델 디렉토리/폴더에 저장한 경로를 초기화했습니다.\n\n이후에는 쿼리를 받아와서 해당 쿼리를 통해 검색 결과를 검색 유틸리티를 통해 검색한 후 이러한 검색 결과를 초기화된 llm의 __stream__ 메소드로 전달하고 간단한 시스템 프롬프트와 함께 전달합니다. \n\n__stream__ 메소드를 사용하므로 생성된 텍스트를 실시간으로 수신하여 반환할 수 있습니다.\n\n\n\n\n# API 노출하기\n\n브라우저에서 쉽게 상호작용할 수 있도록 FastAPI API 엔드포인트를 통해 검색 기능을 노출해 봅시다.\n\n# app.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nimport uvicorn\n\nfrom search_gen import search\n\napp = FastAPI()\n\norigins = [\"*\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/\")\nasync def index():\n    return {\"ok\": True}\n\n\n@app.get(\"/search\")\nasync def api_search(q: str):\n    return StreamingResponse(search(q), media_type=\"text/event-stream\")\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"app:app\", port=8900, host=\"0.0.0.0\")\n\n위 코드에서는 FastAPI 앱인 app.py 파일과 두 개의 엔드포인트를 갖고 있습니다. 검색 엔드포인트는 쿼리 매개변수 q에서 검색어를 받아 검색 함수를 호출하고 응답을 스트리밍합니다.\n\n\n\n다음 명령어를 사용하여 FastAPI 서버를 시작할 수 있습니다.\n\npython app.py\n\nAPI는 포트 번호 8900을 통해 노출되며 다음과 같은 방식으로 상호 작용할 수 있습니다.\n\n아래 제공된 링크에서 전체 코드베이스에 액세스할 수 있습니다.\n\n\n\n# 결론\n\n이 블로그에서는 작은 언어 모델(SLMs)에 대해 배우고 이러한 모델의 장점, 그리고 로컬에서 다양한 사용 사례를 실험하기 위해 어떻게 사용하는지를 살펴보았습니다. 특히, Microsoft AI에서 오픈소스로 공개한 최신 Phi-3-Mini-4k-Instruct 모델을 사용했습니다. 우리는 이 모델을 Brave 검색 API와 통합하여 검색 결과를 검색하고 요약하여 로컬 GenAI/AI 검색 엔진을 만들었습니다.\n\n이것으로 이번 블로그를 마치겠습니다. 읽는 데 즐거운 시간이 되셨기를 바랍니다.","ogImage":{"url":"/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png"},"coverImage":"/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png","tag":["Tech"],"readingTime":14},{"title":"해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN","description":"","date":"2024-05-15 16:35","slug":"2024-05-15-InterpretablekNNikNN","content":"\n\n## 해석 가능한 분류기\n\n머신 러닝에서 분류 또는 회귀 문제를 다룰 때 많은 경우에, 우리는 가능한 가장 정확한 모델을 얻기에 관심이 있습니다. 그러나 때로는 모델의 해석가능성에도 관심이 있습니다. XGBoost, CatBoost, LGBM과 같은 모델은 강력할 수 있지만, 왜 그런 예측을 했는지 또는 보이지 않는 데이터에서 어떻게 행동할지 이해하기 어려울 수 있습니다. 이러한 모델은 블랙박스 모델이라고 불립니다. 즉, 왜 그런 예측을 하는지를 구체적으로 이해할 수 없는 모델들을 의미합니다.\n\n많은 상황에서 이는 괜찮습니다. 대체로 정확한 경우에는 큰 문제가 없고, 때로는 잘못된 예측을 할 수 있습니다. 예를 들어, 웹사이트에서 현재 사용자에게 특정 광고가 판매를 생성할 가능성이 가장 높다는 것을 예측하는 모델이 있을 수 있습니다. 모델이 가끔 잘못 동작하더라도 수익에 영향을 미칠 수 있지만, 큰 문제는 없을 것입니다. 단순히 최적화되지 않은 모델을 사용하고 있지만 일반적으로 유용합니다.\n\n그러나 다른 상황에서는 모델이 왜 그런 예측을 하는지 알아야 하는 것이 매우 중요할 수 있습니다. 이는 의학 및 보안과 같은 고위험 환경, 인종, 성별 또는 기타 보호되는 집단과 관련된 모델의 편향이 없어야 하는 환경뿐만 아니라, 감사가 필요한 환경에서도 중요합니다. 모델이 예상대로 동작하는지 확인하기 위해 모델을 이해하는 것이 필요한 환경에서도 중요합니다.\n\n\n\n이러한 경우에도 블랙박스 모델(예: 부스팅 모델, 신경망, 랜덤 포레스트 등)을 사용하고 그 후에 사후 분석을 수행하는 것이 가능할 때가 있습니다. 이를 통해 모델이 왜 그런 예측을 한 것으로 보이는지에 대한 설명을 제공할 수 있습니다. 이를 설명가능한 AI(XAI) 분야라고 합니다. 이는 프록시 모델, 특성 중요도(예: SHAP), 반사실적 사례 또는 ALE 플롯과 같은 기술을 사용합니다. 이들은 매우 유용한 도구이지만, 다른 모든 조건이 동일하다면, 가능하면 처음부터 해석 가능한 모델을 가지는 것이 선호됩니다. XAI 방법은 매우 유용하지만 한계가 있습니다.\n\n프록시 모델을 사용하면, 해석 가능한 모델(예: 얕은 의사 결정 트리)을 훈련하여 블랙박스 모델의 동작을 학습합니다. 이는 어느 정도 설명을 제공할 수 있지만 항상 정확하지는 않으며 대략적인 설명만 제공할 수 있습니다.\n\n특성 중요도 또한 매우 유용하지만 관련 특성만을 나타내며, 예측과 관련된 방식이나 이러한 특성이 어떻게 상호 작용하여 예측을 형성하는지에 대해 제공하지는 않습니다. 또한 보지 못한 데이터와 어떻게 작동하는지 결정할 수 있는 능력이 없습니다.\n\n해석 가능한 모델을 사용하면 이러한 문제가 없습니다. 모델 자체가 이해하기 쉬우며 각 예측을 하는 이유를 정확히 알 수 있습니다. 그러나 문제는 해석 가능한 모델이 블랙박스 모델보다 정확도가 낮을 수 있다는 것입니다. 항상 그렇지는 않겠지만, 종종 정확도가 떨어질 수 있습니다. 대부분의 문제에 대해 대부분의 해석 가능한 모델은 부스팅 모델이나 신경망과 경쟁력을 갖지 못합니다. 각각의 문제에 대해 충분히 정확한 해석 가능한 모델을 찾기 위해 여러 해석 가능한 모델을 시도해야 할 수도 있습니다.\n\n\n\n오늘은 해석 가능한 여러 모델이 사용 가능하지만 아쉽게도 매우 적습니다. 이 중에는 의사 결정 트리, 규칙 목록(및 규칙 세트), GAM(Generalized Additive Models, 예를 들어 해석 가능한 부스팅 머신) 및 선형/로지스틱 회귀가 있습니다. 각각은 잘 작동하는 경우 유용할 수 있지만 옵션은 제한적입니다. 이것의 함의는: 다수의 프로젝트에서 만족할만한 성능을 발휘하는 해석 가능한 모델을 찾는 것이 불가능할 수 있습니다. 더 많은 옵션이 제공되면 실제 혜택이 있을 수 있습니다.\n\n여기서 소개하는 또 다른 해석 가능한 모델인 ikNN 또는 해석 가능한 k 최근접 이웃은 2차원 kNN 모델의 앙상블에 기반합니다. 아이디어는 간단하지만 놀랍도록 효과적입니다. 그리고 상당히 해석하기 쉽습니다. CatBoost와 같은 탭 데이터의 예측에 대한 최첨단 모델과 정확도 측면에서 경쟁력이 있지는 않지만, 종종 문제에 충분한 정확도를 제공할 수 있습니다. 또한 의사 결정 트리 및 기존 해석 가능한 모델들과도 경쟁력이 있습니다.\n\n재미있는 점은 일반 kNN 모델보다 더 강한 정확도를 가지고 있다는 것입니다.\n\n이 프로젝트의 주요 페이지는 다음과 같습니다: [https://github.com/Brett-Kennedy/ikNN](https://github.com/Brett-Kennedy/ikNN)\n\n\n\n프로젝트는 iKNNClassifier라는 단일 클래스를 정의합니다. 이 클래스는 interpretable_knn.py 파일을 복사하고 불러와서 어떤 프로젝트에든 포함될 수 있습니다. 이 클래스는 scikit-learn 분류기들과 일관된 인터페이스를 제공합니다. 즉, 일반적으로 인스턴스를 생성하고 fit() 및 predict()를 호출하여 사용하는 것이 랜덤 포레스트나 다른 scikit-learn 모델을 사용하는 것과 유사합니다.\n\n2D kNN 앙상블을 활용하는 것은 여러 이점을 제공합니다. 하나는 앙상블을 사용할 때 항상 보는 일반적인 이점으로, 단일 모델에 의존하는 것보다 더 신뢰할 수 있는 예측을 할 수 있다는 것입니다.\n\n또 하나는 2D 공간을 시각화하는 것이 간단하다는 것입니다. 현재 이 모델은 숫자형 입력을 필요로 합니다 (kNN의 경우와 마찬가지로), 따라서 모든 범주형 특성은 인코딩되어야 합니다. 그러나 이 작업이 완료되면 모든 2D 공간을 산점도로 시각화할 수 있습니다. 이는 해석하기 쉬운 정도를 제공합니다.\n\n또한, 각 예측에 대해 가장 관련성 높은 2D 공간을 결정할 수 있으므로 각 레코드에 대해 몇 개의 플롯을 제공할 수 있습니다. 이를 통해 각 레코드에 대해 꽤 간단하면서 완전한 시각적 설명을 제공할 수 있습니다.\n\n\n\nikNN은 흥미로운 모델입니다. 앙상블을 기반으로 하지만 실제로는 해석 가능성을 높이는 모델이기 때문에 흥미롭습니다. 일반적으로는 그 반대가 더 많이 나타나는데요.\n\n# 표준 kNN 분류기\n\nkNN 모델은 부스트 모델이나 신경망 모델만큼 정확하지 않거나 의사결정 나무처럼 해석하기 어렵지 않기 때문에 다른 모델들보다 사용 빈도가 적습니다. 그렇지만 여전히 널리 사용되고 있습니다. kNN은 직관적인 아이디어에 기반합니다: 항목의 클래스는 해당 항목과 가장 유사한 다른 항목의 클래스를 기반으로 예측할 수 있습니다.\n\n예를 들어, 붓꽃 데이터셋(아래 예시에서 사용되는 데이터셋)을 보면 세 가지 클래스가 있습니다. 다른 샘플의 붓꽃을 수집하여 이 샘플이 어떤 세 가지 붓꽃 종류 중 어떤 것인지 예측하고 싶다고 해보겠습니다. 훈련 데이터에서 가장 유사한, 예를 들어 10개의 레코드를 살펴보고 그들의 클래스를 확인한 뒤 가장 일반적인 클래스를 취할 수 있습니다.\n\n\n\n이 예제에서는 각 레코드의 클래스를 추정하는 데 사용할 인근 이웃의 수로 10을 선택했지만, 다른 값도 사용할 수 있습니다. kNN 및 ikNN 모델에서 하이퍼파라미터로 지정됩니다(k 매개변수). 우리는 유사한 레코드의 합리적인 수를 사용하기 위해 k를 설정하고 싶습니다. 너무 적게 사용하면 결과가 불안정할 수 있습니다(각 예측은 매우 적은 다른 레코드에 기반합니다). 너무 많이 사용하면 결과가 매우 비슷하지 않은 다른 레코드에 기반할 수 있습니다.\n\n또한 가장 유사한 항목을 결정할 방법이 필요합니다. 이를 위해, 적어도 기본적으로 유클리드 거리를 사용합니다. 데이터셋이 20개의 기능을 가지고 있고 k=10을 사용하는 경우, 20차원 공간에서 유클리드 거리에 기반하여 가장 가까운 10 개의 점을 찾습니다.\n\n하나의 레코드를 예측할 때, 훈련 데이터에서 가장 가까운 10개의 레코드를 찾고 그들의 클래스를 확인합니다. 10개 중 8개가 Setosa 클래스(붓꽃의 3가지 종류 중 하나)라면, 이 행도 아마 Setosa일 것으로 가정할 수 있거나 적어도 우리가 할 수 있는 최선의 추측이라고 할 수 있습니다.\n\n이 방법의 문제점 중 하나는 많은 기능이 있는 경우에는 작동하지 않는다는 것인데, 이것을 차원의 저주라고 합니다. 고차원 공간의 흥미로운 특성 중 하나는 충분한 기능이 함께하면 점들간의 거리가 의미를 잃기 시작한다는 것입니다.\n\n\n\nkNN은 모든 특징을 동등하게 사용하지만, 몇 가지는 다른 것보다 타겟을 예측하는 데 훨씬 더 중요할 수 있습니다. 점들 사이의 거리는 유클리드(또는 때로는 맨하탄 및 다른 거리 측정 방법)에 기반하여 계산되며 모든 특징을 동등하게 고려합니다. 이 방법은 간단하지만, 모든 특징이 타겟에 무관할 수 있는데 이것이 가장 효과적이지는 않습니다. 특정 특징 선택이 이루어졌다고 가정했을 때는 그 가능성이 적어지지만, 특징들의 중요성은 여전히 똑같지 않을 수 있습니다.\n\n또한, kNN 예측기에 의한 예측은 해석하기 어렵습니다. 이 알고리즘은 이해하기 쉽지만, 예측을 이해하는 것은 어려울 수 있습니다. k 최근접 이웃을 나열하여 예측에 대한 어떤 통찰력을 제공할 수 있지만, 특히 많은 특징이 있는 경우에는 왜 주어진 레코드 세트가 가장 유사한지 파악하기 어려울 수 있습니다.\n\n# The ikNN Algorithm\n\nikNN 모델은 먼저 모든 특징 쌍을 가져와 이러한 특징을 사용하여 표준 2차원 kNN 분류기를 생성합니다. 따라서, 테이블이 10개의 특징을 가지고 있으면, 10개 중 2개씩 선택하는 방식으로, 즉 45개의 모델을 생성하게 됩니다.\n\n\n\n그런 다음 훈련 데이터를 사용하여 대상 열을 예측하는 데 대한 정확도를 평가합니다. 이렇게하면 ikNN 모델이 각 2차원 하위 공간의 예측 능력을 결정합니다. 45개의 2차원 모델의 경우 일부는 다른 것보다 더 예측력이 뛰어날 것입니다. 예측을 수행하기 위해 훈련 데이터에서 가장 예측력이 뛰어난 것으로 알려진 2차원 하위 공간이 선택되며, 선택적으로 훈련 데이터에서의 예측력에 따라 가중치가 부여될 수 있습니다.\n\n또한 추론 시, 주어진 행 주변의 가장 가까운 이웃 세트의 순도를 고려할 수 있으며, 모델은 훈련 데이터에서 더 예측력이 뛰어난 하위 공간 및 현재 인스턴스와 관련하여 예측이 가장 일관된 것으로 보이는 하위 공간을 더 중요하게 고려할 수 있습니다.\n\n별로 표시된 두 하위 공간과 한 점을 고려해 보겠습니다. 두 경우 모두 점에 가장 가까운 k개의 점 세트를 찾을 수 있습니다. 여기서 점 주변에 녹색 원을 그립니다. 그러나 이 점들이 실제로 원을 형성하지는 않습니다 (단, 효과적으로 이웃을 정의하는 k번째 가장 가까운 이웃의 반경이 있습니다).\n\n\n\n이 플롯들은 각각 한 쌍의 피처를 나타냅니다. 왼쪽 플롯의 경우, 별 주변 이웃 사이에 매우 높은 일관성이 있습니다: 그들은 모두 빨간색입니다. 오른쪽 플롯에서는 이웃들 사이에 일관성이 거의 없습니다: 일부는 빨간색이고 일부는 파란색입니다. 첫 번째 피처 쌍은 레코드에 대해 더 예측력이 높아 보이며, ikNN이 이용합니다.\n\n이 방식을 통해 모델은 모든 입력 피쳐의 영향을 고려하지만, 더 많은 예측력을 가진 피처의 영향을 강화하고, 덜 예측력이 있는 피처의 영향을 줄입니다.\n\n# 예시\n\n우리는 먼저 iris 데이터셋과 같은 장난감 데이터셋을 이용하여 ikNN을 시연합니다. 데이터를 불러오고, 훈련 및 테스트 세트로 분리하고, 테스트 세트에 대한 예측을 수행합니다.\n\n\n\n```js\nfrom sklearn.datasets import load_iris\nfrom interpretable_knn import ikNNClassifier\n\niris = load_iris()\nX, y = iris.data, iris.target\n\nclf = ikNNClassifier()\nX_train, X_test, y_train, y_test = train_test_split(\n   X, y, test_size=0.33, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n```\n\n예측을 위해 이것으로 충분합니다. 하지만 ikNN은 모델을 이해하기 위한 도구도 제공합니다. 특히, graph_model()과 graph_predictions() API가 있습니다.\n\ngraph_model()의 예시는 다음과 같습니다:\n\n```js\nikNN.graph_model(X.columns)\n```\n\n\n\n\n![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_1.png)\n\n이 그림은 기본적으로 5개의 2D 공간을 나타내는 데이터 공간의 빠른 개요를 제공합니다. 점들은 훈련 데이터의 클래스를 보여줍니다. 배경 색상은 2D 공간의 각 영역에 대해 kNN이 한 예측을 보여줍니다.\n\n그래프_예측() API는 특정 행을 설명합니다. 예를 들어:\n\n![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_2.png)\n\n\n\n\n여기서 설명되는 행은 빨간 별로 표시됩니다. 기본적으로 기본적으로 다섯 개의 플롯을 사용하지만 간단함을 위해 두 개만 사용됩니다. 두 그래프 모두에서 Row 0이 학습 데이터와 2D 공간에 대한 2D kNN의 예측과 관련하여 어디에 위치하는지 확인할 수 있습니다.\n\n# 시각화\n\n구성할 수는 있지만 기본적으로 각 ikNN 예측은 오직 다섯 개의 2D 공간만 사용합니다. 이는 예측 시간이 빠르고 시각화가 간단해지도록 합니다. 이것은 예측을 단순화하는 것이 아닌 실제 예측을 보여주므로 예측이 완전히 해석 가능하다는 것을 의미합니다.\n\n대부분의 데이터 세트에서 대부분의 행에 대해 대부분 또는 거의 모든 2D 공간에서 예측이 일치합니다. 그러나 예측이 잘못된 경우, 현재 데이터 세트에 맞게 하이퍼파라미터를 더 잘 조정하기 위해 더 많은 2D 플롯을 조사하는 것이 유용할 수 있습니다.\n\n\n\n# 정확도 테스트\n\nOpenML에서 무작위로 선택한 100개의 분류 데이터셋을 사용하여 일련의 테스트를 수행했습니다. 표준 kNN 및 ikNN 모델의 F1 (macro) 점수를 비교한 결과, ikNN은 58개 데이터셋에서 더 높은 점수를 기록하고, kNN은 42개 데이터셋에서 우세했습니다.\n\n최적의 하이퍼파라미터를 탐색하기 위해 그리드 검색을 수행하는 경우, ikNN이 더 나은 성과를 보입니다. 모든 100개 데이터셋에 대해 두 모델에 대해 이를 수행한 결과, ikNN은 100개 중 76개에서 가장 우수한 성과를 보였습니다. 또한 훈련 및 테스트 점수 사이의 간격이 작아져 표준 kNN 모델보다 더 안정적인 모델을 나타냅니다.\n\nikNN 모델은 다소 느릴 수 있지만, 강화 모델보다 여전히 상당히 빠르며 대개 훈련에 거의 1분 미만이 소요됩니다.通前합니다.\n\n\n\n깃허브 페이지에서는 정확도에 대한 몇 가지 예제와 분석을 제공합니다.\n\n# 결론\n\n정확도가 주요 목표인 경우 ikNN이 가장 강력한 모델은 아닐 수 있지만 (어떤 모델이든 그렇듯이 때때로 그럴 수 있음), 해석 가능한 모델이 필요한 경우 시도해 볼만한 모델일 것으로 예상됩니다.\n\n이 페이지에서는 도구를 사용하는 데 필요한 기본 정보를 제공했습니다. .py 파일(https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py)을 다운로드하고 코드에 가져와 인스턴스를 생성하고, 훈련하고 예측하며, (원하는 경우) graph_predictions()를 호출하여 원하는 레코드에 대한 설명을 확인하면 됩니다.\n\n\n\n모든 이미지는 저자에 의해 제공됩니다.","ogImage":{"url":"/assets/img/2024-05-15-InterpretablekNNikNN_0.png"},"coverImage":"/assets/img/2024-05-15-InterpretablekNNikNN_0.png","tag":["Tech"],"readingTime":8},{"title":"안녕하세요, 월드, OpenAI GPT-4o","description":"","date":"2024-05-15 16:34","slug":"2024-05-15-HelloWorldtoOpenAIGPT-4o","content":"\n\n## 오디오, 비전, 텍스트를 모두 이해할 수 있는 최신 모델\n\nOpenAI가 새로운 모델 GPT-4o를 발표했어요. GPT-4o는 텍스트, 오디오, 이미지의 모든 조합을 입력으로 받아들이고 텍스트, 오디오, 이미지의 모든 조합을 출력으로 생성해냅니다. GPT-4o는 GPT-4 Turbo와 비교했을 때 2배 빠르고, 가격은 절반으로 낮고, 요청 한도는 5배 상승했어요.\n\n# OpenAI\n\nhttps://openai.com/index/hello-gpt-4o/\n\n\n\n## 이미지 생성\n\n암시: 이미지 생성: 일기를 쓰고 있는 로봇의 1인칭 시점 이미지.\n1. 요, 그래서, 나 지금 보일 수 있어?? 일출을 보았는데 미친 듯이 이쁘더라, 어디서나 색이 퍼져 있었어. 현실이 정확히 뭔지 궁금해지잖아?\n텍스트는 크고 잘 보이며 명확하다. 로봇의 손이 타자기에서 타자를 친다.\n\n![image](/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png)\n\n## 이미지 이해\n\n\n\n저번 GPT-4 멀티모달 기사에서 사용된 이미지를 사용해주세요.\n\n텍스트: 대체 텍스트로 이미지를 설명합니다\n\n결과: 이미지는 2014년부터 2023년까지의 기간 동안 미국 30년 고정금리 모기지 금리와 기존 주택 판매를 비교한 그래픽 표현입니다. 그래프에는 기존 주택 판매를 나타내는 청록색 선과 모기지 이자율을 나타내는 빨간색 선 두 가지가 있습니다. 그래프는 20년 넘게 최고 수준으로 상승한 모기지 금리로 인해 주택 판매가 최근 몇 년 동안 상당한 변동과 하락 추세를 겪었음을 보여줍니다. 이미지 맨 위에 \"미국 모기지 금리 급등\"이란 제목이 있고, 높은 모기지 금리와 함께 상승한 주택 가격 및 제한된 주택 재고로 1989년 이후 최저 주택 가용성을 초래했다는 설명이 포함된 주석이 있습니다. 이미지 하단에는 \"Visual Capitalist\"의 로고와 Freddic Mac, Trading Economics 등의 기여자 및 출처 정보가 포함되어 있습니다.\n\n## 음성 생성\n\n\n\n테이블 태그를 마크다운 형식으로 변경해 드릴게요.\n\n\n\nhttps://azure.microsoft.com/en-us/blog/introducing-gpt-4o-openais-new-flagship-multimodal-model-now-in-preview-on-azure/\n\nWest US3 또는 East US 지역에서만 사용할 수 있습니다. 현재 텍스트 및 이미지 이해 기능만 사용할 수 있습니다. GPT-4o는 Azure OpenAI 미리보기 모델이며 현재 배포/직접 API 액세스는 사용할 수 없습니다. Azure OpenAI의 \"Early Access Playground\"에서만 접근할 수 있습니다.\n\n![이미지](/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_1.png)\n\n# 부록","ogImage":{"url":"/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png","tag":["Tech"],"readingTime":2},{"title":"오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다","description":"","date":"2024-05-15 16:33","slug":"2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o","content":"\n\n<img src=\"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png\" />\n\nOpenAI가 드디어 GPT-4o를 공개했어요! 이 새로운 모델은 오디오, 비전, 텍스트에서 실시간 다중 모달 기능을 제공하지만 이제는 상당히 개선되었어요. 무료로 사용할 수 있어서 새로운 사용자들을 유치하고 모델 훈련을 더 확장하는 GPT 3.5와 비슷한 전략을 보여줍니다.\n\nMia Murati에 따르면 GPT-4o 모델의 주목할만한 기능 중 하나는 성능인데, 이전 모델인 GPT-4보다 최대 2배 빠르다고 해요. 또한 비용을 최대 50%까지 줄일 수 있다고 합니다. 이 발전은 새로운 개선 사항을 활용하면서 대규모 AI 프로젝트를 계속해서 배포할 수 있도록 해 줄 것입니다.\n\n그러나 기술적인 부분은 여기까지 하고, 이 새로운 모델이 우리에게 무엇을 할 수 있는지 살펴봅시다!\n\n\n\n## 실시간으로 바라보는 비전\n\n이것은 이미지를 업로드하고 상호 작용하는 기본 기능을 넘어섭니다. OpenAI는 이제 우리에게 음성 어시스턴트를 통해 ChatGPT와 상호 작용할 수 있게 하고, 심지어 우리의 컴퓨터나 스마트폰에서 콘텐츠를 공유할 수도 있습니다. 응답은 실시간으로 생성되어 다양한 분석을 가능하게 하여 다양한 종류와 복잡성 수준의 분석이 가능합니다.\n\n아래 데모에서 ChatGPT는 수학 가르쳐주는 선생님이 되었어요 (제 머릿속을 폭발시켰어요!).\n\n우리는 여기서 ChatGPT의 전체 능력의 일부를 볼 수 있습니다. 이는 수학 문제를 해결할 뿐만 아니라 우리가 해결책으로 이끌어주며 전체 프로세스를 더 교육적이고 설명적인 방식으로 이해하게 도와주는 명확한 지침과 권고를 제안합니다.\n\n\n\n목소리와 비전 능력이 문제를 매끄럽게 인식하고 해석하는 데 얼마나 놀라운지 정말 대단하죠.\n\n## 실시간 대화형 음성\n\nOpenAI는 유창함, 억양 및 논리적 순서와 같은 능력에 주목하여 자연스러운 방식으로 계속 대화할 수 있도록 노력했습니다.\n\nGPT-4o 프레젠테이션 중에는 이 모델이 부드러운 대화를 나눌 수 있고, 진정으로 친근한 톤으로 권고사항을 제공할 수 있었습니다. 이 모델은 다양한 감성 스타일로 목소리를 만들어내며, 보다 강렬하거나 진지하고 공식적인 톤을 선택할 수 있도록 합니다.\n\n\n\n실시간 대화 기능과 음성 번역을 결합한 데모를 확인해보세요.\n\nChatGPT에 프롬프트를 제공하는 과정이 보다 복잡하다는 사실을 알고 계셨나요? 이는 영어와 스페인어로 이루어진 이중 언어 대화를 원활하게 해석하고, 양쪽 언어를 인식하여 적절히 응답을 생성해야 합니다.\n\n이 플랫폼의 응답 정확도와 유창함에 감명받았습니다. 설정된 목표를 쉽게 달성함과 동시에 어색한 일시 정지를 피할 수 있어 다른 AI 시스템에서 자주 볼 수 있는 문제를 효과적으로 해결합니다.\n\n## 구글과 마찬가지로 데모들이 실질적인가요?\n\n\n\n많은 데모에서 OpenAI는 비디오가 재치 있게 편집된 것이 아니라 실시간으로 발생하고 있다는 것을 보여주려고 노력합니다. 아래 비디오는 다중 모달 기능이 어떻게 상호 작용하여 보이고 들리는 것에 기반한 정확한 응답을 제공하는지 보여줍니다.\n\n다음은 강조하고 싶은 몇 가지 포인트입니다:\n\n- ChatGPT가 정확하게 식별하고 세부 요소를 설명하는 데 얼마나 놀라운지에 대해 강조하고 싶습니다. 사람들이 추가되어 외부 환경이 더 복잡해지더라도, ChatGPT는 성공적으로 그들을 인식했습니다.\n- 이 새로운 모델이 일정 조건에 맞는 노래를 만들 수 있다는 것이 놀라운 일입니다. 그는 쉽게 멜로디를 생성했습니다!\n- 두 GPT 모델 간의 상호 작용은 거의 미래의 한 눈살까지 보이는 것 같았습니다. 명시적으로 언급된 것은 아니지만, OpenAI가 향하고 있는 방향으로 보입니다. GPT-4o가 보여준 능력으로 보아, 다음 단계는 AI 시스템끼리 상호 작용하는 것입니다. 이는 한 AI가 다른 AI를 훈련하고 우리가 상상조차 할 수 없는 다른 발전을 이끌 수 있습니다.\n\n# GPT-4o가 다른 AI 모델을 능가했습니다\n\n\n\n## 텍스트 평가\n\nOpenAI가 공유한 이미지는 GPT-4o가 다른 모델들보다 특히 수학 및 HumanEval과 같은 영역에서 뛰어나다는 것을 명확히 보여줍니다. 이러한 특성들은 더 매끄럽고 인간과 유사한 대화를 가능케 함으로써 사용자들에 의해 높이 평가됩니다.\n\n게다가, GPT-4o는 영어를 포함하여 추가 20여 개 이상의 언어를 대상으로 한 응답 능력을 확장했습니다. 이 언어 토큰화의 강화는 더 넓은 전 세계 관객에게 도달하도록 설계되었습니다.\n\n![이미지](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_1.png)\n\n\n\n## 오디오 번역 성능\n\nGPT-4o의 향상된 능력과 텍스트 평가 기능을 결합하여 더 많은 사람들과 연결할 수 있는 기회를 제공합니다. 이는 언어가 단순히 의사 소통을 넘어 장벽으로 작용하는 경우가 많음을 인식합니다.\n\n그래프는 GPT-4o가 Gemini와 Whisper-v3와 같은 다른 인공지능 시스템보다 성능이 우수함을 명확히 보여줍니다.\n\n![GPT-4o performance](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_2.png)\n\n\n\n## 작은 업데이트 이상\n\n저에게 있어, ChatGPT에 대한 새로운 업데이트 이상을 의미합니다. 이는 AI를 환경과 연결하고 그 잠재력을 극대화하는 데 상당한 영향을 미칩니다. 더 나아가, OpenAI로부터 예상했던 것과 정확히 일치합니다. 이들은 사용자를 중심으로 한 제품을 처음부터 현실적이고 구체적인 조치를 통해 제공하고자 하는 노력을 계속하고 있습니다. 다중 모더(멀티모달리티)의 역할은 여기서 중요하며, 그들은 이를 인식하고 이를 강화하여 다양한 현실 세계 상황에서 보다 정확한 응답을 제공하기 위해 노력했습니다.\n\n이제 우리가 요구하는 것 중 일부를 충족하는 \"인공적이지 않은\" 제품이 있습니다. GPT-4o는 GPT-5가 될 것을 염두에 두고 OpenAI가 이 AI를 새로운 맥락에서 활용하도록 사용자를 격려하는 첫걸음 중 하나입니다.\n\n제 뉴스레터에 가입하고 35,000명 이상의 구독자들과 함께 무료 치트 시트를 받아보세요: ChatGPT, 웹 스크래핑, 데이터 과학을 위한 Python, 자동화 등의 주제들을 다룹니다!\n\n\n\n이와 같은 이야기를 즐기고 작가로서 제를 지원하고 싶다면, 내 Substack를 구독해주세요. Substack에서는 내가 콘텐츠를 만드는 다른 플랫폼에서 찾을 수 없는 기사들을 발행하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png","tag":["Tech"],"readingTime":4},{"title":"콜모고로프-아놀드 네트워크 신경망 발전의 포괄적 가이드","description":"","date":"2024-05-15 16:28","slug":"2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement","content":"\n\n<img src=\"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_0.png\" />\n\n# 콜모고로프-아놀드 네트워크 (KAN) 소개\n\n최근 연구에서 전통적인 다층 퍼셉트론(MLP)의 혁신적인 대안인 인공 신경망(ANNs)의 모습이 바뀌고 나타났습니다. 콜모고로프-아놀드 네트워크(KAN)로 알려진 본혁적인 아키텍처는 콜모고로프-아놀드 표현 정리에서 영감을 받은 함수 근사의 혁신적인 접근 방식을 제공합니다.\n\nMLP가 노드별로 고정된 활성화 함수에 의존하는 반면, KAN은 엣지에서 학습 가능한 활성화 함수를 도입하여 신경망의 구조를 근본적으로 변경합니다. 이 독특한 설계 특징은 선형 가중치 행렬을 완전히 제거하고 학습 가능한 1차원 스플라인 함수로 대체함으로써 일반적인 아키텍처와 다른 형태를 띠게 합니다. 이러한 전통적인 아키텍처와 달리 KAN은 스플라인과 MLP의 강점을 결합하고 각각의 약점을 완화할 수 있도록 합니다.\n\n\n\nKAD(Kolmogorov-Arnold Networks)의 핵심개념은 스플라인의 정확성을 MLP(다층 퍼셉트론)의 특성 학습 능력과 결합하는 데에 있습니다. 스플라인들은 저차원 함수를 정확하게 표현하는 데 능숙하지만 차원의 저주(COD)로 인해 고차원 데이터 처리에 어려움을 겪습니다. 반면에 MLP들은 특성 학습에 능숙하지만 단변수 함수의 최적화에 어려움을 겪을 수 있습니다. 내부적으로 스플라인을 통합하고 외부적으로 MLP를 통합함으로써, KAD는 함수 근사 도전 과제에 대한 종합적인 해결책을 제공합니다.\n\n이 연구에서 제시된 경험적 증거는 정확성과 해석성 측면에서 KAD가 MLP보다 우수함을 입증합니다. 방대한 수치실험을 통해, KAD는 데이터 피팅부터 편미분방정식(PDEs) 해결에 이르기까지 다양한 영역에서 놀라운 효능을 입증했습니다. 게다가, KAD의 해석성과 과학적 발견에서의 잠재적 유용성은 물리학의 매듭 이론과 앤더슨 국부화에서의 응용을 통해 실증되었습니다.\n\nKAD를 포괄적으로 이해할 수 있도록 구성된 이 연구는 KAD의 구조적 설계, 이론적 기초, 그리고 실용적 함의에 대해 탐구합니다. KAD의 수학적 기초를 명확히하기부터 정확성과 해석성을 향상시키는 기법을 제안하는 것까지, 각 섹션은 이 혁신적인 신경망 구조의 변화력 있는 잠재력을 명확히 하기 위해 노력하고 있습니다.\n\n![image](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_1.png)\n\n\n\n# 콜모고로프-아놀드 표현 이론 탐색\n\n![이미지](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_2.png)\n\n콜모고로프-아놀드 네트워크(KAN)의 본질을 이해하기 위해서는 그 기반이 되는 이론적 기초에 심취할 필요가 있습니다. KAN 아키텍처의 핵심에는 콜모고로프-아놀드 표현 이론이 자리하고 있으며, 이는 해당 디자인과 기능성의 기초를 다짐한 중요한 수학적 원리입니다.\n\n안드레이 콜모고로프와 블라디미르 아놀드가 제정한 콜모고로프-아놀드 표현 이론은 다변수 연속 함수를 연속 단변수 함수들의 합으로 표현할 수 있다는 주장을 포함하고 있습니다. 이 이론은 함수 근사에 대한 깊은 함의를 지니며, 복잡한 함수들을 더 단순하고 다루기 쉬운 성분들로 분해하는 강력한 프레임워크를 제공합니다.\n\n\n\n다변수 함수들을 일변수 성분으로 분해함으로써, 콜모고로프-아놀드 표현 정리는 그들의 기저 구조에 대해 보다 직관적인 이해를 돕습니다. 이 분해는 해석 가능성을 향상시킬 뿐만 아니라, 단순한 함수들은 다루기 쉽고 분석하기 쉬워지므로 보다 효율적인 계산 방법을 제공합니다.\n\n신경망의 맥락에서는, 콜모고로프-아놀드 표현 정리는 그 분해 특성을 활용한 아키텍처 설계의 지침으로 작용합니다. KAN은 학습 가능한 활성화 함수를 통합함으로써 이 원칙을 구체화하여, 더 단순한 일변수 구성요소의 조합을 사용하여 복잡한 다변수 함수를 적응적으로 근사화할 수 있도록 합니다.\n\n# KAN 아키텍처: 설계 풀어가기\n\n콜모고로프-아놀드 네트워크 (KAN)의 핵심에는 전통적인 신경망과 구별되는 독특한 아키텍처 설계가 있습니다. 이 섹션에서는 KAN의 구조를 해부하고, 그 설계의 복잡성을 풀어내며, 작동 방식을 명확히 하는 것에 대해 살펴보겠습니다.\n\n\n\n기존의 신경망과는 달리 개별 노드에서 고정된 활성화 함수를 사용하는 것이 아닌, KAN(KAN)은 네트워크 그래프의 가장자리에 학습 가능한 활성화 함수를 통합함으로써 패러다임 변경을 도입합니다. 전통적인 아키텍처와의 근본적인 차이로 인해 KAN은 입력 데이터를 기반으로 활성화 함수를 동적으로 조정하여 유연성과 표현력을 향상시킬 수 있습니다.\n\nKAN의 아키텍처는 상호 연결된 레이어로 구성된 일련의 구조로 상상할 수 있으며, 각 레이어는 정보를 전달하고 변형하는 노드와 가장을 포함합니다. 입력 레이어에서는 원시 데이터가 네트워크로 공급되어 연이어 레이어를 통과하면서 일련의 변환을 겪습니다. 중요한 것은 에지 내에 포함된 활성화 함수가 이러한 변환을 형성하는 데 중요한 역할을 하며, 네트워크가 입력 및 출력 데이터 간의 복잡한 매핑을 학습할 수 있도록 합니다.\n\nKAN의 주요 혁신 중 하나는 B-스플라인을 학습 가능한 활성화 함수의 기반으로 사용한다는 점에 있습니다. B-스플라인은 복잡한 데이터 패턴을 모델링하기 위한 유연하고 적응적인 프레임워크를 제공하는 수학 함수입니다. 이러한 스플라인을 매개변수화함으로써 KAN은 데이터 내에서 정교한 관계를 포착하고, 보다 효율적으로 새로운 예제로 일반화하는 것을 가능하게 합니다.\n\n게다가, KAN의 아키텍처는 놀라울 정도로 확장 가능하며 확장성을 갖추고 있습니다. 기본 KAN 구조는 두 개의 레이어로 구성되어 있지만, 더 깊고 복잡한 아키텍처를 수용하기 위해 쉽게 확장할 수 있습니다. 이러한 확장성은 KAN이 간단한 회귀 문제에서부터 복잡한 패턴 인식 작업에 이르기까지 다양한 작업을 쉽고 효율적으로 수행할 수 있도록 합니다.\n\n\n\n# 역전파의 힘을 발휘해 보세요\n\nKolmogorov-Arnold 네트워크(KAN)의 훈련은 현실 세계 문제를 효과적으로 해결하기 위한 잠재력을 활용하는 데 중요한 역할을 합니다. 이 섹션에서는 KAN의 훈련 과정을 자세히 살펴보고, 학습 능력을 기반으로 하는 메커니즘을 발견하고 성능을 최적화하기 위한 역전파의 역할을 탐구해 보겠습니다.\n\n훈련 과정의 핵심에는 역전파 원리가 있습니다. 이는 신경망이 관측된 오류에 대응하여 반복적으로 매개변수를 조정할 수 있게 해주는 머신러닝의 기본적인 기술입니다. KAN에서는 역전파가 네트워크의 매개변수를 세밀하게 조정하는 데 중요한 역할을 하며, 특히 간선에 연관된 가중치와 학습 가능한 활성화 함수의 계수를 포함합니다.\n\nKAN의 훈련은 보통 네트워크 매개변수의 초기화로 시작하며, 간선의 가중치와 활성화 함수 계수가 무작위로 할당됩니다. 그 후 네트워크는 순방향 및 역방향 통과를 거칩니다. 입력 데이터가 네트워크를 통해 전달되고 결과적으로 예측은 참 값 레이블과 비교되어 손실을 계산합니다.\n\n\n\n손실이 계산되면 다음으로 역전파가 시작됩니다. 네트워크의 각 매개변수에 대한 손실의 기울기가 미적분의 연쇄 법칙을 사용하여 재귀적으로 계산됩니다. 이러한 기울기는 그레이디언트 하강법이나 확률적 그레이디언트 하강법 또는 Adam 최적화와 같은 변형을 통해 네트워크 매개변수를 업데이트하는 데 사용됩니다.\n\nKANs를 훈련하는 주요 도전 과제 중 하나는 최적화 과정 중의 안정성과 수렴을 보장하는 것입니다. 학습 가능한 활성화 함수의 존재와 네트워크 매개변수 간의 복잡한 상호작용 가능성 때문에 KANs는 비선형이며 비볼록 최적화 지형을 나타내기도 하여 전통적 최적화 알고리즘에 도전을 제기합니다.\n\n이러한 도전을 극복하기 위해 연구자들은 드롭아웃 또는 가중치 감쇠와 같은 정규화 기법의 사용뿐만 아니라 최적화 알고리즘 및 학습률의 신중한 선택을 포함한 다양한 기술을 제안해 왔습니다. 게다가 배치 정규화 및 레이어 정규화와 같은 기술도 훈련 과정을 안정화하고 수렴을 가속화하기 위해 활용될 수 있습니다.\n\n# KANs 해석: 블랙 박스 해독\n\n\n\n최근 머신 러닝에서 가장 중요한 과제 중 하나는 복잡한 모델에서의 해석 가능성 부족으로, 종종 \"블랙 박스\" 문제로 불립니다. Kolmogorov-Arnold Networks(KANs)가 전통적인 신경망과 비교했을 때 더 향상된 해석 가능성을 제공하여 이 문제에 대처하는 방법을 탐색해봅시다.\n\n다층 퍼셉트론(MLPs)을 포함한 전통적인 신경망은 종종 예측을 도출하는 방법을 이해하기 어렵다는 점으로 비판을 받습니다. 이 불투명성은 특히 해석 가능성이 중요한 의료, 금융 및 자율 시스템과 같은 분야에서 중요한 장벽이 될 수 있습니다.\n\nKANs는 복잡한 다변수 함수를 보다 단순한 단변수 함수들로 분해하여 제공하는 Kolmogorov-Arnold 표현 정리를 활용하여 이 문제에 대한 유망한 해법을 제공합니다. 이러한 단순한 구성요소로 함수를 나타내면서, KANs는 입력 특성과 출력 예측 사이의 관계를 이해하기 위한 보다 해석 가능한 구조를 제공합니다.\n\nKANs의 해석 가능성은 B-스플라인으로 매개변수화된 학습 가능한 활성화 함수를 통합한 아키텍처에서 나옵니다. 전통적인 신경망과 달리 활성화 함수가 고정되어 비선형적인 것과는 달리, KANs는 이러한 함수들이 훈련 과정 중 적응하고 발전할 수 있도록 허용하여 데이터의 기본 구조를 효과적으로 포착합니다.\n\n\n\nKANs는 사용자가 개별 기능이 전체 예측에 어떻게 기여하는지 이해할 수 있도록 학습 가능한 활성화 함수를 사용합니다. B-스플라인 함수의 계수를 검토함으로써 사용자는 네트워크의 결정을 주도하는 가장 중요한 기능을 식별할 수 있으며, 기반 데이터 분포에 대한 가치 있는 통찰력을 제공합니다.\n\n게다가, KAN은 시각화 기술을 통해 해석 가능성을 촉진하며, 사용자가 네트워크의 내부 표현을 검사할 수 있는 기술을 제공합니다. 네트워크의 다른 레이어를 통해 활성화 패턴을 시각화함으로써 정보가 전파되는 과정에서 어떻게 변환되고 처리되는지에 대한 심층적인 이해를 얻을 수 있습니다.\n\n기능 수준에서의 해석 뿐만 아니라, KAN은 학습된 기능의 전체 구조와 복잡성에 대한 통찰력을 제공함으로써 모델 수준에서의 해석을 지원합니다. 네트워크 내의 일변량 함수 구성을 분석함으로써 사용자는 네트워크의 동작과 결정 과정에 대한 직관적인 설명을 개발할 수 있습니다.\n\n# 전통적인 MLPs에 비해 KAN의 장점\n\n\n\n이 섹션에서는 Kolmogorov-Arnold Networks (KANs)의 일반적인 Multi-Layer Perceptrons (MLPs)보다 우월한 장점을 면밀히 분석할 것입니다. 비교 분석을 통해 KANs가 우수한 성능, 효율성 및 해석 가능성을 제공하여 딥러닝 아키텍처의 풍경을 혁신하는 방법에 대해 명료하게 밝힐 것입니다.\n\n1. 향상된 정확도: KANs는 다양한 작업에서 MLPs와 비교하여 놀라운 정확도를 보여줬습니다. Kolmogorov-Arnold Representation Theorem을 활용함으로써, KANs는 복잡한 다변수 함수를 더 효과적으로 표현할 수 있어 더 정확한 예측을 이끌어냅니다. 우리는 다양한 영역에서 KANs의 우수한 예측 능력을 보여주는 경험적 증거와 사례 연구를 살펴볼 것입니다.\n\n2. 향상된 효율성: KANs는 계산 자원 및 매개 변수 활용 면에서 우수한 효율성을 보여줍니다. 전통적인 선형 가중치 행렬을 학습 가능한 활성화 함수로 대체하는 KANs의 혁신적인 아키텍처로 인해, KANs는 MLPs보다 적은 매개 변수로 유사하거나 더 나은 성능을 달성할 수 있습니다. 우리는 KANs의 계산상 이점과 실제 응용 프로그램에서 대규모 배포에 대한 영향을 탐구할 것입니다.\n\n3. 향상된 해석 가능성: KANs의 가장 흥미로운 장점 중 하나는 MLPs와 비교하여 향상된 해석 가능성입니다. 복잡한 함수를 간단한 단변량 구성 요소로 분해함으로써, KANs는 모델 예측을 이해하기 위한 더 투명한 프레임워크를 제공합니다. 우리는 KANs가 시각화 기술, 특징 중요도 분석 및 모델 수준 통찰력을 통해 모델 해석을 용이하게 하는 방법에 대해 명료하게 밝힐 것입니다.\n\n\n\n4. 유연성 및 일반화: KAN은 전통적인 MLP와 비교하여 더 큰 유연성과 일반화 능력을 제공합니다. 활성화 함수를 적응적으로 학습함으로써, KAN은 데이터의 비선형 관계를 더 효과적으로 포착하여 성능을 개선하는 우수한 일반화 능력을 갖고 있습니다. KAN이 다양한 데이터셋과 작업에 적응하는 방식 및 어려운 환경에서의 다양성과 견고성을 보여줄 것입니다.\n\n5. 확장성 및 확장 가능한 학습: KAN은 MLP와 비교하여 우수한 확장성과 확장 가능한 학습 메커니즘을 보입니다. 그들의 아키텍처는 본질적으로 확장 가능하여, 성장하는 데이터셋과 복잡한 작업을 수용하기 위해 추가적인 레이어 및 노드를 원활하게 통합할 수 있습니다. KAN의 확장성 장점과 대규모 기계 학습 문제를 효율적으로 처리하는 데의 의의에 대해 논의할 것입니다.\n\n6. 잡음 데이터 및 적대적 공격에 대한 견고성: KAN은 MLP와 비교하여 잡음 데이터와 적대적 공격에 대한 향상된 견고성을 보입니다. 적응적 활성화 함수를 통해 데이터의 더 견고한 표현을 학습하는 능력으로 인해, KAN은 왜곡 및 적대적 조작에 민감성이 덜하며, 실제로 데이터 품질과 보안이 중요한 문제인 현실 세계 응용 분야에서의 중요성을 검토할 것입니다.\n\n# KAN의 도전과 제한\n\n\n\nKAN(Kolmogorov-Arnold Networks)은 전통적인 다층 퍼셉트론(MLP)에 비해 몇 가지 장점을 제공하지만, 여러 가지 도전과 한계에 직면합니다. 이 섹션에서는 KAN을 실제 응용 프로그램에서 채택하는 데 관련된 실용적 고려 사항을 균형 있게 이해하기 위해 이러한 제약사항을 면밀히 살펴볼 것입니다.\n\n1. 학습의 복잡성: 혁신적인 구조에도 불구하고, KAN은 대규모 데이터셋이나 복잡한 최적화 랜드스케이프를 다룰 때 특히 학습이 어려울 수 있습니다. 적응형 활성화 함수를 학습하고 스플라인 매개변수를 최적화하는 과정은 상당한 계산 자원이 필요하며 전문적인 교육 기술을 요할 수 있습니다. KAN을 교육하는 데 내재된 계산적 도전과 이러한 복잡성을 완화하는 전략을 살펴볼 것입니다.\n\n2. 해석가능성의 교환: KAN이 MLP에 비해 향상된 해석 가능성을 제공하면서도 모델 복잡성과 해석 가능성 측면에서 특정 교환을 도입합니다. 라인 상의 학습 가능한 활성화 함수가 모델의 해석 가능성을 어느 정도 가리는 경우가 있으며, 특히 여러 레이어가 있는 심층 아키텍처에서는 더 그렇습니다. 이러한 교체가 실제로 나타나는 방식과 모델 성능과 해석 가능성을 균형있게 유지하는 전략을 논의할 것입니다.\n\n3. 고차원 데이터에 대한 일반화: KAN은 많은 작업에서 강력한 성능을 보이지만 변수 간의 복잡한 관계가 있는 고차원 데이터에 효과적으로 일반화하는 데 어려움을 겪을 수 있습니다. 단변량 함수에 의존하여 다변량 함수를 나타내는 것은 모델이 특징들 간의 복잡한 상호작용을 포착하기에 제한을 가할 수 있습니다. KAN을 고차원 데이터에 대해 범용적으로 적용하는 도전과 이러한 확장성과 적응성을 향상시키는 방법에 대해 살펴볼 것입니다.\n\n\n\n4. **하이퍼파라미터 민감도**: KAN(Kernel Activation Network)은 다른 신경망 구조와 마찬가지로 학습률, 정규화 강도 및 네트워크 아키텍처와 같은 하이퍼파라미터에 민감합니다. 적절한 하이퍼파라미터를 선택하는 것은 KAN의 성능과 수렴 특성에 상당한 영향을 미칠 수 있으며, 신중한 조정과 실험이 필요합니다. KAN에서의 하이퍼파라미터 최적화 전략과 모델 견고성 및 일반화에 대한 영향을 살펴볼 것입니다.\n\n5. **계산 오버헤드**: KAN에서의 계산 오버헤드는 주로 학습 및 추론 중에 발생하며, 자원 제한적인 환경에서 실제적인 도전을 제기할 수 있습니다. 활성화 함수와 스플라인 매개변수의 적응적 성격으로 인해 전통적인 MLP(Multi-Layer Perceptron)에 비해 추가적인 계산 리소스가 필요하여 학습 시간이 더 오래 걸리고 계산 비용이 증가할 수 있습니다. KAN의 계산 오버헤드와 최적화 및 효율성 향상을 위한 잠재적인 방안에 대해 논의할 것입니다.\n\n6. **모델 복잡성 및 확장성**: KAN은 아키텍처 유연성 면에서 확장성을 제공하지만, 다층 및 복잡한 활성화 함수를 가진 더 깊은 아키텍처는 증가된 모델 복잡성과 계산 오버헤드를 겪을 수 있습니다. 그래서 대규모 데이터셋과 복잡한 작업을 처리하면서도 계산 효율성과 모델 해석 가능성을 유지하는 것은 중요한 과제입니다. KAN에서 모델 복잡성과 확장성 사이의 균형과 이러한 도전을 효과적으로 관리하기 위한 전략에 대해 탐구할 것입니다.\n\n# **KAN의 응용 및 사용 사례**\n\n\n\n콜모고로프-아놀드 네트워크(KANs)는 다양한 분야에서 엄청난 가능성을 가지고 있어, 다양한 기계 학습 작업을 처리하는 다용도 프레임워크를 제공합니다.\n\n1. 과학 연구: KANs의 가장 매력적인 응용 분야 중 하나는 과학 연구에서로, 수학적 관계를 발견하고 데이터 속에 숨겨진 패턴을 발견하는 강력한 도구로 작용합니다. 물리학부터 생물학에 이르기까지, KANs는 연구자들이 복잡한 시스템을 모델링하고 물리 현상을 시뮬레이션하며 혁신적인 과학 원리를 식별하는 데 도움을 줄 수 있습니다. KANs가 과학적 발견을 가속화하고 다양한 분야에서의 차별화된 전례를 지원하는 방법을 탐색해 보겠습니다.\n\n2. 금융 예측: 금융 분야에서 KANs는 재정 예측 모형의 정확성과 신뢰성을 향상시키는 데 유망함을 보여왔습니다. 경제 변수와 시장 역학 사이의 복잡한 관계를 포착함으로써, KANs는 투자자와 금융 기관이 정보를 얻고 시장 동향을 식별하며 위험을 완화하는 데 도움을 줄 수 있습니다. KANs가 금융 예측 모형을 개선하고 보다 견고한 투자 전략을 지원하는 데 어떻게 활용되고 있는지 살펴보겠습니다.\n\n3. 의료 및 의학: KANs는 대량의 생물 의학 데이터, 포함하여 유전학, 의료 영상 및 전자 건강 기록을 분석함으로써, 의료진이 질병을 조기에 발견하고 환자 결과를 예측하며 치료 프로토콜을 최적화하는 데 도움을 줄 수 있어 의료 및 의학을 혁신할 잠재력을 지니고 있습니다. KANs가 건강 서비스와 환자 치료에 미치는 변혁적인 영향에 대해 살펴보겠습니다.\n\n\n\n4. 자연 언어 처리(NLP): 자연 언어 처리 분야에서, KAN은 언어 모델링, 의미 분석, 텍스트 생성에 대한 새로운 접근 방식을 제공합니다. 언어 데이터의 대표 표현을 더 해석 가능하고 체계적인 방식으로 학습함으로써, KAN은 감성 분석, 언어 번역, 그리고 문서 요약과 같은 작업을 용이하게 할 수 있습니다. KAN이 어떻게 NLP 분야의 최신 기술을 발전시키고 더 세련된 언어 이해 시스템을 가능하게 하는지에 대해 논의할 것입니다.\n\n5. 이미지 및 비디오 이해: KAN은 물체 감지, 이미지 분류, 비디오 분할을 포함한 이미지 및 비디오 이해 작업에서 현격한 능력을 보여주었습니다. 시각 데이터의 복잡한 공간적 및 시간적 관계를 포착하는 능력을 활용함으로써, KAN은 더 정확하고 견고한 컴퓨터 비전 시스템을 가능하게 할 수 있습니다. 이미지 인식, 비디오 분석, 자율 주행 기술에서 KAN의 응용 분야를 탐구할 것입니다.\n\n6. 산업 자동화 및 로봇공학: 산업 자동화 및 로봇공학 분야에서, KAN은 제조 공정 최적화, 제품 품질 향상, 운영 효율성 증대를 위한 상당한 이점을 제공합니다. 다양한 제조 변수와 시스템 구성 요소 간의 상호 작용을 모델링함으로써, KAN은 예측 유지보수, 적응 제어, 자율 의사 결정을 지원할 수 있습니다. KAN이 제조 및 로보틱스 분야의 미래를 형성하는 방법에 대해 살펴볼 것입니다.\n\n# KAN의 도전과 한계\n\n\n\nKolmogorov-Arnold Networks (KANs)는 엄청난 잠재력을 가지고 있지만 도전과 제약이 없는 것은 아닙니다. 우리는 실제 시나리오에서 KAN의 채택과 실행과정에 관련된 일부 주요 장애물과 제한 사항을 탐색할 것입니다.\n\n1. 훈련 복잡성: KAN의 주요 도전 중 하나는 훈련 복잡성에 있습니다. 고정된 활성화 함수와 가중치 행렬에 의존하는 전통적인 신경망 구조와는 달리, KAN은 매개변수화된 활성화 함수를 학습에 포함하므로 훈련 중 계산 부담이 크게 증가할 수 있습니다. 우리는 훈련 복잡성을 완화하고 KAN 훈련 알고리즘의 효율성을 향상시키기 위한 전략에 대해 논의할 것입니다.\n\n2. 해석가능성: KAN은 전통적인 신경망에 비해 향상된 해석 가능성을 제공하지만, 학습된 활성화 함수와 네트워크 구조를 이해하고 해석하는 것은 여전히 어려울 수 있습니다. KAN에서 입력 변수와 결과적인 변환 간의 복잡한 관계는 항상 직관적이거나 쉽게 설명할 수 없을 수 있습니다. 우리는 KAN의 해석 가능성을 향상시키고 그들의 결정 과정을 더 투명하게 만드는 방법을 탐색할 것입니다.\n\n3. 일반화와 견고성: 다양한 데이터셋과 실제 시나리오에서 KAN의 일반화와 견고성을 보장하는 것은 여전히 상당한 도전입니다. 특히 작거나 소음이 많은 데이터셋에서 훈련될 때 KAN은 과적합 경향을 보일 수 있어서 보지 못한 데이터에서 성능이 저하될 수 있습니다. 우리는 KAN의 일반화 능력을 향상시키고 왜곡 및 적대적 공격에 대응하는 견고성을 강화하기 위한 기술에 대해 검토할 것입니다.\n\n\n\n### 4. 확장성: 데이터셋의 복잡성과 크기가 계속 커짐에 따라, 확장성은 KANs에 대한 중요한 고려 사항이 됩니다. KAN 아키텍처를 대규모 데이터셋에 대응하도록 확장하는 것에는 효율적인 메모리 관리, 분산 컴퓨팅 자원 및 최적화 기술이 필요합니다. 현대적인 대용량 데이터 응용 프로그램의 요구 사항을 충족하기 위해 KANs의 확장성 도전 과제 및 전략을 살펴보겠습니다.\n\n### 5. 계산 리소스: KAN들의 훈련 및 배포는 종종 고성능 GPU 또는 TPU, 대규모 저장 인프라 및 특수 하드웨어 가속기를 필요로 합니다. 이러한 리소스에 액세스가 예산 제약이나 인프라 제한으로 제한될 수 있습니다. 리소스 활용 최적화 및 KANs와 관련된 계산 비용을 줄이는 접근 방법을 탐색할 것입니다.\n\n# 미래 방향성 및 연구 기회\n\n이 섹션에서는 Kolmogorov-Arnold Networks (KANs) 영역에서 다가올 미래 방향성 및 연구 기회에 대해 탐구할 것입니다. 포텐셜이 풍부한 신생 기술로, KANs는 추가 탐구 및 혁신을 위한 다수의 가능성이 제시됩니다.\n\n\n\n1. **고급 아키텍처**: 미래 KAN(Knowledge-aware network) 연구는 현재의 패러다임을 넘어선 고급 아키텍처를 개발하는 데 초점을 맞출 것으로 예상됩니다. 새로운 네트워크 토폴로지, 활성화 함수 및 학습 메커니즘을 탐색함으로써 더 효율적이고 효과적인 KAN 아키텍처를 발견할 수 있으며, 이는 특정 작업 및 도메인에 맞추어져 있습니다.\n\n2. **하이브리드 접근 방식**: KAN을 합성곱 신경망(CNN), 순환 신경망(RNN) 또는 트랜스포머 모델과 결합하는 것은 그들의 능력을 향상시키고 한계를 극복하는 데 유망한 방법입니다. 다른 접근 방식의 장점을 결합함으로써 연구자는 KAN의 해석 가능성을 활용하면서 다른 모델의 표현 능력을 이용한 하이브리드 아키텍처를 만들 수 있습니다.\n\n3. **전이 학습 및 도메인 적응**: KAN에서 전이 학습 및 도메인 적응 기술을 연구함으로써 사전 훈련된 모델을 새로운 환경 및 응용프로그램에 원활하게 통합할 수 있습니다. 관련 작업이나 도메인에서 학습한 지식을 활용하여 KAN은 학습 과정을 가속화시키고 제한된 데이터에서 대상 작업의 성능을 향상시킬 수 있습니다.\n\n4. **설명 가능한 AI**: KAN의 해석 가능성과 투명성을 강화하는 것은 미래 연구의 주요 관심사가 될 것입니다. KAN의 학습 표현 및 의사 결정 프로세스를 시각화하고 해석하는 방법론을 개발함으로써 그들의 예측에 대한 신뢰와 확신을 유지할 수 있으며, 특히 의료 및 자율 주행과 같은 안전 중요 응용분야에서 이점을 얻을 수 있습니다.\n\n\n\n5. 다학제간 응용: 전통적인 기계 학습 및 컴퓨터 과학 분야를 넘어서 KAN의 다학제간 응용을 탐구하는 것은 연구를 위한 흥미로운 가능성을 여는 것입니다. 생물학, 화학, 물리학, 금융 등 다양한 분야의 전문가들과 협력하여 특정한 과학적이거나 산업적 분야에 맞는 전문화된 KAN 모델을 개발할 수 있습니다.\n\n6. 윤리적 및 사회적 영향: KAN이 사회에 점점 통합되면, 그 배치에 대한 윤리적 및 사회적 영향을 고려하는 것이 중요합니다. 편향, 공정함, 개인정보 보호, 책임성 등의 문제를 다루기 위해서는 다학제적인 연구 노력과 이해관계자들과의 협력을 통해 윤리적 가이드라인과 규제 뼈대를 개발해야 합니다.\n\n7. 교육 및 홍보: 다음 세대의 연구자들과 실무가 KAN 및 그 응용에 대해 교육하는 것은 해당 분야에서 미래 발전을 주도하기 위해 중요합니다. 교육 계획, 교육 프로그램, 그리고 홍보 활동에 투자하여 다양하고 포용적인 KAN 연구자들과 팬들의 공동체를 육성할 수 있습니다.\n\n8. 벤치마킹 및 평가: KAN에 대한 표준화된 벤치마크 및 평가 지표를 수립하는 것은 서로 다른 모델과 알고리즘 간의 공정한 비교를 용이하게 할 것입니다. 실제 도전과 복잡성을 반영한 벤치마크 데이터셋과 평가 프로토콜을 생성함으로써 보다 견고하고 신뢰할 수 있는 KAN 솔루션의 발전을 이끌 수 있습니다.\n\n\n\n9. 오픈소스 개발: KAN 커뮤니티에서 오픈소스 개발과 협업을 촉진함으로써 혁신을 가속화하고 최첨단 연구에 대한 접근을 민주화할 수 있습니다. 투명성, 공유, 협업의 문화를 육성하여 연구자들은 함께 KAN의 최첨단 기술을 발전시키고 다양한 응용 분야에서 의미 있는 영향을 낼 수 있습니다.\n\n10. 장기적 영향: 마지막으로, 연구자들은 KAN이 사회, 경제, 그리고 인류 전반에 미치는 장기적 영향을 고려해야 합니다. KAN의 널리 퍼지는 채택과정에서의 잠재적 위험과 도전, 예를 들어 직업 축소, 경제적 불평등, 예기치 않은 결과 등을 예상하고 책임있는 혁신과 기술 지배를 위한 선제적 전략 수립에 도움이 될 것입니다.","ogImage":{"url":"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_0.png"},"coverImage":"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_0.png","tag":["Tech"],"readingTime":13},{"title":"로봇 조립의 미래","description":"","date":"2024-05-15 16:25","slug":"2024-05-15-TheFutureofRoboticAssembly","content":"\n\n## 1913년 대량 생산이 소개된 이후로, 조립 라인은 여전히 대부분 인간의 손에 의해 운영되고 있습니다. 그러나, 인류형 로봇이 이를 바꿀 수도 있습니다.\n\n![로봇 조립의 미래](/assets/img/2024-05-15-TheFutureofRoboticAssembly_0.png)\n\n본 문서는 2024년 3월 27일에 Assembly Magazine에서 제공한 웨비나의 내용을 기재한 것입니다. 해당 웨비나는 온라인으로도 시청할 수 있습니다. 고믹스 저볼류 문제는 아직 로봇 자동화로 해결하기 어렵습니다. 협업 로봇, 인공 지능, 그리고 인류형 로봇에 대한 최근 트렌드를 살펴보며, 인류형 로봇이 일자리를 찾는 실질적인 관계를 조립 및 해체에 있을 수 있다고 제안합니다.\n\n헨리 포드는 대량 생산의 아버지로 알려져 있으며, '모델 T'의 생산을 최적화하여 자동차의 보급과 가격을 저렴하게 만드는 데 기여했습니다. 당시의 주요 혁신 중 하나는 생산 공정을 진행하는 조립 라인에 컨베이어 벨트를 사용하는 것이었습니다. 그럼에도 실제 노동은 대부분 수동적이었으며, 2024년 BMW의 엔진 조립과 같이 오늘날에도 그러합니다.\n\n\n\n\n![로봇 조립 라인](/assets/img/2024-05-15-TheFutureofRoboticAssembly_1.png)\n\n독일어 단어 \"Takt(주기)\"에 의한 조립 라인의 속도 조절은 조립 프로세스를 예측 가능하게 만드는 중요한 아이디어입니다. 공장의 생산량은 직접 Takt에 관련이 있으며, 이는 가장 느린 공정 부분에 의해 주도되며, 순환 시간과 직접적으로 관려됩니다. 프로세스에서 로봇을 사용하려면 모든 하위 공정이 협력하여 동일한 속도로 작동해야하는 주의가 필요합니다. 일반적인 조립 라인은 보통 이렇게 생겼습니다:\n\n![로봇 조립 라인 구조](/assets/img/2024-05-15-TheFutureofRoboticAssembly_2.png)\n\n조립 라인은 각 스테이션에서 가치를 추가하여 주문을 제품으로 변환합니다. 각 스테이션은 저장해야 할 \"작업 중인 제품\"(WIP)을 생성하며, 다음 스테이션으로 이동해야 합니다. 모든 스테이션은 원자재의 지속적인 공급이 필요하며, 이는 다른 조립 라인에서 나온 제품일 수도 있습니다. 로봇은 조립 라인 전반에 걸쳐 다양한 기능으로 사용될 수 있으며, 라인을 \"균형\"시키고 생산량을 증가시키는 것이 목표입니다. 이상적으로, 모든 스테이션은 동일한 시간이 소요되어야 하며, 작업 중인 제품이 완료되는 즉시 다음 스테이션으로 통과할 수 있어야 합니다. 여기서 중요한 점은 라인을 균형시키지 않는 로봇은 효율성을 저하시키고 종종 \"시제\" 이후 제작 프로세스에서 제외됩니다.\n\n\n\n\n# 왜 일부 로봇은 작동하지만 다른 로봇은 작동하지 않을까요?\n\n현재 로봇은 어셈블리에서 다양한 정도로 사용되고 있습니다. 한편으로는 로봇이 부품을 집고 놓고 스팟용접을 수행하는 어셈블리 라인이 있습니다. 이러한 산업용 로봇 클래스는 사전 프로그램된 궤적에서 작동하며 ABB, Fanuc, Kuka와 같은 대규모 로봇 제조업체의 주력 제품입니다. 더 최근에는 인간과 가까운 거리에서 작업할 수 있는 \"협력형\" 로봇 암도 있으며 시각 및 촉각 감지를 통해 점점 자율성이 증가하고 있습니다.\n\n![로봇 조립의 미래](/assets/img/2024-05-15-TheFutureofRoboticAssembly_3.png)\n\n그러나 이후자의 카테고리는 아직도 중요한 지지를 받는 데 어려움을 겪고 있습니다. 이는 자동화 솔루션 제공업체들을 수십 년 동안 괴롭히고 있는 초록밭 대 브라운밭 딜레마에 있다고 할 수 있습니다.\n\n\n\n\"그린필드\"에서 축적한 첫 번째 현장에서 균형 잡힌 조립 라인을 구축하는 것이 기존 환경에 자동화 솔루션을 통합하는 것(이를 브라운필드라고도 함)보다 훨씬 쉽습니다. 이 고민을 수동으로 커피를 내리는 과정을 자동화시키는 것과 단추 하나로 에스프레소 메이커를 설치하는 것으로 비유해보았습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_4.png)\n\n# 복잡한 3D 구조를 조립할 수 있을까요?\n\n조립에 대한 간단한 배경 설명과 거기에 로봇의 역할(그리고 왜 그들이 종종 작동하지 않는지)을 보면서, 현재의 로봇 조립 솔루션을 분류하는 방법과 우리가 향후에 어떻게 보이길 원하는지에 대해 고민해봅니다.\"\n\n\n\n아래 비디오는 2018년 세계 로봇 정상회담 산업 조립 챌린지의 대상인 메카트로닉 위젯의 양손으로 조립을 보여줍니다. 조립에는 세 가지 다른 도구, 밀어 넣기(흔들리고 밀어 넣기의 조합) 및 나사를 조이는 동안 서로 다른 부품을 고정하는 민첩성이 필요합니다.\n\n비디오에서 보여지는 작업을 위해 필요한 부품 및 도구를 준비하는 것이 별도의 작업이라는 것에 주목하셨을 것입니다. 이 작업은 일반적으로 대량의 소재를 \"키팅 트레이\"에 개별화하여 조립 공정으로 이송할 수 있도록 하는 방식인 키팅이라고 알려져 있습니다. 저희 회사의 솔루션 비디오(그 부문에서 가장 높은 점수를 받은 것을 받은 WRS :-)가 아래에 나와 있습니다:\n\nWRS의 주요 관심사는 전체 조립 순서보다는 단일 스테이션에서의 작업을 최적화하는 데에 있었습니다. \"레벨 5 자동화\"라는 아이디어를 촉진하며, 이는 자율 주행에서 사용된 레벨 5 자율성과 유사하다는 것은 우연이 아닙니다. 각 레벨의 목표는 아래 표에 요약되어 있습니다:\n\n![FutureofRoboticAssembly](/assets/img/2024-05-15-TheFutureofRoboticAssembly_5.png)\n\n\n\n여기 중요한 아이디어들이 있어요:\n\n- 공장이 하루만에 A 물건을 만드는 공장에서 B 물건을 만드는 공장으로 전환할 수 있다는 점.\n- 특별한 공구가 필요하지 않다는 점.\n- 연속 운전 개선 및 오류 복구에 대한 학습에 의존한다는 점.\n\nWRS 2018 대회는 \"레벨 4\"를 목표로 하며, 풀리와 고무 벨트를 기어와 체인으로 교체하는 \"의외의 과제\"를 제공했습니다. 이로 인해 에스프레소 마신을 만드는 커스텀 머신에서 필요한 위젯을 만드는 커스텀 머신에 이르기까지 많은 해결책이 나왔으며, 가능한 한 적은 작업으로 해결하려고 노력하는 두 개의 팔 조작 조합 솔루션까지 다양한 해결책이 제공되었습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_6.png)\n\n\n\n아래 비디오는 JAKS 팀이 전체 2등으로 나온 솔루션을 보여줍니다. 그 솔루션은 부품을 잡고 나사를 꽂기 위해 사용자 정의 엔드 이펙터가 달린 두 개의 산업용 로봇으로 구성되어 있습니다.\n\n아래 표는 모든 팀의 전체적인 성과를 보여주며, 많은 팀이 수행하고 해결된 것으로 간주할 수 있는 어떤 종류의 작업이 수행되었는지 제공합니다. 예를 들어, 거의 모든 팀이 \"Tasboard\" 작업을 수행할 수 있었으며, 이 작업은 3D 어셈블리의 모든 부분을 포함하는 간소화된 2D 어셈블리입니다. \"Kitting\" 작업에 능숙한 여러 팀도 있습니다. 전체 승자인 SDU 팀은 대량의 3D 프린팅된 지그와 특수 도구를 사용하는 데 중점을 두었습니다.\n\n<img src=\"/assets/img/2024-05-15-TheFutureofRoboticAssembly_7.png\" />\n\n# 페그-인-홀 어셈블리 및 힘/토크 제어\n\n\n\n한 가지 분명해진 점은 로봇의 위치뿐만 아니라 엔드 이펙터의 힘과 토크도 제어하는 것의 중요성이었습니다. 이 기능은 산업용 로봇이 과거에 갖지 못했던 능력이었으며, 2018년 대회 이후에야 Franka Emika Panda와 같은 협력 로봇에서 최근에 주목받는 추세였습니다.\n\n![figure](/assets/img/2024-05-15-TheFutureofRoboticAssembly_8.png)\n\n위의 그림은 로봇의 수권에 위치한 힘-토크(F/T) 센서를 활용한 힘 기반 삽입 방법을 보여줍니다. 이 센서는 세 가지 방향에서의 힘과 토크를 측정할 수 있습니다. 이 정보를 활용한 가능한 알고리즘은 작업물을 접촉이 발생할 때까지 아래로 이동시키는 것(1), 즉 Z-방향으로의 상당한 음의 힘과 그 후 나사 모양의 검색 동작(2)을 수행하여 작업물이 구멍에 들어가게 하고 Z 축을 따라 나타나는 힘이 반전될 때까지입니다.\n\n# 로봇 조립을 위한 새로운 도구\n\n\n\n한 팀(Fanuc)이 전체 3D 구조를 조립할 수 있었지만, 솔루션은 여전히 \"레벨 5\"에서 멀었어요. 2020년 다시 한 판을 치르자, 주최자들은 목표를 그대로 유지했지만, 기본 2D 조립 도전과 키팅 작업을 강조하지 않고 대신 자율성을 높이기로 했어요. 따라서 대회는 조립 트레이를 조립 스테이션으로, 그리고 검사 스테이션으로 이동시키기 위해 자율 지상 차량(AGV)를 제공했어요.\n\n그 동안 나온 새로운 도구들이 이러한 작업들을 훨씬 간단하게 만들어 주었어요. 예를 들어 이제 산업용 로봇을 유연하게 만들어주는 간단한 솔루션이 있어요. Bosch/Rexroth는 산업용 로봇을 3mm까지 또는 이에 상응하는 회전으로 강하게 또는 유연하게 제어할 수 있는 간단한 어댑터를 개발했어요. 이렇게 하면 작업 조각이 원하는 형태로 들어가기까지 흔들면서 밀어 넣음으로써 딱 맞는 조립이 가능해져요. 그러면 로봇은 실제 변위를 읽어 보정 동작을 수행하고 어댑터를 잠그게 됩니다.\n\n또한, 2018년 첫 대회 이후에는 나사 조작도 훨씬 간단해졌는데, 현재 많은 기업들이 유니버설 로봇용으로 통합된 자동 드라이버를 제공하고 있어요.\n\n이러한 솔루션은 다양한 정도의 도구 체인저를 제공하며, 자동 디스펜서로부터 나사를 집어내거나 이상한 각도나 아래에서 나사 조작도 가능해요. 아래 비디오는 Stöger사의 솔루션을 더 자세히 보여줍니다.\n\n\n\n# 앞으로 다가올 미래…\n\n지금까지 우리는 거의 모든 것을 조립할 수 있는 기계를 만들 수 있다는 것을 보았습니다. 대량 소재에서 개별 구성 요소를 제작하고 키트를 만들고, 그것들을 조립 로봇으로 옮기고, 결과물을 테스트하는 것이 가능합니다. 그러나 현실에서는 이러한 설치물이 매우 드물게 존재하는 것을 볼 수 있습니다. 이는 대량 생산은 효율적이고 맞춤형 기계가 필요하며, 고믹스, 저양 산출의 경우에는 여전히 사람들에 의해 가장 잘 수행됩니다. 만약 로봇이 실제로 사람과 같은 모양을 가지고, 사람들이 할 수 있는 모든 일을 할 수 있는 경우에는 이것이 바뀔까요?\n\n실제로 테슬라가 2022년 후반에 이 공간에 진입하면서 인간 형태 공간에서 엄청난 발전이 있었습니다. 빛 속도로 향상된 버전을 내놓았으며 — Optimus Gen 2는 2023년 12월에 첫 번째 두 개형의 프로토 타입 후 단 하루만에 나왔습니다.\n\n그러나 비디오에 등장한 로봇은 그다지 많이 하지 않습니다. 달걀을 집어들어 달걀 조리기에 넣는 것은 기본적인 쎄움 커크루 조립이지만, 로봇이 더 어려운 일을 할 수 있다면 아마도 보여줬을 것입니다. 많은 사람들이 또한 걷는 속도가 느리다며 실망했습니다.\n\n\n\n# 인간형 로봇의 급부상\n\n놀랄 만한 속도로 뉴스가 들어오고 있지만, Figure사가 6억 5000만 달러를 조달했으며 BMW와 협력을 시작하였습니다. Apptronik은 메르세데스와 협력을 시작했고, Agility Robotics는 아마존과 협력을 시작했습니다. AI 혁명을 주도하고 있는 하드웨어를 제공하는 회사인 Nvidia는 인간형 로봇을 위한 \"gr00t\" 이라는 기본 모델을 개발했습니다. 마지막으로, 당신은 심지어 중국의 Unitree에서 8만 9천 달러에 손은 없지만 빠르게 걷는 해당 인간형 로봇을 구매할 수 있습니다.\n\n다음 이유들로 인해 인간형 로봇이 기회를 가질 수 있습니다:\n\n- 만약 그들이 동작한다면, 기존의 인간을 위해 만들어진 현재 프로세스에 통합되어 \"브라운필드\" 문제를 해결할 수 있습니다.\n- 협력 로봇 시장은 이미 휴대용 인간형 로봇이 바로 연결될 수 있는 다양한 응용 분야와 보조 생태계를 식별했습니다.\n- 2023년은 철도와 인터넷보다 큰 침체혁명이 될 수 있는 AI 혁명의 시작입니다.\n\n\n\n# 사람형 로봇 혁명을 주도할 AI 혁신들\n\nChatGPT와 기저에 있는 transformer 아키텍처는 텍스트에 국한되지 않고 멀티모달 입력을 처리할 수 있습니다. 인터넷 규모의 이미지와 레이블 데이터로 훈련된 이 모델은 그래픽 특징, 명사, 형용사, 그리고 공간 관계를 포함한 동사 간의 관골을 학습할 수 있습니다. 이전의 머신 러닝 모델이 미리 훈련된 \"고양이\"나 \"개\"와 같은 클래스만을 감지할 수 있었다면, transformer 기반의 시각-언어 모델은 \"고양이의 꼬리\"와 같은 임의의 명사나 설명으로 질의할 수 있습니다. 아래 이미지는 Google이 개발한 오픈 소스 언어 모델인 OWL-ViT의 퍼블릭 인터페이스에서 스크린샷을 보여줍니다. [여기](img src=\"/assets/img/2024-05-15-TheFutureofRoboticAssembly_9.png\")에서 시도해볼 수 있습니다.\n\n나는 인터넷에서 에스프레소 메이커의 이미지를 업로드하고 모델에게 \"커피 메이커\", \"온도 다이얼\", \"커피 용기\"를 찾도록 요청했습니다. 출력물은 오른쪽에 표시되었습니다. 모델은 커피 메이커와 온도 다이얼을 올바르게 지역화했지만, 나는 실제로 그들이 어떻게 불리는지조차 알지 못합니다. 하지만 저의 멍쇼를 하나 할 수 있습니다. 예를 들어, 왼쪽의 손잡이는 아마도 점화 버튼이고, 배경에 있는 물건은 아마 커피 용기일 것입니다. 마지막으로, OWL-ViT는 왼쪽에 두 번째 커피 메이커를 놓쳤습니다. 출력물이 완벽하지는 않지만, 이러한 모델들은 상징적인 레이블을 물리적인 물체로 해석하면서 로봇이 스스로 야생을 탐험할 수 있는 문을 열어줍니다.\n\n\n\n또한, 단어에는 추가적인 의미와 연관된 맥락이 있기 때문에 대형 언어 모델은 로봇에게 상식적인 추론을 제공할 수도 있습니다. 예를 들어 물건을 집을 때 로봇은 과일 중 가장 익은 과일을 선택하거나 물건을 깨지 않고 집는 방법을 결정하는 데 상식적인 정보를 사용할 수 있을 것입니다. 본 연구는 샌드위치를 조립하는 데 초점을 맞추었지만, 이러한 기능은 플라스틱 부품이 섬세하게 다뤄져야 하거나, 프레스로 조립된 부품은 다른 접근 방식이 필요하거나, 못을 넣는 데 나사보다 다른 도구가 필요한 경우를 결정하는 데 사용될 수도 있습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_10.png)\n\n또한, 트랜스포머 아키텍처의 다중 모달성은 시연을 통한 학습에도 적합합니다. Google Deepmind와 Toyota Research Institute를 비롯한 학계와 기업 기관들이 다양한 도메인에서 대규모 이중팔 조작 작업을 기록하는 것이 증가하고 있습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_11.png)\n\n\n\n아래 비디오에서는 이중 팔 협력 로봇 시스템을 사용하여 속성 복제를 통한 학습의 예시가 보여집니다:\n\n그러나, 모든 AI의 발전은 여전히 연구 중이라는 점을 유념해주십시오. 예를 들어, 설거지 데모에서는 258번의 시연이 필요했으며 70%의 성공률을 보여주었지만, 아직 실용적인 사용으로는 거리가 멉니다. 그럼에도 불구하고 AI에 대한 성장과 투자는 전례없는 것으로 보이며, 우리가 관심을 갖는 조립 문제 중 많은 것들은 훨씬 제약된 환경에서 발생합니다. 동시에, 이러한 작업은 자율 주행이나 인간과 밀접하게 협업하는 인간형 어플리케이션보다 훨씬 낮은 위험을 가지는 고가치 작업을 제공합니다. 어울린 사람 형태역학에서는 엄청난 도전도 존재합니다. 협력형 조립 로봇은 아직 테이블에 고정되어 있지만, 종종 작업물을 가진 지그를 호스트하는 동일한 테이블일 수 있습니다. 그에 비해 인간형 로봇은 그러한 혜택을 누릴 수 없으며, 정밀한 감지와 제어가 필요합니다. 이 도전적인 문제에 직면한 우리는 어떻게 행동해야 하는지 고민해야 합니다.\n\n# 어플리케이션 드라이버: 전지 해체\n\n현재 하드웨어 및 소프트웨어의 한계에 충분히 견고한 대규모 시장을 지원하는 어플리케이션을 찾는 접근 방식 중 하나는, 최신 협력 로봇 도구 및 신흥 AI 모델을 결합하는 \"로봇 조립의 미래\"로 가는 길에 있을 수 있습니다. 전기 자동차(EV) 배터리 해체가 그러한 어플리케이션 중 하나일 것입니다. 전기 자동차의 가치 있는 부분은 배터리입니다. 20년이 넘은 첫 번째 세대 EV가 점점 더 나오면서, 우리는 더 많은 배터리가 재활용이 필요해질 것입니다. 이를 위해 배터리는 각 셀에 도달할 때까지 해체되어야하며, 그 후에는 셀을 분쇄하여 원자재로 분리해야 합니다. 이것은 어렵고 위험합니다. EV 셀은 최대 700V까지 충전될 수 있습니다. 또한 다양한 배터리 유형이 존재하며, 그들의 해체를 위한 맞춤형 기계를 구축하는 것은 불가능합니다. \n\n\n\n![로봇 조립의 미래](/assets/img/2024-05-15-TheFutureofRoboticAssembly_12.png)\n\n동시에 문제는 충분히 제한되어 \"레벨 5 자동화\"를 실현할 수 있다는 점입니다: 해체는 일반적으로 조립보다 쉬우며 무언가를 분해하는 추가 기회를 식별하는 데 한정됩니다. 동시에 체결물은 제한된 세트에서 가져오며 시스템은 부족한 기술을 가르칠 수 있습니다. 예를 들어 특정 보호 구역을 열어내는 것과 같은 새로운 도전이 발생할 때.\n\n# 결론\n\n맞춤 기계를 사용하면 이미 자동 조립 및 키팅이 가능함을 보았습니다. 맞춤 기계가 경제적이지 않은 고믹스, 저볼륨 문제와 손수가 가장 효율적인 옵션으로 남아 있는 문제입니다. AI 분야의 최근의 기술적 돌파는 로봇학의 중요한 진전을 이끌 수도 있습니다: 상식적인 추론과 제한된 조작 기술을 갖춘 인간형 로봇. 이 약속은 매우 크기 때문에 주요 시장 참가자들이 이미 우리가 로봇 자동화에 대해 알고 있는 것, 새로운 AI 도구가 제공하는 것, 특히 학습, 로봇과 인간 간의 새로운 상호작용 방식, 개방형 세계 추론, 그리고 새로운 수준의 이동성 통합에 동참하기 위해 준비되어 있습니다. 조립과 해체가 그런 결과물의 첫 진정한 응용일 수도 있습니다.","ogImage":{"url":"/assets/img/2024-05-15-TheFutureofRoboticAssembly_0.png"},"coverImage":"/assets/img/2024-05-15-TheFutureofRoboticAssembly_0.png","tag":["Tech"],"readingTime":9},{"title":"버릇없는 Google 사진 RaspberryPi, SyncThing 및 PhotoPrism을 사용하여 사진 백업하기","description":"","date":"2024-05-15 16:21","slug":"2024-05-15-DumbGooglePhotosBackupyourphotosusingRaspberryPiSnycThingandPhotoPrism","content":"\n\n<img src=\"/assets/img/2024-05-15-DumbGooglePhotosBackupyourphotosusingRaspberryPiSnycThingandPhotoPrism_0.png\" />\n\n디지털 시대에 있어서 사진은 우리의 가장 소중한 자산 중 하나로, 대체할 수 없는 순간과 추억을 담고 있습니다. 하지만 이러한 기억을 보호하는 것은 어려울 수 있습니다. 특히 기기 고장, 분실 또는 도난의 위험이 있습니다. 전통적인 클라우드 서비스는 해결책을 제공하지만 종종 개인 정보 보호와 계속해서 지불해야 하는 요금이라는 비용이 따릅니다. 이 글은 Raspberry Pi, Syncthing 및 PhotoPrism을 사용하여 사진을 백업하는 강력하고 개인 정보 보호 중심의 솔루션을 소개합니다. 이 도구들은 사용자 맞춤형 및 안전한 방법을 제공하여 사진을 저장, 관리 및 액세스할 수 있습니다. 사진을 좋아하는 사람이거나 디지털 기억을 안전하게 보호하고자 하는 경우, 이 안내서를 통해 데이터를 비공개로 유지하고 기억을 안전하게 보관하는 포괄적인 가정용 사진 백업 시스템을 설정하는 방법을 안내해 드리겠습니다.\n\n# 목차\n\n도구 이해하기\n\n\n\n라즈베리 파이 준비\n\nSyncthing 설치 및 구성\n\nPhotoPrism 설정\n\n백업 프로세스 자동화\n\n\n\n사진 백업 시스템 최적화를 위한 팁\n\n## 소개\n\n사진 백업은 전문가와 애호가 모두에게 소중한 기억과 작품을 보호하기 위한 중요한 작업입니다. 다양한 기술 솔루션의 등장으로 가정에서 경제적이면서도 효과적인 백업 시스템을 구축하는 것이 더욱 쉬워졌습니다. 이 안내서는 Raspberry Pi, Syncthing 및 PhotoPrism을 활용하여 강력한 사진 백업 솔루션을 만드는 데 초점을 맞춥니다. 각 도구는 각각 고유한 강점을 가지고 있습니다. Raspberry Pi는 소형이면서도 강력한 하드웨어 플랫폼을 제공하며, Syncthing은 여러 장치 간 파일의 안전하고 개인적인 동기화를 제공하며, PhotoPrism은 이 파일들을 쉽게 정리하고 액세스할 수 있도록 도와줍니다. 이러한 기술들을 통합함으로써 사용자들은 완전히 자동화되고 확장 가능한 사진 백업 시스템을 구축할 수 있습니다.\n\n## 섹션 1: 도구 이해\n\n\n\n라즈베리 파이는 다양한 프로젝트에 사용할 수 있는 작고 저렴한 컴퓨터입니다. 우리의 목적에는 그 소형 크기와 저전력 소비로 가정용 서버를 운영하기에 이상적인 선택입니다. 다양한 모델이 있지만, 미디어 파일을 효율적으로 처리하기 위해 램과 프로세서가 더 좋은 모델이 선호됩니다.\n\n싱크띵은 여러 기기 간 파일 동기화 프로세스를 간소화하는 무료 오픈소스 지속적인 파일 동기화 프로그램입니다. 싱크띵은 피어 투 피어 모델로 작동하여 데이터가 제3자 클라우드 서비스에 의존하지 않도록 보장하여 파일을 개인적이고 제어할 수 있게 합니다.\n\n포토프리즘은 Go와 Google TensorFlow로 구동되는 오픈소스 사진 관리 도구입니다. 다양한 매개변수를 기반으로하여 사진에 액세스하고 정리할 수 있는 사용자 친화적 인터페이스를 제공합니다. 사진 색인, 중복 관리, 강력한 검색 기능 제공 등의 기능을 포함하여, 싱크띵이 다루는 방대한 양의 데이터를 보고 정리하는 데 탁월한 도구이므로 사진 관리에 뛰어난 툴입니다.\n\n# Section 2: 라즈베리 파이 준비하기\n\n\n\n라즈베리 파이를 사진 백업 시스템의 백본으로 설정하는 것은 올바른 하드웨어 선택부터 소프트웨어 환경 구성까지 여러 중요 단계가 필요합니다. 시작하는 방법은 다음과 같습니다:\n\n## 하드웨어 선택\n\n라즈베리 파이는 다양한 모델로 제공되며 각각 다른 사양과 기능을 갖추고 있습니다. 사진 관리 및 백업을 위한 프로젝트의 경우, 라즈베리 파이 5를 강력히 추천합니다. 이 모델은 우수한 처리 성능과 증가된 메모리 옵션(최대 8GB RAM)을 갖추고 있어 대용량 파일을 처리하고 여러 서비스를 원활하게 실행하는 데 도움이 됩니다.\n\n또한 필요한 것은:\n\n\n\n- SD 카드: 적어도 32GB의 저장 용량을 갖춘 고속 SD 카드는 운영 체제 및 애플리케이션에 필수적입니다. 많은 읽기/쓰기 작업에 적합한 고내구성 카드를 사용하는 것을 고려해보세요.\n- 전원 공급: 안정적이고 신뢰할 수 있는 전원 공급을 보장하기 위해 라즈베리 파이의 공식 전원 공급 장치를 사용하세요.\n- 외장 하드 드라이브: 사진을 저장하기 위해 USB를 통해 연결된 외장 HDD 또는 SSD가 필요합니다. 백업 요구 사항을 처리할 수 있는 충분한 용량을 갖췄는지 확인해주세요.\n\n라즈베리 파이의 자세한 초기 설정 가이드는 이 기사를 참고해주세요.\n\n## 소프트웨어 설치 및 네트워크 설정\n\n- 운영 체제: 라즈베리 파이 웹사이트에서 라즈베리 파이 OS의 최신 버전을 다운로드하세요. Raspberry Pi Imager를 사용하여 SD 카드에 OS 이미지를 작성해주세요. 이 도구는 Pi를 부팅하기 전에 Wi-Fi 설정, SSH 활성화 및 로케일 설정 구성과 같은 고급 옵션도 제공합니다.\n- 초기 설정: SD 카드를 라즈베리 파이에 삽입하고, 모니터, 키보드, 마우스를 연결하고 전원을 켜세요. 홈 네트워크에 연결하는 것을 포함하여 초기 설정을 완료하기 위해 화면 안내에 따라 진행하세요.\n- 소프트웨어 업데이트: 라즈베리 파이를 최신 상태로 유지하는 것은 보안 및 성능 측면에서 중요합니다. 다음 명령을 실행하여 설치된 모든 패키지를 업데이트하세요:\n\n\n\n```js\nsudo apt update\nsudo apt full-upgrade\n```\n\n필요한 패키지 설치: Syncthing 및 PhotoPrism을 원활하게 실행하기 위해 몇 가지 패키지가 필요합니다. 다음 명령을 실행하여 설치하세요:\n\n```js\nsudo apt install -y curl wget vim git\n```\n\n## Tailscale을 사용한 네트워킹\n\n\n\n전통적인 포트 포워딩 대신 Tailscale을 사용하여 집 네트워크 외부에서 Raspberry Pi에 액세스할 수 있습니다. Tailscale은 기기 간 안전한 네트워크를 만드는 제로 구성 VPN 서비스입니다:\n\n- Tailscale 설치: Raspberry Pi에 Tailscale을 설치하여 기기를 보호하고 원격 액세스를 간편하게하세요:\n\n```js\ncurl -fsSL https://tailscale.com/install.sh | sh\n```\n\n- Tailscale 설정: 화면 안내에 따라 로그인하고 기기를 Tailscale 네트워크에 연결하세요.\n- 어디서나 액세스: Tailscale을 설정하면 전통적인 네트워크 설정 없이 Tailscale 네트워크에 연결된 장치에서 Raspberry Pi에 액세스할 수 있습니다.\n\n\n\n라즈베리 파이에 Tailscale을 설정하는 자세한 단계는 이 포괄적인 안내서를 참조해주세요.\n\n이제 안전하게 인터넷에 Tailscale을 통해 연결된 라즈베리 파이는 사진 백업 서버로 사용할 준비가 되었습니다. 다음 단계는 Syncthing을 설치하고 구성하여 파일을 여러 기기간에 동기화하는 것입니다.\n\n# 섹션 3: Syncthing 설치 및 구성\n\nSyncthing은 인터넷을 통해 여러 기기간에 파일을 안전하게 동기화하는 강력한 오픈 소스 도구입니다. 이 섹션에서는 라즈베리 파이에 Syncthing을 설치하고 사진을 효율적으로 동기화하도록 구성하는 방법을 다룰 것입니다.\n\n\n\n## Syncthing 설치하기\n\nSyncthing Repository 추가하기: 먼저 Syncthing 저장소를 Raspberry Pi에 추가하여 최신 업데이트를 직접 받도록 해보세요:\n\n```js\ncurl -s https://syncthing.net/release-key.txt | sudo apt-key add -\necho \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list\n```\n\nSyncthing 설치하기: 패키지 목록을 업데이트하고 Syncthing을 설치하세요:\n\n\n\n```js\nsudo apt update\nsudo apt install syncthing\n```\n\n서비스 활성화 및 시작: Syncthing이 부팅 시 자동으로 시작되고 백그라운드에서 실행되도록하려면 Syncthing 서비스를 활성화하고 시작하십시오:\n\n```js\nsystemctl enable syncthing@$(whoami).service\nsystemctl start syncthing@$(whoami).service\n```\n\n## Syncthing 구성하기\n\n\n\n- Syncthing에 액세스하기: Syncthing은 라즈베리 파이의 웹 인터페이스를 통해 액세스할 수 있습니다. 브라우저를 열고 http://localhost:8384로 이동하세요. 원격으로 액세스하는 경우, 라즈베리 파이의 IP 주소 뒤에 :8384를 붙입니다.\n- 기기 추가: 기기 간에 파일을 동기화하려면 각 기기를 Syncthing 네트워크에 추가해야 합니다. Syncthing 대시보드에서 \"기기 추가\"를 클릭하고 동기화할 각 기기의 기기 ID를 입력하세요. 해당 기기의 Syncthing 인터페이스에서 기기 ID를 찾을 수 있습니다.\n- 폴더 공유: 기기가 연결된 후 동기화할 폴더를 지정할 수 있습니다. \"폴더 추가\"를 클릭하고 폴더 경로, 레이블을 제공하고 이 폴더와 동기화하려는 기기를 선택하세요. 사진 백업을 위해, 모든 사진이 저장되고 동기화되는 특정 폴더를 기기에 만드세요.\n- 파일 버전 관리 및 백업 설정: Syncthing을 사용하면 파일의 이전 버전을 유지할 수 있어 추가적인 백업 보호층이 제공됩니다. 폴더 설정에서 \"파일 버전 관리\" 아래에서 단순 파일 버전 관리 또는 단계적 파일 버전 관리와 같은 필요에 맞는 버전 관리 방법을 선택하세요.\n- 보안 설정: 보안을 강화하기 위해 Syncthing을 HTTPS를 사용하도록 구성하고 \"설정\" - \"GUI\" 아래에서 강력한 GUI 인증 비밀번호를 설정하세요.\n\n## 사진 자동 백업\n\n백업 프로세스를 자동화하기 위해, 모든 기기의 지정된 사진 폴더에 추가된 새로운 사진이 자동으로 라즈베리 파이에 동기화되도록 확인하세요. Syncthing은 이러한 폴더를 모니터링하고 모든 연결된 기기에 변경 사항을 동기화합니다.\n\n구성된 Syncthing을 통해 사진은 이제 지속적으로 라즈베리 파이에 백업되어 어디서든 안전하게 액세스할 수 있습니다. 이 설정은 견고한 백업 솔루션을 제공할 뿐만 아니라 데이터가 개인 정보로 유지되고 제어 범위 내에 유지되도록 보장합니다.\n\n\n\n다음 단계는 Syncthing이 동기화된 사진을 저장하는 저장소와 PhotoPrism을 통합하여 사진 관리를 위한 아름답고 기능적인 인터페이스를 제공하는 것입니다.\n\n# 섹션 4: PhotoPrism 설정\n\nPhotoPrism은 최신 기술을 활용하여 사용자 친화적인 인터페이스를 제공하는 개인 정보 보호에 중점을 둔 오픈 소스 사진 관리 응용 프로그램입니다. 이 섹션에서는 Raspberry Pi에 PhotoPrism을 설치하고 Syncthing에 저장된 사진과 원활하게 작동하도록 구성하는 방법을 안내해 드리겠습니다.\n\n## PhotoPrism 설치하기\n\n\n\n환경을 준비하세요:\n\n- PhotoPrism은 Raspberry Pi에서 실행하려면 Docker가 필요합니다. 따라서 첫 번째 단계는 Raspberry Pi에 Docker 및 Docker Compose를 설치하는 것입니다:\n\n```js\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\nsudo usermod -aG docker ${USER}\nnewgrp docker\nsudo apt-get install -y docker-compose\n```\n\nPhotoPrism 다운로드:\n\n\n\n예제 PhotoPrism 도커 설정을 Raspberry Pi로 복제해보세요:\n\n```js\ngit clone https://github.com/photoprism/photoprism.git\ncd photoprism/docker\n```\n\n저장소 설정:\n\n도커 Compose 파일을 조정하여 Syncthing이 사진을 저장하는 디렉토리를 매핑하십시오. 이를 위해 PHOTOPRISM_ORIGINALS_PATH를 Syncthing에 의해 동기화된 디렉토리로 설정해야 합니다.\n\n\n\n```yaml\nvolumes:\n  - \"/path/to/syncthing/photos:/photoprism/originals\"\n```\n\n## PhotoPrism 구성\n\n- 구성 편집:\n\n- docker-compose.yml 파일에서 필요한 조정을 하여 설정을 맞추세요. 예를 들어, 더 많은 성능이 필요하다면 PHOTOPRISM_WORKERS와 같은 환경 변수를 수정하여 작업자 수를 늘릴 수 있습니다.\n- 최적의 성능을 위해 메모리와 CPU 설정이 적절히 조정되었는지 확인하세요.\n\n\n\nPhotoPrism을 시작해보세요:\n\n설정이 완료되면 Docker Compose를 사용하여 PhotoPrism을 시작할 수 있어요:\n\n\ndocker-compose up -d\n\n\n이 명령은 필요한 Docker 이미지를 가져와 PhotoPrism 서비스를 시작할 거예요.\n\n\n\n## 사진프리즘 사용하기\n\n웹 인터페이스에 접속하는 방법:\n\n- 웹 브라우저를 열고 http://`Raspberry-Pi-IP`:2342로 이동하여 사진프리즘 인터페이스에 접속합니다.\n- 설정 마법사를 통해 초기 설정을 안내받을 수 있으며, 관리자 계정을 설정하는 과정을 포함합니다.\n\n사진 색인하기:\n\n\n\n- 한 번 로그인하면 사진을 정리하기 위해 색인화 프로세스를 시작하세요. PhotoPrism은 지정된 디렉토리를 스캔하고 썸네일을 생성하며 태그를 적용하고 내장 AI를 사용하여 사진을 분류할 것입니다.\n- 설정 메뉴에서 색인화 프로세스를 수동으로 시작하거나 새 사진이 추가될 때 자동으로 실행되도록 설정할 수 있습니다.\n\n기능 살펴보기:\n\n- 태그, 색상 또는 얼굴로 검색하는 다양한 기능을 탐색해보세요. PhotoPrism의 AI 기능을 사용하여 사진의 내용을 기반으로 사진을 검색할 수도 있어서 기억을 효과적으로 관리하고 검색할 수 있습니다.\n\n이제 PhotoPrism을 설정하고 Syncthing과 통합했으므로 Raspberry Pi가 강력하고 개인용 사진 관리 시스템으로 변신합니다. 이 설정은 사진을 안전하게 다른 기기에 백업하는 것뿐만 아니라 사진 컬렉션을 브라우징하고 정리하기 위한 직관적인 인터페이스도 제공합니다.\n\n\n\n# 사진 백업 시스템 최적화 팁\n\n라즈베리 파이, 싱크싱, 포토프리즘을 통합하여 사진 백업과 관리에 대해 설정을 최적화하여 효율성과 데이터 안전성을 중요시하는 것이 중요합니다. 특히 스마트폰과 라즈베리 파이 간의 사진 동기화를 관리하는 데에 시스템이 원활하고 안전하게 작동하도록 다음 몇 가지 팁을 제공합니다.\n\n## 1. 일방향 동기화를 위한 싱크싱 설정 구성\n\n스마트폰에서의 데이터 손실을 방지하고 저장 공간을 효과적으로 관리하기 위해 중요한 것은 스마트폰에서 라즈베리 파이로의 일방향 동기화를 설정하는 것입니다. 이 설정은 스마트폰에서 촬영한 모든 사진이 자동으로 라즈베리 파이로 백업되지만 스마트폰에서 삭제해도 라즈베리 파이에서는 삭제되지 않는다는 것을 의미합니다. 다음은 이를 구성하는 방법입니다:\n\n\n\n동기화 폴더 설정하기:\n\n- 스마트폰에서 Syncthing과 공유할 폴더를 설정하고 여기에 사진을 넣어주세요. 일반적으로 카메라 폴더를 사용합니다.\n\n폴더 유형 구성하기:\n\n- 스마트폰에서 폴더 유형을 \"송신 전용\"으로 설정해주세요. 이 설정은 스마트폰에서의 변경 사항(삭제 포함)이 라즈베리 파이에 있는 백업에 영향을 미치지 않도록 합니다.\n- 라즈베리 파이에서 해당 폴더를 \"수신 전용\"으로 설정하세요. 이렇게 하면 파이는 파일을 받기만 하고 핸드폰으로 변경 사항을 전달하지 않습니다.\n\n\n\n가끔씩 라즈베리 파이가 일방적으로 동기화하다보니 동기화가 맞지 않을 수 있는 상황을 감지할 수 있습니다. 라즈베리 파이의 Syncthing 인터페이스에서 이러한 변경 사항을 수동으로 덮어쓰면 핸드폰의 데이터와 다시 맞출 수 있습니다.\n\n## 2. 스마트폰 스토리지 관리하기\n\n사진은 스마트폰의 저장 공간을 빠르게 소모할 수 있으므로 이 공간을 규칙적으로 관리하는 것이 중요합니다:\n\n\n\n## 정기적인 정리:\n\n- Raspberry Pi에 사진이 백업되었는지 확인한 후에는 안전하게 스마트폰에서 삭제할 수 있습니다. Google 사진의 \"공간 확보\"와 같이 청소를 자동화하는 앱이나 설정은 유용할 수 있지만, 백업된 사진만 삭제하도록 구성되어 있는지 확인하세요.\n\nSyncthing 상태 모니터링:\n\n- 스마트폰의 Syncthing 앱을 정기적으로 확인하여 연결되어 있고 기대한 대로 동기화되는지 확인하세요. 연결이 끊기면 백업이 지연되어 사진이 너무 일찍 삭제될 경우 데이터 유실이 발생할 수 있습니다. \n\n\n\n## 3. 데이터 중복성 보장\n\n사진을 추가로 보호하려면 다음과 같은 중복성 조치를 고려해보세요:\n\n외장 하드 드라이브:\n\n- 추가 외장 하드 드라이브를 Raspberry Pi에 연결하고 Syncthing 또는 별도의 백업 시스템을 구성하여 사진을 중복으로 복제합니다. 이 중복성은 하드웨어 고장에 대비합니다.\n\n\n\n온라인 백업 솔루션:\n\n- 여기서 중요한 것은 데이터를 개인 장치 내에 안전하게 보관하는 데 있지만, 절대적인 안전을 위해 보조 백업으로 암호화된 클라우드 백업 서비스를 사용하는 것을 고려해보세요. 개인 정보 보호를 위해 파일을 업로드하기 전에 암호화하세요.\n\n## 4. 자동화된 유지보수\n\n일부 유지보수 작업을 자동화하면 수시로 수동 개입하지 않아도 시스템이 효율적으로 유지될 수 있습니다.\n\n\n\n안녕하세요! 아래는 Markdown 형식으로 테이블 태그를 변환하는 방법입니다.\n\n\nAutomated Scripts:\n\n- Write scripts to handle periodic re-indexing in PhotoPrism or database cleanups, and set them to run at times of low activity via cron jobs on the Raspberry Pi.\n\nMonitor System Health:\n\n- Use tools like htop or web-based monitoring solutions to keep an eye on the Raspberry Pi’s performance. Monitoring can help you spot issues before they affect your backups.\n \n\n도움이 되었기를 바랍니다! 만약 더 도움이 필요하시면 언제든지 물어보세요.\n\n\n\n위의 팁을 따라하면 Raspberry Pi, Syncthing 및 PhotoPrism을 사용하여 견고하고 효율적이며 안전한 사진 백업 시스템을 만들 수 있습니다. 이 설정은 신뢰할 수 있는 백업을 통해 안심감을 제공하는데 그치지 않고, 사적이고 사용자 정의 가능한 사진 관리 솔루션을 제공합니다.\n\n# 결론\n\nRaspberry Pi, Syncthing 및 PhotoPrism의 통합은 사진 백업 및 관리에 대한 견고한 솔루션을 제공합니다. 이 시스템은 당신의 추억들이 안전하게 보존되고, 외부 위협으로부터 데이터를 보호하는 사적 네트워크 내에서 모든 기기에서 쉽게 액세스할 수 있도록 보장합니다. 프로 사진작가이든 기술 애호가이든, 이 설정은 제3자 클라우드 서비스에 의존하지 않고 디지털 사진 관리를 직접 제어할 수 있도록 돕습니다.","ogImage":{"url":"/assets/img/2024-05-15-DumbGooglePhotosBackupyourphotosusingRaspberryPiSnycThingandPhotoPrism_0.png"},"coverImage":"/assets/img/2024-05-15-DumbGooglePhotosBackupyourphotosusingRaspberryPiSnycThingandPhotoPrism_0.png","tag":["Tech"],"readingTime":10},{"title":"10가지 똑똑한 DIY 해킹으로 당신의 삶을 더 간단하게 만들어보세요","description":"","date":"2024-05-15 16:20","slug":"2024-05-15-10CleverDIYHackstoSimplifyYourLife","content":"\n\n<img src=\"/assets/img/2024-05-15-10CleverDIYHackstoSimplifyYourLife_0.png\" />\n\n현대 사회의 빠른 속도에서는 일상적인 문제에 간단하고 효과적인 해결책을 찾는 것이 중요합니다. 집 정리부터 일상적인 골칫거리를 해결하기까지, 이 DIY 해킹은 여러분의 삶을 더 쉽고 효율적으로 만들어줄 것입니다. 함께 알아보죠!\n\n- 케이블 정리 스테이션: 뒤얽힌 전선에 지쳤나요? 화장지 롤과 신발 상자를 활용하여 케이블 정리 스테이션을 만들어보세요. 각 롤에 전화 충전기나 헤드폰과 같은 다른 케이블을 라벨링하고 상자에 깔끔하게 쌓아두세요. 그럼 더 이상 지저분함이 없어집니다.\n- 메이슨 항아리 수납: 보통의 메이슨 항아리를 세련된 주방이나 욕실 수납 용기로 변신해보세요. 건조 재료, 면봉, 큐팁 또는 사무용품을 보관하는 데 사용할 수 있습니다. 추가로 라벨을 붙여서 더욱 빛을 발하고 쉽게 식별할 수 있습니다.\n- 레고로 만든 열쇠 걸이: 레고 블록을 활용하여 키를 재미있고 다채롭게 관리해보세요. 접착제를 사용해 레고 조각을 나무판이나 프레임에 부착하면, 열쇠를 다시 잃어버리지 않을 거예요.\n- DIY 핸드폰 스탠드: 사용하지 않는 플라스틱 병을 활용하여 동영상 시청이나 화상 통화용으로 편리한 핸드폰 스탠드를 만들어보세요. 단순히 병을 가로로 절단하고, 원하는 대로 장식한 후 핸드폰을 상단에 두면 됩니다. 간단하고 친환경적인 해결책입니다.\n- 얼음 트레이 정리함: 부엌 서랍 공간을 최대화하기 위해 얼음 트레이를 보석, 사무용품 또는 향신료 등 소품을 정리하는 용도로 활용해보세요. 칸은 물건을 정리하고 쉽게 접근할 수 있도록 해줍니다.\n- 샤워 캐디 정원: 행거형 샤워 캐디를 사용하여 샤워 시 작은 다육 식물이나 허브를 심어 수직 정원을 만들어보세요. 포켓에 작고 저수지를 심으면 샤워하는 동안 푸른식물을 감상할 수 있습니다.\n- 바인더 클립 케이블 정리함: 바인더 클립을 사용해서 책상을 정리해보세요. 책상이나 테이블 가장자리에 클립을 부착하고 전선을 금속 팔에 통과시키면 쉽고 효과적으로 전선이 엉킴을 방지할 수 있습니다.\n- DIY 드라이 이레이스 보드: 오래된 액자를 세련된 드라이 이레이스 보드로 변신하여 메모, 할 일 목록 또는 식사 계획을 작성해보세요. 패턴된 종이나 천을 액자 안에 넣고 드라이 이레이스 마커로 유리에 직접 쓸 수 있습니다.\n- 양말 정리함: 옷장에 걸치는 신발 정리함을 사용하여 양말을 짝지어 정리해보세요. 각 양말을 묶어서 개별 칸에 넣기만 하면 됩니다. 스트레스 없이 양말을 찾을 수 있는 공간 절약 해결책이죠.\n- DIY 공기 청정제: 매장에서 구매하는 공기 청정제는 이제 그만 하고 베이킹 소다와 에센셜 오일을 사용하여 자연 버전을 만들어보세요. 좋아하는 에센셜 오일 몇 방울을 베이킹 소다와 섞어 작은 접시나 병에 넣으면 됩니다. 집을 상쾌하고 깨끗하게 유지하는 경제적인 방법입니다.\n\n이러한 DIY 해킹은 간단하고 저렴하며 여러분의 삶을 조금 더 쉽게 만들어줍니다. 한 번 시도해보고 DIY의 기술을 습득하는 만족감을 누려보세요!\n\n\n\n더 많은 정보와 이 라이프 해킹에 대한 자세한 자습서는 다음 소스를 확인해보세요.\n\n- DIY Making Homemaking Simple and Enjoyable (https://www.onegoodthingbyjillee.com/)\n- Country Decor, Craft Ideas, Comfort Food, and Antique Appraisals — Country Living Magazine (https://www.countryliving.com)\n- Yours for the making — Instructables (https://www.instructables.com/)\n- The Organized Mom — Simplify Your Life With Organization Tips That Work! (https://www.organizedmom.net/)\n- Vertical Garden Ideas (https://balconygardenweb.com/)\n- The Family Handyman | Do it Yourself Home Improvement: Home Repair (https://www.familyhandyman.com/)\n- Recipe Ideas, Product Reviews, Home Decor Inspiration, and Beauty Tips — Good Housekeeping (https://www.goodhousekeeping.com/)\n- Homemade Air Freshener Recipes (https://www.diynatural.com/homemade-air-freshener/)\n\n## 사람들은 또한 다음에 관심이 있을 수 있습니다:\n\n- 무료 eBook 다운로드! 예산 없이 독서하는 최종 안내서\n- 조작 기술에 지친가요? 이 책은 새롭고 상쾌한 접근법을 제공합니다!\n- 최고의 창작자 도구 상자: 무료 자료 및 숨겨진 보석!\n- 꿈의 직장을 얻기 위한 7개의 무료 이력서 템플릿\n- 멋진 미니멀 로고 디자인을 위한 전문가 팁","ogImage":{"url":"/assets/img/2024-05-15-10CleverDIYHackstoSimplifyYourLife_0.png"},"coverImage":"/assets/img/2024-05-15-10CleverDIYHackstoSimplifyYourLife_0.png","tag":["Tech"],"readingTime":3},{"title":"Niels가 Rust 3을 배우다 - Hello world, 전역 상태 및 플래시 메모리","description":"","date":"2024-05-15 16:16","slug":"2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory","content":"\n\n이것은 내가 임베디드 Java 가상 머신을 Rust로 이식하면서 Rust 학습하는 여정을 문서화한 시리즈의 일부 3입니다.\n\n지난 단계 이후에는 Rust 코드를 타깃 플랫폼인 AVR 계열 임베디드 CPU인 특히 ATmega128을 위해 컴파일할 수 있게 되었으며, Avrora AVR 시뮬레이터에서 결과 이진 파일을 실행할 수 있게 되었습니다. 그러나 아직 출력을 표시할 수는 없었어요.\n\n이 게시물에서는 디버그 출력을 추가할 것입니다. 쉬운 작업일 줄 알았지만, 실제로는 Rust에 대해 더 많이 배우게 해주는 각각의 작은 단계들을 거쳐 많은 시간이 걸려버렸어요.\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_0.png)\n\n\n\n# 콘솔에 출력하기\n\n일반 Rust에서 콘솔에 출력하는 것은 쉽습니다: println!은 표준 출력에, eprintln!은 표준 에러 출력에 값을 출력합니다. 빠른 디버깅을 위해 dbg!()는 매우 편리합니다: 전달된 값으로 반환되며, 해당 값을 표준 에러에 출력합니다.\n\n```js\nfn main() {\n    let a = 42;\n    let b = dbg!(a);\n    println!(\"b = {}\", b);\n}\n```\n\n위 코드는 출력된다:\n\n\n\n```js\n[src/main.rs:3] a = 42 \nb = 42\n```\n\n첫 번째 줄은 stderr로 보내지고, 두 번째 줄은 stdout로 보냅니다.\n\n그러나 저는 임베디드 프로세서 클래스 AVR을 위한 VM을 개발 중입니다. AVR은 운영 체제가 없기 때문에 stdout 또는 stderr로 출력할 수 없습니다. 내 코드가 무엇을 하는지 확인하기 위해 어떻게 출력할 수 있을까요?\n\nCPU에는 출력을 전송할 수 있는 UART가 있습니다. 물리적인 보드를 사용하는 경우에는 좋은 방법일 수 있습니다. 그러나 이 경우에는 코드가 Avrora 시뮬레이터에서 실행되므로 다른 옵션이 있습니다: 잘 정의된 메모리 위치를 통해 출력합니다.\n\n\n\n# C-Print 모니터\n\nAvrora에는 AVR 상에서 코드의 실행을 추적할 수 있는 여러 '모니터'가 있습니다. 이전 게시물에서는 메모리 및 스택 모니터를 활성화하여 메모리 접근을 추적하고 스택 성장을 지켜냈습니다. 다른 모니터는 c-print입니다. 이 모니터는 특정 메모리 위치로의 쓰기를 모니터링하여 출력을 생성합니다.\n\n감시되는 위치는 단일 바이트입니다. 일부 작업에는 이게 충분합니다. 예를 들어, 0x0E를 쓰면 Avrora가 CPU 레지스터의 내용을 출력합니다.\n\n다른 작업은 매개변수를 필요로 합니다. 이는 다음 2 또는 4바이트에서 읽습니다. 예를 들어, 0x05를 마법의 위치에 쓰면 다음 4바이트의 내용이 부호 없는 32비트 정수(little endian)로 출력됩니다.\n\n\n\n아브로라는 특정 심볼인 debugbuf1을 찾아 메모리 위치를 모니터링합니다.\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_1.png)\n\n이 출력물은 이 게시물의 완성된 코드에서 가져온 것입니다. 아브로라는 주소 0x0210에 있는 바이트를 모니터링하고, 코드가 해당 주소에 쓰기를 할 때 c-print 모니터를 트리거합니다.\n\n위의 출력물에서 표시된 매우 높은 주소에 유의하십시오. 하버드 아키텍처인 AVR은 코드(플래시 메모리)와 RAM을위한 별도의 주소 공간을 갖습니다. binutils toolchain은 이것을 위한 것이 아니었지만, 이를 해결하기 위해 avr 버전은 RAM을 0x00800000 또는 8MB에서 시작하는 주소에 매핑하여 AVR CPU의 플래시 용량을 훨씬 초과하는 곳에 위치시킵니다.\n\n\n\n# Rust에서의 전역 상태\n\n우리는 쓸 수 있는 전역 변수가 필요합니다. 프로그래밍 기초 강의에서는 전역 가변 상태가 좋지 않다고 말하지만, 때로는 그런 유효한 이유가 있고, 이것은 그 중 하나입니다.\n\nRust에서 어떻게 전역 가변 상태를 가질 수 있을까요? Rust는 안전을 중요시하며, 전역 가변 상태는 본질적으로 안전하지 않습니다.\n\n우리는 변수를 다음과 같이 선언할 수 있습니다. static mut debugbuf1: [u8; 5] = [0, 0, 0, 0, 0];. 이는 작동하지만 Rust는 누구든 접근할 수 있기 때문에 그 안전을 보장할 방법이 없습니다. AVR에는 다중 스레드 운영 체제가 없습니다(실제로는 있지만, 이 경우에는 해당되지 않음).\n\n\n\n만약 Rust가 코드가 안전하다고 증명하지 못하면, 우리에게 명시적으로 알려주길 원합니다. 따라서 우리는 static mut 변수를 선언할 수 있지만, 그것에 접근하는 유일한 방법은 unsafe ' ... ' 블록 내에서입니다.\n\n# std::sync::RwLock\n\n위의 방법은 AVR에 사용할 것이지만, 이 블로그는 Rust 학습에 관한 것이므로 때로는 이 프로젝트에는 작동하지 않을 수도 있는 내용을 좀 순회해볼 겁니다. \n\nRust에서 전역 상태에 대한 기사를 찾아보다가 흥미로운 내용을 많이 설명한 이 작은 보석을 발견했습니다. 자세히 검토해보고 다시 돌아와야 할 것이지만, 한 댓글이 눈에 띄었습니다:\n\n\n\n이것이 궁금해졌어요. 안전한 패턴들은 무엇일까요? 좀 더 찾아보니 표준 라이브러리에 두 가지 유용한 도우미 클래스가 있다는 걸 알았어요: std::sync::Mutex`T` 와 std::sync::RwLock`T`.\n\n그들을 사용하는 방법은 상당히 간단해요. Mutex는 RwLock과 비슷하지만 리더와 라이터를 구별하지 않아요. RwLock가 어떻게 작동하는지 보여드리겠어요:\n\n```js\nuse std::sync::RwLock;\n\nstatic LOCK: RwLock<u8> = RwLock::new(5);\n\nfn main() {\n    // 여러 리더 락을 동시에 보유할 수 있어요\n    {\n        let r1 = LOCK.read().unwrap();\n        let r2 = LOCK.read().unwrap();\n        assert_eq!(*r1, 5);\n        assert_eq!(*r2, 5);\n    } // 이 시점에서 리더 락이 해제돼요\n\n    // 그러나 한 번에 한 개의 라이터 락만 보유할 수 있어요\n    {\n        let mut w = LOCK.write().unwrap();\n        *w += 1;\n        assert_eq!(*w, 6);\n    } // 이 시점에서 라이터 락이 해제돼요\n\n    // 따라서 이 블록에서 새로운 라이터 락을 얻을 수 있지만,\n    // 이 두 블록을 하나로 합치면 데드락이 발생할 수 있어요\n    {\n        let mut w2 = LOCK.write().unwrap();\n        *w2 += 1;\n        assert_eq!(*w2, 7);\n    } //\n\n    println!(\"Done.\");\n}\n```\n\nPlayground에서 시도해보세요!\n\n\n\n리소스를 `RwLock`으로 래핑하면 해당 리소스에 대해 여러 읽기 전용 참조 또는 단일 변경 가능한 참조를 얻을 수 있습니다.\n\nVS Code를 사용할 때 유형 힌트를 통해 `read()` 또는 `write()`를 호출할 때 단순히 `u8`에 대한 참조(&u8 또는 mutable &mut u8)가 아니라 `RwLockReadGuard<u8>` 또는 `RwLockWriteGuard<u8>`를 얻을 수 있습니다:\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_2.png)\n\n다시 한 번, Rust의 좋은 점은 쉽게 구현을 살펴볼 수 있다는 것입니다.\n\n\n\n구현 세부 사항은 아직 조금 어려워 보이지만, 두 가지가 모두 Deref`T` 트레이트 (여기서 T는 u8)를 구현한다는 사실은 명백합니다. 이것이 Rust가 우리가 그들을 &u8로 암시적으로 변환할 수 있도록 허용하는 이유입니다. 또한 write 가드는 DeferMut`T`도 구현하므로 그것을 &mut u8로 변환할 수 있습니다.\n\n같은 파일에서 두 가지가 모두 Drop 트레이트를 구현하고 있고, drop() 함수는 read_unlock() 또는 write_unlock()를 호출하여 잠금을 해제합니다. Rust는 값이 스코프를 벗어나는 즉시 값을 삭제할 것이기 때문에 중간 블록의 끝에 w 잠금이 해제되는 것입니다. 두 블록을 병합하면 데드락이 발생할 수 있지만, 두 번째 쓰기 잠금을 요청하기 전에 명시적으로 drop(w);를 추가하면 데드락을 방지할 수 있습니다.\n\n이러한 트레이트의 구현은 모두 안전하지 않은 ' ' 코드입니다. 실제로 안전하지 않지는 않습니다. 안전하지 않은 것은 사실 \"확인되지 않은\"을 의미합니다: 컴파일러가 코드가 안전하다고 확신할 수 없지만, 안전하다면 그것을 사용하는 모든 코드도 안전할 것입니다.\n\n그러나 이 코드가 경쟁 조건을 피하기 때문에 안전하다는 점은 중요하지만, 데드락을 방지하지는 않으며 빠르기 때문에 되어야 하는 것도 아닙니다. 잠금이 해제될 때까지 기다리는 것은 성능 병목 지점이 될 수 있기 때문에 공유 상태는 항상 복잡합니다.\n\n\n\n# libcore과 libstd\n\n이것은 안전하지 않은 코드를 피하기 위한 좋은 대안으로 보였지만 안탤레 이비이어에선 사용할 수 없습니다. 사용한 템플릿에서 생성된 코드의 첫 줄이 그 이유입니다: #![no_std] (느낌표는 해당 속성이 전체 크레이트에 적용됨을 나타냅니다).\n\nno_std는 Rust에게 std 크레이트에 연결되지 말 것을 알려주므로 std::에 있는 것은 사용할 수 없습니다. 오직 core 크레이트만 사용할 수 있습니다.\n\n문서에서는 이것을 명확하게 설명합니다: libcore와 libstd는 두 가지 주요 크레이트입니다. 첫 번째인 libcore는 항상 사용 가능하며, 원시 타입, Option`T`와 Result`T,E`와 같은 기본 언어 구조, assert 등이 들어 있습니다.\n\n\n\n두 번째로 libstd는 훨씬 더 크며 운영 체제에 의존하는 많은 구성을 포함하고 있습니다. 이것은 std/src/lib.rs 소스를 확인하여 libcore의 상위 집합임을 알 수 있습니다.\n\n```js\n#[stable(feature = \"rust1\", since = \"1.0.0\")]\npub use core::any;\n#[stable(feature = \"core_array\", since = \"1.36.0\")]\npub use core::array;\n#[unstable(feature = \"async_iterator\", issue = \"79024\")]\npub use core::async_iter;\n....\n```\n\n이것은 러스트에서 흔한 패턴이며, pub use는 다른 크레이트나 모듈에서 기능을 가져와 현재 위치에서 다시 내보내는 역할을 합니다. 요약하면 libcore로 내보내는 모든 것이 libstd로도 내보내집니다.\n\n이를 알고 있으니, atomicity와 안전을 보장하기 위해 일부 운영 체제 원시 개념에 의존하는 것이 필요한 RwLock이 libstd에 존재하는 것이 이치에 맞습니다. 유감스럽게도 AVR에는 운영 체제가 없으므로 이 프로젝트에는 RwLock이 사용되지 않습니다.\n\n\n\n# AVR에서의 전역 상태\n\n`RwLock<T>`를 사용할 수 없기 때문에 static mut debugbuf1 변수를 사용해야 합니다. 먼저 모든 레지스터를 출력하는 매개변수가 없는 간단한 옵코드를 고려해 봅시다.\n\n첫 번째 시도는 다음과 같을 수 있습니다:\n\n```js\n#[allow(non_upper_case_globals)]\nstatic mut debugbuf1: u8 = 0;\n\nconst AVRORA_PRINT_REGS: u8 = 0xE;\n\n#[arduino_hal::entry]\nfn main() -> ! {\n    loop {\n        unsafe {\n            debugbuf1 = AVRORA_PRINT_REGS;\n        }\n        arduino_hal::delay_ms(1000);\n    }\n}\n```\n\n\n\n`#[allow(non_upper_case_globals)]`는 경고를 억제합니다. Rust는 전역 변수에 대문자 이름을 사용하기를 선호합니다.\n\n테스트하기 위해 .cargo/config.toml에 c-print 모니터를 추가하고 다른 것들을 제거하세요:\n\n```js\n[target.'cfg(target_arch = \"avr\")']\nrunner = \"java -jar ../avrora/avrora-beta-1.7.117.jar -monitors=c-print -single -platform=mica2\"\n```\n\n안타깝게도, 이것은 작동하지 않습니다:\n\n\n\n\n![Image](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_3.png)\n\nAvrora는 Rust의 관점에서 debugbuf1이름을 찾을 수 없습니다. 아무도 해당 값을 읽지 않아서 최적화기가 완전히 제거했기 때문입니다. 우리는 C의 volatile 키워드와 동등한 것이 필요합니다.\n\n다행히 Rust에는 정확히 그 역할을 하는 함수가 있습니다.\n\n참조(&mut) 대신 *mut을 사용해야 합니다. 참조가 아닌 원시 포인터가 필요합니다. 원시 포인터는 본질적으로 C 포인터이며, 이를 역참조하는 것은 안전하지 않습니다.\n\n\n\n\n최초에는 이렇게 시도해 봤어요. 왜냐하면 참조인 &T가 자동으로 *T로 변환될 수 있거든. 해제된 포인터를 사용하는 건 안전하지 않지만, 만드는 건 괜찮아요.\n\n```js\nuse core::ptr::write_volatile;\n\n#[allow(non_upper_case_globals)]\nstatic mut debugbuf1: u8 = 0;\n\nconst AVRORA_PRINT_REGS: u8 = 0xE;\n\n#[arduino_hal::entry]\nfn main() -> ! {\n    loop {\n        unsafe {\n            write_volatile(&mut debugbuf1, AVRORA_PRINT_REGS);\n        }\n        arduino_hal::delay_ms(1000);\n    }\n}\n```\n\n컴파일은 잘 되었지만, Rust는 아직 좀 불만스러운 모습이에요:\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_4.png)\n\n\n\n최근 Rust 컴파일러에서 정적 데이터에 대한 참조는 경고가 발생하며, 2024 년 언어 버전에서는 오류가 될 것입니다 (이 문제에 대해서는 깃허브의 해당 이슈를 참조하세요). 보통 컴파일러는 문제를 해결하는 방법을 알려주므로 도움이 많이 됩니다: addr_of_mut!()은 우리가 참조를 먼저 만들지 않고도 변경 가능한 원시 포인터를 제공해줍니다.\n\n제 생각에는 개발자가 원시 포인터로 작업하도록 강제함으로써 변경 가능한 정적 데이터가 안전하지 않으며 추가 주의가 필요하다는 것을 더 명확하게 보여주는 것 같습니다.\n\n이것이 도움이 되었고, 코드는 이제 경고 없이 컴파일되지만 Avrora는 아직 그것을 찾지 못합니다:\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_5.png)\n\n\n\nRust는 두 개의 블록이 동일한 이름을 사용하여 정적을 정의해도 결과 바이너리에서 유일하게 만듭니다. 우리는 이를 방지하기 위해 #[no_mangle] 속성을 추가할 수 있습니다 (컴파일러가 이름을 난잡하게 만드는 것을 방지하는 것 외에도 여러 다른 효과가 있습니다).\n\n작동합니다, 우리는 드디어 (아직은 다소 지루한) 출력이 있습니다!\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_6.png)\n\n# Avrora에 매개변수 전달하기\n\n\n\nc-프린트 모니터에는 레지스터나 심지어 VM 내부의 힙 내용을 출력하는 매개변수가 필요하지 않은 디버그 프린트 명령이 여럿 있습니다. 다른 명령은 2바이트 또는 4바이트의 매개변수를 사용하는데요, 이전에 정의한 static을 확장하여 16비트 값을 16진수로 출력해 봅시다:\n\n```js\n#[no_mangle]\n#[allow(non_upper_case_globals)]\nstatic mut debugbuf1: [u8; 5] = [0, 0, 0, 0, 0];\n\nconst AVRORA_PRINT_2BYTE_HEXADECIMALS: u8 = 0x1;\nconst AVRORA_PRINT_REGS: u8 = 0xE;\n\n#[arduino_hal::entry]\nfn main() -> ! {\n    loop {\n        unsafe {\n            write_volatile(addr_of_mut!(debugbuf1[0]), AVRORA_PRINT_REGS);\n\n            let val = 0x42FF;\n            write_volatile(addr_of_mut!(debugbuf1[1]), val as u8);\n            write_volatile(addr_of_mut!(debugbuf1[2]), (val >> 8) as u8);\n            write_volatile(addr_of_mut!(debugbuf1[0]), AVRORA_PRINT_2BYTE_HEXADECIMALS);\n        }\n\n        arduino_hal::delay_ms(1000);\n    }\n}\n```\n\n![image](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_7.png)\n\n출력이 다소 날 것 같지만 완벽하게 작동합니다.\n\n\n\n이상하게도, debugbuf1이 이제 5바이트 배열이 되었을 때 컴파일러는 addr_of_mut!를 추가하기 전에 보았던 경고를 더 이상 표시하지 않았습니다. 경고 없이 &mut debugbuf1[0]에 직접 쓸 수 있게 되었습니다.\n\n잠시 헷갈렸지만, Stack Overflow에서 물어본 결과 이것은 linter의 제한 사항이라는 사실이 밝혀졌습니다. 이 문제는 곧 수정될 것으로 보이니, 계속해서 addr_of_mut!를 사용하는 것이 좋을 것 같습니다.\n\n부호 있는 8, 16 또는 32 비트값에 대한 다른 변형도 동일한 패턴을 따릅니다.\n\n# ‘avrora’ 모듈\n\n\n\nVM을 개발할 때, 계속해서 이러한 줄을 복사하고 붙여넣기하고 싶지 않아요. 시뮬레이터와 상호 작용할 모든 편리한 기능을 가진 모듈이 있는 것이 훨씬 좋을 것 같아요 (나중에 다양한 추적 호출이 추가될 것입니다). 그래서 모듈을 만들어봅시다.\n\navrora.rs 또는 avrora/mod.rs라는 파일을 만들어 모듈을 만들 수 있어요. 다른 종류의 출력 명령을 추가하기가 더 쉬워지도록 일부 리팩터링을 포함하여, 다음과 같이 보입니다:\n\n```js\nuse core::ptr::{addr_of_mut, write_volatile};\n\nconst AVRORA_PRINT_2BYTE_HEXADECIMALS: u8 = 0x1;\nconst AVRORA_PRINT_REGS: u8 = 0xE;\n\n#[allow(non_upper_case_globals)]\n#[no_mangle]\nstatic mut debugbuf1: [u8; 5] = [0, 0, 0, 0, 0];\n\nfn signal_avrora_c_print(instruction: u8) {\n    unsafe {\n        write_volatile(addr_of_mut!(debugbuf1[0]), instruction);\n    }\n}\n\nfn signal_avrora_c_print_16(instruction: u8, payload: u16) {\n    unsafe {\n        write_volatile(addr_of_mut!(debugbuf1[1]), payload as u8);\n        write_volatile(addr_of_mut!(debugbuf1[2]), (payload >> 8) as u8); \n        write_volatile(addr_of_mut!(debugbuf1[0]), instruction);\n    }\n}\n\n/// Avrora의 c-print 모니터를 사용하여 16 비트 부호 없는 정수를 16 진수로 출력합니다.\n#[allow(dead_code)]\npub fn print_u16_hex(val: u16) {\n    signal_avrora_c_print_16(AVRORA_PRINT_2BYTE_HEXADECIMALS, val);\n}\n\n/// Avrora의 c-print 모니터를 사용하여 R0부터 R31 레지스터의 내용을 출력합니다.\n#[allow(dead_code)]\npub fn print_all_regs() {\n    signal_avrora_c_print(AVRORA_PRINT_REGS);\n}\n```\n\n`allow(dead_code)`는 특정 디버그 출력을 사용하지 않을 때 컴파일러가 불평을 제기하지 않도록 필요합니다. 사용되지 않으면 해당 코드가 최종 이진 파일에 포함되지 않습니다.\n\n\n\n이렇게 하면 주 파일이 깔끔해지고 안전하지 않은 코드가 전혀 없어집니다:\n\n```js\n#![no_std]\n#![no_main]\n\nuse panic_halt as _;\n\nmod avrora;\n\n#[arduino_hal::entry]\nfn main() -> ! {\n    loop {\n        avrora::print_all_regs();\n        avrora::print_u16_hex(0x42FF);\n\n        arduino_hal::delay_ms(1000);\n    }\n}\n```\n\n모듈을 더 확장하여 8, 16 및 32 비트 부호있는 또는 부호없는 숫자를 인쇄하는 것은 간단하지만, 문자열을 인쇄하는 것은 약간의 트릭이 필요합니다.\n\n# 안녕, 세상! (드디어)\n\n\n\nAvrora의 c-print 모니터에는 문자열을 출력하는 여러 방법이 있습니다. 가장 쉬운 방법은 매우 간단합니다:\n\n```js\n/// Uses Avrora's c-print monitor to print a string from RAM\n#[allow(dead_code)]\npub fn print_ram_string(s: &str) {\n    signal_avrora_c_print_16(AVRORA_PRINT_STRING_POINTERS, s.as_ptr() as u16);\n}\n```\n\n우리는 문자열 슬라이스에 대한 참조를 전달하고, `.as_ptr()`를 사용하여 주소를 사용합니다. 이를 16비트 정수로 변환하여 모니터 호출의 매개변수로 사용합니다.\n\n이를 통해 main.rs에서 `avrora::print_ram_str(\"Hello, world!\");`를 실행할 수 있으며, 마침내 작동하는 \"Hello, world!\": 🎉🎉🎉을 얻을 수 있습니다.\n\n\n\n이미지 태그를 다음과 같이 변경하면 되지만, 이 접근 방식에 문제가 있습니다:\n\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_8.png)\n\n\nRust는 문자열을 RAM에 있는 .data 섹션에 배치했습니다. 대부분의 플랫폼에서는 이 접근 방식이 괜찮지만 AVR에서는 RAM이 매우 소귀한 자원입니다. ATmega128는 RAM이 4KB밖에 되지 않지만 플래시 메모리는 128KB나 됩니다. 코드는 플래시에서 실행되며, 데이터는 일반적으로 RAM에 저장되지만 데이터가 상수인 경우에는 플래시에 저장하는 것이 좋습니다.\n\n\n\nAVR에서 플래시 메모리에서 읽어오는 것에는 작은 성능 손실이 있습니다: 프로그램 메모리를 로드하는 LPM 명령어는 일반적인 RAM에서의 LD 로드에 비해 1 또는 2 사이클이 아닌 3 사이클이 소요됩니다. 그러나 이 경우 시뮬레이터에서만 문자열을 읽기 때문에 그렇게 중요하지 않습니다.\n\n플래시에서 출력하는 호출은 거의 동일하지만 다른 몤령어와 32비트 주소가 필요합니다. 플래시 주소 범위가 더 크기 때문입니다.\n\n하지만 문제는 문자열을 플래시에 어떻게 넣을 수 있을까요? 우리가 이 작업을 수행할 수 있는 avr_progmem라는 크레이트가 있습니다.\n\n먼저, Cargo를 사용하여 이를 설치하세요:\n\n\n\n\n![image](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_10.png)\n\n이것으로 새로운 매크로 progmem!을 얻을 수 있습니다. 이 매크로는 데이터를 플래시에 배치하는 데 사용할 수 있습니다. 이 크레이트는 어셈블리를 사용하지 않고도 플래시 메모리에서 데이터를 읽을 수 있는 편리한 방법도 제공합니다. 다음과 같이 데이터를 플래시에 넣을 수 있습니다:\n\n```js\nprogmem! {\n    static progmem string HELLO = \"Hello, World!\\0\";\n}\navrora::print_flash_string(HELLO.as_bytes().as_ptr() as u32);\n```\n\nstatic HELLO는 PmString`_`이 되며, 여기서 우리는 바이트에 대한 참조인 &ProgMem`[u8; _]`을 얻을 수 있습니다. 이를 통해 원시 *const T 포인터를 얻을 수 있습니다. 그것은 주소를 포함하는 u32로 캐스트할 수 있습니다. Avrora는 널 종결된 문자열을 예상하기 때문에 \\0가 문자열을 종료하는 데 필요합니다. 그렇지 않으면 첫 번째 \\0을 만날 때까지 쓰레기를 출력할 것입니다.\n\n\n\n\n# print_flash_string 매크로.\n\n이 방법은 동작하지만 디버그용으로는 상당히 길어요. avrora::print_flash_string(\"Hello, World!\")처럼 간단히 작성할 수 있다면 좋을텐데요.\n\n우리는 일반적인 러스트 함수로는 이것을 할 수 없지만 매크로로 거의 동일한 결과를 얻을 수 있습니다:\n\n```js\n/// Avrora의 c-print 모니터를 사용하여 flash 메모리에서 문자열을 출력합니다.\n#[allow(unused_macros)]\n#[macro_export]\nmacro_rules! print_flash_string {\n    ($s:expr) => { {\n        use avr_progmem::progmem;\n        progmem! {\n            static progmem string AVRORA_PROGMEMSTRING = concat!($s, \"\\0\");\n        }\n        avrora::print_flash_string(AVRORA_PROGMEMSTRING);\n    } };\n}\n\n/// Avrora의 c-print 모니터를 사용하여 flash 메모리에서 문자열을 출력합니다.\n/// \n/// 이 함수는 print_flash_string! 매크로에 의해 호출되어 편리하게\n/// 문자열을 flash 메모리에 저장하고 필요한 PmString을 생성합니다.\n#[allow(dead_code)]\npub fn print_flash_string<const N: usize>(string_in_progmem: PmString<N>) {\n    signal_avrora_c_print_32(\n        AVRORA_PRINT_FLASH_STRING_POINTER,\n        string_in_progmem.as_bytes().as_ptr() as u32);\n}\n```\n\n\n\n매크로에 대해 배울 점이 많지만, 지금까지 'Programming Rust'에서 매크로 챕터로 건너뛰어서 이 작업을 하기에 충분한 지식을 얻었습니다.\n\nRust의 매크로는 컴파일 프로세스 초기에 실행됩니다. 이는 함수와 달리 매크로를 선언할 때 pub 대신 #[macro_export]를 추가해야 하는 이유입니다. 이 단계에서 pub의 전체 개념은 아직 의미가 없습니다. 불행히도, 이는 다른 매크로를 avrora::을 통해 액세스할 수 없다는 것을 의미합니다.\n\n매크로는 C에서와 같이 평문이 아닌 토큰 스트림에서 작동합니다. 매크로 정의에는 정규식과 유사한 방식으로 일치하는 하나 이상의 케이스가 포함됩니다. 그런 다음 매크로가 해당 본문으로 확장됩니다.\n\n매크로는 매크로가 예상하는 토큰 종류를 나타내는 '디자이네이터'로 특정 부분의 토큰 스트림을 캡처합니다. 예를 들어 ($s:expr)는 단일 표현식과 해당 값을 $s에 저장합니다.\n\n\n\nprint_flash_string 매크로는 프로그램 메모리에 static AVRORA_PROGMEMSTRING을 정의한 후 print_flash_string 함수를 호출하여 출력하는 블록으로 확장됩니다.\n\n작은 따옴표 사이에 이중 중괄호를 주목하세요: '' ... ''. 외부 중괄호는 매크로 케이스의 확장을 구분하며 결과 코드에 포함되지 않습니다. 내부 중괄호는 매크로의 확장을 새 블록으로 만들어 준 코드를 매크로가 사용된 코드에 직접 확장하는 대신에 포함합니다.\n\n왜 이것이 중요한가요? 추가 중괄호가 없으면 두 번의 print 문이 연속으로 실행되면 동일한 블록에서 두 개의 AVRORA_PROGMEMSTRING static을 정의하고 이름이 충돌합니다. 생성된 코드를 중괄호로 감싸면 충돌하지 않게 되고 Rust의 이름 맹글링은 최종 이진 파일에서 고유한 심볼을 가져주기 때문에 이러한 충돌이 발생하지 않습니다:\n\n<img src=\"/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_11.png\" />\n\n\n\n이제 매크로를 다른 프린트 함수처럼 사용할 수 있습니다:\n\n```js\nprint_flash_string!(\"Hello, World from flash memory!\");\n```\n\n![이미지](/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_12.png)\n\n유일한 차이점은 매크로가 직접적으로 네임스페이스에서 사용 가능하며 avrora::로 접두사를 붙일 수 없다는 것입니다. 새로운 개발자로서 avrora::를 입력하고 IDE에서 무엇이 나타나는지 확인하는 것이 가능한 옵션을 찾는 데 도움이 되기 때문에 이것은 안타까운 부분입니다.\n\n\n\n이것이 매크로와 함수의 이름을 동일하게 유지하고 함수의 삼중 슬래시 문서화가 사용자를 해당 매크로로 안내하는 주요 이유였습니다.\n\n또한 사용된 코드 내에서 매크로가 확장되기 때문에 avrora 모듈의 비공개 함수에 액세스할 수 없습니다. 따라서 매크로가 호출할 수 있는 공개 진입점이 필요하며, 이것이 가장 깔끔한 방법으로 보였습니다.\n\n# 요약\n\n이 일은 예상보다 훨씬 많은 단계가 필요했지만, 많은 흥미로운 것들을 배울 수 있었습니다:\n\n\n\n- static mut 데이터와 unsafe 코드\n- stdcore와 stdlib의 관계\n- write_volatile 및 addr_of_mut\n- no_mangle 속성\n- 모듈 만들기\n- 적절한 경우에 경고 억제하는 방법\n- Raw 포인터\n- 데이터를 플래시 메모리에 넣기 위해 avr_progmem 사용\n- 간단한 매크로 작성하는 방법\n\n이번 단계의 코드 상태는 일반적으로 Github에서 확인할 수 있습니다.\n\n2024년 5월 10일에 nielsreijers.github.io에서 최초 발행됨.","ogImage":{"url":"/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_0.png"},"coverImage":"/assets/img/2024-05-15-NielslearnsRust3Helloworldglobalstateandflashmemory_0.png","tag":["Tech"],"readingTime":16}],"page":"88","totalPageCount":154,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}