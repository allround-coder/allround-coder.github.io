{"pageProps":{"posts":[{"title":"가방 속의 고양이","description":"","date":"2024-05-16 17:28","slug":"2024-05-16-CatinaBag","content":"\n\n제 최종 프로젝트는 '가방 안 고양이(Cat in a Bag)' 라는 게임입니다. 이 게임의 아이디어는 제 룸메이트가 기르는 퍼시라는 이름의 고양이에서 영감을 받았어요. 퍼시는 가방을 좋아하는 회색 고양이예요. 그녀가 가장 좋아하는 가방은 녹색 퍼블릭스 가방인데, 그 속에 숨어들어 가끔 마주쳤었죠. 아래에서 진짜 퍼시가 가방 안에 있는 모습을 보실 수 있어요!\n\n이 게임의 목표는 조이스틱을 이용하여 퍼시 스프라이트를 제어하고 화면 주위를 움직이는 녹색 가방 스프라이트를 성공적으로 클릭하는 것이에요. 화면 주위에는 여러 다른 색의 가방 스프라이트들이 많이 튀어다니는데, 그 중에서 녹색 가방만 움직이죠. 플레이어들은 올바른 가방을 클릭하기 위해 30초와 3번의 시도 기회를 가집니다. 모든 시도 기회를 소진하거나 시간이 다 되면 게임에 패배합니다. 시간 내에 녹색 가방을 클릭할 수 있다면 이기는 거죠.\n\n이 프로젝트를 만들기 위해 아두이노 프로그램을 작성하여 조이스틱 입력을 p5 JavaScript 프로그램으로 전송하고, JavaScript 프로그램은 다시 정보를 아두이노로 전송하여 3개의 LED 라이트를 제어하도록 했어요. 또한, 아두이노가 p5 프로그램에 연결되면 시작되는 배경 음악을 위한 Tone.js 사운드 시퀀스를 만들었습니다. LED 라이트는 올바른 가방을 클릭할 시도 횟수를 나타냅니다. 틀린 가방을 클릭할 때마다 라이트 중 하나가 꺼지게 됩니다. 플레이어가 올바른 가방을 클릭할 수 있으면, 라이트가 몇 번 깜빡이고 다른 음악 시퀀스가 재생됩니다. 게임의 시작할 때는 화면에 게임 방법을 설명하는 텍스트가 나타납니다. 게임에서 이기면 승리 화면으로 전환되고, 그렇지 않으면 시도 기회가 모두 소진되었다는 안내가 표시됩니다. 모든 스프라이트 그래픽은 Piskel에서 그리고 스프라이트 시트로 내보냈어요.\n\n![Cat in a Bag](/assets/img/2024-05-16-CatinaBag_0.png)\n\n<div class=\"content-ad\"></div>\n\n\n![CatinaBag_1](/assets/img/2024-05-16-CatinaBag_1.png)\n\n![CatinaBag_2](/assets/img/2024-05-16-CatinaBag_2.png)\n\nBelow is a video of my project in action.\n\nFor future development, I think it would be cool to add more Arduino elements, such as buzzers or photoreceptors, to enhance the game with additional features. Instead of just moving a sprite on the screen, players could control sprite speed or collect power-ups. Implementing a points system would make the game more competitive, so players could strive to beat their own high scores in addition to beating the clock. Some aspects of the current game can be refined for more consistent gameplay. For example, if two wrong bags are clicked simultaneously, the Arduino might struggle to process the request to turn off two LEDs at the same time.\n\n\n<div class=\"content-ad\"></div>\n\n전반적으로 내 프로젝트 결과물에 매우 만족하고 만들기 위해 들인 모든 노력을 배우는 것을 즐겼어요. 'Cat in a Bag' 게임 즐기시길 바랍니다!","ogImage":{"url":"/assets/img/2024-05-16-CatinaBag_0.png"},"coverImage":"/assets/img/2024-05-16-CatinaBag_0.png","tag":["Tech"],"readingTime":2},{"title":"부가 제조를 위한 3D 스캐닝 소개","description":"","date":"2024-05-16 17:26","slug":"2024-05-16-AnIntroductionto3DScanningforAdditiveManufacturing","content":"\n\n![3D 스캔을 통한 추가 제조에 대한 소개](/assets/img/2024-05-16-AnIntroductionto3DScanningforAdditiveManufacturing_0.png)\n\n3D 스캔이 어떻게 하면 물리적 물체를 신속하고 정확하게 디지털화하여 추가 제조가 간편해지는 데 도움을 줄 수 있는지에 대한 입문 가이드입니다.\n\n추가 제조, 일명 3D 프린팅은 디지털 파일로부터 3D 물체를 생성하는 것을 가능하게 합니다. 이러한 추가 제조 공정의 전체 잠재력을 활용하기 위해서는 먼저 복제하거나 수정하려는 물리적 물체의 정확한 디지털 모델을 보유해야 합니다.\n\n3D 스캔은 정확한 표면 세부 정보까지 물체를 빠르고 경제적인 방식으로 디지털화할 수 있는 방법을 제공합니다. 이는 스캔되는 항목에 접촉이나 손상이 없는 상태로 치수와 표면을 캡처합니다. 그런 다음 이 3D 스캔 데이터는 3D 모델 파일로 시각화, 분석 또는 3D 프린팅 방법을 사용한 추가 제조와 같은 다른 디지털 3D 모델 파일과 같이 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n변형 기능은 기존 개체를 역공학하여 건축 측정 및 CAD 도면을 처음부터 만들 필요없이 복제, 수정 또는 디지턈 보관할 수 있도록 해줍니다. 3D 스캐닝은 가장 복잡한 제품 기하학에 대해서도 더 빠르고 정확한 복제 프로세스를 제공합니다.\n\n저렴한 3D 스캐닝 하드웨어 및 소프트웨어를 추가하여 첨단 제조 계획을 실현하려면 준비가 되셨나요? 이 초보자용 안내서를 통해 이 기술을 이해하기 시작해보세요.\n\n목차\n\n* 3D 스캐닝을 추가 제조에 사용하는 이유\n* 3D 스캐닝 프로세스는 어떻게 작동하나요?\n* 3D 스캐닝 기술과 방법 개요\n* 추가 제조를 위해 3D 스캔 활용하기\n* 3D 스캔을 이용한 주요 포인트들\n\n<div class=\"content-ad\"></div>\n\n# 왜 부가 제조에 3D 스캔을 사용해야 할까요?\n\n부가 제조는 물체 제작의 디지턈 방법을 약속하지만, 실제 입력물은 여전히 창조과정에서 필요합니다. 3D 스캔 기술을 사용하는 것은 다음과 같은 주요 이점을 제공합니다:\n\n- 속도 – 시간이 오래 걸리는 물리적 측정 대신 몇 분 안에 정확한 디지털 파일을 무료 3D 스캔으로 얻을 수 있습니다.\n- 비용 – 신제품 디자인 비용에 비해 빠른 스캔으로 지출을 줄일 수 있습니다.\n- 정확도 – 복잡한 측정치가 고해상도 메시 파일로 변환되어 정밀한 출력을 가능하게 합니다.\n- 손상 없음 – 파괴되지 않는 스캔을 통해 디지턀화 과정 중에 기존 물체의 상태와 사용을 보존합니다.\n\n부가 제조 워크플로우의 강화된 성능 혜택을 고려한다면, 저렴한 스캔 툴을 통합하는 것은 쉬운 선택입니다. 모든 소스 물체가 디지털 본들로만 다루어지며, 디지털화 후에 훼손없이 재사용됩니다. 이후 엑세스 가능한 3D 인쇄용 파일 포맷을 통해 하부 공정에서 조작되거나 복제됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 3D 스캐닝 프로세스는 어떻게 작동하나요?\n\n3D 스캐닝은 주로 레이저나 다른 광학 스캔 기술을 사용하여 물리적 물체의 치수를 닿지 않고 디지털 데이터 파일 형태로 완전히 캡처합니다. 주요 단계는 다음과 같습니다:\n\n- 스캐너가 레이저 스트라이프와 같은 스캔 요소 방향으로 객체로 쏜다\n- 스캐너 센서가 반사된 크기, 모양 및 때로는 색상 데이터를 감지\n- 소프트웨어가 이러한 스캔을 완전한 3차원 디지털 메쉬로 처리하여 표면을 재현\n- 메쉬 후처리가 최종 모델을 생성하여 3D 프린팅이나 시각화 용도로 형식화\n\n다양한 3D 스캐너 장비 모드는 다양한 좌표 측정 방법으로 다양한 접근 방식을 취하지만, 핵심 스캐닝 프로세스는 물체를 정확하게 디지털화하는데 이러한 4 가지 필수 단계를 따릅니다.\n\n<div class=\"content-ad\"></div>\n\n\n![An Introduction to 3D Scanning for Additive Manufacturing](/assets/img/2024-05-16-AnIntroductionto3DScanningforAdditiveManufacturing_1.png)\n\nThe resulting digital file enables further design work including model repair, editing, ready-to-print conversions or even parametric builds based off the captured source. This delivers extensive flexibility for your additive manufacturing methods once you have scanned your object’s mesh source data.\n\n# Overview of 3D Scanning Techniques and Methods\n\nVarious approaches and types of 3D scanning hardware have emerged leveraging common technologies adapted for real-world applications. Each method comes with its own pros and cons targeting specific uses which dictate scanner type selections.\n\n\n<div class=\"content-ad\"></div>\n\n가장 일반적인 3D 스캐닝 메커니즘 범주를 간단히 소개하겠습니다. 스캐너 하드웨어 기술은 기능 세트에 영향을 줍니다. 3D 스캔 기능을 추가할 때 대부분의 고려 사항 중 하나가 되는 방법 선택:\n\n- 레이저 삼각측량 – 스캔된 표면에 레이저 점이나 선이 투사되고 전문 카메라가 레이저의 변위에 따른 깊이를 계산합니다. 작은 개체 크기에 대한 자세한 스캔 데이터를 제공합니다.\n- 구조빛 – 점, 선 또는 그리드의 패턴을 투사하고 카메라로 왜곡을 감지하여 세부 사항을 캡처합니다. 고해상도 결과를 가진 중대형 개체에 적합합니다.\n- 포토그래메트리 – 소프트웨어를 통해 다양한 각도에서 촬영한 2D 사진 이미지 패턴을 처리하여 완전한 3D 모델을 재구성합니다. 대규모 개체 스캔을 가능하게 하는 더 매뉴얼한 프로세스입니다. 경우에 따라 스마트폰 카메라를 활용할 수 있습니다.\n- 접촉 기반 방법 – 정밀한 포인트 위치를 기록하기 위해 기계 프로브를 사용하여 측정 표면 전체에서 레코드합니다. 해결된 높은 정확도 트레이드 오프는 더 긴 스캔 시간이 필요합니다.\n\n추가로 특수화된 스캐너는 메트로로지 검사부터 자율 주행 차량 깊이 매핑에 이르기 까지 다양한 응용 분야를 위해 X-선, 음향 또는 기타 감지 방법을 활용합니다. 그러나 현재 사용 중인 접근 가능한 스캐너의 대부분을 소개한 것은 포토그래메트리, 레이저 또는 구조빛이 혼합된 가격 합리성, 사용 편의성 및 다목적 스캔 기능을 제공합니다.\n\n# 가산 제조를 위해 3D 스캔 활용하기\n\n<div class=\"content-ad\"></div>\n\n그렇다면, 3D 스캔으로 디지털화된 레플리카가 어떻게 다운스트림 프린팅이나 부가 제조용으로 변환되는지 알고 계신가요? 마지막 단계는 완료된 3D 스캔 데이터 세트를 메쉬 조작 단계를 통해 처리했습니다:\n\n- 메쉬 파일 내보내기– 3D 스캐너 소프트웨어가 3D 프린터가 읽을 수 있는 STL 또는 OBJ 파일 형식으로 저장된 완전한 삼각형 모델을 완성합니다.\n- (선택사항) 메쉬 수리– 제3자 소프트웨어가 메쉬 파일을 스캔하여 프린트 실패의 원인이 될 수 있는 오류를 자동으로 수리할 수 있습니다.\n- 슬라이서 가져오기– 메쉬 파일이 적절히 위치하고 최종 부가 제조 준비를 위해 Cura와 같은 프린터 슬라이서 소프트웨어로 가져옵니다.\n- 부품 인쇄 시작– 빌드 환경을 캘리브레이션한 후, 데이터가 실제 노즐 및 베드 모션을 구동하여 스캔된 물체를 재창조합니다.\n\n이 스캐닝 디지털 트윈 핸드오프에서 3D 프린터 작업 흐름을 통해 원본 오브젝트 디자인과 일치하는 내구성 있는 플라스틱 또는 레진으로 인쇄된 부품을 제공할 수 있습니다. 강도나 유연성이 필요한 부분을 복제하는 경우 오리지널에서는 불가능한 대체 플라스틱 필라멘트나 조절된 인피룰 패턴에 인쇄물을 조정할 수 있습니다.\n\n이제 저렴하고 쉽게 사용할 수 있는 스캐닝 장비 또는 서비스를 통해 물체를 디지털로 변경하여 부가 제조 재이용을 제공할 수 있습니다. 이 스캔을 통해 생산을 주도하면, CAD 모델을 처음부터 만들어야 하는 경우에 필요한 시간을 단축할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 3D 스캔을 활용하는 중요 포인트\n\n- 부가 제조 기술은 정확한 디지털 소스 모델 파일을 보유하는 데 의존합니다. 이를 통해 완성된 프린트물을 제공할 수 있습니다 — 3D 스캐닝 하드웨어는 물리적 물체를 프린트 가능한 데이터로 효과적으로 연결해줍니다.\n- 레이저 스캐닝부터 포토그래미티리까지 다양한 기술을 통해 물체를 정밀한 표면 세부 정보까지 손상시키지 않고 디지털화하는 접근 가능하고 자동화된 방법을 제공합니다.\n- 완료된 3D 스캔은 고해상도 메쉬 모델로 세밀하게 정제된 후, 추가 프린팅을 가능하게하는 슬라이서에 위치시킵니다.\n- 완성된 프린트물은 원본 물체의 정확한 치수를 유지하지만, 비디지털 제조로는 불가능한 대체 재료 선택이나 사용자 지정 설정이 가능해집니다.\n\n현재 제품 설계 또는 후속 프린트 생산에 저렴한 현장 3D 스캐닝을 통합하기 원하시나요? 귀하의 사용 사례 요구 사항에 맞는 최상의 실천 방안을 다룬 개인적 조언을 받으려면 연락해 주세요.\n\n기존 부품을 캡처하고 다음 혁신적인 이터레이션을 위해 준비된 3D 스캔을 활용해 추가 제조 빠른 프로토타입 이점을 구축하기 시작하세요!\n\n<div class=\"content-ad\"></div>\n\n이 게시물은 원본으로 3dprintjunction.com에 게시되었습니다. \n저희 작업을 지원해 주신 모든 분들께 감사드리며, 더 많은 이러한 기사를 보기 위해 3dprintjunction.com을 방문해 주시기를 부탁드립니다.","ogImage":{"url":"/assets/img/2024-05-16-AnIntroductionto3DScanningforAdditiveManufacturing_0.png"},"coverImage":"/assets/img/2024-05-16-AnIntroductionto3DScanningforAdditiveManufacturing_0.png","tag":["Tech"],"readingTime":5},{"title":"iOS 바로가기에서 ChatGPT - 세계에서 가장 똑똑한 HomeKit 음성 어시스턴트","description":"","date":"2024-05-16 17:24","slug":"2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant","content":"\n\nChatGPT과 GPT-3를 시도한 이후로는, Siri, Alexa, Google Home, 그리고 다른 \"스마트\" 어시스턴트들이 너무 어설프고 쓸모 없게 느껴질 것입니다.\n\n놀랍게도, 한 시간 이내에 당신만의 스마트 어시스턴트를 만들 수 있습니다!\n\n![이미지](/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_0.png)\n\n# 다른 AI 어시스턴트\n\n<div class=\"content-ad\"></div>\n\n다른 사용 사례에 이 도움말러와 같은 어시스턴트를 사용하는 방법을 보여 주기 위해 차 관련 문제를 논의할 수 있는 AI 자동차 정비사를 만들었습니다:\n\n# 배경\n\n나는 수십 개의 조명, 온도 조절 장치, 바닥 난방, 환기 장치, 카메라 등이 있는 완전히 구축된 HomeKit 스마트 홈을 갖고 있어서 Siri를 GPT-3으로 대체할 수 있다면 좋을 것 같다고 생각했습니다.\n\n자세한 내용을 살펴보기 전에 데모를 보여 드리겠습니다. 응답은 항상 개인화되어 있으며 데모에 제한되지 않습니다. 원하는 질문을 원하는 대로 할 수 있습니다!\n\n<div class=\"content-ad\"></div>\n\n# \"홈 어시스턴트 프로그래밍\"\n\nGPT-3 및 특히 ChatGPT는 대화 데이터로 훈련된 언어 모델로, 인간의 지시를 이해하고 응답하는 데 굉장히 뛰어납니다.\n\n이 챗봇 중 하나를 시도해 본 적이 있다면, 다양한 형식으로 질문하고 응답을 받는 법을 쉽게 알 수 있을 것입니다. 문제는 스마트 홈을 제어할 때 다뤄야 할 매우 구체적인 구성 요소가 있습니다. 이 문제를 어떻게 해결할 수 있을까요?\n\n모든 이를 달성하기 위해 사용한 정확한 프롬프트는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_1.png\" />\n\n알 수 있듯이, 모든 것을 평범한 영어로 설명했습니다. 요청 유형, 응답의 구체적인 구조를 설명하고, 감각적인 AI처럼 행동하도록 요청했으며, 개인 질문에 대한 조언을 하도록 했습니다. 또한 집안의 시간, 위치 및 장치 및 각 방에 대한 몇 가지 세부 정보를 제공했습니다. 이를 통해 우리는 완벽하게 구조화된 응답을 받게 될 것입니다.\n\n프로그래밍하는 데 필요한 모든 것이 여기에 있습니다!\n\n# 다른 요청 유형\n\n<div class=\"content-ad\"></div>\n\n이제 네 요청 카테고리를 살펴보고 영상에서 몇 가지 예시를 통해 어떻게 처리할지 살펴볼게요.\n\n## :: 명령 실행하기\n\nGPT-3가 스마트 홈의 특정 부분을 제어하려고 한다는 것을 감지하면 “명령” 카테고리의 액션으로 응답합니다.\n\n예시 요청은:\n\n<div class=\"content-ad\"></div>\n\n그리고 우리는 다음과 같은 응답을 받았어요:\n\n```js\n{\n  \"action\": \"command\",\n  \"location\": \"office\",\n  \"target\": \"light\",\n  \"value\": \"on\",\n  \"comment\": \"Turning the light on for you.\",\n  \"scheduleTimeStamp\": \"\"\n}\n```\n\n솔직히 말해서, 처음 이 응답을 보았을 때 눈을 믿을 수 없었고 얼마나 예외적으로 잘 작동하는지 놀랐어요!\n\n해당 요청은 단순히 \"사무실의 불을 켜라.\" 와 같은 간단한 명령이 아니었습니다. 매우 비틀린 방식으로 구사되어 있었죠. 이러한 방식은 Siri, Alexa 또는 Google 홈을 즉시 헷갈리게 만드는 요소였습니다.\n\n<div class=\"content-ad\"></div>\n\n위의 정보로 우리는 정확히 무엇을 해야 할지 알게 되었고, Siri 바로 가기에서 이를 나중에 처리할 것입니다.\n\n또한 여러분은 아마도 scheduleTimeStamp가 비어 있다는 것을 알았을 것입니다. 이것은 작업이 즉시 발생해야 한다는 것을 의미하지만, 나중에 작업이 필요한 경우에는 미리 지정된 미래의 날짜와 시간이 채워진 응답을 받을 수 있습니다.\n\n이 경우 GPT-3는 아마도 침실이 꺼져야 한다는 점을 이해하고 요청에 전달한 시간보다 20분 뒤의 올바른 타임스탬프를 추가했습니다.\n\n<div class=\"content-ad\"></div>\n\n```json\n{\n  \"action\": \"command\",\n  \"location\": \"bedroom\",\n  \"target\": \"light\",\n  \"value\": \"off\",\n  \"comment\": \"자녀의 침실 불 끄기.\",\n  \"scheduleTimeStamp\": \"Mon Jan 16 2023 12:16:31 GMT+0000\"\n}\n```\n\n또 다른 흥미로운 예시는 침실 온도를 설정하는 데 저 대신에 결정을 내린 방법이었습니다:\n\n그리고 그것은 자신의 지식을 기반으로 침실을 편안한 19도로 설정했습니다!\n\n## :: 쿼리 액션\n  \n\n<div class=\"content-ad\"></div>\n\nGPT-3가 스마트 홈 장치의 상태를 읽으려는 의도를 감지하면 \"query\"라는 동작 범주로 응답합니다.\n\n예시 요청:\n\n받는 응답은:\n\n```js\n{\n  \"action\" : \"query\",\n  \"location\" : \"kitchen\",\n  \"target\" : \"thermostat\",\n  \"property\" : \"temperature\"\n}\n```\n\n<div class=\"content-ad\"></div>\n\n훌륭합니다! 작업 유형이 쿼리로 변경되어서 이제 주방 온도계의 온도를 가져오는 작업을 진행할 수 있습니다. 다시 한 번 말씀드리지만, 이 작업은 Siri Shortcut에서 나중에 처리할 예정입니다.\n\n## :: 대답 작업\n\nGPT-3가 요청이 스마트 홈과 관련이 없고 일반적인 질문일 때 \"대답\" 작업 범주로 응답합니다.\n\n예시 요청:\n\n<div class=\"content-ad\"></div>\n\n요청에서 스마트 홈의 위치를 영국 세인트올번으로 알렸기 때문에 GPT-3은 정확히 어떻게 응답해야 하는지 알고 있어요!\n\n```js\n{\n  \"action\": \"answer\",\n  \"answer\": \"세인트올번은 역사적인 명소와 볼거리가 풍부해요. 세인트올번 대성당은 노르만 양식의 멋진 건물이에요. 베룰라미움 공원은 로마 시대 유적과 아름다운 정원이 있는 멋진 장소에요. 더 현대적인 경험을 원한다면 세인트올번 박물관이나 알반 아레나에서 쇼를 감상할 수 있어요.\"\n}\n```\n\n간단하고 좋죠! 이 정보를 다시 읽어주는 Shortcut에 전달해 줄게요.\n\n## :: 작업 명확히하기\n\n<div class=\"content-ad\"></div>\n\n마침내, GPT-3가 세 가지 작업 중 어느 것도 감지하지 못했다면, 질문을 반복하거나 다시 말해달라고 요청할 것입니다.\n\n# Siri 단축어에서 데이터 처리하기\n\niOS, macOS 또는 iPadOS에서 단축어 애플리케이션을 열고 요청의 세부 정보를 추가해보세요:\n\n![이미지](/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_2.png)\n\n<div class=\"content-ad\"></div>\n\n기억해 두면 좋은 한 가지 팁은 바로 단축어를 실행할 때 Siri에게 바로 단축어의 이름을 말하는 것입니다. 그래서 \"오케이 스마트 홈\"이 좋은 아이디어처럼 보였어요.\n\n다음으로는 \"텍스트 요청하기...\" 작업을 사용하여 사용자로부터 응답을 받습니다. 이는 Siri를 통해 단축어를 시작하면 소리로 읽어주거나, 단축어 타일을 클릭하여 실행하면 입력 필드로 표시됩니다.\n\n이제 OpenAI에 요청을 보내기 준비가 되었습니다. 이를 위해서는 계정을 등록하고 API 토큰을 받아야 합니다. 로그인 후 \"API 키 보기\" 메뉴 항목 아래에 있습니다.\n\nAPI 사용 비용은 요청 당 약 $0.014이 소요되므로 $1당 70회 이상의 요청을 수행할 수 있습니다. 주의할 점은 우리의 요청이 매우 길기 때문에 이것은 비용이 비싸게 느껴집니다. 그렇지만 더 짧은 요청의 경우 비례적으로 적게 지불하게 됩니다. 실험할 때 기억해 둘만한 것이 될 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_3.png\" />\n\nAPI 토큰을 준비했으면, 요청의 세부 사항을 입력하세요:\n— model: text-davinci-003\n— prompt: `텍스트` 요청: `제공된 입력` 응답:\n— max_tokens: 1000\n\n헤더 섹션에 다음을 추가하세요:\n— Content-Type: application/json\n— Authorization: Bearer `API 토큰 추가`\n\nAPI에서 응답 데이터를 받으면, JSON 응답에 중첩되어 있으므로 그것을 찾아야 합니다. 데이터를 JSON 변수에 할당한 다음, 액션 카테고리 값을 추출합니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 이제 if 문에서 동작 카테고리의 값을 확인할 준비가 된 것입니다:\n\n![image](/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_5.png)\n\n한번 동작이 \"command\" 였음을 알면, 우리는 위치, 대상, 값 필드를 확인하고 동작을 트리거하면 됩니다: 첫 번째 예제에서 사무실 불을 켜는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n`<img src=\"/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_6.png\" />`\n\n조건 바깥과 \"End If\" 블록 뒤에, 우리는 또한 요청과 함께 전송된 GPT-3의 코멘트를 보여줍니다:\n\n`<img src=\"/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_7.png\" />`\n\n쿼리 명령은 내장된 HomeKit 액션과 비슷하게 작동합니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_8.png)\n\n마침내, 답변 및 명확화 작업은 처리 없이 반환된 값을 보여줍니다:\n\n![이미지](/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_9.png)\n\n이상입니다\n\n\n<div class=\"content-ad\"></div>\n\n지금 들리긴 좀 길게 들릴 수 있지만, 솔직히 말해서, 난 집 전체를 실제로 다 하진 않았어. 하지만, 네 논리를 입력 매개변수로 더 추상화하고, 코드에서의 함수와 비슷하게 값을 반환할 수 있는 작은 바로 가기 몇 가지로 더 단순화할 수 있다는 건 알아. 이 부분을 살펴보고, 일을 간단하게 할 수 있는 방법이 있는지 확인해 보겠어. 이에 대해 아이디어가 있으면 알려줘!\n\n내 바로 가기 링크로 넘어가서 사용해보고 적용해 볼 수 있어:\nhttps://www.icloud.com/shortcuts/e5d4033cf8024b5796e270c8fed9e478\n\n## 결론\n\n이거 얼마나 멋진 일이야?? 평범한 영어로 정의한다면 스마트 어시스턴트를 만들 수 있다니, 이게 미친 일이 아니겠어!\n\n<div class=\"content-ad\"></div>\n\n하지만 이것은 단지 한 예에 불과해요. GPT-3에 보낼 수 있는 다양한 프롬프트와 Siri Shortcuts에서 수행할 수 있는 다양한 작업들을 상상해 보세요.\n\n제가 확실히 여러분이 이미 몇 가지 아이디어를 가지고 있을 거라고 확신하고 있어요. 그래서 제게 언제든지 알려주시기 바라며, 정말로 관심이 있어요!","ogImage":{"url":"/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_0.png"},"coverImage":"/assets/img/2024-05-16-ChatGPTinaniOSShortcutWorldsSmartestHomeKitVoiceAssistant_0.png","tag":["Tech"],"readingTime":6},{"title":"스마트 홈 편의성 연구","description":"","date":"2024-05-16 17:21","slug":"2024-05-16-SmartHomesConvenienceCon","content":"\n\n\n![Smart Home](/assets/img/2024-05-16-SmartHomesConvenienceCon_0.png)\n\n누가 스마트 홈이 대부분의 의존도를 요구한다고 부인할 수 있을까요? 어떤 서버에서 소프트웨어로 규제된 시스템에 의존, 하드웨어에 의존, 업데이트에 의존, 가전제품을 제공하는 제조업체에 의존, 인터넷에 의존, 코드에 의존, 즉: 안전이라고 불리는 것은 당연히 인터넷에 연결되는 모든 \"스마트\" 기기들은 해킹될 수 있다는 것을 의미합니다. 다수는 그것을 부인할 것입니다. 예상대로 말이죠.\n\n사람들에게 무슨 일이 일어났는지요? 왜 자신의 손가락으로 불을 키는 버튼을 누르는 게 이상한 건가요? 손가락을 접거나 꿈치를 돌려 다리를 접었느냐고요? 왜 덤볼트를 수동으로 잠그는 게 문제가 되는 건가요? 열쇠를 돌릴 때 손목을 돌릴까봐 무서워하나요? 놀라운 잠금장치와 매우 강한 문을 사실 수 있습니다. 아직 듣지 못했다면 참고로 \"바보같은\" 방범 시스템도 아주 잘 작동합니다. 도어벨이 연결된 무엇을 원하는 거야? 문에 감시구멍을 달 수 없어? 이러한 수동 작업은 스마트 기기의 유지보수보다 덜 많은 에너지와 번거로움, 덜 불안 (및 문제)을 필요로 합니다. 그리고 두 해마다 스마트폰과 마찬가지로 교체가 필요합니다.\n\n무엇때문에 주스 제조기가 인터넷에 연결돼야 하나요? 냉장고로부터 식품이 상하는 알림을 받길 원하나요? 당신은 직접 눈, 코, 혀로 확인할 수 없을 정도로 바보거나 게으르신가요? 왜냐하면 그것은 자연이 그것들을 주신 이유입니다! 냉장고가 물자가 부족하다고 알려주길 원하시나요? 손에 공책을 쥐고 검사하고 목록을 작성할 수 없나요? (쇼핑 목록이라고 부릅니다). 스마트 홈 열광자들이 완전한 바보이거나 소파감자가 되어 가장 기본적인 가정 업무조차 수행할 수 없게 된 건가요? 너무 많은 편리함은 대가가 따른다는 것을 알 수 없나요?\n\n\n<div class=\"content-ad\"></div>\n\n어째서 누가 스마트 카메라를 모든 방에 설치하여 개인 정보를 스위스 치즈처럼 구멍투성이로 만들고 싶어 할까요? 고양이나 개를 위해서라고요? 아이들을 위해서라니요? 당신의 동물을 훈련시키는 법 알고 계신가요? 할아버지와 할머니를 위한 방법을 알고 계신가요? 신뢰할 수 있는 이웃들? 육아 도우미, 그리고 육아 서비스? 도둑들을 잡기 위해서인가요? 아, 집을 비욕하는 도둑들 말이죠? 왜냐하면 그들은 귀하가 집에 없는 시간을 알고 있는데, 그건 아마도 귀하의 카메라를 해킹해 알아냈기 때문이죠.\n\n스마트 홈 열풍 전부는 사람들에게 전혀 필요하지 않은 시스템에 돈을 쓰도록 유도하는 계책에 지나지 않습니다. 그것은 자신의 창의력, 자원, 결정력, 상상력을 억제함으로써 결국 그것들이 \"스마트화\" 된 집에서 자신이 자체적으로 그들이 대한 참가자가 되었다는 것을 말합니다. 물론 대단한 기술이지만 본질적으로 저능한 기술이 자신의 두뇌력을 제거하고 자아에 대한 감각을 망칩니다.\n\n다음 문제로 넘어갑시다. 집주인의 주인이 나 주인이 아니라는 것은, 거대 기술 회사, 제조사, 심지어 정부, 그리고 귀하의 ISP에게 자유를 넘기었습니다. 그러다 보니 개인 정보에 대한 보안을 상실하였고, 거의 매 순간 감시를 받고 있습니다. 음식물이 상한 계란에 대해 냉장고가 갑자기 대화를 걸어오지 않으면 구독을 받아야 하지 않는데, 당신을 속이려고 합니다. 당신의 도덕성은 위험에 처해 있습니다. 이제 당신은 집에서 조작의 공범이 되었습니다. 당신의 인간다움이 위협받고 있습니다. 당신은 이제 누군가의 조종을 받는 꼭두각시가 되었습니다. 그들은 소프트웨어 문제 때문에 기기를 교체하도록 당신에게 이야기 합니다. 그들은 다른 물건을 구매하도록 꼬드겨 넣습니다. 그들은 구독을 받도록 강요합니다(아니면 냉장고가 약속 시한까지 썩어가는 알에 대해 당신에게 얘기하지 않을지도 모릅니다). 당신의 사무실에서 동료들이 당신보다 더 많고 똑똑한 스마트 기기를 가진 상태에서 속아 넘어 느끼게 만들어줍니다. 당신은 수명을 다해 트러블 슈팅에만 투자하게 만듭니다. 그러나 결과적으로는 위태로운 차고 문 반대편의 올바른 사람에게 연락하기 위해 또 다른 여러 시간이 지나게 됩니다. 당신은 물리적 열쇠를 항상 소지하지 않는다면, 지금은 사무지 도어의 스마트 잠금 장치가 완전히 막혀 버리고 해가 마끈 물어 버렸기 때문에 몰래 잠들어 버리게 될 수도 있습니다. 모든 것이 음성 제어나 스마트폰의 코드로 작동하여 집안 일에서 이전처럼 미약한 상황에 대해 고민해보지 않습니다.\n\n그러니 이제 이유가 결정되었습니다. 자, 자랑스런 스마트 홈 소유자여, 당신이 집에 잡혀들어온 버그가 나무 구멍 밖으로 기어나오지 않고 소프트웨어에 들어가는 상황이 정확히 어떤 위험/단점인가요? 여기요:\n\n<div class=\"content-ad\"></div>\n\n- 보안 취약점: 오래된 소프트웨어, 약한 암호, 암호화되지 않은 연결 등 이러한 요인들이 함께 나타나면 스마트 기기가 사이버 공격에 취약해질 수 있습니다;\n- 환경적 요인: 극한 온도, 물리적 장애물로 인한 막힘, 다른 전자기기로부터의 간섭;\n- 기기 노화: 어리버리한 기기와 마찬가지로 스마트 기기도 마모되고 고장이 발생할 수 있으며 적절한 유지보수로 수명을 연장시킬 수 있지만 결국에는 고장이 납니다;\n- 소프트웨어 버그: 컴퓨터나 스마트폰을 사용하는 사람이라면 버그, 오류 메시지, 이상 현상, 빈 페이지, 무반응, 기능 오작동 등에 친숙할 것입니다. 이는 매우 짜증나며 가정의 연결된 기기 사슬에 미치는 영향을 상상해보세요;\n- 호환성 문제: 여러 제조업체로부터 제품을 구매할 수 있으므로 A가 B와 함께 작동하는지, 혹은 C가 D와 작동하는지 여부가 알 수 없습니다. 간단히 말해: 통합 문제;\n- 연결 문제: 와이파이 신호 강도가 흔들리거나, 네트워크가 혼잡하거나, 라우터에 문제가 있거나, 인터넷 서비스 제공업체가 서버 오류를 경험할 수 있으며, 이 모든 것이 연결 오류로 이어질 수 있습니다;\n- 전원 장애: 물론 상황은 최악입니다. 이러한 정전은 스마트 기기를 손상시킬 수도 있으며, 적어도 리셋이 필요합니다. 정전 중에는 집 전체가 \"작동\"하지 않습니다! (상상해보세요, 살기 위해서는 \"작동\"해야 하는 집이라니!)\n- 클라우드 서비스 중단: 많은 스마트 홈 기기가 클라우드 서비스에 연결되어 원격 접근 및 데이터 저장을 보장합니다. 그렇다면 클라우드 서비스에 장애가 발생하거나 사업이 폐쇄된다면 어떨까요? 스마트 홈을 \"이주\"해야 할지도 모릅니다. 행운을 빕니다!\n- 사용자 오류: 모든 평범한 사람이 설정을 놓칠 수 있는 기술 마법사가 될 것을 기대할 수는 없습니다. 또는 자체적인 실수로 오작동한 소프트웨어를 재구성해야 할 수도 있습니다.\n- 핸드폰이 고장이 나거나 도난당했을 때: 많은 스마트 기기 중에는 핸드폰이 실제 원격 조절 장치로 사용되는 경우가 많으니, 이를 소유하고 있지 않게 된다면 여기서 설명된 시나리오 중에서 가장 최악의 상황에 놓일 것입니다 (특히 다른 사람 소유 여부일 때). 자세한 설명은 필요 없습니다.\n- 목소리가 도둑맞을 때: 그렇게 읽으셨습니다. 사이버 범죄자들이 현재 \"음성 사기\"를 완성하고 있어, 당신의 목소리에 접근하고 스마트 기기를 조작하는 데 사용할 수 있습니다. 해커는 알렉사 사용자에게 가짜 링크를 보내며, 대개 이메일이나 메신저 앱을 통해 보내기 때문에 이메일을 클릭하면 해커는 알렉사 앱에서 당신의 음성 녹음 내역에 액세스할 수 있습니다. 해커들은 당신의 음성 내력을 사용하여 오디오 위조를 만들고 당신의 기기에 명령을 보낼 수 있습니다.\n\n이제 두 번째 질문: 덤 홈 주민으로서 직면하고 있는 위험 / 단점은 무엇인가요?\n\n- 보안 취약점: 집으로 침입하려는 도둑 – 열쇠가 없는 경우나 현관문을 잠그지 않고 창문을 닫거나 경보가 울리지 않은 경우에 성공할 수 있습니다;\n- 기기 노화: 설명이 필요 없음;\n- 전원 장애: 조명, 컴퓨터(아마도 유선 전화기), 배터리 충전, 전기를 사용하는 모든 기계에 영향을 미치므로 전기 스토브, TV, 라디오 및 방범 경보기도 포함됩니다;\n- 연결 문제: 컴퓨터, 스마트폰, 태블릿만 해당됩니다.\n\n따라서 제가 사는 덤 홈 속에서 겪고 있는 의존성은 다음과 같습니다: 나쁜 인물들을 배제하기 위해 만 6000년 전에 발명된 옛날 스타일의 잠금 장치와 열쇠, 시간 (우주 생성부터 모든 것을 죽여버림), 전력 발전소 (한때 전기가 사람들의 집에 도착했을 때) 및 내 인터넷 서비스 제공자(1990년대 초부터).\n\n<div class=\"content-ad\"></div>\n\n스마트 홈에서 살면 시한폭탄처럼 살고 있는 것 같다고 결론지을 자유를 느껴요. 정말 그 시한폭탄이 터질 거라는 것을 알고 있지만 (폭발하는 건 아니고 그냥 당신의 코앞에서 죽어간다는 차이가 있지만) 그 질문은...언제일까요. 집은 제 안식처니까요. 아냐.\n\"1984\"라는 책을 들어본 적이 있나요? 어떤 조지 오웰이 쓴 책인데요. 그게 바로 스마트 홈이죠. 서방이니까요. 최소한 서양 사회에서는 주요 감시자가 국가 수장이 아니라, 당신이 자신의 두뇌와 손을 사용하여 당신의 삶을 조직할 의사가 없다는 점에 기반하여 자신의 집 안에서 일어나는 많은 당사자들입니다. 그 집은 ... 당신의 집, 큰 나쁜 세계에서 도피할 수 있는 곳, 당신이 자신이 될 수 있는 곳, 당신이 누려지는 소중한 순간을 함께 즐길 수 있는 곳, 혼자이든 아니면 가족과 함께든 말이죠.\n\n당신은 침입자를 집 안으로 들이거나 당신 스스로 그를 초대했기 때문에 이제 큰 나쁜 세계가 당신의 생활 공간을 점령하고 있어요. 왜냐하면 '당신'과 '당신의 동거인들', '친척', '가까운 친구'가 아닌 모든 것이 그 큰 나쁜 세계의 일부라는 것입니다. 축하해요!\n인터넷에서 발췌한 글: \"내 집 안에서 가장 똑똑한 존재는 나 뿐이고, 그마저도 의문스러운 부분이 있어.\"\n\nㅋㅋ.\n\n<div class=\"content-ad\"></div>\n\n\"The Internet of Shit\" 사이트에서 보고있는 모든 스마트홈 재미를 놓치지 마세요: https://twitter.com/internetofshit\n\n©MabelAmber®\n\nPS - 스마트 홈 소유자가 아니라면 내 기사를 읽은 후에도 당신의 개인 정보가 보호되는 것으로 가정하여 자기 자신을 축하하지 마세요; 현재 세계 인구 69.4 억 명 중 약 85%가 스마트폰을 소유하고 있다는 것을 의미합니다. 즉, 당신도 아마 그 중 한 명일 것입니다. \n\n반면에 저와 소수의 다른 사람들은 사회적인 거추장스런 압력에 대항하기 위해 진보의 특정 유형을 반대할 수밖에 없었지만 집에서 엄격하게 운영되는 노트북의 인터넷 사용에 제한되어 있습니다. \"스마트폰\"이라는 단어는 이 집에서 금지되어 있습니다 (예외는 그 저주받은 것들이 완전히 악마화된 문구 내에서입니다). \n\n우리에게 좋은 점이죠, 응?\"\n\n헤더에 있는 삽화: AI의 도움으로 만들어졌습니다.","ogImage":{"url":"/assets/img/2024-05-16-SmartHomesConvenienceCon_0.png"},"coverImage":"/assets/img/2024-05-16-SmartHomesConvenienceCon_0.png","tag":["Tech"],"readingTime":6},{"title":"SQL Server의 비밀 기능 - SQL Server에서 Python 및 애드온을 네이티브로 실행하기","description":"","date":"2024-05-16 17:19","slug":"2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer","content":"\n\n## 파이썬 라이브러리를 가져와서 SQL 테이블을 조작하고 출력하며, SQL 서버를 떠나지 않고 작업하세요.\n\n## 문제\n\n이 프로젝트에서는 두 가지 서로 다른 출처에서 가져온 37,000개의 회사 이름을 관리하는 어려움에 직면합니다. 이 복잡성은 이러한 소스 간에 동일한 회사가 어떻게 나열되는지에 대한 잠재적인 불일치에 있습니다.\n\n## 목표\n\n<div class=\"content-ad\"></div>\n\n이 문서의 목표는 Microsoft SQL Server 내에서 Python을 네이티브로 실행하는 방법을 가르치는 것입니다. 또한 SQL을 사용하여 결과 테이블에 대해 추가 처리를 수행하고, 애드온 및 외부 라이브러리를 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_0.png)\n\n# 초기 알고리즘 빌드\n\n다음은 알고리즘을 구축할 때 따를 전략입니다:\n\n<div class=\"content-ad\"></div>\n\n- 블로킹 — 공통 속성을 기반으로 데이터 집합을 더 작은 블록이나 그룹으로 나누어 레코드 비교의 계산 복잡성을 줄이는 것입니다. 이는 검색 공간을 좁히고 유사도 검색 작업의 효율성을 향상시킵니다.\n- 전처리 — 생 데이터를 정리하고 표준화하여 대문자를 소문자로 변환하거나 구두점을 제거, 불용어를 제거하는 등의 작업을 통해 분석에 준비시킵니다. 이 단계는 데이터 품질을 향상시키고 잡음을 줄입니다.\n- 유사도 검색 모델 적용 — 모델을 적용하여 레코드 간의 유사도나 거리를 토큰화된 표현을 기반으로 계산하는 것입니다. 이는 코사인 유사도나 편집 거리와 같은 메트릭을 사용하여 레코드 링킹이나 중복 제거와 같은 작업을 위해 유사한 쌍을 식별하는 데 도움이 됩니다.\n\n## 블로킹\n\n내 데이터 집합은 매우 불균형합니다 — 첫 번째 테이블에는 1,361,373개의 엔티티가 있고 두 번째 테이블에는 37,171개의 회사 이름이 있습니다. 전처리하지 않은 테이블에서 일치를 시도하면 알고리즘은 매우 오랜 시간이 걸릴 것입니다.\n\n테이블을 블로킹하려면 2개의 데이터 집합 간에 어떤 공통된 특성이 있는지 확인해야 합니다. 내 경우, 회사들은 모두 내부 프로젝트와 관련이 있습니다. 그러므로 다음과 같이 작업할 것입니다:\n\n<div class=\"content-ad\"></div>\n\n- 작은 테이블에서 고유한 회사 이름과 프로젝트 코드를 추출합니다.\n- 프로젝트 코드를 순회하면서 큰 테이블에서 해당 코드를 찾습니다.\n- 해당 프로젝트의 모든 자금을 매핑하고 큰 테이블에서 제외합니다.\n- 다음 프로젝트에 대해 반복합니다!\n\n이렇게 하면 매 반복마다 큰 데이터 세트를 줄이게 되며, 프로젝트 수준에서의 필터링된 데이터 세트로 인해 매핑이 빠릅니다.\n\n이제 두 테이블을 프로젝트 코드로 필터링할 것입니다.\n\n이 방식으로 접근하면, 작은 테이블에는 프로젝트 'ABC'에 대해 매핑할 406개의 행만 있고, 큰 테이블에는 매핑 대상인 15,973개의 행이 있습니다. 이는 최초 테이블에서 상당한 감소입니다.\n\n<div class=\"content-ad\"></div>\n\n## 프로그램 구조\n\n이 프로젝트는 SQL 서버에서 Python 및 SQL 함수를 모두 포함할 것입니다. 아래는 프로그램이 어떻게 작동하는지 더 명확히 이해하기 위해 간략히 스케치된 내용입니다:\n\n![프로그램 실행](/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_1.png)\n\n프로그램 실행:\n\n<div class=\"content-ad\"></div>\n\n- 프로젝트 코드를 루프로 출력하는 것은이 기능의 가장 간단한 버전입니다.\n\nSQL 커서는 너무 많은 리소스를 사용한다는 것이 빨리 드러납니다. 간단히 말해서, 이는 커서가 행 수준에서 작동하고 모든 행을 통과하여 작업을 수행하기 때문입니다.\n\n성능을 향상시키기 위해 임시 테이블을 사용하고 커서를 제거할 것입니다. 여기에 결과 함수가 있습니다:\n\n이제 이 함수는 대형 매핑 테이블에서 해당 프로젝트로 필터링된 프로젝트 코드와 데이터를 선택하는 데 약 3초가 소요됩니다.\n\n<div class=\"content-ad\"></div>\n\n데모 목적으로는 2개의 프로젝트에만 집중할 것이나, 프로덕션 환경에서 작업할 때는 모든 프로젝트에서 함수를 실행할 것입니다.\n\n다음에 사용할 최종 함수는 다음과 같습니다:\n\n## 매핑 테이블 준비\n\n다음 단계는 Python 전처리 및 매핑 함수용 데이터를 준비하는 것입니다. 이를 위해 2개의 데이터셋이 필요합니다:\n\n<div class=\"content-ad\"></div>\n\n- 대형 매핑 테이블에서 프로젝트 코드로 필터링된 데이터\n- 소기업 테이블에서 프로젝트 코드로 필터링된 데이터\n\n여기에 2개의 테이블에서 선택된 데이터가 포함된 업데이트된 함수가 있습니다:\n\n이 함수를 통해 각 프로젝트에 대한 프로젝트, 회사 이름 및 소스를 얻을 수 있습니다.\n\n이제 파이썬에 대비되었습니다!\n\n<div class=\"content-ad\"></div>\n\n# SQL에서의 Python 실행\n\nSQL Server에서 Python은 sp_execute_external_script을 통해 SQL Server 내에서 Python 코드를 직접 실행할 수 있게 해줍니다.\n\nSQL과 Python 간의 데이터 교환을 통해 Python의 기능을 SQL 워크플로에 통합할 수 있습니다. 제공된 예시에서는 Python 스크립트가 실행되어 입력 데이터에서 pandas DataFrame을 생성합니다.\n\n결과는 단일 출력으로 반환됩니다.\n\n<div class=\"content-ad\"></div>\n\n대박이네요!\n\nSQL에서 Python을 실행하는 중요한 몇 가지 사항이 있습니다:\n\n- 문자열은 작은 따옴표(')가 아닌 큰 따옴표(\")로 정의됩니다. 특히 정규표현식을 사용할 때 이를 확인해야 하며, 오류 추적에 시간을 낭비하지 않도록 주의해주세요.\n- 하나의 출력만 허용됩니다. 따라서 Python 코드는 출력에서 하나의 테이블을 생성하게 됩니다.\n- 디버깅을 위해 print 문을 사용하고 결과를 SQL 서버의 '메시지' 탭에 인쇄된 결과를 확인할 수 있습니다. 아래와 같이:\n\n![image](/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_2.png)\n\n<div class=\"content-ad\"></div>\n\n## SQL에서 파이썬 라이브러리\n\nSQL Server에서는 여러 라이브러리가 미리 설치되어 있고 쉽게 접근할 수 있습니다. 이러한 라이브러리들의 완전한 목록을 보려면 다음 명령을 실행할 수 있습니다:\n\n다음은 결과의 예시입니다:\n\n![이미지](/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_3.png)\n\n<div class=\"content-ad\"></div>\n\n# 파이썬으로 텍스트 매칭하기\n\n우리가 생성한 테이블에 돌아와서, 이제 파이썬을 사용하여 여러 소스에서 가져온 회사 이름을 매칭할 수 있습니다. 우리의 파이썬 절차는 긴 테이블을 입력받아 매핑된 엔티티들이 있는 테이블을 출력할 것입니다. 이는 작은 회사 테이블의 각 레코드 옆에 있는 대형 매핑 테이블에서 가장 가능성이 높은 매치를 보여주어야 합니다.\n\n![이미지](/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_4.png)\n\n이를 위해 먼저 SQL 절차에 파이썬 함수를 추가해 보겠습니다. 첫 번째 단계는 데이터 집합을 단순히 Python에 넣는 것입니다. 이를 샘플 데이터셋으로 하고, 그런 다음 실제 데이터로 하겠습니다. 여기에 코드가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n이 시스템을 통해 두 테이블을 모두 파이썬 함수의 입력으로 전달할 수 있습니다. 그러면 두 테이블을 출력으로 출력합니다.\n\n## 파이썬에서의 전처리\n\n문자열을 효과적으로 일치시키기 위해 Python에서 몇 가지 전처리를 수행해야 합니다. 이는 다음을 포함합니다:\n\n- 강세 음절 및 다른 언어별 특수 문자 제거\n- 공백 제거\n- 구두점 제거\n\n<div class=\"content-ad\"></div>\n\nSQL 작업 중 정렬을 사용하여 첫 번째 단계를 진행하고, 나머지 두 단계는 Python 함수의 전처리 단계에서 처리될 것입니다.\n\n다음은 전처리가 포함된 함수의 모습입니다:\n\n결과는 3개의 열로 나타납니다. 하나는 회사 이름을 소문자로 작성하고, 두 번째 열은 프로젝트 열이며, 세 번째 열은 출처입니다.\n\n## Python에서 문자열 매칭하기\n\n<div class=\"content-ad\"></div>\n\n여기서는 사용할 수 있는 라이브러리의 수가 제한되어 있기 때문에 창의적이어야 합니다. 따라서 먼저 결과물이 어떻게 보여야 하는지 결정해봅시다.\n\n소스 2에서 가져온 데이터를 소스 1의 데이터와 일치시키려고 합니다. 따라서 소스 2의 각 값에 대해 일치하는 값의 그룹이 있어야 하며, 일치 정도를 나타내는 점수도 있어야 합니다.\n\n![image](/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_5.png)\n\n작업을 단순화하기 위해 라이브러리를 가져오지 않고 내장된 python 라이브러리를 먼저 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n로직:\n\n- 각 프로젝트를 반복\n- 소스에 따라 자금을 포함한 테이블을 만듭니다. 여기서 소스 1은 매핑 데이터가 있는 큰 테이블이고, 2는 초기 회사 데이터 세트입니다.\n- 작은 데이터 세트에서 데이터를 배열로 선택합니다.\n- 결과 배열의 각 요소를 대형 매핑 데이터 프레임의 각 요소와 비교합니다.\n- 각 엔터티의 점수를 반환합니다.\n\n코드:\n\n그리고 여기가 최종 결과입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![table](/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_6.png)\n\n이 테이블에서는 각 회사 이름, 해당 프로젝트, 그리고 소스(대형 매핑 테이블에서인지 소규모 회사 테이블에서인지)가 나열되어 있습니다. 오른쪽의 점수는 소스 2와 소스 1 사이의 회사 이름 유사성 메트릭을 나타냅니다. 소스 2에서 비롯된 company4는 항상 점수가 1인 100% 일치임을 유의하는 것이 중요합니다.\n\nSQL Server 내에서 Machine Learning Services를 통해 Python 스크립트를 실행하는 것은 인-데이터베이스 분석 및 기계 학습 작업을 가능하게 하는 강력한 기능입니다. 이 통합은 데이터 이동이 필요 없이 직접적인 데이터 접근을 가능하게 하여 데이터 집중적 작업의 성능과 보안을 크게 최적화합니다.\n\n그러나 주의해야 할 제한 사항이 있습니다. 이 환경은 하나의 입력을 지원하므로 SQL 컨텍스트 내에서 직접 수행될 수 있는 작업의 복잡성을 제한할 수 있습니다. 또한 기본 라이브러리에서 지원되지 않는 일부 유형의 데이터 분석이나 기계 학습 작업에 대한 대안 솔루션이 필요할 수 있습니다. 게다가 사용자들은 복잡한 Python 코드가 포함된 T-SQL 쿼리에서의 복잡한 간격과 같은 SQL Server 환경의 복잡성을 해결해야 하며, 이러한 것들은 오류와 혼란의 원인이 될 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n이러한 도전에도 불구하고, SQL Server에서 Python을 실행하는 것이 유리한 여러 응용 프로그램이 있습니다:\n\n1. 데이터 정제 및 변환 — Python을 SQL Server에서 직접 사용하여 복잡한 데이터 전처리 작업을 수행할 수 있습니다. 예를 들어, 누락된 데이터를 처리하거나 값을 정규화하여 후속 분석이나 보고 전에.\n\n2. 예측 분석 — SQL Server 내에서 Python 기계 학습 모델을 직접 배포하면 실시간 예측이 가능해집니다. 고객 이탈 또는 판매 예측과 같은 실시간 데이터베이스 데이터를 사용하는 예측이 가능합니다.\n\n3. 고급 분석 — Python의 능력을 활용하여 데이터베이스에서 직접 고급 통계 분석과 데이터 마이닝을 수행할 수 있습니다. 이를 통해 데이터 전송 지연 없이 의사 결정 프로세스를 도와줍니다.\n\n<div class=\"content-ad\"></div>\n\n4. 자동 보고 및 시각화 — Python 스크립트는 SQL Server 데이터에서 직접 데이터 시각화와 보고서를 생성할 수 있어 자동 업데이트와 대시보드를 가능하게 합니다.\n\n5. 머신 러닝 모델 운영화 — SQL Server에 Python을 통합함으로써, 모델을 데이터베이스 환경 내에서 직접 업데이트하고 관리할 수 있어 운영적 워크플로우를 간소화시킵니다.\n\n마지막으로, Python이 SQL Server에서 실행되는 것은 일부 도전을 동반하지만, 데이터 처리, 분석 및 예측 모델링을 데이터베이스 환경 내에서 직접 향상시키고 간소화시키는 다양한 가능성을 열어줍니다.","ogImage":{"url":"/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_0.png"},"coverImage":"/assets/img/2024-05-16-SQLServersSecretFeatureRunPythonandAdd-OnsNativelyInSQLServer_0.png","tag":["Tech"],"readingTime":7},{"title":"Microsoft Fabric를 위한 확장 가능한 데이터 수집 프레임워크 구축하기","description":"","date":"2024-05-16 17:14","slug":"2024-05-16-BuildingascalabledataingestionframeworkforMicrosoftFabric","content":"\n\n이 기사는 고객 교류 중 자주 논의되는 주제인 데이터 엔지니어링 확장에 대해 다룹니다. 데이터 수집 및 유효성 검사 프로세스를 간소화하기 위한 방법을 어떻게 향상시킬 수 있을까요? 이 질문은 복잡하며, 원하는 대상 아키텍처, 데이터 품질 및 모델링 요구 사항, 메타데이터 관리 등과 같은 다양한 측면과 얽힌 문제입니다.\n\n데이터 엔지니어링 확장의 주요 도전 과제 중 하나는 코드 작성, 매개변수 설정, 파이프라인 테스트 및 성능 모니터링과 같은 반복적이고 종종 지루한 작업을 처리하는 out-of-the-box 솔루션이 부재한 점입니다. 이 문제는 서로 다른 조직들 사이에 요구 사항의 크고 다양한 변화 때문에 주로 발생합니다. 어떤 회사들은 원본 시스템에 대해 기술 표준을 매우 다양화시키지만, 다른 회사들은 공급업체가 적고 복잡한 원본 시스템을 유지합니다. 어떤 회사들은 데이터를 광범위하게 층으로 나누는 것을 선호하지만, 다른 회사들은 권장하는 3단계 Medallion Architecture로 충분히 만족합니다. 어떤 회사들은 다른 팀 간에 데이터 큐레이션을 민주화하는 것보다 중앙 통제를 선호합니다.\n\n복잡성과 다양성과 무관하게, 대부분의 기업의 목표는 데이터 엔지니어링을 간소화하기 위한 확장 가능한 수집 프레임워크를 구축하는 것입니다. Microsoft Fabric을 위한 최소한의 실용 제품을 만들면서 이 작업이 어떻게 이루어지는지 알아봅시다.\n\n주의! 이 글은 깊이 있는 포괄적인 블로그 글입니다. Microsoft Fabric를 직접 다뤄보고 데이터 파이프라인을 시작하고, 메타데이터 중심의 수집 프레임워크를 구현하고, 모든 것을 매력적이고 유익하게 통합해 보겠습니다. 서로 다른 섹션 간에 추가적인 고려 사항을 제공할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 모든 공학 작업을 표준화할 수는 없습니다\n\n확장성의 핵심은 메타데이터 중심의 수집 프레임워크를 구축하는 데 있습니다. 이를 통해 대부분의 작업을 자동화할 수 있습니다. 그러나 데이터 수집 또는 추출의 모든 단계를 쉽게 표준화하거나 동일한 수준의 자동화를 달성할 수 있다고 이해하는 것이 중요합니다. 이러한 과정을 자세히 살펴보고 왜 복잡성이 다양한지 이해해보겠습니다.\n\n여러 단계와 우리가 구현할 수 있는 자동화와 표준화의 정도를 설명하는 시각화를 만들었습니다. 왼쪽에는 추출 및 수집 과정이 있습니다. 데이터 엔지니어링의 첫 번째 단계는 기술과 공급업체 솔루션의 다양성 때문에 복잡합니다.\n\n예를 들어, 일부 공급업체는 고유한 API (Application Programming Interfaces) 또는 고유의 데이터 형식을 사용하여 데이터 추출을 수행합니다. 공급업체가 CSV 내보내기 형식 만 지원하는 경우, 여러분은 이에 맞게 프로세스를 조정해야 합니다. 한 번 이 장벽을 극복하고 데이터가 표준화 (Delta) 형식으로 제공되면, 더 많은 표준화와 자동화를 진행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n우리 다이어그램으로 돌아와서, 브론즈(원시 데이터)에서 실버(정제 및 히스토라이즈된 데이터)로의 전환은 상당히 더 직관적으로 보입니다. 이 단계에서의 변환은 일반적으로 예측 가능하고 비교적 간단하여, 컬럼 이름 변경, 필터 적용, 데이터 기본값 설정 등의 작업을 포함합니다. 보다 예측 가능한 성격 때문에, 데이터 엔지니어링의 이 단계는 매개변수화하기가 더 쉽고 효율적이며 관리하기 용이합니다.\n\n데이터 엔지니어링 프로세스의 마지막 단계는 정제된 데이터를 소비자에게 전달하는 것을 포함합니다. 이 단계는 통합에 필요한 까다로운 비즈니스 로직 때문에 주로 매개변수화하기가 어렵습니다. 이 로직은 메타데이터만을 사용하여 쉽게 설명하기 어렵습니다. 그러나 우리는 워크플로우에 표준화 수준을 도입할 수 있습니다. 템플릿과 서비스를 활용하여 프로세스를 최적화하여 효율성을 높이고 더 효율적이고 관리하기 쉽도록 만들 수 있습니다.\n\n따라서 데이터 엔지니어링의 다른 단계들의 변화를 이해하고 관리하는 것은 확장 가능한 데이터 수집 프레임워크를 구축하는 데 중요합니다. 이제 손을 댄 예제를 사용하여 구체적인 내용으로 살펴봅시다.\n\n## MVP의 사전 요구 사항\n\n<div class=\"content-ad\"></div>\n\n저는 제안하는 해결책이 Microsoft Fabric을 사용한다고 합니다. Microsoft Fabric에 익숙하지 않다면, 최신 데이터 솔루션을 구축하기 위해 설계된 클라우드 네이티브 플랫폼으로 Lakehouse 아키텍처를 활용하는 강력한 하이브리드 솔루션입니다. 이는 데이터 웨어하우스와 데이터 레이크의 기능을 결합한 것입니다.\n\n이 블로그 포스트에서는 Azure SQL에 호스팅된 AdventureWorks 샘플 데이터베이스를 활용하고, 모든 메타데이터를 저장하기 위해 또 다른 Azure SQL 데이터베이스를 활용할 것입니다. 따라서 이 글에서 안내하는 작업을 복제하려면 먼저 이러한 서비스를 배포해야 합니다.\n\nMicrosoft Fabric을 구성하려면 적어도 하나의 워크스페이스, Bronze, Silver 및 Gold라는 세 개의 Lakehouse, 그리고 비어 있는 데이터 파이프라인을 생성해야 합니다. 이 기본 설정은 데이터 엔지니어링 프로세스를 더 발전시키고 정제하는 데 필요한 토대를 마련할 것입니다.\n\n## 구성 — Copy Data 도구를 사용하여 파이프라인 생성하기\n\n<div class=\"content-ad\"></div>\n\n데이터 파이프라인 설계의 초기 단계는 데이터를 플랫폼으로 가져오는 것입니다. 이를 위해 우리는 Out-of-the-Box 커넥터를 활용할 것입니다. 저희 프로젝트에서는 다양한 데이터베이스 및 시스템을 지원하는 Data Factory를 사용할 것입니다. 우리의 데이터 소스는 AdventureWorks 샘플 데이터베이스를 호스팅하는 Azure SQL 서비스입니다.\n\n모든 데이터를 복사하는 대신, 관련 기능 테이블만 선택하기로 결정했습니다. Microsoft Fabric에서 새 데이터 파이프라인 항목을 정의하면, 첫 번째 단계는 Lookup 활동을 끌어다 놓는 것입니다. 설정으로 이동하여 외부를 선택하고 새로 만들어서 데이터 소스에 연결을 설정하기 위한 모든 필요한 정보를 입력하십시오. 원하는 테이블을 검색하기 위해 쿼리를 사용하십시오. 저는 그 목적으로 다음 문을 사용하고 있습니다:\n\n```js\nSELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE' AND TABLE_SCHEMA = 'SalesLT'\n```\n\n또한, '첫 번째 행만'을 선택 해제해 주세요. 이러한 단계를 따르면 나머지 파이프라인을 설계할 준비가 됩니다.\n\n<div class=\"content-ad\"></div>\n\n각 데이터 원본이 다르게 작동한다는 점을 염두에 두는 것이 중요합니다. 따라서 작업 중인 원본에 따라 파이프라인의 초기 부분이 달라질 수 있습니다. 이것을 기억하며 파이프라인 디자인을 진행해 주세요.\n\n## Parquet 파일과 폴더를 사용한 스냅샷의 히스토라이징\n\n수집할 데이터를 결정한 후, 다음 단계는 이를 데이터 플랫폼으로 전송하는 것인데, 여기서는 Microsoft Fabric을 사용합니다. Medallion 아키텍처의 맥락에서, 이 데이터를 Bronze이라는 첫 번째 레이어에 저장할 것입니다. 이를 위해 두 가지를 달성하고자 합니다: 먼저, 파일 섹션에서 Parquet을 사용하여 모든 스냅샷의 히스토리를 만들 것이고, 두 번째로, 가장 최근 데이터를 Delta 테이블로 승격시킬 것입니다.\n\n이 두 가지 작업을 왜 수행하는 걸까요? 첫째, 히스토리컬 아카이브를 생성하면 어떤 손상된 레이어든 복구할 수 있습니다. 가장 최근 복사본만 유지했다면 오류 발생 시 데이터를 복구할 수 없었을 것입니다. 그래서, 시간을 되돌아가 데이터셋을 다시 처리할 수 있습니다. 둘째, 가장 최근 데이터를 Delta 파일로 승격시킴으로써 기계 학습이나 리포팅과 같은 서비스를 사용한 아훕적인 발견이 쉬워집니다.\n\n<div class=\"content-ad\"></div>\n\n두 가지 목표를 달성하기 위해 모든 필수 단계를 포함하는 ForEach 활동을 만들 것입니다. 이미 최종 결과를 보여줬지만, 이 작업을 복제하려면 ForEach 작업 내 첫 번째 활동은 Copy Data Activity가 될 것입니다.\n\n소스 파일을 선택하려면 소스 섹션으로 이동하세요. 이미 설정한 기존 연결을 재사용하세요. 테이블에 대해 두 가지 인수를 사용하겠습니다:\n\n```js\n@item().table_schema\n@item().table_name\n```\n\n대상으로는 Bronze Lakehouse를 사용할 것입니다. 워크스페이스에 아직 Bronze Lakehouse 항목을 만들지 않았다면 먼저 생성해야 합니다. 위치로는 Root Folder를 Files로 선택하세요. 그런 다음 파일 경로에 다음 식을 사용하세요. 저는 간단한 yyyyMMdd 분할 방식을 사용하고 있지만, 더 많은 데이터를 저장하려면 타임스탬프 또는 다른 방식을 사용해도 괜찮습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n@formatDateTime(utcnow(), 'yyyyMMdd')\n@concat(item().table_schema,'.',item().table_name,'.parquet')\n```\n\n위 단계를 완료하신 후에는 파이프라인을 실행하고 브론즈 레이어를 확인하여 결과를 유효성 검사하세요. 제 경우에는 결과가 다음과 같이 나타납니다. 즉시 다양한 폴더가 많이 보입니다. 각 폴더는 전달 날짜별로 구성되어 해당 일의 모든 기능 데이터를 포함합니다. 이 접근 방식은 데이터를 깔끔하고 연대적으로 배치하여 특정 정보를 찾기 쉽게 만듭니다.\n\n한 번 더 강조하자면, 채택된 방법론은 다루고 있는 소스 시스템의 종류에 크게 의존합니다. 이 시나리오에서는 Parquet 파일을 사용하여 데이터를 일괄 적재합니다. 그러나 형식은 CSV, XML, JSON 등 다양할 수 있습니다. 또한 적재 방법도 달라질 수 있습니다. 완전한 로드를 사용하는 대신 델타를 로드하거나 실시간 적재를 선택할 수도 있습니다.\n\n## 최신 Parquet 파일을 Delta 테이블로 승격하기\n\n\n<div class=\"content-ad\"></div>\n\n다음 단계는 가장 최근 파일들을 Delta 테이블로 프로모션하는 것입니다. 이렇게 하면 이전에 처리된 데이터셋이 그대로 유지됩니다. 노트북을 사용하여 모든 것을 Bronze으로 프로모션할 것입니다. 따라서 파이프라인으로 돌아가서 ForEach 활동을 열고 노트북 활동을 드래그하여 Copy 활동에 연결하세요. \"새로 만들기\" 버튼을 클릭하여 하나를 생성해주세요.\n\n그 다음, 아래 제공된 내용을 새롭게 생성된 노트북에 복사하여 붙여넣으세요. 첫 번째 셀은 인수 셀이어야 하므로 해당 설정을 조정해야 할 수 있습니다. 또한 출력 위치를 변경하는 것을 기억하세요. 아래 코드에서 OneLake 위치를 하드코딩했지만, 실제 제품 환경에서는 해당 인수를 전달하려고 할 것입니다. 동일한 메타스토어 구성을 사용하고 소스 시스템을 대상 워크스페이스 및 레이크하우스 위치에 링크하는 방법을 고려할 수 있습니다.\n\n작업을 완료하면 노트북 옵션으로 돌아가세요. cw_table 및 cw_location 매개변수를 설정하고 준비가 되면 파이프라인을 실행하세요.\n\n```js\n@item().table_name\n@formatDateTime(utcnow(), 'yyyyMMdd')\n```\n\n<div class=\"content-ad\"></div>\n\n만약 모든 것이 원활하게 진행된다면, 브론즈 레이어 내에 새로 생성된 델타 테이블이 표시될 것입니다. 모든 테이블을 삭제하고 다시 구축하고 있음을 알려드립니다. 델타의 테이블 히스토리 기능을 활용할 필요가 없으니, 모든 데이터의 이전 복사본을 테이블 섹션에서 유지하고 있기 때문입니다.\n\n지금까지 추출 및 적재 접근 방식은 작업 중인 소스 시스템에 따라 달라질 수 있음을 염두에 두는 것이 중요합니다. 우리의 경우, 프로세스는 비교적 간단했습니다. 그러나 다른 상황에서는 데이터가 임시 위치나 랜딩 존에 착륙하거나 다른 형식으로 제공될 수 있습니다. 확장성을 보장하기 위해, 기술적 파일 형식 변환, 복잡한 구조의 해제, 추가 작업 등을 위한 반복 가능한 스크립트를 개발하는 것을 제안합니다. 또한 가능한 경우 최선의 방법을 채택하세요.\n\n## 메타데이터 저장소 구현\n\n이 글의 두 번째 부분에 도달했습니다. 이제 표준 파일 형식을 사용하여 브론즈 레이어에 데이터를 성공적으로 착륙시킨 상태에서 우리는 실버 레이어로 모든 것을 유효성 검사하고 히스토라이징하여 확장성을 더욱 강화하기 ready합니다. 이 부분은 더 간단하고 예측 가능합니다. 필터링, 열 이름 바꾸기, 데이터 비교와 같은 변환 작업은 비교적 간단하기 때문에 이러한 단계를 더 쉽게 자동화할 수 있습니다. 이를 위해 모든 중요한 구성을 보유하는 메타데이터 저장소를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n두 번째 Azure SQL 데이터베이스에는 모든 메타데이터를 보유하는 테이블을 생성하는 스크립트를 사용할 것입니다. 따라서 Azure SQL 데이터베이스로 이동하여 아래 제공된 스크립트를 실행하십시오. 제 목표는 철저한 접근 방식을 제공하는 것이 아니라 일반적인 개념을 설명하는 것이므로 참고해 주세요.\n\n```js\nCREATE TABLE SchemaMetadata  \n(  \n    Id INT IDENTITY(1,1) PRIMARY KEY,  \n    TableName NVARCHAR(128),  \n    ColumnName NVARCHAR(128),  \n    DataType NVARCHAR(128),  \n    CharacterMaximumLength INT,  \n    NumericPrecision INT,  \n    NumericScale INT,  \n    IsNullable NVARCHAR(3),  \n    DateTimePrecision INT,  \n    IsPrimaryKey BIT DEFAULT 0  \n)  \nGO \n```\n\n다음으로는 AdventureWorks 예제 데이터베이스에서 스키마 메타데이터를 검색해야 합니다. 이를 위해 정보 스키마에서 모든 메타데이터를 읽는 간단한 스크립트를 개발했습니다. 이 스크립트는 메타데이터 저장소에 대한 INSERT 문을 생성하는 데 사용될 수 있습니다. 이 예에서는 하나의 테이블로 설명하고 있음을 참고해 주세요.\n\n```js\nSELECT   \n    TABLE_NAME as 'TableName',  \n    COLUMN_NAME as 'ColumnName',  \n    DATA_TYPE as 'DataType',  \n    CHARACTER_MAXIMUM_LENGTH as 'CharacterMaximumLength',  \n    NUMERIC_PRECISION as 'NumericPrecision',  \n    NUMERIC_SCALE as 'NumericScale',  \n    IS_NULLABLE as 'IsNullable',  \n    DATETIME_PRECISION as 'DateTimePrecision',  \n    COLUMNPROPERTY(OBJECT_ID(TABLE_NAME), COLUMN_NAME, 'IsIdentity') as 'IsPrimaryKey'  \nFROM   \n    INFORMATION_SCHEMA.COLUMNS  \nWHERE   \n    TABLE_NAME = 'Address'\n```\n\n<div class=\"content-ad\"></div>\n\n수집한 스키마 메타데이터를 활용하여 저희 메타데이터 저장소의 Azure SQL 데이터베이스로 돌아가서 모든 레코드를 삽입해주세요. 아래는 참고용 예시입니다.\n\n```js\nINSERT INTO SchemaMetadata  \n(TableName, ColumnName, DataType, CharacterMaximumLength, NumericPrecision, NumericScale, IsNullable, DateTimePrecision, IsPrimaryKey)  \nVALUES  \n('Address', 'AddressID', 'int', NULL, 10, 0, 'NO', NULL, 1),  \n('Address', 'AddressLine1', 'nvarchar', 60, NULL, NULL, 'NO', NULL, 0),  \n('Address', 'AddressLine2', 'nvarchar', 60, NULL, NULL, 'YES', NULL, 0),  \n('Address', 'City', 'nvarchar', 30, NULL, NULL, 'NO', NULL, 0),  \n('Address', 'StateProvince', 'int', NULL, 10, 0, 'NO', NULL, 0),  \n('Address', 'PostalCode', 'nvarchar', 15, NULL, NULL, 'NO', NULL, 0),  \n('Address', 'CountryRegion', 'geography', NULL, NULL, NULL, 'YES', NULL, 0),  \n('Address', 'rowguid', 'uniqueidentifier', NULL, NULL, NULL, 'NO', NULL, 0),  \n('Address', 'ModifiedDate', 'datetime', NULL, NULL, NULL, 'NO', 3, 0);  \nGO\n```\n\n아래에 최종 결과가 표시됩니다.\n\n지금까지 수집한 메타데이터는 테이블 이름, 열, 데이터 형식, 널 가능 속성, 기본 키 등을 포함한 기본적인 내용뿐입니다. 계속해서 특정 도메인 정보, 여러 소스에 대한 메타데이터, 민감도 레이블과 같은 보안 메타데이터, 행 필터링을 위한 메타데이터 등으로 확장할 수 있습니다. 또한 소스 열을 대상 열로 이름 바꾸기, 간단한 조인 또는 연합 실행, 관련 없는 데이터 필터링 등과 같은 더 복잡한 처리 로직에 대한 메타데이터도 통합할 수 있습니다. 최종적으로 메타데이터의 범위는 특정 요구 사항 및 미래 사용 사례에 따라 다를 것입니다.\n\n<div class=\"content-ad\"></div>\n\n저희 실습 예제로 돌아가 봅시다. 준비가 되셨다면 ForEach 활동으로 돌아가세요. 여기에 새 Lookup 활동을 추가해 보세요. 이 작업은 워크플로에 끌어다 놓으면 새 연결을 설정하여 메타데이터 저장소에서 읽어옵니다. 호출을 만들기 위해 테이블 이름을 입력으로 사용할 것이기 때문에 다음 표현식을 사용하세요:\n\n```js\n@concat('SELECT * FROM [dbo].[SchemaMetadata] WHERE TableName=''',item().table_name,'''')\n```\n\n이 설정에서는 table_name 인수만 전달하고 있음을 유의해 주세요. 보다 상세한 설정에서는 데이터베이스 이름과 버전 등 추가 인수를 사용할 것으로 예상합니다.\n\n## Great Expectations을 활용한 데이터 품질 검증하기\n\n<div class=\"content-ad\"></div>\n\n다음으로 데이터 처리 및 처리에 자동화 기능을 도입해 보겠습니다. 브론즈 단계의 데이터를 메타스토어의 메타데이터를 사용하여 유효성을 검사하는 것이 첫 번째 단계입니다. 이를 위해 새 'If Condition' 활동을 끌어다 놓으세요.\n\n조건을 설정하기 위해 LookupMetadata 활동의 수를 검증하는 식을 사용할 것입니다. 수가 0을 초과하면 처리할 메타데이터가 있는 것을 나타냅니다.\n\n```js\n@greater(activity('LookupMetadata').output.count, 0)\n```\n\n이제 앞으로 나아가서 워크스페이스 내에서 Fabric 환경을 설정해야 합니다. 이 환경에는 데이터 유효성을 검사하는 데 필요한 패키지가 포함될 것입니다. 이번 데모에서는 데이터 품질 검증을 위한 인기 있는 오픈 소스 도구인 'Great Expectations'를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n시작하려면 작업 공간으로 이동한 후 상단의 '새로 만들기'를 클릭하십시오. 드롭다운에서 '새 환경'을 선택하십시오. 이 작업으로 대화 상자가 나타납니다. 여기서 공개 라이브러리 아래에 'great-expectations'를 추가해야 합니다. 이 작업을 완료한 후 '게시'를 클릭하십시오. 새 환경을 사용할 준비가 될 때까지 시간이 걸릴 수 있다는 점을 참고해 주세요.\n\n이후에는 데이터로 돌아가 'If Condition' 내부에 새로운 'Notebook' 활동을 추가하십시오. '설정 및 매개변수' 섹션에 'metadata'라는 새 매개변수를 추가해야 합니다. 해당 표현식을 사용하여 다음과 같이 입력하십시오.\n\n```js\n@string(activity('LookupMetadata').output.value)\n```\n\n이제 Python 스크립트 아래에서 제공된 내용을 사용하여 새로운 Notebook을 작성할 시간입니다. 이 스크립트에서는 우선 'metadata' 매개변수를 확인하고 JSON 형식으로 변환합니다. 그런 다음, 메타데이터를 반복하면서 데이터 품질 규칙을 작성합니다. 게다가, 데이터가 잘못된 경우 프로세스를 중지할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n노트북 자체에 대해: 새로 생성한 환경을 노트북 환경 내에서 선택해야 합니다.\n\n모든 단계를 완료한 후에 데이터 파이프라인을 시작하세요. 모든 것이 원활하게 진행되면, 메타데이터가 사용되어 데이터 품질을 동적으로 검증합니다. 이 유효성 검사는 기본적이며 현재는 열 이름만을 확인합니다. 그러나 이 스크립트는 기본 키 확인, 널 값 허용 여부, 고유성 등을 포함하여 얼마든지 확장할 수 있습니다.\n\n## Silver에 처리된 데이터의 히스토라이징\n\n데이터 파이프라인 생성의 이 부분에서는 Slowly Changing Dimensions(Type 2)를 사용하여 데이터를 히스토라이징할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n형태를, 친절한 톤으로 번역하겠습니다.\n\nSCDs(Slowly Changing Dimensions)는 현재 및 과거 데이터를 시간이 지남에 따라 단일 테이블에 통합합니다. 이 접근 방식은 여러 가지 이유로 중요합니다. 먼저, SCD는 데이터 진화의 전체적인 관점을 제공하여 트렌드 분석을 수행하고 머신러닝 모델을 구축하는 데 중요합니다. 이 과정은 데이터 진행을 시간에 따라 이해하는 데 도움이 됩니다. 둘째, SCD는 데이터 무결성과 일관성을 유지하는 데 도움이 됩니다. 마지막으로, SCD를 사용하면 효율성이 향상됩니다. 특정 시간대의 필요한 데이터만 추출하여 수백 개의 파일을 다시 처리할 필요가 없어집니다. 아래는 SCD 테이블이 어떻게 보이는지의 결과를 보실 수 있습니다.\n\n데이터 품질 유효성 검사와 마찬가지로, 이 파이프라인의 이 부분도 자동화할 수 있습니다. 데이터를 비교하고 히스토리를 저장하기 위한 또 다른 노트북 활동을 추가해 봅시다.\n\n이 작업을 위해 조금의 메타데이터를 사용할 것입니다. 'LookupMetadata' 활동으로부터 테이블 이름과 메타데이터를 제공하겠으며, 다른 노트북에 전달하여 아래와 같이 공유될 것입니다.\n\n```js\n@item().table_name\n@string(activity('LookupMetadata').output.value)\n```\n\n<div class=\"content-ad\"></div>\n\n위의 스크립트에 대해 이야기해 봐요. 메타데이터 내에 기본 키가 제공된 경우, 해당 키는 비교를 위한 기능 키로 사용됩니다. 그렇지 않은 경우, 행 내의 모든 값들을 입력으로 사용하여 고유 해시 값을 생성합니다. 저는 MERGE SQL 문을 사용하지 않기로 결정했는데요, 이전 데이터셋에서 같은 데이터를 다시 제출하거나 이전 데이터를 삭제해도 레코드를 완전히 제어하고 닫고 다시 열기를 원하기 때문입니다.\n\n실버 레이어에서는 Delta Lake의 시간 여행 기능을 활용합니다. 따라서 테이블 자체에 기능적인 히스토리 데이터를 저장하는 것 외에도 트랜잭션 로그에서 변경 사항을 기록합니다. 이 접근 방식은 잘못 처리된 데이터셋을 지우는 유연성을 제공합니다. 예를 들어, 같은 이름을 공유하는 모든 고객을 포함하는 고객 파일이 제공되었다고 가정해 보세요. 기술적 데이터 품질 유효성 검사는이 문제를 감지하지 못할 수도 있으며, 결과적으로 이 데이터가 실버 레이어로 침투할 가능성이 높아집니다. 그러면 이전 레코드가 모두 업데이트되고 닫힐 것입니다. 이 문제를 해결하기 위해 시간 여행을 사용하여 이전 상태로 되돌릴 수 있습니다. 그런 다음 원본 시스템에 보정된 전달을 요청하고, 유효성을 검사하고 검토하여 보정된 데이터를 실버 레이어에 다시 통합시킬 수 있습니다.\n\n## 실버 레이어를 위한 몇 가지 고려 사항\n\n위에서 언급한 전략을 구현하면, 실버 레이어까지 데이터는 여전히 소스 중심입니다. 이러한 모범 사례를 따르는 것이 좋습니다. 실버 레이어에서 소스 중심 데이터를 유지하면 데이터 소유권을 결정하는 데 도움이 됩니다. 소스와 일치하는 데이터 제품을 관리하고 구축하는 것이 목표라면 엔지니어들에게 다른 도메인 애플리케이션 간에 데이터를 교차 조인하지 않도록 안내하는 것이 중요합니다. 더 엄격한 설계 접근 방식은 각 소스 시스템을 자체 Lakehouse 엔터티와 일치시키는 것입니다. 이렇게 하면 더 깨끗하고 효율적인 데이터 관리 전략을 보장하며, 데이터 중심 프로젝트를 감독하고 실행하기가 더 쉬워집니다.\n\n<div class=\"content-ad\"></div>\n\n실버 레이어를 계산 또는 비즈니스 로직과 같은 요소로 강화하는 계획을 세울 때 고려해야 할 사항이 몇 가지 있어요. 이러한 변환은 쉽게 매개변수화할 수 없어요. 그 결과, 이전 자동화된 단계를 기반으로하는 추가적인 실버 레이어를 만들어야 할지 고민해보실 필요가 있을지도 모르겠네요. 이 전략은 느슨한 결합과 유연성을 촉진하여 효율적인 데이터 관리를 위한 필수 구성 요소를 제공해요. 이 추가 레이어를 설정하는 데 더 많은 노력이 필요할 수 있지만, 제공하는 유연성은 투자의 가치가 있답니다. 이 접근 방식은 데이터 시스템의 기능을 향상시킬 뿐만 아니라 (원본에 맞게 정렬된) 데이터 제품을 비즈니스 요구 사항에 추가로 적응시킬 수 있는 것을 보장해요.\n\n## 실버에서 골드로 처리하기\n\n메달리온 아키텍처에서 골드 레이어는 \"프로젝트별\" 또는 \"사용 사례별\" 데이터로 구성되어 있어서 즉시 사용할 수 있는 데이터를 보관하고 있어요. 이 통합 단계는 복잡하며 특정 요구 사항에 매우 의존하고 있기 때문에 이전에 언급한 대로 복잡한 비즈니스 규칙을 포함할 가능성이 있어요. 변환에는 후속 처리 단계, 계산, 보강, 사용 사례별 최적화 등이 포함될 수 있어요.\n\n골드 레이어는 아키텍처의 범위에 크게 의존하기 때문에 가장 복잡한 부분을 제공할 가능성이 높아요. 가장 간단한 설정에서는 골드 레이어가 통합과 사용 사례 레이어로 모두 작동할 수 있어요. 즉, 데이터를 먼저 통합한 다음 조직의 고유한 사용 사례에 따라 특정 하위 집합을 선택하여 맞춤화하여 구조화할 수 있어요. 이 맞춤화된 접근 방식은 데이터 아키텍처가 견고하며 조직의 특정 요구 사항과 목표에 최적으로 정렬되어 있는 것을 보장해줘요.\n\n<div class=\"content-ad\"></div>\n\n여러 도메인을 포함하는 아키텍처를 사용한다면, 골드 계층은 여러 하위 계층으로 나눌 수 있습니다. 예를 들어, 하나의 계층은 다양한 소스 시스템에서 초기 데이터 통합에 특화될 수 있습니다. 이어지는 계층은 특정 사용 사례를 위해 설계되며, 따라서 특정 요구 사항을 충족하기 위해 하위 집합을 맞춤화할 수 있습니다. 또 다른 계층은 새로운 데이터 제품을 개발하기 위해 할당될 수 있습니다. 이러한 계층들은 집계로 통칭되는 것을 형성하게 됩니다. 이에 대해 더 알고 싶다면, 메달리온 아키텍처의 층화에 관한 다른 블로그 포스트를 참조해 주세요.\n\n다음 블로그 포스트에서는 Microsoft Fabric 내에서 DBT를 사용하여 실버와 골드 간의 데이터 변환과 통합을 표준화하는 방법을 시연할 예정입니다. DBT는 템플릿과 명령줄 인터페이스를 활용하는 데이터 변환 도구로, 프로세스를 간단하게 만들어 줍니다.\n\n## 결론\n\n메타데이터 기반의 수집 프레임워크 없이 수십 개 또는 수백 개의 소스 시스템을 도입하는 것은 어려운 작업일 수 있습니다. 복잡성은 각 소스 시스템을 모든 스크립트, 파이프라인, 활동 등을 수동으로 관리해야 한다는 점에서 발생합니다. 이는 시간이 많이 소요되며 오류가 발생하기 쉽고 표준화가 부족합니다.\n\n<div class=\"content-ad\"></div>\n\n메타데이터 주도 방식은 이 프로세스를 간소화합니다. 전통적인 코드 기반 전략에 비해 여러 가지 장점을 제공합니다:\n\n- 메타데이터 주도 방식을 채택함으로써 파이프라인의 다양성과 적응성을 향상시킬 수 있습니다. 이를 통해 다양한 데이터 원본과 목적지를 지원할 수 있으며, 서로 다른 형식, 스키마 및 빈도에 맞도록 할 수 있습니다. 또한, 처리 기술을 조정함으로써 전체 로드, 증분 로드 또는 전체 덮어쓰기와 같은 여러 흡수 패턴을 수용할 수 있습니다.\n- 메타데이터 주도 프레임워크는 작성하고 유지해야 하는 코드 양을 크게 줄일 수 있습니다. 여러 노트북을 작성하는 대신, 메타데이터를 기반으로 다양한 데이터 원본 및 목적지를 처리할 수 있는 통합 파이프라인을 생성하면 됩니다.\n- 이 방식을 통해 개발 및 배포 프로세스가 크게 단순화되고 표준화됩니다. 메타데이터를 사용하여 손쉽게 검증 및 수정을 추가할 수 있어 코드를 변경하거나 파이프라인을 재배포할 필요가 없습니다.\n- 파이프라인 설계에 따라 병렬 처리와 동시성을 달성하여 여러 복사 또는 노트북 활동을 동시에 실행할 수 있습니다. 이렇게 하면 파이프라인의 성능과 효율성을 최적화할 수 있습니다.\n\n이 글이 도움이 되었기를 바랍니다. 데이터 품질, Microsoft Fabric, Microsoft Purview 등에 관한 기사를 더 많이 게시했음을 유의해 주세요! 또한, 이 블로그 게시물의 일부 내용은 \"대규모 데이터 관리\"이라는 책에서 인용되었음을 참고해 주세요.","ogImage":{"url":"/assets/img/2024-05-16-BuildingascalabledataingestionframeworkforMicrosoftFabric_0.png"},"coverImage":"/assets/img/2024-05-16-BuildingascalabledataingestionframeworkforMicrosoftFabric_0.png","tag":["Tech"],"readingTime":15},{"title":"Tidymodels를 사용하여 고객 이탈 분류 모델 구축과 평가하기","description":"","date":"2024-05-16 17:11","slug":"2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels","content":"\n\n![표](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_0.png)\n\n모델을 처음 배울 때, 아주 오래 전에는 서로 다른 패키지에서 서로 다른 매개변수 이름으로 다양한 방법으로 모델을 구축하는데 여러 가지 방법이 있었습니다. 그런 다음 tidymodels를 사용하기 시작하면서 모델 빌딩을 일관된 방식으로 다양한 상황과 엔진에서 쉽게 작성할 수 있는 방법에 대해 기쁘게 놀랐습니다. 이는 서로 다른 모델에 대해 백 가지 형식과 매개변수를 기억할 필요가 없어지며 결과를 비교하기가 훨씬 쉬워졌다는 것을 의미했습니다.\n\n이 글에서는 매우 일반적인 고객 이탈 예측 시나리오를 예로 들어, tidy한 방법으로 모델을 구축하고 결과를 비교하는 방법에 대해 안내하겠습니다.\n\n# 코드\n\n<div class=\"content-ad\"></div>\n\n이 글의 모든 코드는 내 GitHub Repo에서 찾을 수 있어요.\n\n# 모델링을 시작해봅시다\n\n## 단계 0: 환경 설정\n\n필요한 패키지를 설치하려면 install.packages(\"package_name\")과 같은 개별 명령을 실행하거나 아래 명령을 실행하여 모든 패키지를 로드하세요. 해당 명령은 이미 존재하지 않는 경우에만 패키지를 설치한 후 로드합니다.\n\n<div class=\"content-ad\"></div>\n\n```r\n# 패키지 불러오기\n\npackages <- c(\"tidyverse\", \"tidymodels\", \"skimr\", \"GGally\")\n\npackage.check <- lapply(packages, FUN = function(x) {\n  if (!require(x, character.only = TRUE)) {\n    install.packages(x, dependencies = TRUE)\n    library(x, character.only = TRUE)\n  }\n})\n```\n\n## 단계 1: 데이터셋\n\n이 연습에서는 Simarpreet Singh의 Kaggle에서 Creative Commons Attribution 4.0 International 라이선스(CC BY 4.0)로 사용 가능한 Binary Classification of Bank Churn Synthetic Data를 사용합니다.\n\n데이터셋은 \"Exited\" 열을 포함하고 있으며 이는 고객이 떠났는지 여부를 나타냅니다. 이를 예측할 것입니다. 먼저 데이터셋을 로드하고 데이터셋의 모습을 살펴볼 것입니다:\n\n<div class=\"content-ad\"></div>\n\n```R\nbankchurn_df <- read.csv(\"./data/bank_churn.csv\")\n\nbankchurn_df |> \n  glimpse()\n```\n\n<img src=\"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_1.png\" />\n\n## Step 2: 데이터 정제\n\n저는 모델의 특성으로 사용되지 않을 ‘Surname’이라는 용어를 포함하는 열을 제거할 것입니다. 또한 이진 분류를 위해 출력 변수인 Exited를 팩터 변수로 변환할 것입니다.\n  \n\n<div class=\"content-ad\"></div>\n\n```R\nbankchurn_df_upd <- bankchurn_df |> \n  select(Exited, everything()) |> \n  mutate(Exited = as.factor(Exited)) |> \n  select(-contains(\"Surname\")) \n```\n\n## 단계 3: 탐색적 데이터 분석\n\n특정 열에 대해 파고들기 전에 데이터를 초기 이해하기 위해 skim() 명령을 사용할 것입니다. 이 명령은 각 열의 분포를 제공하여 모든 변수가 동일한 척도에 있지 않다는 점을 알 수 있도록 도와줍니다.\n\n```R\nbankchurn_df_upd |> \n  skim()\n```\n\n<div class=\"content-ad\"></div>\n\nmd\n![이미지](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_2.png)\n\n이제 ggpairs()를 사용하여 몇 가지 예측 변수를 조사하여 출력 변수와의 관계를 이해해 보겠습니다.\n\n```js\nbankchurn_df_upd |> \n  select(Exited, CreditScore, Age, Tenure, Balance) |> \n  ggpairs(mapping = aes(colour = Exited, alpha = 0.3)) +\n  scale_fill_manual(values=c('darkgreen', 'red')) +\n  scale_colour_manual(values=c('darkgreen', 'red'))\n```\n\n![이미지](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_3.png)\n\n\n<div class=\"content-ad\"></div>\n\n이 패키지는 변수간의 관계를 빠르게 이해하는 데 도움이 됩니다. 이제 성별, 위치 및 출력 변수 간의 관계를 확인하겠습니다.\n\n![image](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_4.png)\n\n상관 매개변수는 예측 변수 간에 강한 관계가 있는지 확인합니다.\n\n이제 모델링 단계로 넘어가겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 단계 4: 훈련/테스트 분리\n\n모든 모델링 과정에서처럼, 첫 번째 단계는 학습된 모델 정확도를 예측할 무작위 테스트 세트를 보류하는 것입니다. 이를 위해 initial_split() 명령을 사용할 것입니다. 재현성을 위해 시드를 설정하고 training() 및 testing() 함수를 사용하여 분할에 액세스할 것입니다.\n\n```js\nset.seed(123)\nbc_split <- initial_split(bankchurn_df_upd, prop = 3/4, strata = \"Exited\")\n\ntrain_data <- training(bc_split)\ntest_data <- testing(bc_split)\n```\n\n## 단계 5: 피처 엔지니어링\n\n<div class=\"content-ad\"></div>\n\n이전에 수행한 탐색적 데이터 분석을 바탕으로, 몇 가지 기본 기능 엔지니어링을 수행할 것입니다. 이를 위해 recipe()를 사용하면 매우 쉽게 반복 가능한 단계 세트를 생성할 수 있습니다. 이를 통해 일반적인 기능 엔지니어링 작업에 대해 자세한 코드를 작성할 필요가 없습니다. 이 시나리오에서 다음을 수행하려고 합니다:\n\n- 명목 변수를 더미 변수로 변환하되 결과 변수는 따로 두기.\n- 단일 값만 포함하거나 분산이 0인 변수를 제거.\n- 숫자 예측 변수를 정규화하기. 이는 일부 변수가 서로 다른 척도에 있기 때문에 필요합니다.\n\n```r\nbc_recipe <- recipe(Exited ~ ., data = bankchurn_df_upd) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric()) %>%\n  prep()\n\nbc_recipe %>%\n  bake(new_data = NULL) \n```\n\n<img src=\"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_5.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 단계 5: 모델 명세 및 워크플로우 작성\n\n저는 파스니프(parsnip)를 사용하여 두 가지 모델 명세와 워크플로우를 작성할 것입니다. 이를 통해 다양한 모델에서 이를 표준화된 접근으로 생성할 수 있는 방법을 보여줄 것입니다.\n\n```js\nlr_mod <- logistic_reg() |> \n  set_mode(\"classification\") |> \n  set_engine(\"glm\")\n\nlr_mod\n```\n\n![이미지](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_6.png)\n\n<div class=\"content-ad\"></div>\n\n이제 레시피를 모델 학습과 결합하는 워크플로를 만들겠습니다.\n\n```js\nlr_workflow <- \n  workflow() |> \n  add_model(lr_mod) |> \n  add_recipe(bc_recipe)\n\nlr_workflow\n```\n\n![Image](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_7.png/)\n\n이제 랜덤 포레스트 모델에 대해서도 위 단계를 반복할 것입니다. 모델 사양과 워크플로 생성의 유사한 구조에 주목하세요.\n\n<div class=\"content-ad\"></div>\n\n```R\nrand_forest_ranger_model <- rand_forest(\n  mode = \"classification\", mtry = 10, trees = 500, min_n = 20) |>\n  set_engine(\"ranger\", importance = \"impurity\") \n\nrand_forest_ranger_model\n\nrf_workflow <- workflow() |> \n  add_model(rand_forest_ranger_model) |> \n  add_recipe(bc_recipe)\n\nrf_workflow\n```\n\n<img src=\"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_8.png\" />\n\n<img src=\"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_9.png\" />\n\n## Step 6: 데이터 적합하기\n\n<div class=\"content-ad\"></div>\n\nfit() 함수를 사용하여 생성한 워크플로우를 활용하여 데이터를 학습 데이터 세트에 맞추겠습니다.\n\n```js\nlr_fit <- \n  lr_workflow |>\n  fit(data = train_data)\n\nrf_fit <- \n  rf_workflow |>\n  fit(data = train_data)\n```\n\n## 단계 7: 피처 중요도\n\n선형 모델의 경우 tidy()라는 명령을 사용할 수 있어 이전에 맞춘 모델의 구성 요소에 쉽게 액세스할 수 있습니다. 이를 액세스한 후, 예측 변수들을 특정 지표(이 경우 p-값)에 따라 정렬할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nlr_fit |> \n  extract_fit_parsnip() |> \n  tidy() |> \n  arrange(p.value)\n```\n\n![Image](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_10.png)\n\nRandom forest를 위해, extract_fit_parsnip() 함수를 사용하여 fit를 추출할 거예요. 그런 다음 importance() 명령어를 사용하여 특성과 그들의 지표를 추출할 거예요.\n\n```js\nextract_fit_parsnip(rf_fit)$fit |> \n  ranger::importance() |> \n  enframe() |> \n  arrange(desc(value))\n``` \n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_11.png\" />\n\n## Step 8: 테스트 데이터셋에서 예측하기\n\n두 모델 모두, 이전에 맞춘 모델에 predict() 함수를 사용하여 테스트 데이터셋에서 예측을 추출할 것입니다. 이진 분류의 경우, 기본적으로 이는 예측값으로 0 또는 1의 클래스 유형을 반환합니다. 확률 값을 얻기 위해, type 매개변수를 \"prob\"로 업데이트할 것입니다.\n\n```js\n# 로지스틱 회귀\nclass_pred_lr <- predict(lr_fit, test_data)\nprob_pred_lr <- predict(lr_fit, test_data, type = \"prob\")\n\n# 랜덤 포레스트\nclass_pred_rf <- predict(rf_fit, test_data)\nprob_pred_rf <- predict(rf_fit, test_data, type = \"prob\")\n```\n\n<div class=\"content-ad\"></div>\n\n아래는 클래스와 확률 출력 예시입니다:\n\n![Class and Probability Outputs](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_12.png)\n\n동일한 테이블 내에서 두 출력을 모두 접근하기 위해 이 두 종류의 예측을 단일 데이터프레임으로 결합할 것입니다. 나는 비즈니스 시나리오에 따라 두 출력 클래스가 중요하다고 생각하기 때문에 이를 결합하려 합니다. 그 후 이 출력을 원래의 테스트 데이터셋과 병합하여 결과를 비교할 수 있도록 클래스, 확률 및 원래 \"Exited\" 값이 동일한 테이블 안에 있는 것을 확인할 것입니다. 아래는 최종 병합된 테이블이 어떻게 생겼는지에 대한 내용입니다.\n\n```r\n# 로지스틱 회귀\nlr_preds_combined <- \n  data.frame(class_pred_lr, prob_pred_lr) |> \n  select(class = .pred_class, prob_no = .pred_0, prob_yes = .pred_1) |> \n  bind_cols(test_data)\n\n# 랜덤 포레스트\nrf_preds_combined <- \n  data.frame(class_pred_rf, prob_pred_rf) |> \n  select(class = .pred_class, prob_no = .pred_0, prob_yes = .pred_1) |> \n  bind_cols(test_data)\n```\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_13.png)\n\n이제 예측값을 가지고 있으니, 이 모델들의 정확도를 측정해 봅시다.\n\n## 단계 9: 성능 지표 이해\n\nROC 곡선은 서로 다른 임계값에서 이진 분류 방법의 성능을 보여줍니다. 이는 실제 양성 비율(TPR)을 거짓 양성 비율(FPR)에 대해 그립니다. roc_curve() 함수를 사용하여 roc 곡선의 값을 얻을 것입니다.\n  \n\n<div class=\"content-ad\"></div>\n\n```js\n# 로지스틱 회귀\nlr_roc <- lr_preds_combined |> \n  roc_curve(truth = Exited, prob_no) |> \n  mutate(model = \"로지스틱 회귀\")\n\n# 랜덤 포레스트\nrf_roc <- rf_preds_combined |> \n  roc_curve(truth = Exited, prob_no) |> \n  mutate(model = \"랜덤 포레스트\")\n\n# ROC curve 값 결합\nlr_roc |> \n  bind_rows(rf_roc) |> \n  glimpse()\n```\n\n<img src=\"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_14.png\" />\n\nautoplot() 함수를 사용하여 ROC 커브를 시각화할 수 있지만, 이 경우에는 두 모델의 ROC 커브를 동시에 보기 위해 ggplot2를 사용하여 처음부터 플로팅하겠습니다.\n\n```js\nlr_roc |> \n  bind_rows(rf_roc) |> \n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line() +\n  geom_abline(lty = 2) +\n  labs(y = \"True Positive Rate\", \n       x = \"False Positive Rate\",\n       title = \"ROC 커브\") +\n  theme_bw()\n```\n\n<div class=\"content-ad\"></div>\n\n헷갈리는 행렬(Confusion matrix)은 분류 모델의 성능을 요약하는 표입니다. Tidymodels 내 caret 패키지는 confusionMatrix()라는 함수를 제공하여 많은 유용한 정보를 제공합니다. 이를 사용하여 최종 모델을 선택하기 전에 두 모델의 지표를 비교할 것입니다.\n\n```R\n# 로지스틱 회귀\ncaret::confusionMatrix(lr_preds_combined$Exited,\n                       lr_preds_combined$class,\n                       positive = \"1\")\n\n# 랜덤 포레스트\ncaret::confusionMatrix(rf_preds_combined$Exited,\n                       rf_preds_combined$class,\n                       positive = \"1\")\n```\n\n<div class=\"content-ad\"></div>\n\n랜덤 포레스트 모델은 로지스틱 회귀 모델과 비교하여 정확도, 민감도 및 특이도를 포함한 대부분의 메트릭에서 성능이 더 좋습니다. 그러나 랜덤 포레스트의 계산 요구량은 높을 수 있습니다. 그래서 특정 비즈니스 시나리오에서 정확도 향상의 중요성에 따라, 어떤 모델이 강조되어야 할지 선택할 수 있습니다.\n\n# 다음 단계\n\n이 기사가 tidymodels 프레임워크 내의 패키지 시리즈가 제공하는 강력하고 일관된 접근 방법을 이해하는 데 도움이 되었기를 바랍니다. 가치를 보여주기 위해 두 모델만 다루었지만, 유사한 구문을 사용하여 구축할 수 있는 다른 모델들이 많이 있습니다.\n\n이러한 모델을 더욱 신뢰할 수 있게 만드는 또 다른 방법은 tidymodels에서 제공하는 도구를 사용하여 교차 검증하는 것입니다. 이에 대해 다른 기사에서 다룰 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n이 글의 모든 코드는 제 GitHub Repo에서 찾을 수 있어요. LinkedIn에서 제를 만나고 싶다면 연락해주세요.\n\n이 글의 모든 이미지는 다른 경우가 아닌 한 저자가 찍었어요.","ogImage":{"url":"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_0.png"},"coverImage":"/assets/img/2024-05-16-BuildingandEvaluatingCustomerChurnClassificationModelswithTidymodels_0.png","tag":["Tech"],"readingTime":10},{"title":"암스테르담에 관한 최고의 재미있는 사실 ","description":"","date":"2024-05-16 17:09","slug":"2024-05-16-IusedWindowsforaYearButImSwitchingBacktoMac","content":"\n\n그리고 여기에 그 이유가 있어요\n\n![image](/assets/img/2024-05-16-IusedWindowsforaYearButImSwitchingBacktoMac_0.png)\n\n저는 평생 MacOS 사용자입니다. 윈도우 컴퓨터는 한 번도 소유한 적이 없었는데요 — 작년쯤, 갑자기 강한 욕구와 필요성으로 윈도우 PC를 구매하게 되었습니다.\n\n제 맥북에 문제가 있거나 만족스럽지 않았기 때문은 아니에요. 다음과 같은 이유 때문이었죠:\n\n<div class=\"content-ad\"></div>\n\n- 최근에는 거의 사용하지 않았던 Windows를 시도해보고 싶었고, 내 친구들이 다 사용하고 있어서 그랬어요.\n- Windows를 더 익숙해지는 것이 대학 수업을 위해 도움이 될 것 같았어요 (우리 수업에서는 주로 Windows를 사용하거든요)\n- Minecraft가 아닌 다른 비디오 게임을 해보고 싶었어요.\n\n몇 일간의 조사 끝에, 중간 정도 성능의 프리빌트 PC를 구입했어요. 모든 부품을 따로 사는 것도 고려했지만, PC 조립 부분은 경험이 없어서 손대기 싫었어요.\n\n게다가, 이 프리빌트 PC를 사는 게 부품을 따로 사는 것보다 더 싸다는 것을 알게 되었어요. 부품들이 할인 중이었고, 개별 구매보다 더 좋은 거래였죠.\n\n어쨌든, 윈도우 11이 장착된 PC를 거의 1년 사용한 후의 생각과, 왜 다시 맥으로 돌아가기로 결정했는지 알려드릴게요.\n\n<div class=\"content-ad\"></div>\n\n먼저, Windows에 대해 가장 감사했던 점에 대해 이야기하고 싶습니다.\n\n## 소프트웨어 이용 가능성\n\nWindows 생태계에 대해 말할 때 가장 좋은 점은 Windows에서 작동하는 소프트웨어의 양이 매우 많다는 것입니다. 필요한 모든 응용프로그램, 프로그램 또는 도구가 거의 확실히 Windows에서 작동합니다. 이는 Windows가 세계에서 가장 많이 사용되는 데스크톱 운영 체제임을 감안하면 놀라울 것이 없습니다.\n\nMacOS 사용자로서, 이것은 매우 편리하게 느꼈습니다. 필요한 모든 소프트웨어를 쉽게 찾을 수 있었고, 내 장치에서 작동할지 걱정할 필요가 없었습니다.\n\n<div class=\"content-ad\"></div>\n\n몰래 말하자면, PC를 구입한 몇 달 후 우리는 대학 과정의 일환으로 C# 프로그래밍 언어와 .NET Windows Forms 프레임워크에 작업을 시작했어요. 이 기술은 Windows에서만 작동하기 때문에 나만의 Windows 기기를 갖게 되어 정말 편리했어요. 수업 외에서도 문제없이 프로젝트를 연습하고 작업할 수 있었거든.\n\n## 게임 지배\n\n두 번째로, 내가 원하는 거의 모든 게임을 할 수 있는 능력은 정말 놀랍게 느껴졌어요. 사탕가게에서 어떤 사탕을 살지 결정하는 것 같은 기분이었어요.\n\n<img src=\"/assets/img/2024-05-16-IusedWindowsforaYearButImSwitchingBacktoMac_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n이 측면에서는 확실히 Mac 환경에서 제약을 받았어요. 그렇다고 나쁠 수록, 이것이 저가 PC를 구매하고 싶었던 주요 이유 중 하나였어요. 말하고 싶지 않은데, 실망하지 않았습니다. 1440p 해상도에서 중상위 설정에서 100프레임 이상으로 거의 모든 게임을 할 수 있었어요.\n\n게임을 하는 대부분의 시간은 2019년 Call of Duty Modern Warfare와 더 최근 Modern Warfare II를 플레이하는 데 보냈어요. 꽤 많이 플레이하고 집중한 시간도 있었어요.\n\n하지만 그만큼 많이 플레이해서 다소 죄책감을 느꼈기 때문에 그걸 줄이고 더 생산적인 활동으로 대체해야 했어요.\n\n## 훌륭한 구성 가능성\n\n<div class=\"content-ad\"></div>\n\n맥에서의 맞춤 설정은 주로 표면적으로만 집중되어 있는 것 같아요 — 멋진 벽지를 선택하고, 밝거나 어두운 테마를 선택하는 정도가 전부에요. 이것이 나쁜 것은 아니지만, 그에 비해 윈도우는 깊게 파고들 수 있는 옵션들을 제공해줘요.\n\n그리고 UI 설정에 대해 이야기하는 게 아닙니다. 윈도우는 다음과 같은 것들을 커스터마이즈할 수 있게 해줘요:\n\n- 하드웨어 제어: CPU, GPU, 메모리의 오버클럭 설정 및 성능 최적화를 위한 팬 커브 조정이 가능해요.\n- 시스템 설정: 네트워크 구성에 대한 더 깊은 접근, 전원 관리 옵션, 배터리 프로필 등\n- 주변기기 커스터마이즈: 고급 키보드 및 마우스 설정 및 구성 옵션\n- 써드파티 도구: 다양한 것들에 대한 추가 맞춤 설정을 허용하는 기타 유틸리티들\n\n# macOS가 더 잘하는 것들\n\n<div class=\"content-ad\"></div>\n\n기술적인 장점이 있더라도 맥보다는 윈도우가 더 나은 부분이 많았습니다.\n\n## 간결한 디자인 & 사용자 경험\n\n윈도우는 11 버전에서 UI와 디자인을 많이 개선했지만, 여전히 맥과 비교할 만하지 않습니다.\n\n맥OS 인터페이스는 분명히 우아하며, 미니멀리즘과 직관적인 디자인에 중점을 둔 것이 눈에 띕니다. 독부터 창 제어까지 모두 신중하게 고려되고 광택이 돌아, 시각적 조화를 만들어 냅니다. 이것이 윈도우에서 그리운 점입니다.\n\n<div class=\"content-ad\"></div>\n\n아름다움 이상으로 Mac은 항상 더 부드러운 사용자 경험을 제공합니다. 폴더 탐색, 파일 관리, 그리고 멀티태스킹과 같은 간단한 작업도 편안하고 순조로운 느낌이 들어요. 이것은 운영 체제와 싸우는 대신 작업에 더 집중할 수 있게 해줘요.\n\n## Apple 통합\n\n이 부분은 정말 중요해요!\n\nMacOS 생태계의 진정한 매력은 저에게 넓은 Apple 생태계와의 원활한 연결에 있어요. 여러 Apple 기기를 가지고 있음으로써 저는 상당한 시간과 노력을 절약하고, 생산성을 높이고 짜증나는 중단을 줄일 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n여기서 가장 많이 활용하는 두 가지 기능은 Universal Clipboard과 AirDrop입니다. Universal Clipboard을 사용하면 한 장치에서 텍스트, 이미지 또는 다른 미디어를 복사하여 다른 장치에 붙여넣기할 수 있습니다. 처음에는 쓸모 없어 보일 수 있지만, 실제로 매우 편리합니다.\n\nAirDrop도 마찬가지로 애플 장치 간 빠른 파일 전송을 가능케 합니다. 이 기능 없이는 살 수 없어요.\n\nWindows를 사용할 때 이렇게 하려고 했는데, 할 수 없다는 것을 깨닫고 파일을 이메일이나 WhatsApp을 통해 자신에게 보내야 했어요.\n\n이것이 내가 메인 장치로 다시 macOS로 전환하기로 결정한 주요 이유 중 하나였습니다.\n\n<div class=\"content-ad\"></div>\n\n## 더 나은 개발 생태계\n\n개발자로서 저는 종종 터미널을 사용합니다. 그리고 윈도우에서는 똑같이 작동하지 않는다는 사실을 깨달을 때 정말 답답했었어요. 저는 맥에서의 유닉스 기반 도구 및 원활한 터미널 통합에 너무 익숙해져서 윈도우는 끊임없는 고통으로 느껴졌어요. 제 프로젝트를 탐색하기 위해 기본 명령을 다시 배워야 하거나 서툰 해결책을 찾아야 하는 것은 제 생산성을 크게 저해했어요.\n\n터미널 외에도 많은 오픈소스 개발 도구 및 프레임워크는 유닉스와 유사한 시스템을 기반으로 설계된 것처럼 보이죠. 이러한 도구들을 윈도우에서 설치하고 설정하는 것은 종종 추가 단계나 호환성 문제를 유발했어요.\n\n윈도우에는 많은 강력한 개발 도구들이 있고 생산적인 환경을 구성하는 것이 분명히 가능하지만, 저에게는 맥이 더 잘 작동하고 문제를 훨씬 덜 겪게 되는 편이에요.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\nWindows를 1년 동안 사용하면서 즐거운 시간을 보냈습니다. Mac에서는 할 수 없는 일들을 할 수 있게 해주었죠. 거의 모든 소프트웨어를 사용하고 게임 가능성을 열어줬어요. 그 이상으로 새로운 것을 배우게 해주었고, 다른 시스템과 함께 작업하는 소중한 경험을 만들어 주었습니다.\n\n하지만 문제가 없었던 것은 아니에요.\n\n최종적으로 Mac의 우아함, 직관적인 인터페이스, 그리고 다른 Apple 기기들과의 원활한 통합이 제게 다시 돌아오게 만들었어요. 제 업무와 아이디어가 가장 자연스럽게 흐르는 곳이고, 어릴 때부터 익숙한 OS였죠. 그래서 계속해서 Mac을 주 사용 장치로 사용하기로 결정했습니다.\n\n<div class=\"content-ad\"></div>\n\n위 내용이 어떻게 생각하시는지 댓글로 알려주세요. 어떤 OS를 선호하시나요?\n\n이 Medium에서 제 첫 이야기 중 하나에요. 읽어주셔서 감사합니다. 즐겁게 읽으셨길 바라요!","ogImage":{"url":"/assets/img/2024-05-16-IusedWindowsforaYearButImSwitchingBacktoMac_0.png"},"coverImage":"/assets/img/2024-05-16-IusedWindowsforaYearButImSwitchingBacktoMac_0.png","tag":["Tech"],"readingTime":5},{"title":"맥용 NVM을 2분 안에 설정하기","description":"","date":"2024-05-16 17:08","slug":"2024-05-16-NVMformacOSin2minutes","content":"\n\n![NVM for MacOS](/assets/img/2024-05-16-NVMformacOSin2minutes_0.png)  \n  \n애플리케이션이 Node.js 아키텍처를 기반으로 한 경우, 서로 다른 버전 간을 전환할 수 있는 능력은 기존 프로젝트와 최신 플랫폼 기능을 활용하기 위해 필수적입니다. 그러나 MacOS와 같은 운영 체제에서는 네이티브로 통합되지 않아 버전 관리가 복잡해질 수 있습니다.\n\n# NVM이란?\n\nNVM은 하나의 시스템에서 여러 버전의 Node.js를 쉽게 설치하고 관리할 수 있게 해주는 명령줄 도구입니다. 그러나 MacOS에서의 구현은 환경 및 의존성 관리에 대한 차이로 인해 다른 운영 체제와 비교했을 때 특별한 도전을 안겨줍니다. 왜냐하면 MacOS에는 내장되어 있지 않기 때문에 대신 Homebrew를 설치해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 어떻게 설치하나요?\n\n터미널을 열고 다음 명령어로 Homebrew를 설치하세요:\n\n```js\n/bin/bash -c \"$(curl -fsSL <https://raw.githubusercontent.com/Homebrew/install/master/install.sh>)\"\n```\n\nHomebrew가 설치되면 NVM을 설치할 차례입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nbrew install nvm\n```\n\nNVM을 사용하려면 항상 ~/.bash_profile 또는 ~/.zshrc 파일에 포함해야 합니다:\n\n```js\nsource $(brew --prefix nvm)/nvm.sh\n```\n\n이 파일들을 찾을 수 없는 경우 sudo su 명령어를 사용하여 수퍼유저로 접근하여 찾을 수 있습니다. 만약 해당 파일이 없다면, 직접 생성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nNVM을 설치한 후에는 NodeJS의 최신 버전을 설치할 차례입니다:\n\n```js\nnvm install node\n```\n\n설치가 완료되면 우리가 원하는 NodeJS 버전들을 설치할 수 있습니다. 이를 위해 컴퓨터에 설치된 모든 버전을 확인하려면 다음 명령을 사용하십시오:\n\n```js\nnvm ls-remote\n```\n\n<div class=\"content-ad\"></div>\n\n만약 우리가 설치하고 싶은 버전을 모른다면 공식 NodeJS 웹사이트를 참고할 수 있어요. 설치하고 싶은 버전을 알고 있다면 다음 명령어를 입력하면 돼요:\n\n```js\nnvm install [설치할 버전]\n\n## 예시:\nnvm install 18.10.0\n```\n\n설치되면 아래와 같이 사용할 수 있어요:\n\n```js\nnvm use [사용할 버전]\n\n## 예시:\nnvm use 18.10.0\n```\n\n<div class=\"content-ad\"></div>\n\n만약 우리가 다른 버전으로 변경하길 원한다면 먼저 nvm list로 설치된 모든 버전을 나열해야 합니다. 원하는 버전을 찾으면 nvm use [원하는 버전 번호]를 사용하여 해당 버전을 사용할 수 있습니다.\n\n# 결론\n\nHomebrew를 통해 MacOS에 NVM을 설치하면 Node.js 버전 관리가 크게 간소화됩니다. NVM을 사용하면 시스템에서 여러 버전의 Node.js를 쉽게 설치, 전환 및 관리할 수 있어서 다양한 프로젝트에 적응하고 최신 플랫폼 기능을 활용할 수 있습니다. 몇 가지 터미널 명령만으로 새로운 버전을 설치하고 이를 전환하며 개발 환경이 항상 호환되고 최신 상태임을 보장할 수 있습니다. 확실히 NVM을 통해 macOS에서 Node.js 개발 환경을 완전히 제어할 수 있어서 호환성 문제나 오래된 버전에 대해 걱정하지 않고 놀라운 애플리케이션을 만드는 데 집중할 수 있습니다.","ogImage":{"url":"/assets/img/2024-05-16-NVMformacOSin2minutes_0.png"},"coverImage":"/assets/img/2024-05-16-NVMformacOSin2minutes_0.png","tag":["Tech"],"readingTime":2},{"title":"리눅스 시스템 관리자가 되는 법 사용자와 그룹 제 1부","description":"","date":"2024-05-16 17:06","slug":"2024-05-16-BecomingLinuxSystemAdministratorusersandgroupspartI","content":"\n\n“리눅스는 여러 사용자 계정을 동시에 사용할 수 있는 다중 사용자 운영 체제로, 서버로 사용하기에 이상적입니다.\n\n어머니의 날 특별 할인 이벤트, 아마존 기기 최대 50% 할인 (쇼핑)\n\n![이미지](/assets/img/2024-05-16-BecomingLinuxSystemAdministratorusersandgroupspartI_0.png)\n\n- 컴퓨터 및 액세서리 베스트셀러 (쇼핑용 뷰)\n- 노트북 컴퓨터 베스트셀러 (쇼핑용 뷰)\n- 컴퓨터 네트워킹 베스트셀러 (쇼핑용 뷰)\n- 리눅스 운영 체제 베스트셀러 (쇼핑용 뷰)\"\n\n<div class=\"content-ad\"></div>\n\n리눅스에서는 각 계정을 사용자 이름과 UID(즉, 고유 번호로 할당된 사용자 ID)로 정의할 수 있습니다. 또한, 각 계정은 기본 그룹에 속하며 실행할 셸과 홈 디렉터리도 가지고 있습니다. 관련 정보는 /etc/passwd 파일에 저장되어 있으며 cat 명령어를 사용하여 확인할 수 있습니다. 터미널에서 단순히 \"cat /etc/passwd\" 명령어를 실행해봅시다:\n\n```js\ndavid@debian:~$ cat /etc/passwd\nroot:x:0:0:root:/root:/bin/bash\n--중략--\ndavid:x:1000:1000:David Beckham:/home/david:/bin/bash\n--중략--\n\n사용자이름:비밀번호:UID:GID:설명:홈_디렉터리:셸\n```\n\n/etc/passwd 파일의 첫 번째 항목인 \"root:x:0:0:root:/root:/bin/bash\"은 루트 계정에 대한 정보를 보여줍니다. 위의 마지막 줄에 표시된 형식에 따라 /etc/passwd 파일의 다른 항목에도 동일한 적용됩니다. 이 형식의 각 필드는 콜론(:)으로 구분됩니다. 따라서 루트 계정과 일반 사용자 david의 형식과 출력을 쉽게 매치할 수 있습니다 (위의 \"cat /etc/passwd\" 출력 참조).\n\n<div class=\"content-ad\"></div>\n\n패스워드 필드의 x는 암호화된 비밀번호가 별도의 파일 /etc/shadow(아래 설명)에 저장되어 있음을 나타냅니다. 이 파일은 슈퍼유저만 읽을 수 있습니다. 이것은 누구나 시스템에서 /etc/passwd 파일을 읽을 수 있지만(암호화되어 있긴 하지만) 보안 위험이 초래될 수 있기 때문에 필요합니다.\n\n다음은 UID를 살펴봅시다. 루트 계정의 경우, UID는 항상 0으로 표시됩니다. 터미널에서 \"cat /etc/passwd\"를 실행하면, 루트나 일반 사용자 외에도 UID가 1000 미만인 계정이 많이 나올 것입니다. 이들은 시스템에서 사용되는 계정들로, 파일 /etc/login.defs를 업데이트하여 구성할 수 있습니다. 이 파일을 직접 확인해보세요:\n\n```js\ndavid@debian:~$ cat /etc/login.defs\n#\n# /etc/login.defs - Configuration control definitions for the login package.\n#\n# 세 가지 항목은 반드시 정의되어야 합니다: MAIL_DIR, ENV_SUPATH, ENV_PATH.\n# 정의되지 않은 경우, 임의의 (가능성 있는) 값으로 간주됩니다. 다른 모든 항목은 선택 사항입니다.\n# 주석 문장(문장이 \"#\"으로 시작하는 문장)과 공백 문장은 무시됩니다.\n#\n# 리눅스용으로 수정되었습니다.  --marekm\n\n# useradd/userdel/usermod에 필수\n#   메일함이 위치하는 디렉토리, _또는_ 파일의 이름, 홈 디렉토리를 상대로한 파일의 이름. 만일\n#   MAIL_DIR 및 MAIL_FILE을 정의할 경우, MAIL_DIR가 우선됩니다.\n#\n#   기본적으로:\n#      - MAIL_DIR는 사용자 메일 스불 파일의 위치를 정의하며, 아래에 정의된 것처럼\n#        MAIL_DIR에 사용자 이름을 추가함으로써(mbox 사용시) 정의됩니다.\n#      - MAIL_FILE은 사용자 메일 스불 파일의 위치를 정의하며, 사용자 홈 디렉토리를\n#        $MAIL_FILE 앞에 붙여 편리하게 얻은 완전한 경로 파일 이름으로 정의됩니다 \n#      \n# 알림: 이는 더 이상 사용자 메일 환경 변수 설정에 사용되지 않습니다\n#       이는 Debian에서 쉐도우 4.0.12-1부터 완전히 pam_mail PAM 모듈의 업무가 되었으며\n#       login, su 등에 대한 기본 PAM 구성 파일을 참고하세요.\n#\n--snip--\n```\n\n다시 /etc/passwd 파일로 돌아와서, GID는 해당 계정의 기본 그룹을 나타냅니다. 사용자가 파일을 새로 만들면, 해당 파일은 자동으로 사용자의 기본 그룹에 속하게 됩니다. 그러나 사용자가 다른 그룹을 사용하려면 newgrp 명령을 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n“username:password:UID:GID:comments:home_dir:shell” 형식으로 우리가 논의 중인 바에 따르면, 5번째 필드 (comments)는 계정에 대한 설명이나 원할 경우 비워둘 수 있습니다. 이 필드를 가끔 GECOS 필드라고도 부르는데, 이는 초기 Unix 시절의 관행을 따른 것입니다. 그리고 짐작할 수 있겠지만, 시스템에 로그인하면 일반적으로 홈 디렉터리(home_dir)에 배치됩니다. 루트의 경우는 /root이며, 예시로는 David의 경우 /home/david입니다. 그러나 사용자의 홈 디렉터리가 존재하지 않는 경우에는 루트 디렉터리가 대신 사용됩니다.\n\n마지막으로, shell 필드는 사용자가 계정으로 로그인할 때 실행될 쉘을 나타냅니다. 시스템에 설치된 쉘을 확인하려면 /etc/shells 파일을 확인할 수 있습니다:\n\n```js\ndavid@debian:~$ cat /etc/shells \n# /etc/shells: valid login shells\n/bin/sh\n/bin/dash\n/bin/bash\n/bin/rbash\n/usr/bin/tmux\n```\n\n여기서 주목할 점은 /etc/passwd 파일의 shell 필드에 나열된 내용이 실제 쉘이 아닌 경우에도 로그인 시 실행된다는 것입니다. 이러한 이유로 /usr/sbin/nologin 또는 /bin/false과 같이 shell 필드에 여러 계정에 대해 다양한 항목이 채워진 것을 터미널에서 볼 수 있을 것입니다. 이러한 계정은 누구에게도 상호 작용적으로 사용될 수 없습니다. 더불어 필요에 따라 사용자가 로그인할 때 특정 프로그램을 실행할 수 있도록 shell 필드를 사용할 수 있습니다. 예를 들어, 필요한 경우 사용자를 특정 응용프로그램에 강제 접근시킬 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 '/etc/shadow' 파일 내용을 확인해보겠습니다.\n\n```js\ndavid@debian:~$ sudo cat /etc/shadow\nroot:$6$9g1IC8AYzqPorEZSHjWeZP8o21:16502:0:99999:7:::\n--중략--\n```\n\n여기서 각 필드(총 9개의 필드)가 콜론(:)으로 구분되어 있고 아래와 같이 나타납니다.\n\n- 사용자 이름\n- 암호화된 비밀번호\n- 1970년 1월 1일부터 현재까지의 일 수(여기서는 16502)로 비밀번호가 변경된 날짜\n- 비밀번호를 변경할 수 있는 일 수(여기서는 0)\n- 비밀번호를 변경해야 하는 일 수. 여기서 99999는 사용자가 비밀번호를 변경할 필요가 없음을 나타냅니다.\n- 사용자에게 비밀번호가 만료될 것임을 알리는 일 수\n- 비밀번호가 만료된 후 계정이 비활성화되는 일 수(미할당)\n- 계정이 비활성화된 날로부터 현재까지의 일 수(미할당)\n- 미래 사용을 위한 예약된 필드\n\n<div class=\"content-ad\"></div>\n\n위를 읽으면 특정 날짜 01/01/1970이 사용된 이유에 궁금증을 느낄 수 있습니다. 컴퓨팅 에포크와 유닉스 시간을 읽어보시기를 권해 드립니다.\n\n이 강의는 책을 기반으로 합니다. 즉, Linux Administration: The Linux Operating System and Command Line Guide for Linux Administrators (킨들, 페이지)\n\n![이미지](/assets/img/2024-05-16-BecomingLinuxSystemAdministratorusersandgroupspartI_1.png)\n\n- Linux 운영 체제 베스트셀러 (쇼핑 보기)\n- Linux 네트워킹 및 시스템 관리 베스트셀러 (보기)","ogImage":{"url":"/assets/img/2024-05-16-BecomingLinuxSystemAdministratorusersandgroupspartI_0.png"},"coverImage":"/assets/img/2024-05-16-BecomingLinuxSystemAdministratorusersandgroupspartI_0.png","tag":["Tech"],"readingTime":5}],"page":"80","totalPageCount":154,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}