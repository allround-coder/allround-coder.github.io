{"pageProps":{"posts":[{"title":"프로젝트에 Nextjs 14를 더이상 사용하면 안되는 이유","description":"","date":"2024-05-27 18:42","slug":"2024-05-27-StopUsingNextjs14","content":"\n\n\n![Next.js](/assets/img/2024-05-27-StopUsingNextjs14_0.png)\n\nNext.js는 여전히 가장 훌륭한 풀스택 프레임워크 중 하나로 손꼽힙니다.\n\n하지만, 우리에게는 Next.js 버전 14 사용을 중단할 시간이 될 수도 있습니다…\n\n왜냐하면 Next.js 15 릴리스 후보 (RC) 버전이 출시되었기 때문입니다!\n\n\n<div class=\"content-ad\"></div>\n\nNext.js 15 RC에는 많은 흥미로운 새로운 기능이 약속되어 있어요. 오늘은 그 중 4가지를 살펴볼 거에요!\n\n그럼 이제... 바로 시작해 봅시다!\n\n## 1. 부분 사전 렌더링\n\n부분 사전 렌더링 (PPR)은 Next.js 14에서 소개된 특별한 기능으로, 정적 및 동적 페이지 콘텐츠가 완벽하게 공존할 수 있도록 해줘요.\n\n<div class=\"content-ad\"></div>\n\nPPR 작동 방식에 대해 더 알고 싶다면, 이 문서를 자유롭게 확인해 보세요.\n\n하지만 Next.js 15에서는 PPR의 점진적 적용이 마침내 가능해졌습니다!\n\n이는 experimental_ppr 플래그를 true로 설정하여 특정 page.tsx 및 layout.tsx 파일을 PPR에 선택적으로 선택할 수 있다는 의미입니다.\n\n```js\nimport { Suspense } from \"react\"\nimport { StaticComponent, DynamicComponent } from \"@/app/ui\"\n\n// 이 페이지만 PPR로 선택\nexport const experimental_ppr = true\n \nexport default function Page() {\n  return {\n     <>\n      <StaticComponent />\n      <Suspense fallback={...}>\n       <DynamicComponent />\n      </Suspense>\n     </>\n  };\n}\n```\n\n<div class=\"content-ad\"></div>\n\n다음으로 next.config.js 파일에서 experimental.ppr 구성을 'incremental'로 설정하세요:\n\n```js\nconst nextConfig = {\n  experimental: {\n    ppr: 'incremental',\n  },\n};\n\nmodule.exports = nextConfig;\n```\n\n# 2. next/after\n\nnext/after은 응답 스트리밍이 종료된 후 작업을 예약할 수 있는 새로운 API입니다.\n\n<div class=\"content-ad\"></div>\n\n다시 말해, 서버리스 함수가 계산을 마치면, 이제 새로운 after() 함수 내에서 추가 코드를 실행할 수 있습니다.\n\n![이미지](/assets/img/2024-05-27-StopUsingNextjs14_1.png)\n\n이것은 후속 fetch 로깅 및 분석에 매우 유용합니다.\n\n지금 next/after를 사용하려면, next.config.js 파일에 다음과 같이 experimental.after 설정을 추가할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 Next.js 서버 액션 내에서 `after()` 함수를 사용하는 예시입니다:\n\n```js\n\"use server\"\n\nfunction next_after() {\n  // 여러분의 함수 로직...\n  const something = true\n\n  // 보조 작업 - 데이터가 반환된 후에 데이터를 로깅합니다.\n  after(() => {\n    console.log(something)\n  })\n\n  // 주요 작업 - 데이터를 반환합니다.\n  return something\n}\n```\n\n# 3. 캐싱\n\n<div class=\"content-ad\"></div>\n\nThe Next.js 팀이 우리의 의견을 들어주었어요!\n\n- fetch 요청\n- GET 핸들러\n- 그리고 클라이언트 네비게이션...\n\n기본적으로 더 이상 캐시되지 않아요!\n\n이 변화는 기다리고 있던 변화였고, Next.js 15가 이를 마침내 구현했어요.\n\n<div class=\"content-ad\"></div>\n\n# 4. 리액트 19 지원\n\n공식으로, Next.js 15 RC는 리액트 19 RC와 완벽하게 호환됩니다!\n\n리액트 19 릴리스에 대해 들은 적이 없다면, 여기 리액트의 공식 트윗이 있습니다.\n\n더 알아보기 위해 여기에서 리액트 컨퍼런스 키노트를 시청할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 글이 Next.js 14 시대를 떠나 Next.js 15에 흥분하게 만들었으면 좋겠어요.\n\nNext.js 15는 무수히 많은 흥미로운 변화를 가져왔으며, 오늘 우리는 그 중 4가지를 상세히 다뤘습니다.\n\n# 제휴사\n\n<div class=\"content-ad\"></div>\n\n- 올인원 SaaS 프로젝트 템플릿\n- Figma 홈: 제가 모든 프로젝트에서 사용하는 UI 디자인 도구입니다.\n- Figma 프로페셔널: 당신이 필요로 할 유일한 UI 디자인 도구입니다.\n- FigJam: 직관적인 다이어그램 및 브레인스토밍으로 마음을 펼쳐 보세요.\n- 노션: 제 인생 전체를 조직하는 데 사용되는 도구입니다.\n- Notion AI: ChatGPT를 뛰어넘고 노션 워크플로우를 견고하게 만들어 줄 AI 도구입니다.\n\n# 참고 자료\n\n- https://nextjs.org/blog/next-15-rc","ogImage":{"url":"/assets/img/2024-05-27-StopUsingNextjs14_0.png"},"coverImage":"/assets/img/2024-05-27-StopUsingNextjs14_0.png","tag":["Tech"],"readingTime":3},{"title":"파이썬 농업이 최고의 지속 가능한 육식을 제공할 수 있습니다","description":"","date":"2024-05-27 18:40","slug":"2024-05-27-PythonFarmingCouldProvideTheMostSustainableMeatYet","content":"\n\n파이썬을 사육하면 지속 가능한 고기를 제공할 수 있어서 이는 전 세계적인 식량 안보를 지타주는데 도움이 될 수 있습니다. 지구를 위한 더 나은 단백질은 뱀일까요?\n\n이 글은 GrrlScientist의 Forbes 기사에서 제공되었습니다. [링크](LinkTr.ee)\n\n국제 연구팀은 동남 아시아에 위치한 두 상업용 파이썬 농장에서 12개월 동안 연구를 진행한 결과, 뱀들이 닭, 돼지, 소 등과 같은 전통적인 축산 종에 비해 식량을 체중 증가로 효율적으로 전환하는데 놀라울 정도로 좋다는 것을 발견했습니다. 뿐만 아니라, 사육된 파이썬은 불규칙하게 먹여도 빠르게 성장한다는 것이 입증되었습니다.\n\n연구의 주 저자인 파충동물학자 다니엘 나투시는 맥쿼리 대학교 자연과학 연구소 명예 연구원이자 발표문에서 “식량 및 단백질 전환 비율을 고려하면 파이썬은 현재까지 연구된 모든 주류 농가종을 압도한다”고 말했습니다.\n\n<div class=\"content-ad\"></div>\n\n\"우리는 유충이 부화 후 첫 해에 '도축 중량'에 도달하기 위해 빠르게 성장하는 것을 발견했어요.\"\n\nNatusch 박사는 뱀 고기가 흰색이며 단백질 함량이 매우 높다고 지적했습니다.\n\n최근 발표된 연구에서 Natusch 박사와 협업자들은 태국과 베트남에서 상업적으로 양식된 4,600마리의 파이썬을 연구하고 비교하였으며, 서로 다른 급여 방법의 효과를 테스트했습니다. 뱀들은 지역에서 구입한 야생 쥐, 돼지 부산물 및 어류 펠릿 등을 섞어 매주 한 번 급여를 받았으며, 연구 기간 동안 정기적으로 측정 및 체중을 쟀습니다.\n\n![이미지](/assets/img/2024-05-27-PythonFarmingCouldProvideTheMostSustainableMeatYet_0.png)\"\n\n<div class=\"content-ad\"></div>\n\n연구에서 Natusch 박사와 협업자들은 격자 뱀인 Malayopython reticulatus와 파이선인 Python bivittatus가 태어나 첫 해에 빠르게 성장한다는 것을 발견했습니다. 일일 46그램(1.6온스)까지 늘어나며 수컷보다 암컷이 더 높은 성장률을 보였습니다.\n\nNatusch 박사와 협업자들은 파이썬은 4개월 이상 음식 없이도 체중을 크게 잃지 않고, 먹이가 다시 시작되자 빠르게 성장을 재개하여 전통적 가축보다 더 적은 노동력이 필요하며 극단적 날씨 사건 등에 의한 장기적 식량 공급 장애에 쉽게 적응할 수 있다는 것을 밝혔습니다.\n\n\"Natusch 박사는 '양은 매우 적게 먹고 작물을 공격하는 쥐와 기타 해충을 먹는다.\"고 관찰했습니다.\n\n뱀을 결코 강제로 먹이를 먹인 적이 없었다고 Natusch 박사와 협업자들은 말하며, 두 파이썬 종의 성장률은 섭취한 음식의 양에 크게 영향을 받았습니다.\n\n<div class=\"content-ad\"></div>\n\n그들은 극도로 효율적입니다. 그들의 튼튼한 소화 체계는 뼈조차 분해할 수 있을 정도이며, 포유동물보다 거의 물 폐기물이 없고 고체 폐기물도 훨씬 적게 생산합니다.\n\n\"돼지를 기르는 대신 파이썬을 기르는 농부들에게는 명확한 경제적 및 적응성 이점이 있습니다,\"라고 생물학 교수이자 파충류학자인 리처드 샤인(Richard Shine) 교수가 백마리 대학교의 생물학 교수이자 연구 공동 저자로 말했습니다. 샤인 교수의 연구는 특히 파충류를 중심으로 진화와 생태학 사이의 상호작용에 대해 조사합니다.\n\n\"조류와 포유동물은 먹이로부터 얻는 에너지의 약 90%를 단지 일정한 체온을 유지하는 데 사용합니다,\"라고 샤인 교수가 설명했습니다.\n\n\"그러나 파충류와 같은 냉혈동물은 따뜻해지려고 해서 태양 아래에서 한 균을 찾습니다. 그들은 먹이를 살을 더 불고 체조부조직으로 바꾸는 데 있어서 온난한 혈통 생물보다 훨씬 더 효율적입니다.\"\n\n<div class=\"content-ad\"></div>\n\nDr. 나투치 박사와 공동 연구자들은 파이썬이 닭고기, 쇠고기, 돼지고기, 연어를 비롯한 다른 가축보다 적은 양의 사료를 필요로 한다는 것을 발견했습니다. 놀랍게도 귀뚜라미를 비롯한 다른 동물들보다도 더 적은 양의 사료가 필요합니다. 그들은 어린 파이썬에 대한 집중적인 사육이 빠른 성장률을 촉진하면서도 별다른 복지에 미치는 영향이 없다는 것을 발견했습니다. 또한 가축과 특히 소와는 달리 파이썬은 매우 적은 양의 물만 필요합니다.\n\n\"뱀은 최소한의 물만 필요하며 아침에 그들의 비늘에 떨어진 이슬로도 생존할 수 있습니다.\" 라는 나투치 박사의 말씀도 있습니다.\n\n요약하자면, 파이썬은 극히 적은 양의 것을 최대한 활용하는 전문가들입니다.\n\n뱀 고기는 지방이 적고 고품질 단백질이 풍부한 지속 가능한 고기로, 이미 동남아시아와 중국에서 널리 소비되고 있습니다. 사실, 동남아시아 많은 지역에서 그 태의 미각의 대표적인 음식으로 소품됩니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 파이썬 농업은 서양 농업에서 여전히 알려지지 않았어요.\n\n“아시아에서는 대규모의 파이썬 농업이 잘 확립되어 있지만, 주류 농업 과학자들의 주목을 받지 못했습니다,” 라는 내츠치 박사의 말입니다.\n\n파이썬 농업은 현지인 고용 기회를 제공해줍니다.\n\n“우리는 또한 일부 농장이 베이비 파이썬을 현지 마을사람들에게 외주하고 있다는 것을 발견했어요. 이들은 종족들에게 현지 쥐와 남은 음식물을 먹이며, 1년 후에 농장으로 다시 판매하여 추가 수입을 올리는 경우가 많습니다.”\n\n<div class=\"content-ad\"></div>\n\n뱀은 매우 짧은 시간 안에 많은 새끼를 낳을 수 있습니다. 예를 들어, 암컷 파이썬은 1년에 50에서 100개의 알을 낳을 수 있습니다. 이에 비해 암소는 평균 0.8 마리의 새끼를 낳고, 돼지는 동일한 기간 동안 22에서 27마리의 새끼를 낼 수 있습니다 (출처).\n\n\"우리의 연구는 존자료 가축 시스템을 보완하는 파이썬 농업이 전 세계적인 식량 부족에 유연하고 효율적으로 대응할 수 있다는 것을 시사합니다,\" Natusch 박사가 지적했습니다.\n\n가공되면 파이썬의 체중의 약 82%가 고기용 드레스 처리된 시체, 가죽용 귀중한 가죽, 그리고 지방 (뱀오일)과 담낭 (뱀담즙)으로 나오게 되는데, 이 둘은 일부 국가에서 약용으로 사용됩니다.\n\n\"우리는 뱀 농장이 농업 폐기물을 효율적으로 단백질로 전환하면서 상대적으로 적은 폐기물을 생산할 수 있다는 것을 보여줬습니다,\" Natusch 박사가 말했습니다.\n\n<div class=\"content-ad\"></div>\n\n파이썬은 흥미로운 다양한 단백질 소스를 소화할 수 있어요. 이 연구에서 연구팀은 고기와 생선의 부산물로 만든 다른 종류의 '소시지'로 파이썬을 먹였어요. 야생에서는 오직 육식성인 파이썬이지만, 연구팀이 고기에 숨은 10%의 채소 단백질이 포함된 단백질 소시지를 만들면서 놀랍게도 대두 및 기타 채소 단백질도 소화할 수 있다는 것을 밝혀냈어요.\n\n\"자식들이 야채를 먹을 수 있도록 소시지에 브로콜리를 숨기는 것과 비슷하죠.\"\n\nNatusch 박사와 공동 연구자는 혈중 단위로 보았을 때 파충류가 포유류에 비해 훨씬 적은 온실가스를 생산한다고 언급했어요. 기후 변화가 계속 악화되는 가운데, 이 연구자들은 말했습니다.\n\n\"기후 변화, 질병 및 줄어드는 천연자원은 기존 가축과 작물에 압력을 가하고 있고, 이미 단백질 결핍으로 고통받는 저소득 국가 사람들에게 치명적인 영향을 미치고 있어요\"라고 Natusch 박사가 말했습니다.\n\n<div class=\"content-ad\"></div>\n\n전통적인 농업 식품 시스템의 실패로 식량 안보가 널리 퍼지고 있으며, 이는 대체 식품원에 대한 관심을 촉발하고 있다고 그는 덧붙였다.\n\n그러나, 파이썬을 재배하고 섭취하는 것으로 제시된 많은 이점과 혜택에도 불구하고, 미국인, 호주인 및 유럽인들이 털보 동물을 죽이고 먹는 대안으로 이러한 고기를 받아들이기는 어려울 것으로 보인다.\n\n\"내가 생각하기에 최애 식당에서 파이썬 버거를 내놓는 것을 보기까지는 아직 시일이 좀 걸릴 것 같아.\"\n\n# 출처:\n\n<div class=\"content-ad\"></div>\n\nD. Natusch, P. W. Aust, C. Caraguel, P. L. Taggart, V. T. Ngo, G. J. Alexander, R. Shine & T. Coulson (2024). Python farming as a flexible and efficient form of agricultural food security, Scientific Reports 14:5419 | doi:10.1038/s41598-024-54874-4\n\nSocials: Bluesky | CounterSocial | LinkedIn | Mastodon | MeWe | Post.News | Spoutible | SubStack | Tribel | Tumblr | Twitter\n\n원문은 2024 년 3 월 22 일에 Forbes.com에 게시되었습니다.","ogImage":{"url":"/assets/img/2024-05-27-PythonFarmingCouldProvideTheMostSustainableMeatYet_0.png"},"coverImage":"/assets/img/2024-05-27-PythonFarmingCouldProvideTheMostSustainableMeatYet_0.png","tag":["Tech"],"readingTime":5},{"title":"AI와 함께 하는 프로그래밍,  API 호출하기","description":"","date":"2024-05-27 18:37","slug":"2024-05-27-ProgrammingwithAICallingAPIs","content":"\n\n몇 주 전에 AI 프로그래밍 수업을 가르쳐달라는 요청을 받았어요. 그래서 슬라이드와 코드를 열심히 준비했는데, 물질들이 커져갔어요. 그래서 이 모든 자료들을 하나로 모아 시리즈 형식의 글로 만들어보자는 생각이 들었죠. 수업 이후에 이를 참고할 수 있는 사람들이 많을 것이라 생각해요. 또한 이 글들은 세션 이후에도 수업에 도움이 될 수 있는 참고 자료가 될 거예요.\n\n그래서 이 수업의 첫 번째 부분을 공유합니다. 이 부분은 REST API 및 라이브러리를 통해 AI 공급업체 API를 호출하는데 관한 내용입니다.\n\n참고: 이것은 초보자를 위한 자료이므로 제가 생략한 내용이 많습니다. 이는 포괄적인 내용이 아니고 이해를 돕기 위한 것입니다.\n\n![Programming with AI Image](/assets/img/2024-05-27-ProgrammingwithAICallingAPIs_0.png)\n\n<div class=\"content-ad\"></div>\n\n따뜻한 시작부터 시작해봅시다. 그것은 몇 가지 API를 호출하는 것을 의미합니다. AI를 활용하기 위해 API를 호출하는 것은 AI 능력에 가장 흔하고 쉬운 방법이에요. 많은 사람들이 이를 비웃고 \"충분히 AI가 아니다\" 라고 생각하지만, 그건 좀 어리석은 생각이죠 - 시스템의 가치는 사용자에게 기능을 제공하는 것이지 얼마나 많은 AI가 사용되었는지에 달려 있지 않습니다.\n\n현재 OpenAI (GPT), Google (Gemini), Anthropic (Claude), Mistral (Mistral), Cohere (Command) 등 다양한 API 제공업체들이 있어요. 이외에도 Replicate, Anyscale, Modal, Banana 등 다양한 기능을 제공하는 플랫폼 제공자들도 있습니다.\n\n이 글에서는 우리가 REST API를 호출하는 것으로 간단히 시작할 거에요.\n\n# REST APIs\n\n<div class=\"content-ad\"></div>\n\n내가 아는 바에 의하면, 각 제공업체는 REST API를 갖고 있어요. 그들을 호출하는 것은 매우 간단해요. curl과 API URL 엔드포인트 그리고 JSON 페이로드를 전달하기만 하면 돼요.\n\n여기 채팅 완성을 위해 OpenAI API를 호출하는 예시가 있어요.\n\n```js\n$ curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Why is the sky blue?\"\n      }\n    ]\n  }'\n```\n\n각 제공업체로부터 유효한 API 키가 필요해요. 대부분의 경우 계정에 가입하고 API 키를 생성하기만 하면 돼요. API 키를 얻었다면 직접 전달하거나 환경 변수로 설정할 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n```js\n$ export OPENAI_API_KEY=<당신의 API 키>\n```\n\nAPI를 호출하면 다음과 같은 결과가 반환되어야 합니다:\n\n```js\n{\n  \"id\": \"chatcmpl-9RWBzicE7v7A1ZRLWUMX3a6zwooWd\",\n  \"object\": \"chat.completion\",\n  \"created\": 1716345631,\n  \"model\": \"gpt-4o-2024-05-13\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Rayleigh 산란이라는 현상으로 인해 하늘은 푸르게 보입니다. 더 자세한 설명은 다음과 같습니다:\\n\\n1. **태양빛 구성**: 태양빛 또는 백색광은 서로 다른 파장을 가진 색 스펙트럼으로 구성되어 있습니다. 가시 스펙트럼은 짧은 파장(파랑과 보라색)에서 긴 파장(빨강과 주황색)까지 범위에 걸쳐 있습니다.\\n\\n2. **대기와의 상호작용**: 태양빛이 지구 대기에 들어오면 분자와 작은 입자와 상호작용합니다. 짧은 파장의 빛(파랑과 보라색)은 이러한 입자에 의해 더 효과적으로 산란되며, 긴 파장(빨강, 주황, 노랑)은 그보다 적게 산란됩니다. \\n\\n3. **인간의 지각**: 보라색 빛은 파랑 빛보다 더 많이 산란되지만, 우리 눈은 파랑 빛에 민감하며, 태양빛에는 처음부터 보라색 빛이 많이 없습니다. 게다가 일부 보라색 빛은 상층 대기에 흡수됩니다. 결과적으로 우리는 하늘을 파랗게 보게 됩니다.\\n\\n4. **결과적인 푸른 하늘**: 산란된 파랑 빛이 각 방향에서 우리 눈에 도달하여, 주로 지면에서 낮에 하늘을 보면 하늘이 파랗게 보입니다.\\n\\n이 산란 효과는 태양이 하늘에 낮게 있을 때 더 명확하게 나타납니다. 그래서 일출과 일몰 시 빨간색 계열을 보게 됩니다. 이러한 경우에는 빛이 더 많은 대기를 통과하면서 파랑과 녹색빛이 더 많이 산란되고, 빨강과 주황색이 하늘을 지배하게 됩니다.\"\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 23,\n    \"completion_tokens\": 286,\n    \"total_tokens\": 309\n  },\n  \"system_fingerprint\": \"fp_729ea513f7\"\n}\n```\n\n아마도 OpenAI가 이러한 API를 처음으로 개발한 것이거나, 더 인기가 많아서 다른 많은 공급자들이 그들의 REST API에서 비슷한 형식을 사용하는 것일 수도 있습니다. 예를 들어, Anthropic의 형식은 다음과 같습니다.\n\n<div class=\"content-ad\"></div>\n\n\n$ curl https://api.anthropic.com/v1/messages \\\n  -H \"content-type: application/json\" \\\n  -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -d '{\n    \"model\": \"claude-3-opus-20240229\",\n    \"max_tokens\": 1024,\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n    ]\n}'\n\n\n위에서 보듯이, API 키는 다른 헤더를 통해 전달되지만 페이로드는 거의 동일하지만 모델 작동 방식에 따라 약간 차이가 있습니다. 예를 들어, Anthropic에서 메시지의 일부로 시스템 역할 콘텐츠를 전달할 수 없습니다.\n\n또 다른 예시는 Mistral의 것입니다.\n\n\n$ curl https://api.mistral.ai/v1/chat/completions \\\n     --header 'Content-Type: application/json' \\\n     --header 'Accept: application/json' \\\n     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n     --data '{\n    \"model\": \"mistral-large-latest\",\n    \"messages\": [\n     {\n        \"role\": \"user\",\n        \"content\": \"Why is the sky blue?\"\n      }\n    ]\n  }'\n\n\n<div class=\"content-ad\"></div>\n\n그런데, Google은 실제로 API에 대해 약간 다른 방식을 사용하며 API 키를 URL 쿼리의 일부로 전달하고 모델을 URL의 일부로 포함시킵니다. 페이로드도 다르지만 아이디어는 거의 동일합니다.\n\n```js\n$ curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=$API_KEY\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{ \"contents\":[\n    { \"parts\":[{\"text\": \"Why is the sky blue?\"}]}\n  ]\n}'\n```\n\nREST API는 정말 유용하고 거의 보편적입니다. 제공 업체에서 직접 지원하지 않는 언어로 프로그래밍하는 경우 HTTP 클라이언트 라이브러리를 사용하여 REST API를 직접 호출할 수 있습니다. 대부분의 합리적인 프로그래밍 언어에는 표준 라이브러리나 서드 파티 라이브러리에 HTTP 클라이언트 라이브러리가 있으므로 문제 없습니다.\n\n그러나 대부분의 제공 업체는 대부분 Python을 지원하기도 합니다. 그 이유는 대부분의 AI 관련 작업이 Python으로 프로그래밍되기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n# 파이썬\n\n예를 들어, OpenAI를 호출하는 방법은 이렇습니다. Python 라이브러리를 사용합니다.\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n```\n\n너무 간단하죠? 클라이언트를 만들 때 매개변수와 옵션을 설정할 수 있지만 그게 전부입니다. API 키를 더 이상 지정할 필요가 없다는 것을 알아채셨을 것입니다. 환경 변수로 API 키를 설정했다면 Python 라이브러리가 해당 환경 변수에서 가져올 거에요.\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! Anthropic과 Mistral도 마찬가지에요.\n\n```js\nimport anthropic\n\nmessage = anthropic.Anthropic().messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n    ]\n)\n\nprint(message.content)\n```\n\n프랜들리하게말하자면,\n\n```js\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\nclient = MistralClient()\n\nchat_response = client.chat(\n    model=\"mistral-large-latest\",\n    messages=[\n        ChatMessage(role=\"user\", content=\"Why is the sky blue?\")\n    ],\n)\n\nprint(chat_response.choices[0].message.content)\n```\n\n<div class=\"content-ad\"></div>\n\nGoogle의 Python 라이브러리도 사용하기 매우 쉽지만, 다른 라이브러리들과 조금 다릅니다.\n\n```python\nimport google.generativeai as genai\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nchat = model.start_chat(history=[])\nresponse = chat.send_message(\"Why is the sky blue?\")\n\nprint(response.text)\n```\n\n파이썬은 매우 잘 지원되고 있는 것을 보실 수 있습니다. 다른 잘 지원되는 언어는 JavaScript입니다.\n\n# Javascript\n\n<div class=\"content-ad\"></div>\n\n파이썬이 가장 잘 지원되는 언어라고 해도, 인기가 많기 때문에 자바스크립트/타입스크립트도 많이 사용됩니다. 자바스크립트와 node.js를 사용하여 OpenAI API에 액세스하는 방법을 살펴봅시다.\n\n```js\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nconst completion = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n      { role: \"system\", content: \"You are a helpful assistant.\" },\n      { role: \"user\", content: \"Why is the sky blue?\" }\n  ],    \n});\n\nconsole.log(completion.choices[0]);\n```\n\n파이썬 라이브러리에서와 같이 API 키를 더 이상 입력할 필요가 없었습니다. 환경 변수로 API 키를 설정한 경우, 자바스크립트 라이브러리가 환경 변수에서 API 키를 자동으로 인식합니다.\n\n이것이 JSON 결과 출력입니다 (completion.choices[0]만 표시).\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 표를 변경하려면 다음과 같이 하면 됩니다.\n\n```js\n{\n  index: 0,\n  message: {\n    role: 'assistant',\n    content: \"하늘이 파란 이유는 Rayleigh 산란이라는 현상 때문입니다. 이 산란은 태양광이 지구 대기로 들어와 공기 속 분자와 작은 입자들과 상호 작용할 때 발생합니다.\\n\" +\n      '\\n' +\n      \"하늘이 파란 색으로 보이는 이유를 단계별로 살펴보겠습니다:\\n\" +\n      '\\n' +\n      '1. **태양광 조성**: 태양광 또는 백색광은 여러 색상으로 구성되어 있으며 각각 다른 파장을 가지고 있습니다. 색상은 보라색과 파랑 (파장이 짧은)에서 빨강과 주황 (파장이 긴)까지 이어집니다.\\n' +\n      '\\n' +\n      '2. **산란**: 태양광이 대기를 통과할 때 가스 분자와 작은 입자와 충돌합니다. 빛의 짧은 파장(파랑과 보라색)은 이러한 분자와 입자들에 의해 더 많이 길거나 (빨강과 주황과 같은)보다 더 넓은 범위로 산란됩니다.\\n' +\n      '\\n' +\n      '3. **인간의 지각**: 비록 보라색 빛이 파란 빛보다 더 많이 산란되지만, 우리 눈은 파란 빛에 민감하고 보라색 빛에 덜 민감합니다. 또한 일부의 보라색 빛은 상층 대기에 흡수됩니다. 따라서 우리에게는 하늘이 주로 파란색으로 보입니다.\\n' +\n      '\\n' +\n      '4. **시야각**: 하늘을 올려다볼 때, 우리는 하늘의 모든 부분에서 나오는 이 산란된 파란 빛을 보며 그 특징적인 색상을 부여합니다.\\n' +\n      '\\n' +\n      \"요약하면, 하늘이 파란 이유는 태양광의 짧은 파장인 파란색이 지구 대기의 분자들에 의해 모든 방향으로 더 넓게 산란되고, 우리 눈이 파란색 빛을 보는 데 더 잘 적응되어있기 때문입니다.\"\n  },\n  logprobs: null,\n  finish_reason: 'stop'\n}\n```\n\n마찬가지로, Javascript와 node.js를 사용하여 Anthropic API를 호출하는 방법은 아래와 같습니다.\n\n```js\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst anthropic = new Anthropic();\n\nconst completion = await anthropic.messages.create({\n  model: \"claude-3-haiku-20240307\",\n  max_tokens: 1024,\n  messages: [\n    {\"role\": \"user\", \"content\": \"하늘이 파란 이유는 무엇인가요?\"}\n  ]\n});\n\nconsole.log(completion);\n```\n\n다른 공급자들로는 진행하지 않겠지만, 아이디어를 얻으실 수 있습니다. 공식적으로 지원되는 라이브러리를 사용하시려면 Python 및 Javascript가 좋습니다. REST API 외에도 Python 및 Javascript를 포함한 대부분의 공급자는 다른 언어에 대한 공식 지원이 없지만 Google은 훨씬 더 다양한 언어를 지원합니다.\n\n\n<div class=\"content-ad\"></div>\n\n그러나 다른 언어용 라이브러리가 없는 것은 아닙니다.\n\n# 서드 파티 라이브러리\n\n주변에는 다양한 서드 파티 라이브러리가 있습니다. OpenAI 문서를 살펴보면 대부분의 인기 있는 언어에 대한 서드 파티 라이브러리 지원이 있습니다. 예를 들어, go-openai 패키지를 사용하면 Go에서 OpenAI 라이브러리를 호출할 수 있습니다.\n\n```js\npackage main\n\nimport (\n \"context\"\n \"fmt\"\n \"os\"\n\n openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n client := openai.NewClient(os.Getenv(\"OPENAI_API_KEY\"))\n resp, err := client.CreateChatCompletion(\n  context.Background(),\n  openai.ChatCompletionRequest{\n   Model: openai.GPT4o,\n   Messages: []openai.ChatCompletionMessage{\n    {\n     Role:    openai.ChatMessageRoleUser,\n     Content: \"Why is the sky blue?\",\n    },\n   },\n  },\n )\n\n if err != nil {\n  fmt.Printf(\"ChatCompletion error: %v\\n\", err)\n  return\n }\n\n fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n<div class=\"content-ad\"></div>\n\n저기요! 여기 SwiftOpenAI라는 Swift용 써드파티 OpenAI 라이브러리가 있어요. XCode 프로젝트에서 패키지 종속성으로 추가해서 사용할 수 있어요. 이 함수는 OpenAI API를 호출하는 예시에요.\n\n```js\n    func sendMessage() async {\n        let input = userInput.trimmingCharacters(in: .whitespacesAndNewlines)\n        guard !input.isEmpty else { return }\n        \n        let message = Message(content: input, isUser: true)\n        messages.append(message)\n        userInput = \"\"\n        \n        let openAI = SwiftOpenAI(apiKey: Config.openAIKey)\n        let msgs: [MessageChatGPT] = [\n            MessageChatGPT(text: \"You are a helpful assistant.\", role: .system),\n            MessageChatGPT(text: input, role: .user)\n        ]\n        \n        let optionalParameters = ChatCompletionsOptionalParameters(\n            temperature: 0.7,\n            stream: true,\n            maxTokens: 1024\n        )\n        \n        do {\n            let stream = try await openAI.createChatCompletionsStream(\n                model: .gpt4o(.base),\n                messages: msgs,\n                optionalParameters: optionalParameters\n            )\n            \n            let resp = Message(content: \"\", isUser: false)\n            messages.append(resp)\n            \n            for try await response in stream {\n                let content = response.choices[0].delta?.content ?? \"\"\n                if let lastMessage = messages.last, !lastMessage.isUser {\n                    let updatedContent = lastMessage.content + content\n                    messages[messages.count - 1] = Message(content: updatedContent, isUser: false)\n                }\n            }\n        } catch {\n            print(\"Error: \\(error)\")\n            if let lastMessage = messages.last, !lastMessage.isUser {\n                messages[messages.count - 1] = Message(content: \"Cannot get response from OpenAI: \\(error)\", isUser: false)\n            }\n        }\n    }\n```\n\n이 모든 써드파티 라이브러리들은 좋지만 대부분이 한 제공업체만 지원해요. 여러 제공업체에 접근하려면 보통 동시에 몇 개의 라이브러리를 사용하거나 LLM 프레임워크를 시도해볼 수도 있어요.\n\n# LLM 프레임워크\n\n<div class=\"content-ad\"></div>\n\nLLM 프레임워크는 LLM 기반 어플리케이션을 작성할 수 있게 해주는 어플리케이션 프레임워크의 일종입니다. 이러한 프레임워크는 지원과 구조를 제공하며, 일반적으로 LLM 기반 어플리케이션을 작성하는 방법을 표현합니다.\n\n다양한 LLM 프레임워크가 있으며, 그 중에는 공식으로 지원되거나 제3자 라이브러리보다 인기가 있는 것도 있습니다. 이는 이러한 프레임워크가 개발자에게 다양한 능력을 제공하기 때문입니다. 이를 통해 여러 LLM 제공 업체에 동시에 연결하고, 여러 데이터 소스에 연결하며, 기본 LLM 위에 에이전트를 구현할 수 있습니다.\n\nLangchain, LlamaIndex, Haystack 등 여러 프레임워크가 있지만, 이 글에서는 LLM 기반 어플리케이션을 만들기 위한 현재 가장 인기 있는 두 프레임워크인 Langchain과 LlamaIndex에 대해 이야기하겠습니다.\n\n## Langchain 🦜️🔗\n\n<div class=\"content-ad\"></div>\n\n가장 인기 있는 프레임워크는 아마도 Langchain일 것입니다. Langchain은 2022년 10월에 처음 릴리스되었으며 그 이후로 급속하게 발전하여 LLM 세계의 거의 모든 것을 다루는데 이르렀습니다. 현재 시점에서 거의 400개의 릴리스에 이를 정도로 성장했습니다. 한 때 릴리스는 거의 매일 발생했으며 가끔은 하루에 두 번씩 이루어졌습니다!\n\n지난 1년 동안 Langchain은 비교적 단순한 Python 라이브러리에서 핵심 라이브러리부터 배포 서버, 관측성 도구 세트까지의 기능 생태계로 성장했습니다.\n\n이제 Langchain을 사용하여 OpenAI에 연결하고 그 채팅 API를 호출하는 방법에 대해 간단히 설명해드리겠습니다.\n\n```js\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nllm = ChatOpenAI(model_name=\"gpt-4o\")\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    (\"user\", \"{input}\")\n])\noutput_parser = StrOutputParser()\n\nchain = prompt | llm | output_parser\nresults = chain.invoke({\"input\": \"why is the sky blue?\"})\n\nprint(results)\n```\n\n<div class=\"content-ad\"></div>\n\n코드를 한눈에 보면 그리 다른 것 같지는 않지만, 챗 프롬프트, LLM 및 출력 파서를 연결하여 결과를 생성했음을 알아차릴 수도 있을 것입니다. 이것은 Langchain의 더 강력한 기능 중 하나의 예시이며, Langchain에 이름을 부여한 것 중 하나인 연쇄입니다.\n\n연쇄는 서로 연결된 호출의 일련이다. 연쇄는 Langchain 표현 언어(LCEL)를 사용하여 생성되며, 가장 기본적인 연쇄는 위에서 보여진 것과 같습니다:\n\n```js\nchain = prompt | llm | output_parser\n```\n\n연쇄를 실행하기 위해, 우리는 연쇄에 대해 몇 가지 메서드 중 하나를 호출하면 됩니다(위의 코드의 경우 invoke를 사용했습니다). 적절한 입력을 사용하여 호출하면 결과를 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Programming with AI: Calling APIs](/assets/img/2024-05-27-ProgrammingwithAICallingAPIs_1.png)\n\n체인은 강력하고 구성 가능합니다. 컨텍스트와 함께 질문을 LLM에 전달하여 간단한 검색 증강 생성(RAG)을 수행하는 방법을 살펴보겠습니다.\n\n이 경우 싱가포르의 통신 및 정보부 (MCI) 위원회 공급위원회에 대한 2024년 1월의 국회 회의록 텍스트 문서를 사용합니다. 해당 사이트에서 텍스트를 가져와 hansard.txt라는 텍스트 파일로 저장했습니다.\n\n```js\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_openai import ChatOpenAI\n\ndef extract(file_path):\n    with open(file_path, 'r') as file:\n        return [line.strip() for line in file if line.strip()]\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nvectorstore = DocArrayInMemorySearch.from_texts(\n    texts=extract('data/hansard.txt'),\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\ntemplate = \"\"\"다음 컨텍스트를 기반으로 질문에 답하십시오:\n{context}\n\n질문: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\noutput_parser = StrOutputParser()\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\nchain = setup_and_retrieval | prompt | model | output_parser\n\nresults = chain.invoke(\"스마트 네이션은 어떻게 시민들의 삶을 개선했습니까?\")\nprint(results)\n```\n\n\n<div class=\"content-ad\"></div>\n\n위의 코드에서는 먼저 hansard.txt 문서의 각 줄에서 인메모리 벡터 저장소를 만들고 OpenAI의 임베딩을 사용합니다. 벡터 저장소로부터 리트리버를 생성하여 프롬프트에 입력으로 적합한 줄을 가져올 수 있습니다.\n\n이제 사용자의 입력이 주어지면, 해당 입력을 리트리버에 전달하여 벡터 저장소에서 줄들을 가져올 수 있습니다. 사용자 입력은 또한 프롬프트로 전달됩니다. RunnableParallel을 통해 동시에 이 두 가지가 실행되고, 출력은 질문과 문맥으로 프롬프트로 전송됩니다.\n\n<img src=\"/assets/img/2024-05-27-ProgrammingwithAICallingAPIs_2.png\" />\n\n나머지 부분은 거의 동일하지만 여기에 출력이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n% python langchain_test_rag.py\n싱가포르의 스마트 네이션 이니셔티브는 2014년부터 2023년까지 정부 서비스에 대한 만족도를 73%에서 83%로 높여 시민들의 삶을 개선했습니다. 뿐만 아니라, 싱가포르인의 84%가 디지털 기술이 그들의 삶을 더 편하게 만들었다고 느끼고 있습니다. 이 이니셔티브는 일상적인 편의성과 삶의 질을 향상시키고, 사람들이 더 의미 있는 삶을 살도록 돕고, 누구도 뒤처지지 않도록 하는 것을 목표로 합니다.\n```\n\n여러분이 보실 수 있듯이, 체인은 강력한 메커니즘입니다. 이 체인 메커니즘은 Langchain에만 해당하는 것은 아닙니다. Haystack 프레임워크는 파이프라인이라고 부르며, LLMFlows와 같은 몇 개의 다른 프레임워크는 플로우라고 합니다.\n\n## LlamaIndex\n\n다른 인기 있는 LLM 프레임워크인 LlamaIndex가 있습니다. LlamaIndex는 2022년 11월에 GPTIndex라는 이름의 프레임워크로 시작되었습니다. LlamaIndex의 기본 개념은 LLM을 데이터에 연결하는 것입니다. 실제로 LlamaIndex와 Langchain은 거의 동시에 시작되었다는 것에 주목할 수 있습니다. 사실, Langchain의 창시자인 해리슨 체이스와 LlamaIndex의 창시자인 제리 류는 인공 지능 보안 회사인 Robust Intelligence에서 동료였습니다.\n\n\n<div class=\"content-ad\"></div>\n\n빠른 대화 완성을 위해 LlamaIndex 사용 방법을 간단히 살펴봅시다.\n\n```js\nfrom llama_index.core import Settings\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.llms.openai import OpenAI\n\nSettings.llm = OpenAI(model=\"gpt-4o\")\nmessages = [\n    ChatMessage(\n        role=\"system\", content=\"You are a helpful assistant.\"\n    ),\n    ChatMessage(role=\"user\", content=\"Why is the sky blue?\"),\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n```\n\n보시다시피, Langchain이나 기타 API와 크게 다르지 않지만 LlamaIndex의 장점은 데이터와의 연결에 중점을 둔다는 점입니다. 예상대로, 간단한 RAG를 수행하는 코드는 매우 간단합니다.\n\n```js\nfrom llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.llms.openai import OpenAI\n\nSettings.llm = OpenAI(model=\"gpt-4o\")\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"How has Smart Nation improved citizen's lives?\")\nprint(response)\n```\n\n<div class=\"content-ad\"></div>\n\n먼저, 데이터 디렉토리(즉, 우리의 hansard.txt 파일)에서 파일을 가져와서 벡터 저장소에 저장합니다. 그런 다음 해당 벡터 저장소를 쿼리 엔진으로 사용하여 쿼리를 보내면 문서에서 데이터를 사용하여 응답을 형성할 것입니다.\n\nLangchain과 LlamaIndex는 진화의 급격한 속도 이후 강력한 프레임워크입니다. 각각의 강점이 있으며 현재 시점에서는 주로 개인적인 선호에 따라 사용하는 것이 대부분입니다.\n\n# 요약\n\nAI 프로그래밍에 대한 수업의 첫 번째 부분입니다. 다음 글에서는 지역 LLM에 대해 더 깊이 알아볼 것입니다. 즉, 자신의 기기에 배포할 수 있는 LLM에 대해 다뤄볼 것입니다. 예를 들어, 자신의 노트북에도 배포할 수 있는 LLM입니다.","ogImage":{"url":"/assets/img/2024-05-27-ProgrammingwithAICallingAPIs_0.png"},"coverImage":"/assets/img/2024-05-27-ProgrammingwithAICallingAPIs_0.png","tag":["Tech"],"readingTime":17},{"title":"파이썬과 Redpanda를 사용하여 실시간 센서 데이터 집계하기","description":"","date":"2024-05-27 18:35","slug":"2024-05-27-AggregatingReal-timeSensorDatawithPythonandRedpanda","content":"\n\n## 간단한 Python을 사용한 스트림 처리 및 tumbling windows\n\n![이미지](/assets/img/2024-05-27-AggregatingReal-timeSensorDatawithPythonandRedpanda_0.png)\n\n이 튜토리얼에서는 Python(및 메시지 브로커로 Redpanda)만 사용하여 센서 데이터 스트림을 다운샘플링하는 방법을 보여드리고 싶습니다. 목표는 스트림 처리가 얼마나 간단할 수 있는지를 보여주는 것이며, 시작하기 위해 무겁고 복잡한 스트림 처리 프레임워크가 필요하지 않다는 것을 보여주고 싶습니다.\n\n최근까지 스트림 처리는 일반적으로 Java 전문 지식이 필요했던 복잡한 작업이었습니다. 그러나 Python 스트림 처리 생태계가 점차 성숙해지면서 Faust, Bytewax 및 Quix와 같은 Python 개발자에게 더 많은 옵션이 있어졌습니다. 나중에는 이러한 라이브러리가 기존의 Java 중심 옵션과 경쟁하기 위해 등장한 이유에 대해 조금 더 배경을 제공할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 먼저 일을 시작해 보겠습니다. 우리는 스트림 프로세서로 Quix Streams 라이브러리를 사용할 것입니다. Quix Streams는 Faust와 매우 유사하지만 구문이 더 간결하고 StreamingDataframes라는 Pandas와 유사한 API를 사용하도록 최적화되었습니다.\n\n다음 명령어로 Quix Streams 라이브러리를 설치할 수 있습니다:\n\n```js\npip install quixstreams\n```\n\n무엇을 구축할 것인가\n\n<div class=\"content-ad\"></div>\n\n간단한 애플리케이션을 개발하게 될 거에요. 이 애플리케이션은 다양한 센서에서 수신되는 온도 측정값의 롤링 집계를 계산할 거예요. 온도 측정값은 비교적 높은 빈도로 들어오며, 이 애플리케이션은 이를 집계하여 낮은 시간 해상도(10초마다)로 출력할 거에요. 너무 높은 해상도의 데이터로 작업하고 싶지 않아서 이를 압축한 것으로 생각할 수 있어요.\n\n이 GitHub 저장소에서 완성된 코드에 접근할 수 있어요.\n\n이 애플리케이션에는 합성 센서 데이터를 생성하는 코드가 포함되어 있지만, 실제 시나리오에서 이 데이터는 차량 편대나 기계가 가득한 창고와 같은 다양한 종류의 센서에서 나올 수 있어요.\n\n기본 아키텍처의 개략적인 그림을 아래에서 확인할 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-27-AggregatingReal-timeSensorDatawithPythonandRedpanda_1.png\" />\n\n# 스트림 처리 파이프라인의 구성 요소\n\n이전 다이어그램은 스트림 처리 파이프라인의 주요 구성 요소를 나타냅니다. 데이터 생산자인 센서, 스트리밍 데이터 플랫폼으로 Redpanda, 그리고 스트림 프로세서인 Quix가 있습니다.\n\n데이터 생산자\n\n<div class=\"content-ad\"></div>\n\n이 코드 조각들은 ECU(Engine Control Units)의 펌웨어, 클라우드 플랫폼의 모니터링 모듈 또는 사용자 활동을 기록하는 웹 서버와 같은 데이터를 생성하는 시스템에 연결되어 있습니다. 이러한 코드는 원시 데이터를 가져와 해당 플랫폼이 이해할 수 있는 형식으로 스트리밍 데이터 플랫폼으로 보냅니다.\n\n스트리밍 데이터 플랫폼\n\n여기서는 스트리밍 데이터를 저장합니다. 정적 데이터에 대한 데이터베이스가 하는 역할과 거의 동일한 역할을 합니다. 그러나 테이블 대신 토픽을 사용합니다. 그렇지 않으면 정적 데이터베이스와 유사한 기능을 갖추고 있습니다. 데이터를 소비하고 생성할 수 있는 사용자, 데이터가 준수해야 하는 스키마 등을 관리해야 합니다. 그러나 데이터베이스와는 달리 데이터가 끊임없이 변하기 때문에 쿼리할 목적으로 설계되지 않습니다. 일반적으로 데이터를 변환하고 데이터 과학자가 탐색할 수 있는 데 옮기거나 RisingWave나 Apache Pinot과 같은 스트리밍 데이터에 최적화된 쿼리 가능 시스템에 원시 데이터를 넣기 위해 스트림 프로세서를 사용합니다. 그러나 스트리밍 데이터의 패턴에 의해 트리거된 자동화 시스템(추천 엔진과 같은)의 경우에는 이것이 이상적인 해결책이 아닙니다. 이 경우 전용 스트림 프로세서를 사용하는 것이 확실합니다.\n\n스트림 프로세서\n\n<div class=\"content-ad\"></div>\n\n이러한 엔진들은 데이터가 도착하는 대로 계속적으로 작업을 수행하는 엔진들입니다. 이들은 어플리케이션 백엔드에서 데이터를 처리하는 일반적인 마이크로서비스들과 비교될 수 있습니다. 하지만 한 가지 큰 차이가 있습니다. 마이크로서비스의 경우 데이터가 비아주로 도착하며, 각 \"물방울\"이 개별적으로 처리됩니다. 비가 많이 내려도 서비스가 \"물방울\"을 넘치지 않고 따라잡는 것은 그리 어렵지 않습니다 (물에서 불순물을 거르는 필터 시스템을 생각해보세요).\n\n스트림 프로세서의 경우 데이터가 연속적이고 넓은 물줄기로 도착합니다. 필터 시스템은 빠르게 압도되며 디자인을 변경하지 않는 이상 물줄기를 분할하고 작은 스트림을 여러 필터 시스템으로 보내야 합니다. 이것이 바로 스트림 프로세서가 작동하는 방식입니다. 그들은 수평적으로 확장 가능하게 설계되어 있으며 병렬로 작동하고 있습니다. 그리고 절대 멈추지 않고 계속해서 데이터를 처리하며, 걸러진 데이터를 스트리밍 데이터 플랫폼에 출력하는데, 이것은 스트리밍 데이터의 저장지 역할을 합니다. 조금 더 복잡하게 만들기 위해, 스트림 프로세서들은 종종 여기서 시도해볼 창 기능 예제와 같이 이전에 받았던 데이터를 추적해야 할 수도 있습니다.\n\n또한 \"데이터 컨슈머\"와 \"데이터 싱크\"라는 것도 있습니다. 처리된 데이터를 사용하는 시스템(프론트엔드 어플리케이션 및 모바일 앱과 같은)이나 오프라인 분석을 위해 데이터를 저장하는 시스템(스노우플레이크나 AWS 레드시프트와 같은 데이터 웨어하우스)이 있습니다. 이 튜토리얼에서는 이들을 다루지 않을 것이므로 일단은 생략하겠습니다.\n\n# 로컬 스트리밍 데이터 클러스터 설정하기\n\n<div class=\"content-ad\"></div>\n\n이 튜토리얼에서는 로컬 설치된 Redpanda를 활용하여 스트리밍 데이터를 관리하는 방법을 보여드릴 거에요. Redpanda를 선택한 이유는 로컬에서 쉽게 실행할 수 있기 때문이에요.\n\n먼저 도커 컴포즈를 사용하여 Redpanda 콘솔을 포함한 클러스터를 빠르게 생성할 거에요. 그러니 먼저 도커를 설치해야 해요.\n\n# 스트리밍 애플리케이션 생성\n\n먼저 스트리밍 데이터를 생성하고 처리할 개별 파일을 만들 거에요. 이렇게 하면 실행 중인 프로세스를 독립적으로 관리하기 쉬워져요. 즉, 프로듀서를 중지하더라도 스트림 프로세서를 중지하지 않아도 되요. 이제 만들 파일의 개요를 살펴보겠어요:\n\n<div class=\"content-ad\"></div>\n\n- 스트림 프로듀서: sensor_stream_producer.py\n가상 온도 데이터를 생성하고 해당 데이터를 Redpanda의 \"raw data\" 소스 토픽에 씁니다. Faust 예제와 마찬가지로 약 5초마다 약 20개의 측정 값을 생성하거나 초당 약 4개의 측정 값을 생성합니다.\n- 스트림 프로세서: sensor_stream_processor.py\n\"source\" 토픽에서 원시 온도 데이터를 소비하고 데이터의 해상도를 줄이기 위해 텀블링 윈도우 계산을 수행합니다. 10초 간격의 창에서 받은 데이터의 평균 값을 계산하여 각 10초마다 한 번의 측정 값을 얻습니다. 그런 다음 이 집계된 측정 값을 Redpanda의 agg-temperatures 토픽에 생성합니다.\n\n스트림 프로세서가 대부분의 작업을 처리하고이 튜토리얼의 핵심입니다. 스트림 프로듀서는 적절한 데이터 수집 프로세스의 대리인입니다. 예를 들어, 프로덕션 시나리오에서는 MQTT 커넥터와 같은 것을 사용하여 센서에서 데이터를 가져와 토픽에 생성할 수 있습니다.\n\n- 간단한 예제를 위해 데이터를 시뮬레이션하는 것이 더 간단하므로 먼저 설정해 봅시다.\n\n# 스트림 프로듀서 생성\n\n<div class=\"content-ad\"></div>\n\n새로운 파일인 sensor_stream_producer.py를 생성하고 주요 Quix 애플리케이션을 정의하세요. (이 예제는 Python 3.10에서 개발되었지만, Python 3의 다른 버전도 pip install quixstreams을 실행할 수 있다면 작동해야 합니다.)\n\nsensor_stream_producer.py 파일을 만들고 필요한 종속 항목(Quix Streams 포함)을 모두 추가하세요.\n\n```python\nfrom dataclasses import dataclass, asdict # 데이터 스키마를 정의하는 데 사용됩니다.\nfrom datetime import datetime # 타임스탬프를 관리하는 데 사용됩니다.\nfrom time import sleep # 데이터 생성기를 느리게 만드는 데 사용됩니다.\nimport uuid # 메시지 ID 생성에 사용됩니다.\nimport json # 데이터 직렬화에 사용됩니다.\n\nfrom quixstreams import Application\n```\n\n그런 다음, Quix 애플리케이션 및 데이터를 보낼 대상 토픽을 정의하세요.\n\n<div class=\"content-ad\"></div>\n\n```python\napp = Application(broker_address='localhost:19092')\n\ndestination_topic = app.topic(name='raw-temp-data', value_serializer=\"json\")\n```\n\n`value_serializer` 매개변수는 예상 소스 데이터의 형식을 정의합니다 (바이트로 직렬화될 데이터). 이 경우 JSON을 보낼 것입니다.\n\n온도 데이터에 대한 매우 기본적인 스키마를 정의하고 JSON으로 직렬화하는 함수를 추가하기 위해 dataclass 모듈을 사용해봅시다.\n\n```python\n@dataclass\nclass Temperature:\n    ts: datetime\n    value: int\n\n    def to_json(self):\n        # 데이터 클래스를 사전으로 변환\n        data = asdict(self)\n        # datetime 객체를 문자열로 변환\n        data['ts'] = self.ts.isoformat()\n        # 사전을 JSON 문자열로 직렬화\n        return json.dumps(data)\n```\n\n<div class=\"content-ad\"></div>\n\n다음으로, 가짜 온도 센서 데이터를 Redpanda 소스 토픽으로 보내는 코드를 추가해보세요.\n\n```js\ni = 0\nwith app.get_producer() as producer:\n    while i < 10000:\n        sensor_id = random.choice([\"Sensor1\", \"Sensor2\", \"Sensor3\", \"Sensor4\", \"Sensor5\"])\n        temperature = Temperature(datetime.now(), random.randint(0, 100))\n        value = temperature.to_json()\n\n        print(f\"생성된 값 {value}\")\n        serialized = destination_topic.serialize(\n            key=sensor_id, value=value, headers={\"uuid\": str(uuid.uuid4())}\n        )\n        producer.produce(\n            topic=destination_topic.name,\n            headers=serialized.headers,\n            key=serialized.key,\n            value=serialized.value,\n        )\n        i += 1\n        sleep(random.randint(0, 1000) / 1000)\n```\n\n이 코드는 0부터 1초 사이의 랜덤한 시간 간격으로 분리된 1000개의 레코드를 생성합니다. 또한 5가지 옵션 중에서 센서 이름을 무작위로 선택합니다.\n\n이제 명령줄에서 다음을 실행하여 프로듀서를 테스트해보세요.\n\n<div class=\"content-ad\"></div>\n\n```js\npython sensor_stream_producer.py\n```\n\n이렇게 로되 데이터를 콘솔에 확인할 수 있어요:\n\n```js\n[데이터 생성됨]\n```\n\n작동하는 것을 확인하면 일단 프로세스를 중지해주세요 (나중에 스트림 처리 프로세스와 함께 실행할 거에요).\n\n\n<div class=\"content-ad\"></div>\n\n# 스트림 프로세서 생성\n\n스트림 프로세서는 세 가지 주요 작업을 수행합니다: 1) 소스 토픽에서 원시 온도 데이터를 소비, 2) 데이터를 지속적으로 집계하고, 3) 집계된 결과를 싱크 토픽으로 전송합니다.\n\n각 작업에 대한 코드를 추가해 보겠습니다. IDE에서 sensor_stream_processor.py라는 새 파일을 만들어 주세요.\n\n먼저, 이전과 같이 종속성을 추가해주세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nimport os\nimport random\nimport json\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\nimport logging\nfrom quixstreams import Application\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n```\n\n그리고 스트림 처리 응용 프로그램에서 필요한 몇 가지 변수를 설정해보겠습니다:\n\n```js\nTOPIC = \"raw-temperature\" # 입력 토픽을 정의합니다\nSINK = \"agg-temperature\"  # 출력 토픽을 정의합니다\nWINDOW = 10  # 시간 창의 길이를 초 단위로 정의합니다\nWINDOW_EXPIRES = 1 # 데이터가 창에서 제외되기 전에 도착할 수 있는 시간을 초 단위로 정의합니다\n```\n\n나중에 창 변수가 무엇을 의미하는지 자세히 살펴보겠지만, 지금은 주요 Quix 애플리케이션을 정의하는 데 집중해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\napp = Application(\n    broker_address='localhost:19092',\n    consumer_group=\"quix-stream-processor\",\n    auto_offset_reset=\"earliest\",\n)\n```\n\n지금은 몇 가지 더 많은 애플리케이션 변수들이 있는데요, consumer_group와 auto_offset_reset이 그 중 일부에요. 이러한 설정 사이의 상호작용에 대해 더 알아보려면 \"카프카의 auto offset reset 구성 이해: 사용 사례 및 함정\"이라는 기사를 확인해 보세요.\n\n다음으로, 코어 스트림 처리 함수의 양쪽에 입력 및 출력 토픽을 정의하고, 들어오는 데이터를 DataFrame에 넣는 함수를 추가하세요.\n\n```js\ninput_topic = app.topic(TOPIC, value_deserializer=\"json\")\noutput_topic = app.topic(SINK, value_serializer=\"json\")\n\nsdf = app.dataframe(input_topic)\nsdf = sdf.update(lambda value: logger.info(f\"Input value received: {value}\"))\n```\n\n<div class=\"content-ad\"></div>\n\n우리는 들어오는 데이터가 손상되지 않았는지 확인하기 위해 로깅 라인을 추가했습니다.\n\n다음으로, 메시지 페이로드에서 Kafka 타임스탬프 대신 사용할 사용자 정의 타임스탬프 추출기를 추가해봅시다. 집계를 위해 이것은 기본적으로 읽기가 생성된 시간을 사용하고 Redpanda가 수신한 시간이 아닌 것을 의미합니다. 더 간단하게 설명하면 \"Redpanda의 수신된 시간이 아닌 센서의 시간 정의를 사용하세요\".\n\n```js\ndef custom_ts_extractor(value):\n \n    # 센서의 타임스탬프를 추출하여 datetime 객체로 변환\n    dt_obj = datetime.strptime(value[\"ts\"], \"%Y-%m-%dT%H:%M:%S.%f\") # \n\n    # 효율적인 Quix 처리를 위해 Unix epoch부터의 밀리초로 변환\n    milliseconds = int(dt_obj.timestamp() * 1000)\n    value[\"timestamp\"] = milliseconds\n    logger.info(f\"새로운 타임스탬프 값: {value['timestamp']}\")\n\n    return value[\"timestamp\"]\n\n# 이전에 정의된 input_topic 변수를 덮어쓰어 사용자 정의 타임스탬프 추출기를 사용하도록 설정\ninput_topic = app.topic(TOPIC, timestamp_extractor=custom_ts_extractor, value_deserializer=\"json\") \n```\n\n왜 이렇게 하는 걸까요? 처리에 사용할 시간에 대해 철학적인 논쟁으로 빠져들 수 있겠지만, 그건 다른 기사의 주제입니다. 사용자 정의 타임스탬프로 하고자 한 것은 실시간 처리에서 시간을 해석하는 다양한 방법이 있고, 데이터 도착 시간을 반드시 사용할 필요는 없다는 것을 보여주고 싶었습니다.\n\n<div class=\"content-ad\"></div>\n\n새 창이 시작될 때 집계를 위한 상태를 초기화하세요. 창에 첫 번째 레코드가 도착할 때 집계를 준비합니다.\n\n```js\ndef initializer(value: dict) -> dict:\n\n    value_dict = json.loads(value)\n    return {\n        'count': 1,\n        'min': value_dict['value'],\n        'max': value_dict['value'],\n        'mean': value_dict['value'],\n    }\n```\n\n이것은 창을 위한 초기 값들을 설정합니다. 최솟값, 최댓값 및 평균의 경우, 처음의 센서 리딩을 시작점으로 삼기 때문에 모두 동일합니다.\n\n이제 \"리듀서\" 함수 형태로 집계 로직을 추가해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\ndef reducer(aggregated: dict, value: dict) -> dict:\n    aggcount = aggregated['count'] + 1\n    value_dict = json.loads(value)\n    return {\n        'count': aggcount,\n        'min': min(aggregated['min'], value_dict['value']),\n        'max': max(aggregated['max'], value_dict['value']),\n        'mean': (aggregated['mean'] * aggregated['count'] + value_dict['value']) / (aggregated['count'] + 1)\n    }\n\n\n이 기능은 창에서 여러 집계를 수행할 때만 필요합니다. 우리의 경우 각 창에 대해 count, min, max 및 mean 값을 생성하기 때문에 이러한 값을 미리 정의해야 합니다.\n\n다음으로, 중요한 부분입니다 - tumbling window 기능 추가:\n\n\n### 창 유형 및 길이와 같은 창 매개변수 정의\nsdf = (\n    # 10초의 텀블링 창 정의\n    sdf.tumbling_window(timedelta(seconds=WINDOW), grace_ms=timedelta(seconds=WINDOW_EXPIRES))\n\n    # 'reducer' 및 'initializer' 함수로 'reduce' 집계 생성\n    .reduce(reducer=reducer, initializer=initializer)\n\n    # 닫힌 10초 창에 대해서만 결과 발생\n    .final()\n)\n\n### 스트리밍 DataFrame에 창 적용 및 출력에 포함할 데이터 포인트 정의\nsdf = sdf.apply(\n    lambda value: {\n        \"time\": value[\"end\"], # 'agg-temperature' 토픽으로 보낼 메시지의 타임스탬프로 윈도우 종료 시간 사용\n        \"temperature\": value[\"value\"], # 온도 매개변수에 대한 {count, min, max, mean} 값을 포함하는 사전 전송\n    }\n)\n\n\n<div class=\"content-ad\"></div>\n\n스트림 처리가 가능한 DataFrame을 정의하는데, 이는 텀블링 윈도우를 기반으로 한 집계의 집합입니다 — 시간의 10초간의 겹치지 않는 세그먼트에 대해 수행되는 집계의 집합입니다.\n\n팁: 다양한 윈도우 계산 유형에 대해 다시 알아보려면 다음 기사를 확인해보세요: “스트림 처리에서 윈도우링하는 방법 안내”.\n\n마지막으로 결과를 다운스트림 출력 주제로 내보냅니다:\n\n```js\nsdf = sdf.to_topic(output_topic)\nsdf = sdf.update(lambda value: logger.info(f\"생성된 값: {value}\"))\n\nif __name__ == \"__main__\":\n    logger.info(\"애플리케이션 시작\")\n    app.run(sdf)\n```\n\n<div class=\"content-ad\"></div>\n\n알림: 생산자 코드가 합성 온도 데이터를 전송하는 데 사용된 생산자 코드와 매우 다르게 보일 수 있습니다 (with app.get_producer() as producer()을 사용하는 부분). 이는 Quix가 변환 작업을 위해 다른 생산자 함수를 사용하기 때문입니다 (즉, 입력 및 출력 주제 사이에 위치한 작업).\n\n따라오시면서 알게 되겠지만, 우리는 Streaming DataFrame인 sdf 변수를 최종 형태로 바꿀 때까지 반복적으로 변경합니다. 따라서 sdf.to_topic 함수는 Streaming DataFrame의 최종 상태를 단순히 출력 주제로 스트리밍하여 행 단위로 돌려줍니다.\n\n반면에 생산자 함수는 CSV 파일, MQTT 브로커 또는 우리의 경우와 같이 외부 소스에서 데이터를 수집하는 데 사용됩니다, 생성기 함수가 있습니다.\n\n# 스트리밍 애플리케이션 실행\n\n<div class=\"content-ad\"></div>\n\n마침내, 스트리밍 애플리케이션을 실행하고 모든 부분들이 원활하게 작동하는지 확인할 수 있게 되었네요.\n\n먼저 터미널 창에서 다시 프로듀서를 실행해주세요:\n\n```js\npython sensor_stream_producer.py\n```\n\n그런 다음, 두 번째 터미널 창에서 스트림 프로세서를 시작하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\npython sensor_stream_processor.py\n```\n\n각 창에서 로그 출력을 주의 깊게 확인하여 모든 것이 원할하게 실행되는지 확인하세요.\n\n또한 집골 토픽에 집계된 데이터가 올바르게 스트리밍되는지 확인하려면 Redpanda 콘솔을 확인할 수 있습니다(주소: http://localhost:8080/topics).\n\n<img src=\"/assets/img/2024-05-27-AggregatingReal-timeSensorDatawithPythonandRedpanda_2.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n# 마무리\n\n여기서 시도한 것은 스트림 처리를 수행하는 하나의 방법에 불과합니다. 당연히 Apache Flink 및 Apache Spark Streaming과 같은 강력한 도구들도 있습니다. 그러나 이 도구들은 주로 Java 기반입니다. Python 래퍼를 사용할 수 있지만 문제가 발생하면 여전히 Python 오류가 아닌 Java 오류를 디버깅해야 합니다. Java 기술은 데이터 엔지니어들과 함께 작업하는 소프트웨어 엔지니어들 사이에서 점점 더 중요해지는 데이터 분석가에게 일반적으로 보급되어 있는 기술은 아닙니다.\n\n이 튜토리얼에서는 스트림 처리 알고리즘으로 간단한 집계를 실행했지만, 실제로 이러한 알고리즘들은 주로 기계 학습 모델을 활용하여 데이터를 변환합니다. 또한, 기계 학습을 위한 소프트웨어 생태계는 주로 Python으로 이루어져 있습니다.\n\n자료 전문가, 기계 학습 엔지니어 및 소프트웨어 엔지니어들이 함께 작업할 때 Python이 선호되는 언어임을 자주 간과합니다. 이는 SQL보다도 더 효율적입니다. 왜냐하면 Python을 사용하여 데이터와 관련 없는 작업(예: API 호출 및 웹훅 트리거)을 수행할 수 있기 때문입니다. 이것이 Faust, Bytewax, Quix와 같은 라이브러리들이 발전해 나간 이유 중 하나입니다. 즉, 이러한 다양한 분야 사이의 임피던스 갭을 줄이기 위해 만들어졌다는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n희망을 가지고, 파이썬이 스트림 처리에 적합한 언어임을 보여드릴 수 있었으면 좋겠고, 파이썬의 스트림 처리를 위한 생태계가 꾸준히 성숙해지고 있으며, 기존의 Java 기반 생태계에 버금가는 성능을 보여줄 수 있음을 보여드릴 수 있었기를 희망합니다.\n\n- 이 튜토리얼의 모든 코드는 이 GitHub 저장소에서 확인하실 수 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-AggregatingReal-timeSensorDatawithPythonandRedpanda_0.png"},"coverImage":"/assets/img/2024-05-27-AggregatingReal-timeSensorDatawithPythonandRedpanda_0.png","tag":["Tech"],"readingTime":13},{"title":"풀리 시스템 모델링 방법","description":"","date":"2024-05-27 18:33","slug":"2024-05-27-HowtoModelPulleySystems","content":"\n\n\n![Pulley System](/assets/img/2024-05-27-HowtoModelPulleySystems_0.png)\n\n풀리 시스템은 비탄성 케이블이나 줄을 통해 연결된 물체들의 시스템으로, 일반적으로 고정된 축 위의 바퀴인 부드러운 풀리를 통해 통과됩니다. 일상생활에서 가장 흔히 발견되는 풀리 시스템은 강철 케이블로 연결된 두 묵직한 물체로 구성됩니다. 케이블이 시스템 상단의 부드러운 풀리를 통과하고 물체들은 풀리 양쪽에서 수직으로 올라가거나 내려갑니다.\n\n풀리의 한 쪽에 엘리베이터가 있고 다른 쪽에 카운터웨이트가 있는 엘리베이터 시스템이 일반적인 예입니다. 또 다른 일반적이고 비슷한 예로는 인간이 카운터웨이트에 맞서 풀리를 통해 케이블을 당기는 웨이트 머신이 있습니다.\n\n풀리 시스템 문제는 고등학교에서 흔히 배우는 수학 도구를 사용하여 해결하는 데 재미있고 깔끔할 수 있습니다. 최근에 매우 만족스러운 해결책이 있는 이 형태의 문제를 발견했습니다. 이 문제를 해결하기 위해서는 뉴턴의 법칙, 운동의 기본 방정식, 그리고 에너지와 운동량에 대한 기본 지식이 필요합니다. 또한 문제를 수학으로 모델링하기 위해 체계적으로 생각할 수 있어야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n문제를 설명하겠습니다.\n\nA 파일드라이버는 가벼운 무게 m의 경량 카운터웨이트에 의해 무거운 질량 M의 무게로 구성되어 있으며 매끄럽고 가벼운 고정 풀리를 통해 전달되는 불편한 조건자 그리고 카운터웨이트가 연결된 무거운 질량 M으로 이루어져 있습니다. 편은 파일 아래에 위치합니다. 파일 드라이버는 파일 위에 위치한 상태에서 정지된 상태에서 시작되며, 그 이후의 충돌은 파일드라이버와 파일 간에만 발생하며 이러한 충돌은 완전히 탄력적입니다. 파일드라이버가 정지 상태에 이르기까지 걸리는 시간이 처음으로 파일을 제동시키기까지 걸리는 시간의 세 배임을 보여줍니다.\n\n## 문제 이해\n\n역학 문제의 경우 다이어그램이 많은 도움이 됩니다. 이것이 파일 위에 위치한 초기 상황의 다이어그램입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![How to Model Pulley Systems Part 1](/assets/img/2024-05-27-HowtoModelPulleySystems_1.png)\n\n이 시스템은 정지 상태에서 출발되며, 우리는 편막기가 말뚝을 타격할 것으로 예상할 수 있습니다. 이 충돌이 완전히 탄성 없다고 합니다. 이는 모든 운동 에너지가 손실되고 편막기가 말뚝에서 튕겨 나오지 않고 충돌 후 그 자리에서 그대로 남게 될 것을 의미합니다.\n\n그러나 카운터웨이트는 충돌 후에도 계속해서 상승할 것이며, 그 결과 줄은 느슨해질 것입니다. 이것은 중력이 카운터웨이트를 멈출 때까지 계속될 것이며, 그리고 다시 하강하기 시작합니다. 이 상황의 다이어그램은 다음과 같습니다:\n\n![How to Model Pulley Systems Part 2](/assets/img/2024-05-27-HowtoModelPulleySystems_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n어느 순간, 줄이 다시 팽창되어 긴장이 생기면서 편입기가 중력에 의해 느려지고 제로가 될 때까지 위로 올라갈 것입니다. 그런 다음, 파일과 충돌할 때까지 떨어지며, 이 과정은 시스템이 영구적으로 정지할 때까지 반복됩니다.\n\n## 첫 번째 충돌 전에 움직임 이해하기\n\n시스템이 해제되면 힘이 작용하고, 가속될 것으로 예상할 수 있습니다. 아래쪽 힘을 양수, 위쪽 힘을 음수로 간주합시다. 풀리의 각 측면에서 힘을 분해하고 뉴턴의 제2법칙을 사용하여 동시 방정식을 구할 수 있습니다 (중력 가속도를 나타내는 데 g를 사용):\n\n![image](/assets/img/2024-05-27-HowtoModelPulleySystems_3.png)\n\n<div class=\"content-ad\"></div>\n\n두 번째 방정식을 재배열하고 첫 번째 방정식에 대입하면, 시스템의 가속도에 대한 식을 유도할 수 있습니다. 다음과 같이:\n\n![이미지](/assets/img/2024-05-27-HowtoModelPulleySystems_4.png)\n\n이제 이 질문에 대한 진전을 이루기 위해, 첫 충돌이 발생하는데 걸리는 시간에 대한 식을 구해야 합니다. 이를 위해 이동 방정식을 사용할 수 있습니다. 우리는 가속도를 알고 있고, 초기 속도 u = 0임을 알고 있습니다. 충돌 지점에서의 속도를 v로 부르겠습니다. 첫 번째 충돌이 발생하는 데 걸리는 시간을 찾기 위해 v = u + at와 같은 방정식을 사용해 봅시다:\n\n![이미지](/assets/img/2024-05-27-HowtoModelPulleySystems_5.png)\n\n<div class=\"content-ad\"></div>\n\n## 첫 번째와 두 번째 충돌 사이의 움직임 연구\n\n알다시피, 첫 충돌 직후 집게는 정지 상태를 유지하지만, 카운터웨이트는 중력에 반항하여 계속 상승하다가 중력이 그것을 느리게 하여 멈추고 다시 하강하기 시작합니다.\n\n이제 충돌 시점에서 집게의 속도를 v로 가정하겠습니다. 따라서 충돌 시점에서 카운터웨이트도 속력 v로 상승 중입니다. 충돌 이후에도 카운터웨이트는 이 초기 속도로 계속 상승하며, 줄은 늘어나게 되고, 중력이 카운터웨이트를 느리게 하여 멈추게 한 후 다시 하강하기 시작합니다. 줄이 다시 팽팽해지는 시점에, 다시 속도 v에 도달하게 됩니다. 이전과 같은 운동 방정식을 사용하고, 초기 속도가 v이고 최종 속도가 0이며 가속도가 -g임을 고려하면, 카운터웨이트가 멈추기까지 걸리는 시간이 v/g임을 쉽게 찾을 수 있고, 줄이 다시 팽팽해지기까지 걸리는 시간은 2v/g임을 알 수 있습니다.\n\n이제 줄이 다시 팽팽해지는 시점에서, 장력은 집게와 카운터웨이트가 새로운 속도로 함께 움직이도록 유발할 것입니다. 우리는 전단량 보존 법칙을 사용하여 우리가 v₁이라고 부를 새로운 속도를 계산할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![2024-05-27-HowtoModelPulleySystems_6](/assets/img/2024-05-27-HowtoModelPulleySystems_6.png)\n\n이제 이 시스템이 중력으로 인해 느려지고, 더블카드가 두 번째로 펠을 충돌할 때 속도 v₁로 다시 돌아올 것을 알았습니다. 우리는 이전에 계산한 것과 같은 방법을 사용하여 더블카드가 멈출 때까지 걸리는 시간을 계산합니다. 초기 속도는 v₁, 최종 속도는 0이며, 가속도는 전체 시스템의 가속도입니다. 따라서 다음과 같습니다:\n\n![2024-05-27-HowtoModelPulleySystems_7](/assets/img/2024-05-27-HowtoModelPulleySystems_7.png)\n\n첫 번째와 두 번째 충돌 사이의 총 시간 T₁을 얻으려면 이 값을 두 배하여 역추적하는 시간을 더해야 합니다:\n  \n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-27-HowtoModelPulleySystems_8.png)\n\n## 연속적인 충돌 사이의 움직임 공부\n\n우리의 논리를 반복하면, 만약 v₁가 두 번째 충돌 직전 시스템의 속도라면, 반추중량은 2v₁/g 시간에 탄력줄 위치로 돌아올 것입니다. 이 시점에서 우리는 시스템의 새로운 속도 v₂가 다음과 같은 조건을 만족한다고 말할 수 있습니다:\n\n![이미지](/assets/img/2024-05-27-HowtoModelPulleySystems_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n이러한 방법을 사용하여 결합된 시스템이 제로로 느려지는 데 걸리는 시간을 계산할 수도 있습니다. 이 계산 결과는 다음과 같습니다:\n\n![image](/assets/img/2024-05-27-HowtoModelPulleySystems_10.png)\n\n그리고 같은 과정을 따라 두 번째 충돌과 세 번째 충돌 사이의 시간 T₂를 결정할 수 있습니다:\n\n![image](/assets/img/2024-05-27-HowtoModelPulleySystems_11.png)\n\n<div class=\"content-ad\"></div>\n\n그리고 이제 충돌 사이의 소요 시간이 등비 수열인 것을 쉽게 알 수 있습니다. 즉:\n\n![image](/assets/img/2024-05-27-HowtoModelPulleySystems_12.png)\n\n## 마지막 단계\n\n충돌 사이의 시간 순열이 공비율 m/(M+m) 과 첫 항 T₁을 가진 등비 수열임을 알았으며, 공비율이 명백히 1보다 작음을 고려하면, 모든 충돌의 시작부터 끝까지의 총 시간이 극한값으로 수렴함을 말할 수 있고, 등비 급수의 합 T = T₁ + T₂ + … 를 다음과 같이 계산할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-27-HowtoModelPulleySystems_13.png)\n\n그냥 언급할 사항만 남았네요. 이는 망치가 처음으로 말끔히 칠 때까지 걸리는 시간의 두 배입니다(이전에 계산한 것). 따라서 시스템이 정지할 때까지 걸리는 총 시간은 망치가 처음으로 말끔히 칠 때까지 걸리는 시간의 세 배입니다.\n\n이 문제에 대해 어떻게 생각하셨나요? 다르게 접근할 방법이 있나요? 자유롭게 의견을 남겨주세요!","ogImage":{"url":"/assets/img/2024-05-27-HowtoModelPulleySystems_0.png"},"coverImage":"/assets/img/2024-05-27-HowtoModelPulleySystems_0.png","tag":["Tech"],"readingTime":5},{"title":"YOLOv10 Custom Object Detection","description":"","date":"2024-05-27 18:31","slug":"2024-05-27-YOLOv10CustomObjectDetection","content":"\n\nYOLOv10 및 사용자 지정 데이터로 모델 학습 개요\n\n![YOLOv10CustomObjectDetection_0.png](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png)\n\n## 개요\n\nUltralytics Python 패키지를 사용하여 개발된 YOLOv10은 실시간 객체 검출을 위한 새로운 접근 방식을 제공합니다. Qinghua 대학 연구원들이 개발한 이 모델은 모델 아키텍처 개선과 non-maximum suppression (NMS) 제거를 통해 성능을 향상시켰습니다. 이러한 최적화로 인해 더 낮은 계산 요구사항으로 최신 기술 성능을 제공합니다. YOLOv10은 다양한 모델 규모에 대해 우수한 정확도-대기간 교환을 제공하는 것으로 실험 결과 보여졌습니다.\n\n<div class=\"content-ad\"></div>\n\n내 이전 기사를 읽은 사람들은 알겠지만, YOLO 모델을 사용한 다양한 프로젝트를 공유해 왔습니다. 사전 학습된 모델 중에서 성능과 효율성 면에서 두드러지는 YOLO 모델입니다. 그러나 실시간 객체 감지는 비최대 억제 (NMS)와 구조적 비효율성에 의해 도전을 겪어왔습니다. YOLOv10은 이러한 문제를 해결하기 위해 NMS를 제거하고 효율성과 정확도 양쪽을 모두 고려한 설계 전략을 채택했습니다.\n\n## 구조\n\n![YOLOv10CustomObjectDetection_1](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_1.png)\n\n- 백본: 특징 추출을 담당하는 백본은 CSPNet (Cross Stage Partial Network)의 향상된 버전을 사용하여 기울기 흐름을 개선하고 계산 중복성을 줄였습니다.\n- 넥: 다양한 스케일의 특징을 집계하고 헤드로 전달하는 넥은 효과적인 다중 스케일 특징 퓨전을 위해 PAN (Path Aggregation Network) 레이어를 포함하고 있습니다.\n- One-to-Many 헤드: 훈련 중 하나의 객체에 대해 여러 예측을 생성하여 풍부한 지도 신호를 제공하고 학습 정확도를 향상시킵니다.\n- One-to-One 헤드: 추론 중 하나의 객체에 대해 최상의 예측을 생성하여 NMS의 필요성을 제거하고 지연 시간을 줄이며 효율성을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n## 모델 변형과 성능\n\nYOLOv10은 여섯 가지 모델로 제공됩니다:\n\n- YOLOv10-N: 매우 자원이 제한된 환경을 위한 나노 버전.\n- YOLOv10-S: 속도와 정확도를 균형있게 유지한 작은 버전.\n- YOLOv10-M: 일반적인 용도를 위한 중간 버전.\n- YOLOv10-B: 정확도를 높이기 위해 넓이를 증가시킨 균형잡힌 버전.\n- YOLOv10-L: 컴퓨팅 자원을 늘리는 대가로 더 높은 정확도를 가진 대형 버전.\n- YOLOv10-X: 최대 정확도와 성능을 위한 초대형 버전\n\n![이미지](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_2.png)\n\n<div class=\"content-ad\"></div>\n\n## 비교\n\n서로 다른 모델 간의 지연 시간과 정확도에 대한 비교를 살펴봅시다. 이는 COCO와 같은 표준 벤치마크에서 테스트되었습니다.\n\n![Image 1](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_3.png)\n\n![Image 2](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_4.png)\n\n<div class=\"content-ad\"></div>\n\nYOLOv10은 실시간 객체 검출 애플리케이션에 대한 첨단 기술로, 더 적은 매개변수로 더 높은 정확도와 속도 성능을 제공합니다.\n\n## 사용자 정의 객체 검출을 위한 YOLOv10 훈련\n\n먼저, 공식 YOLOv10 GitHub 저장소를 복제하여 필요한 yolov10n 모델을 다운로드하세요.\n\n```js\n!pip install -q git+https://github.com/THU-MIG/yolov10.git\n\n!wget -P -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\n```\n\n<div class=\"content-ad\"></div>\n\n로보플로 유니버스에서 원하는 사용자 정의 프로젝트를 실험하고, 직접 데이터셋을 생성하며, 인텔이 후원하는 RF100 데이터셋을 사용할 수 있어요. 이 게시물에서는 X-레이 이미지에서 위험한 항목을 감지하기 위해 준비된 데이터셋을 사용할 거에요.\n\n로보플로 API를 사용하여 YOLOv8 형식으로 모델을 다운로드하세요.\n\n```python\n!pip install -q roboflow\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"your-api-key\")\nproject = rf.workspace(\"vladutc\").project(\"x-ray-baggage\")\nversion = project.version(3)\ndataset = version.download(\"yolov8\")\n```\n\n매개변수와 파일 경로를 지정한 다음, 모델 훈련을 시작하세요.\n\n<div class=\"content-ad\"></div>\n\n\n!yolo task=detect mode=train epochs=25 batch=32 plots=True \\\nmodel='/content/-q/yolov10n.pt' \\\ndata='/content/X-Ray-Baggage-3/data.yaml'\n\n\n예시 data.yaml 파일\n\n\nnames:\n- Gun\n- Knife\n- Pliers\n- Scissors\n- Wrench\n\nnc: 5\n\nroboflow:\n  license: CC BY 4.0\n  project: x-ray-baggage\n  url: https://universe.roboflow.com/vladutc/x-ray-baggage/dataset/3\n  version: 3\n  workspace: vladutc\n\ntest: /content/X-Ray-Baggage-3/test/images\ntrain: /content/X-Ray-Baggage-3/train/images\nval: /content/X-Ray-Baggage-3/valid/images\n\n\n결과를 살펴봅시다.\n \n\n<div class=\"content-ad\"></div>\n\nmd\n![Training results](/content/runs/detect/train/results.png){width=1000}\n\n![Prediction results](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_5.png)\n\n테스트 데이터를 예측하고 결과를 5x2 그리드로 표시합니다.\n\n```python\nfrom ultralytics import YOLOv10\n\nmodel_path = '/content/runs/detect/train/weights/best.pt'\nmodel = YOLOv10(model_path)\nresults = model(source='/content/X-Ray-Baggage-3/test/images', conf=0.25, save=True)\n```\n\n\n<div class=\"content-ad\"></div>\n\n```js\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimages = glob.glob('/content/runs/detect/predict/*.jpg')\n\nimages_to_display = images[:10]\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 10))\n\nfor i, ax in enumerate(axes.flat):\n    if i < len(images_to_display):\n        img = mpimg.imread(images_to_display[i])\n        ax.imshow(img)\n        ax.axis('off')  \n    else:\n        ax.axis('off')  \n\nplt.tight_layout()\nplt.show()\n```\n\n<img src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_6.png\" />\n\n## 결론 및 권장 사항\n\n- 이 글을 작성하는 동안 여러 데이터셋에서 YOLOv10n 모델을 학습하여 Colab의 15GB 무료 T4 GPU 한도를 고갈시켰습니다. Colab 환경에서 모델을 학습할 때 한도를 초과하면 T4 GPU에 제한이 있습니다. 이 문제를 해결하기 위해 다른 구글 계정으로 로그인할 수 있습니다.\n- 기술이 빠르게 발전함에 따라 컴퓨터 비전과 대형 언어 모델 양쪽에서 단일 기술에 갇히지 않고 주요 개념을 배우는 것이 유익하다고 생각됩니다. 이를 적응하기 위해 이러한 기술의 개발자들로부터 배우는 것이 도움이 됩니다. Ultralytics와 Roboflow의 콘텐츠는 이 분야에서 매우 가치 있으며, 그들을 팔로우하는 것이 바람직합니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 참고 자료\n\n- 공식 레포: https://github.com/THU-MIG/yolov10\n- 울트라리틱스 (Ultralytics)\n- 로보플로우 (Roboflow)\n\n```js\n@article{THU-MIGyolov10,\n  title={YOLOv10: 실시간 엔드 투 엔드 객체 검출},\n  author={Ao Wang, Hui Chen, Lihao Liu 등},\n  journal={arXiv 사전 인쇄 arXiv:2405.14458},\n  year={2024},\n  institution={Tsinghua University},\n  license={AGPL-3.0}\n}\n```\n\n```js\n@misc{\nx-ray-baggage_dataset,\ntitle={X-레이 수하물 데이터셋},\ntype={오픈 소스 데이터셋},\nauthor={vladutc},\nhowpublished={\\url{ https://universe.roboflow.com/vladutc/x-ray-baggage }},\nurl={https://universe.roboflow.com/vladutc/x-ray-baggage},\njournal={Roboflow Universe},\npublisher={Roboflow},\nyear={2024},\nmonth={5},\nnote={방문일: 2024년 5월 26일},\n}\n```\n\n<div class=\"content-ad\"></div>\n\n저는 청화 대학교의 연구원들, Ultralytics와 Roboflow 팀, 그리고 오픈 소스 커뮤니티의 모든 기여자들에게 감사드립니다.","ogImage":{"url":"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png"},"coverImage":"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png","tag":["Tech"],"readingTime":6},{"title":"Nodejs 19가 출시, 새로운 기능 및 내용 정리","description":"","date":"2024-05-27 18:29","slug":"2024-05-27-Nodejs19isOutHerearetheNewUpdates","content":"\n\n## 노드 v19의 새로운 기능을 강조한 릴리스 노트의 친숙한 버전\n\n![Node v19](/assets/img/2024-05-27-Nodejs19isOutHerearetheNewUpdates_0.png)\n\n보통 런타임의 새 버전을 위한 릴리스 노트가 나오면, 사용자들로서는 꽤 내부적이고 투명한 부분들에 대한 업데이트가 많이 포함되어 있습니다.\n\n그러나 Node.js 19의 릴리스 노트에서는 세부적으로 살펴보지 않는다면 발표 공지에서 놓칠 수 있는 흥미로운 정보가 몇 가지 있습니다.\n\n<div class=\"content-ad\"></div>\n\n그럼 이제 우리가 가장 기대되는 것들을 살펴보겠습니다. 우리가 좋아하는 JavaScript 런타임의 버전 19에서 발표된 변경 사항 중에서 어떤 것들이 있는지요?\n\n# 이제 WATCH 플래그가 있습니다\n\n비록 실험적인 모드에 있지만, 적어도 워치 플래그의 초석이 있다고 말할 수 있습니다. 이것이 의미하는 바가 무엇인가요? Node가 Deno 팀에서 몇 가지 아이디어를 천천히 받아들이고 있는 것일까요?\n\nDeno가 CLI를 설계할 때 Node와 다른 접근 방식을 취했습니다. 그들의 사용자들에게 스크립트를 실행하는 간소화된 방법을 제공하는 대신에, Deno 팀은 모든 필요한 것을 단일 실행 파일로 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n이는 파일 감시자, 테스트 러너, 코드 포멧터 등을 의미합니다. 지난 10년간 노드가 수행해온 것과는 확실히 다른 접근 방식이지만 작동하는 것으로 보입니다. Deno 개발자들과 대화를 나눈 후, 이 도구들이 한 곳에서 제공된다는 매력을 느낄 수 있었습니다.\n\n그러니까, 이것이 자체 CLI 도구로 커뮤니티에 의해 개발된 다양한 도구들을 통합하는 과정에서 한 발자국 중 하나라고 안전하게 말할 수 있을까요?\n\n확실한 것은 말할 수 없습니다. 비슷한 접근 방식을 살펴볼 수 있지만, 한 번에 한 번 일어날 수도 있습니다.\n\n특히 이 플래그는 흥미롭습니다. 기본적으로 진입점 파일과 필요한 또는 가져온 종속성을 모두 감시합니다. 특정 폴더에서 변경 사항을 감시하려면(구성 파일 변경 사항을 감시하는 것과 유사한지 상정합니다), 실험 단계에 있는 watch-path 플래그를 명시해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n지금은 \"감시 모드\"에서 변경이 감지되면 Node.js 프로세스가 다시 시작될 뿐, 할 일이 많지 않아요.\n\n제가 잘못 이해시킨게 아니에요. 이미 많은 일을 하는 것이긴 하지만, 이벤트에 연결하여 추가적인 흥미로운 작업을 수행할 수 있다면 더 좋을 것 같아요.\n\n그래도 아쉽네요. 아마 다음 버전에는 가능할지도 몰라요!\n\n# 사용자 정의 ESM 해상도 조정 (이것을 빠트리지 마세요!)\n\n<div class=\"content-ad\"></div>\n\n이 제목을 보고 나는 처음에 그냥 내부 플래그가 제거된 것 같다고 생각해서 건너뛰었어요. 정말 별로 관심이 없었거든요.\n\n그러다가 몇 가지 링크를 따라가서 이 변화가 어떤 것인지 읽어봤어요.\n\n사실 이름이 Node.js 핵심 개발자가 아니라면 약간 암호적일 수도 있지만, 기능은 그렇지 않아요. Node에서 파일을 이렇게 require 해본 적이 몇 번이나 있었나요?\n\n```javascript\nconst myPkg = require(\"./folder/file\")\n```\n\n<div class=\"content-ad\"></div>\n\n폴더에 확장자가 없다는 점에 주목하세요. 노드는 포기하기 전에 여러 가지 대안을 찾기 때문에 확장자를 생략한 것입니다.\n\n또는 이렇게 폴더를 직접 참조할 수도 있어요:\n\n```javascript\nconst myPkg = require(\"./my-folder\")\n```\n\n이 폴더 안에 index.js 파일이 있다면 동작할 거예요. 노드가 자동으로 해당 파일을 찾아줄 거에요.\n\n<div class=\"content-ad\"></div>\n\n하지만 동일한 패키지를 가져오려고 시도하면 그렇지 않습니다. 노드의 \"ESM specifier resolution\"은 해당 추가 동작을 지원하지 않습니다. 올바른 확장명을 직접 지정하지 않는다면 파일을 찾을 수 없습니다.\n\n--experimental-specifier-resolution=node 플래그를 사용하여 CommonJS 동작을 모방할 수 있습니다. 하지만 그렇다고 해도 이것은 \"실험적\"인 단어가 들어간 긴 지저분한 플래그입니다. 사용자들에게 이것이 실행해도 안전하다고 설득하는 데 행운을 빕니다!\n\n다행히도 이제 Node.js 19에서는 이 문제가 더 이상 발생하지 않습니다. 새로운 로더로 이와 같은 작업을 수행할 수 있습니다:\n\n\nimport file from `./file` // \"file\"에 올바른 확장명이 있으면 동작합니다\n\n\n<div class=\"content-ad\"></div>\n\n아니요\n\nimport myFile from `./폴더` //만약 \"폴더\" 안에 \"index.js\"가 있다면\n\n보셨나요? 이 업데이트의 이름 때문에 다른 걸로 생각할 수도 있겠지만, 실제로는 매우 좋고 환영받는 개선 사항이죠.\n\n읽은 내용이 마음에 드셨나요? IT 산업에서 2 10년 간의 지혜를 모두와 공유하는 무료 뉴스레터를 구독해 보세요. “늙은 개발자의 혼잣말”에 가입해 보세요!\n\n<div class=\"content-ad\"></div>\n\n# 우리는 V8의 새 버전을 사용 중입니다\n\n우리는 10.2 버전에서 10.7 버전으로 넘어갔어요\n\n누가 관심 있겠어요?\n\n하지만 당신도 모를까봐요! 이번에 이루어진 이 업데이트는 Node.js가 런타임의 최신 릴리스를 사용하도록 유지시키고 우리가 뒤쳐지지 않게 하는 것 외에도, 특정 버전이 ECMAScript의 Stage 3 제안으로부터 새로운 업데이트를 소개하게 됐다는 사실이요: Intl.numberFormat API의 업데이트입니다.\n\n<div class=\"content-ad\"></div>\n\n언어의 새로운 기능을 사용해볼 때 항상 흥분되는데요, 특히 프로젝트에서 숫자를 다루는 경우에는 이 기능이 매우 흥미로울 거에요.\n\n기존 메소드들의 정밀도가 향상되고 새로운 형식 옵션이 추가되는 등 여러 가지 면에서 이 기능은 매우 흥미로운 것들을 제공해줍니다. 아직은 공식적인 3단계 단계에 있지만, 이 제안은 이미 Node의 최신 버전을 이용하여 시도하고 테스트할 수 있으니 한 번 시도해보세요!\n\n# DTrace/SystemTap/ETW 지원이 사라지고 있어요\n\n알지 못하셨다면, DTrace, SystemTap 및 ETW는 각각 다른 OS에서 작동하는 프로파일링 도구이며, Node.js 팀 내부에서 런타임이 모두와 호환되도록 유지되고 작동할 수 있도록 노력했습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 이 외부 도구를 사용하여 프로필을 만들고 Node.js 기반 코드가 다른 상황에서 어떻게 실행되는지 이해할 수 있게 되었습니다.\n\n하지만 발표에 따르면, 이러한 도구를 유지하고 업데이트하는 노력은 노드 사용자들에게 가져다 주는 이익 대비 너무 컸던 것 같아요.\n\n그래서 이 도구들은 더 이상 지원되지 않게 되었습니다.\n\n과거에 이를 활용하셨나요? 이제는 어떻게 할 건가요?\n\n<div class=\"content-ad\"></div>\n\n다른 업데이트도 공지를 확인하면 찾을 수 있어요, 그런데 솔직히 이것들이 제 눈길을 사로잡았어요. 특히 조금 더 파헤쳐봐야 이해할 수 있는 부분들이요.\n\nNode.js 19의 변경 사항에 대해 기대되시나요? 어떻게 당신에게 영향을 미치는지 댓글로 남겨주세요! 다른 분들이 Node를 어떻게 사용하는지 읽어보는 건 항상 기쁘답니다!\n\n# 레고처럼 재사용 가능한 컴포넌트로 앱을 제작하세요\n\n![Node.js 19 업데이트 이미지](/assets/img/2024-05-27-Nodejs19isOutHerearetheNewUpdates_1.png)\n\n<div class=\"content-ad\"></div>\n\nBit의 오픈 소스 도구는 25만 명 이상의 개발자들이 구성 요소를 활용하여 앱을 구축할 수 있도록 도와줍니다.\n\n모든 UI, 기능 또는 페이지를 재사용 가능한 구성 요소로 변환하고 애플리케이션 전체에 공유할 수 있습니다. 협업하고 더 빠르게 개발하는 것이 더 쉬워집니다.\n\n→ 더 알아보기\n\n앱을 구성 요소로 분할하여 앱 개발을 더 쉽게 만들고 원하는 작업 흐름에 대한 최상의 경험을 누릴 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n## → 마이크로 프론트엔드\n\n## → 디자인 시스템\n\n## → 코드 공유 및 재사용\n\n## → 모노 레포(repository)\n\n<div class=\"content-ad\"></div>\n\n# 더 알아보기","ogImage":{"url":"/assets/img/2024-05-27-Nodejs19isOutHerearetheNewUpdates_0.png"},"coverImage":"/assets/img/2024-05-27-Nodejs19isOutHerearetheNewUpdates_0.png","tag":["Tech"],"readingTime":5},{"title":"Nodejs를 활용한 마이크로서비스 만드는 방법","description":"","date":"2024-05-27 18:27","slug":"2024-05-27-LeveragingMicroserviceswithNodejs","content":"\n\n## 궁극적인 확장성과 민첩성의 해제\n\n마이크로서비스는 거대한 앱을 독립적인 서비스로 분할합니다. Node.js와 함께 사용하면 다음과 같은 이점을 얻을 수 있습니다:\n\n- 개별 서비스의 확장\n- 각 서비스에 최적의 기술 스택 사용\n- 내구성을 위해 장애 격리\n- 병렬 개발을 통한 빠른 전달\n- 쉬운 유지보수 및 업그레이드\n\n## 소개\n\n<div class=\"content-ad\"></div>\n\n이른바 빠르게 변화하는 디지털 세상에서는 기업들이 시장 상황에 신속히 대응하고 대규모 트래픽 증가를 처리하며 새로운 기능을 매끄럽게 통합할 수 있는 애플리케이션을 요구합니다. 마이크로서비스는 개발자들이 이러한 유연하고 확장 가능한 시스템을 구축할 수 있도록 함께 부상한 혁명적인 아키텍처 패러다임입니다. Node.js의 효율성과 성능과 결합되면, 마이크로서비스는 새로운 수준의 잠재력을 발휘합니다. 이 블로그 포스트는 마이크로서비스의 기본 원칙, 혁신적인 이점 및 Node.js에서의 구현을 위한 모베스트 프랙티스를 안내해 드릴 것입니다.\n\n## 마이크로서비스란?\n\n전통적인 단일체 응용 프로그램을 거대하고 연결된 구조로 상상해보십시오. 여기에서 각 구성 요소는 꽉 결합되어 있고 서로 의존적입니다. 애플리케이션의 일부를 변경하거나 특정 부분을 확장하는 것은 어려운 일이 됩니다. 심지어 미세한 조정이라도 전체 시스템 전반에 파급 효과를 일으킬 수 있습니다. 마이크로서비스는 애플리케이션을 더 작고 독립적인 서비스로 분해하여 각각이 특정 비즈니스 기능을 담당하는 혁신적인 방식을 제시합니다.\n\n모든 고객을 위해 특정 중앙 주방에서 서비스되는 거대한 레스토랑을 상상해보십시오. 마이크로서비스 아키텍처에서는 레스토랑이 각각이 특정 요리나 음식에 전념한 여러 전문화된 주방을 가지고 있습니다. 이탈리아 주방은 모든 파스타와 피자 주문을 처리하고, 스시 주방은 신선한 초밥 롤을 준비하는 데만 전념합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-27-LeveragingMicroserviceswithNodejs_0.png)\n\n이 모듈식 접근 방식은 효율적인 자원 할당, 빠른 서비스 및 쉬운 유지 보수를 가능하게 합니다. 각 주방이 독립적으로 작동하고 수요에 따라 확장할 수 있기 때문입니다.\n\n## 마이크로서비스의 변화를 가져다주는 혜택들\n\n- 비교할 수 없는 확장성: 마이크로서비스를 사용하면 수요에 따라 개별 서비스를 독립적으로 확장할 수 있습니다. 하나의 서비스에 트래픽이 급증하는 경우, 해당 서비스에 더 많은 자원을 할당할 수 있습니다. 예를 들어 전자 상거래 애플리케이션에서 대규모 할인 행사 중 주문 처리 서비스가 많은 주문을 받을 수 있습니다. 마이크로서비스를 사용하면 개별 서비스를 효율적으로 확장하여 늘어난 부하를 처리할 수 있으며, 원활한 운영과 최적의 성능을 보장하면서 비용을 절감할 수 있습니다.\n- 유연성과 기술 다양성: 각 서비스는 가장 적합한 기술 스택을 사용하여 개발할 수 있으므로 다른 언어, 프레임워크 및 데이터베이스의 장점을 활용할 수 있습니다. 예를 들어 고성능 API 서비스에 Node.js를 사용하고, 기계 학습 서비스에 Python을 사용하고, 유연한 데이터 저장소가 필요한 서비스에는 NoSQL 데이터베이스를 사용할 수 있습니다. 적절한 도구를 선택할 수 있는 이 자유는 개발자가 고품질 맞춤형 솔루션을 제공할 수 있도록 도와줍니다.\n- 개선된 오류 격리 및 복원력: 단일 지점의 장애가 전체 시스템을 다운시킬 수 있는 모놀리식 애플리케이션과 달리, 마이크로서비스를 사용하면 서비스 하나가 실패해도 전체 애플리케이션에 미치지 않습니다. 이 격리는 전체 시스템의 건강과 복원력을 유지하는 데 도움이 됩니다. 예를 들어 결제 처리 서비스에 문제가 발생하는 상황을 상상해보세요. 마이크로서비스를 사용하면 고객은 여전히 제품 카탈로그를 찾아보고 주문을 할 수 있으며, 결제 서비스는 별도로 처리됩니다.\n- 빠른 시장 진입: 마이크로서비스를 통해 팀이 각기 다른 서비스에 동시에 작업할 수 있어 개발 주기를 단축하고 빠른 릴리스를 가능하게 합니다. 예를 들어 한 팀은 케이터링 서비스에 대한 새로운 기능을 개발하고, 다른 팀은 제품 카탈로그 서비스를 개선함으로써 병렬 개발이 가능하며, 새로운 기능을 최종 사용자에게 빠르게 제공할 수 있습니다.\n- 쉬운 유지 보수 및 업그레이드: 작고 모듈식 코드베이스를 사용하면 개별 서비스의 유지 보수 및 업그레이드가 훨씬 쉬워집니다. 프레임워크나 언어의 새 버전으로 서비스를 리팩토링하거나 이주할 수 있으며 전체 애플리케이션을 방해하지 않고 진행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## Node.js에서 마이크로서비스 구현하기\n\n마이크로서비스는 많은 장점을 제공하지만 효과적으로 구현하려면 신중한 고려와 모베스트 프랙티스 준수가 필요합니다. Node.js는 이벤트 기반, 논블로킹 I/O 모델과 가벼우며 효율적인 런타임으로, 마이크로서비스 구축에 탁월한 선택입니다. Node.js에서 마이크로서비스를 구현할 때 주의할 점 몇 가지를 살펴보겠습니다:\n\n- 서비스 디자인: 비즈니스 기능을 중심으로 서비스를 디자인하여 각 서비스가 단일 책임과 명확한 인터페이스를 갖도록 합니다. 이 관심사 분리 원칙은 모듈성, 테스트 가능성 및 유지보수성을 촉진합니다.\n- 통신: 마이크로서비스는 네트워크를 통해 서로 통신하며, 종종 HTTP/REST와 같은 가벼운 통신 프로토콜을 사용합니다. 비동기 통신 시나리오의 경우 RabbitMQ, Apache Kafka 또는 BullMq와 같은 메시지 브로커를 사용하여 서비스를 분리하고 이벤트 기반 아키텍처를 처리하는 것을 고려해보세요.\n- 배포: Docker와 같은 컨테이너화 기술은 마이크로서비스를 배포하는 데 필수적입니다. Docker 컨테이너는 각 서비스와 해당 종속성을 자체 포함된 단위로 패키징하여 서로 다른 환경에서 서비스를 배포하고 관리하기 쉽게 만듭니다.\n- 모니터링 및 로깅: 분산 시스템에서 여러 서비스가 실행되는 경우 전체 응용 프로그램의 건강 상태와 성능을 이해하기 위해 모니터링 및 로깅이 중요해집니다. Prometheus 및 Grafana 또는 Newrelic과 같은 모니터링 솔루션과 함께 Cloudwatch 로깅을 구현하여 생태계에 대한 더 많은 통찰력을 얻을 수 있습니다.\n- 보안: 서비스 수준에서 인증 및 권한 부여를 구현하여 서비스 간 안전한 통신을 보장합니다. npm 패키지를 사용하여 요청 제한, 요청 유효성 검사 및 액세스 제어와 같은 일반적인 보안 문제를 처리하세요.\n\n## 최상의 실천 방법\n\n<div class=\"content-ad\"></div>\n\n- 탈중앙화된 데이터 관리: 각 마이크로서비스는 자체 데이터베이스를 관리해야 하며, 단일 장애 지점을 피하고 서비스가 완전히 독립적이 될 수 있도록 해야 합니다. 이 \"서비스 당 데이터베이스\" 패턴은 데이터 격리와 확장성을 촉진합니다.\n- 버전 관리: API 버전 관리를 구현하여 기존 클라이언트에 영향을 주지 않고 변화와 업데이트를 처리할 수 있습니다. 이를 통해 역호환성을 유지하고 새로운 기능을 도입하거나 서비스를 리팩토링할 때 원활한 전환을 가능하게 합니다.\n- 자동화된 테스트: 각 서비스에는 단위, 통합 및 종단간 테스트를 포함한 포괄적인 자동화된 테스트 스위트가 있어야 합니다. 이는 개발 주기 초기에 문제를 발견하고 개별 서비스 및 전체 시스템의 신뢰성과 품질을 보장하는 데 도움이 됩니다.\n- 지속적 통합/지속적 배포 (CI/CD): CI/CD 파이프라인을 도입하여 테스트, 빌드 및 배포 프로세스를 자동화하고 일관된 신뢰할 수 있는 릴리스를 보장해야 합니다. 젠킨스, AWS 파이프라인과 같은 도구를 활용하여 이러한 프로세스를 간소화하고 빠른 이터레이션을 가능하게 할 수 있습니다.\n\n## 결론\n\n마이크로서비스는 응용 프로그램을 구축하고 확장하는 방식을 혁신적으로 바꿨으며, 우수한 유연성, 확장성 및 견고성을 제공합니다. 이러한 아키텍처 접근 방식이 Node.js의 효율성과 성능과 결합될 때, 이는 혁신과 민첩성에 새로운 차원을 열어줍니다.\n\n기본 개념, 이점 및 모범 사례를 이해하면 마이크로서비스의 잠재력을 활용하여 비즈니스 성공을 이끄는 견고하고 확장 가능하며 유지보수 가능한 응용 프로그램을 제공하는 데 잘 준비될 수 있습니다.","ogImage":{"url":"/assets/img/2024-05-27-LeveragingMicroserviceswithNodejs_0.png"},"coverImage":"/assets/img/2024-05-27-LeveragingMicroserviceswithNodejs_0.png","tag":["Tech"],"readingTime":5},{"title":"모든 레벨의 개발자를 위한 필수 Nodejs 가이드","description":"","date":"2024-05-27 18:25","slug":"2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels","content":"\n\n![Node.js Guide](/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_0.png)\n\n안녕하세요, Node.js 팬 여러분! 당신이 전문 개발자이건, JavaScript 백엔드에 막 입문한 사람이건, 이 블로그는 실제 Node.js 앱을 만들기에 완벽한 장소입니다.\n\n우리는 애플리케이션 아키텍처 및 코딩을 위한 권장 사항을 살펴보고, 앱을 성능적이고 안전하게 만들기 위한 몇 가지 추가 제안도 제공할 것입니다.\n\nNode.js 게임을 한 단계 업시키기 위해 준비하세요!\n\n<div class=\"content-ad\"></div>\n\n\n![image1](/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_1.png)\n\n[Gmail](mailto:your.email@gmail.com) | [LinkedIn](https://www.linkedin.com/in/yourprofile)\n\n## Why These Practices Matter\n\n![image2](/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n환영합니다!\n\nNode.js는 비동기 입출력(I/O)-바운드 작업(예: 데이터베이스 상호 작용 및 네트워크 요청과 유사)을 뛰어난 성능으로 수행합니다. 이는 논블로킹 I/O 모델과 이벤트 루프 덕분입니다.\n\n일반적인 멀티스레드 디자인과 대조적으로, Node.js는 병목 현상을 초래할 수 있는 여러 스레드 디자인과 달리 여러 요청을 동시에 처리하면서 성능이 떨어지지 않습니다.\n\n모듈화 디자인과 같은 특정 방법이 왜 Node.js에 유익한지 이해하는 것은 Node.js의 기능을 활용하는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n# 애플리케이션 아키텍처 모범 사례\n\n![이미지](/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_3.png)\n\n자, 이제 멋진 것을 만들어 봅시다! 다음은 염두에 둘 아키텍처적인 모범 사례입니다:\n\n## 모듈식 디자인\n\n<div class=\"content-ad\"></div>\n\n애플리케이션을 작은, 재사용 가능한 구성 요소로 나눠보세요.\n\n이렇게 하면 코드가 더 이해하기 쉽고 유지보수 및 테스트가 용이해집니다.\n\n작은, 집중된 부분이 서로 교차하여 복잡한 구조물을 만들어내는 레고 블록을 사용해보세요.\n\n모듈은 레고와 유사합니다. 각 모듈은 단일 작업을 수행하고 잘 정의된 기능을 가져야 합니다.\n\n<div class=\"content-ad\"></div>\n\n이것을 통해 기능을 분리하고 응용 프로그램 전체에서 코드를 재사용할 수 있어요. 이렇게 함으로써 반복을 줄이고 유지 보수를 향상시킬 수 있어요.\n\n## 계층 구조\n\n프레젠테이션(앱의 인터페이스), 비즈니스 로직(핵심 기능) 및 데이터 액세스(데이터베이스와의 상호 작용)와 같은 계층을 사용하여 문제를 분리하세요.\n\n이렇게 하면 코드를 청소하고 복잡한 앱을 유지하기 쉽게 만들 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n주방(비즈니스 로직)과 식당(프레젠테이션)을 분리하면 일을 원할하게 처리할 수 있어요.\n\nMVC(Model-View-Controller)와 마이크로서비스 같은 인기 있는 패턴을 여기에 적용할 수 있어요.\n\nMVC는 애플리케이션을 모델(Model), 데이터를 나타내는 뷰(View), 데이터 표시 방식을 제어하는 컨트롤러(Controller) 세 개의 층으로 나눠요. 사용자 입력을 받고 모델과 뷰를 적절하게 조정하는 역할을 하는 거죠.\n\n마이크로서비스는 API를 사용하여 상호 통신하는 작은 독립적인 서비스로 애플리케이션을 나누는 개념이에요.\n\n<div class=\"content-ad\"></div>\n\n표 태그를 Markdown 형식으로 변경하겠습니다.\n\n## Dependency Injection\n\n이 방법은 크고 복잡한 응용 프로그램에 가장 적합하며 확장성을 촉진합니다.\n\n이 멋진 용어는 의존성(예: 데이터베이스 또는 외부 서비스)를 코드에 하드코딩하는 대신 추가하는 것을 의미합니다.\n\n이렇게 하면 테스트가 개선되고 느슨하게 연결된 코드를 유지하므로 개별 구성 요소의 쉬운 대체가 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n의존성 주입을 사용하면 다른 데이터베이스로 쉽게 전환할 수 있어요.\n\nNode.js에서 의존성 주입을 구현하는 다양한 전략이 있지만, 일반적인 전략 중 하나는 의존성 주입 컨테이너를 사용하는 것입니다. 이 컨테이너는 의존성의 라이프사이클을 제어하고 필요할 때 코드에 주입합니다.\n\n## 이벤트 주도 아키텍처\n\n이벤트 이벤터와 메시지 큐를 사용하여 앱 내에서와 외부 서비스와의 비동기 통신을 제공하세요.\n\n<div class=\"content-ad\"></div>\n\n프로그램의 각 구성 요소가 효율적으로 이벤트에 응답하고 서로 대기하지 않고 작동할 수 있도록 합니다.\n\n알림 시스템과 유사하게, 각 구성 요소는 이벤트에 가입하고 발생할 때 대응 조치를 취할 수 있습니다.\n\n이벤트 주도 아키텍처는 실시간 애플리케이션 개발 및 대규모 프로세스 관리에 매우 유용합니다.\n\n메시지 큐를 사용하여 시스템에 일시적으로 장애가 있더라도 이벤트가 일관되게 전달됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 코드 수준의 최상의 실천 방법\n\n지금 우리가 견고한 아키텍처를 갖고 있으므로, 깨끗하고 효율적인 코드를 개발하는 데 중점을 둘 수 있습니다:\n\n## 깨끗한 코드와 가독성\n\n깨끗하고 간단하며 잘 서식이 지정된 코드를 목표로 해보세요.\n\n<div class=\"content-ad\"></div>\n\n린터, 포매터 및 스타일 가이드를 사용하여 일관성을 보장하세요. 잘 쓰인 레시피는 읽고 따르기 쉽습니다.\n\n명확한 변수와 함수 이름이 있는 잘 서식이 맞춰진 코드는 미래에 그것을 작업해야 할 수도 있는 다른 개발자들과 여러분 모두가 이해하기 쉽게 만들어 줍니다.\n\n린터와 포매터는 코드 표준을 유지하면서 프로세스를 자동화하는 데 도움이 될 수 있습니다.\n\n## 비동기 프로그래밍\n\n<div class=\"content-ad\"></div>\n\nPromises나 async/await을 사용하여 비동기 프로그래밍을 마스터하세요.\n\n이 작업을 통해 Node.js에서 I/O 작업을 이벤트 루프를 막지 않고 처리할 수 있습니다.\n\n이것을 공을 던지면서 생각해보세요: 여러 작업을 함께 완료할 수 있고 아무것도 떨어뜨리지 않을 수 있습니다! 비동기 작업에는 데이터베이스 호출, 네트워크 요청 및 파일 I/O가 포함됩니다.\n\nPromises와 async/await을 사용하면 이러한 작업의 비동기적인 성격을 처리하면서 코드를 깔끔하고 가독성 있게 유지할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 오류 처리 및 로깅\n\n효과적인 오류 처리 구조 및 로깅 접근 방식을 구현하세요.\n\n이것은 애플리케이션 성능을 디버깅하고 모니터링하는 데 중요합니다.\n\n로그 및 오류 메시지는 앱의 문제를 경고하는 엔진 라이트 버전입니다.\n\n<div class=\"content-ad\"></div>\n\n적절한 오류 처리는 부드럽게 오류를 잡아내고, 디버깅을 위한 필수 정보를 문서화하며 사용자에게 유용한 오류 메시지를 제공하는 것을 의미합니다.\n\n# 코드 예제: 최상의 관행을 실천으로\n\n우리는 많은 것을 다루었지만, 이제 이러한 최상의 관행들이 코드에 어떻게 적용되는지 살펴봅시다. 기본 개념을 보여주는 몇 가지 실제 예제가 여기 있습니다:\n\n## 모듈화된 디자인:\n\n<div class=\"content-ad\"></div>\n\n```js\n// user.service.js\nfunction getUserById(id) {\n  // 데이터베이스에서 사용자 데이터를 가져오는 로직\n}\n\nfunction updateUser(user) {\n  // 데이터베이스에서 사용자 데이터를 업데이트하는 로직\n}\n\nmodule.exports = {\n  getUserById,\n  updateUser,\n};\n```\n\n위 코드는 사용자 데이터를 가져오고 업데이트하는 여러 방법을 제공하는 사용자 서비스 모듈입니다. 이는 재사용성을 향상시키고 코드를 보다 쉽게 읽고 유지보수할 수 있도록 만듭니다.\n\n## 의존성 주입(Dependency Injection):\n\n```js\n// database.js\nclass Database {\n  constructor(config) {\n    // config를 사용하여 데이터베이스에 연결하는 로직\n  }\n\n  getUserById(id) {\n    // 사용자를 위한 데이터베이스 쿼리 로직\n  }\n}\n\n// user.service.js (의존성 주입 사용)\nfunction __getUserById(database, id) {\n  // 데이터베이스 인스턴스를 사용하여 사용자 데이터를 가져오는 로직\n}\n\nconst userService = {\n  getUserById: __getUserById.bind(null, new Database(config)), // 데이터베이스 의존성 주입\n};\n\nmodule.exports = userService;\n```\n\n<div class=\"content-ad\"></div>\n\nuserService는 데이터베이스와 직접 상호 작용하지 않습니다. 대신, 의존성으로 데이터베이스 인스턴스를 수신합니다.\n\n이를 통해 더 쉬운 테스트가 가능해지고 서비스를 더 유연하게 만들 수 있습니다 - 핵심 로직을 수정하지 않고 데이터베이스 구현을 교체할 수 있습니다.\n\n# 성능 최적화 팁\n\n## 캐싱\n\n<div class=\"content-ad\"></div>\n\n데이터베이스 호출 및 API 요청 수를 줄이기 위해 캐싱 전략을 구현해보세요.\n\n자주 액세스되는 데이터의 성능을 크게 향상시킬 수 있습니다.\n\n잘 갖춘 식료품 저장실을 상상해보세요 — 재료가 필요할 때마다 식료품점에 가실 필요가 없습니다! Node.js에서 자주 사용되는 캐싱 전략에는 인메모리 캐싱과 브라우저 캐싱 메커니즘을 사용한 클라이언트 측 캐싱이 포함됩니다.\n\n## I/O 작업 최소화\n\n<div class=\"content-ad\"></div>\n\n애플리케이션이 수행하는 I/O 작업 횟수를 줄이세요.\n\n데이터베이스 호출, 파일 I/O 및 네트워크 요청이 여기에 포함됩니다.\n\n이들은 비동기적이지만, 너무 많은 호출은 여전히 이벤트 루프에 부담을 줄 수 있습니다.\n\n너무 많은 공을 토스하듯이 생각해보세요 — 추적하기 어려워집니다! I/O 작업을 최적화하면 이벤트 루프의 효율성을 유지하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 효율적인 이벤트 루프 활용\n\n이벤트 루프를 사용하는 방법에 유의하세요.\n\n콜백 내에서 긴 실행 시간이 소요되는 작업을 피하십시오. 해당 작업은 이벤트 루프를 차단하고 다른 요청이 처리되는 것을 방해할 수 있습니다.\n\n계산 집약적인 작업에는 워커 스레드와 같은 기술을 활용하십시오.\n\n<div class=\"content-ad\"></div>\n\n따로 할당된 어시스턴트가 있다면 좋겠죠. 이렇게 하면 주 이벤트 루프가 다른 요청을 처리할 수 있게 해줍니다.\n\n성능을 높이려면 프로파일링 도구를 사용하여 프로그램의 오류를 발견해 보세요.\n\n이 도구들은 코드가 대부분의 시간을 보내는 곳을 파악하고 개선할 위치를 찾는 데 도움이 될 것입니다.\n\n# 보안 팁\n\n<div class=\"content-ad\"></div>\n\n보안을 잊지 마세요! Node.js 어플리케이션은 많은 위협에 노출될 수 있으므로 전략을 세워보세요:\n\n- 알려진 보안 취약점을 해결하기 위해 주기적으로 종속성을 업데이트하세요.\n- SQL 인젝션 및 XSS와 같은 인젝션 위협에 대비하려면 사용자 입력을 살균하세요.\n- 민감한 데이터 접근을 제한하기 위해 강력한 인증 및 권한 부여 기술을 사용하세요.\n\nNode.js 보안에 대해 더 많이 배울 수 있는 온라인 자료가 여러 개 있습니다.\n\n이런 제안을 따르면 보통의 보안 위험을 줄이고 더 안전한 어플리케이션을 만들 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n# 확장성 전략\n\n프로젝트가 성장할수록 확장성은 중요한 요소입니다. 여기에 몇 가지 접근 방식이 있습니다:\n\n## 클러스터링\n\n노드 JS 어플리케이션을 여러 서버에 다른 인스턴스로 실행하여 작업 부하를 분산시킵니다. 이렇게 하면 보다 많은 동시 요청을 처리할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n요리사 팀이 함께 일한다고 상상해보세요 — 그들은 더 적은 시간에 더 많은 식사를 준비할 수 있어요!\n\n## 부하 분산\n\n단일 서버에 과부하를 피하기 위해 들어오는 트래픽을 여러 응용 프로그램 인스턴스로 분산합니다.\n\n이는 교통 안내원과 유사하게 작동하여 요청을 가장 이용 가능한 서버로 라우팅하여 원활한 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n이것은 Node.js 앱을 확장하는 몇 가지 기본적인 방법 중 일부에 불과해요.\n\n선택하는 특정 방법은 귀하의 애플리케이션의 기능 및 트래픽 패턴에 의존합니다.\n\n## 마지막으로\n\n이 블로그에서 소개된 모범 사례를 따르면 사용자 베이스가 확대됨에 따라 계속 유지될 강력하고 확장 가능하며 안전한 Node.js 앱을 만들 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n웹 개발의 다양한 측면에 대해 더 자세히 다룰 향후 블로그를 기대해 주세요!\n\n![image 1](/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_4.png)\n\n![image 2](/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_5.png)\n\n# 간단하게 설명하기 🚀\n\n<div class=\"content-ad\"></div>\n\n인 플레인 영어 커뮤니티에 참여해 주셔서 감사합니다! 떠나기 전에:\n\n- 작가를 클랩하고 팔로우해 주세요 ️👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- 알고리즘 콘텐츠와 거래하도록 강요하는 블로깅 플랫폼에 지쳤나요? Differ를 시도해보세요.\n- 더 많은 콘텐츠: PlainEnglish.io","ogImage":{"url":"/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_0.png"},"coverImage":"/assets/img/2024-05-27-TheEssentialNodejsGuideforDevelopersofAllLevels_0.png","tag":["Tech"],"readingTime":8},{"title":"Routing으로 나만의 파일 경로 기반 라우터 만들기","description":"","date":"2024-05-27 18:24","slug":"2024-05-27-DemystifyingRoutingCreatingYourOwnfile-path-basedRouter","content":"\n\n## 모든 현대 프레임워크에서 사용하는 기능, 왜 비밀로 유지해야합니까?\n\n![image](/assets/img/2024-05-27-DemystifyingRoutingCreatingYourOwnfile-path-basedRouter_0.png)\n\n대부분의 JavaScript 프레임워크가 채택하고 있는 한 가지 트렌드는 경로 기반 라우팅을 제공하는 것입니다. 이는 방문하려는 URL과 프로젝트 내의 특수 폴더 사이에 1:1 관계가 있다는 의미입니다. 이 특수 폴더에는 라우트 핸들러 함수들이 포함되어 있습니다.\n\n즉, http://`호스트`/users/list를 방문한다면 users/list.js 파일(또는 사용자의 선호에 따라 users/list/index.js 내부에 있을 수도 있음) 내에 핸들러 함수가 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이것은 정말 멋진 기능이에요. Next, Fresh 또는 다른 프레임워크와 같이 당연하게 사용하는 기능 중 하나에요.\n\n하지만, 저는 당연한 것을 싫어해요. 그래서 이 기능이 어떻게 작동하는지 역공학을 시도해 보겠어요.\n\nExpress를 사용하여 경로 기반 라우팅의 자체 버전을 구현하는 방법을 살펴보겠어요.\n\n# 이 작업을 하는 이유?\n\n<div class=\"content-ad\"></div>\n\n거의 모든 현대 프레임워크에 깊게 자리한 기능을 명확히 설명하는 데 있어서 이것은 개발 경험을 향상시키는 매우 좋은 기능입니다. 이 형식은 라우트 핸들러를 설정하는 방법을 단순화하며 이전에 사용했던 단일 라우트 매핑 파일과 같은 하위 최적화 방법 대신 새로운 방식을 제공합니다.\n\n이전에는 모든 라우트와 해당 핸들러 파일을 포함하는 단일 라우트 매핑 파일을 사용하여 해결책을 찾았습니다. 이런 식으로:\n\n그것은 작은 앱과 몇 개의 라우트만 있는 경우 좋은 해결책이었습니다. 심지어 라우트 핸들러를 다른 위치에 저장할 수 있는 유연성을 가지고도 했습니다. 그러나 반면에 대규모 기업 애플리케이션을 작업 중이라면 이 파일 내에 수백 줄에 달하는 코드를 다뤄야 할 수도 있습니다. 아마도 짐작할 수 있겠지만, 그런 파일을 유지하는 것은 아무도 원치 않았고 그것에 버그를 추가하는 것은 너무 쉬웠습니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 여기서 \"구성보다 규약\"을 사용하여 우리의 코드가 어떤 라우트를 처리하는지 정의하는 \"더 간단한\" 방법을 유지할 필요가 없게 되었어요. 이것은 또 다른 파일을 유지할 필요없이 코드의 어느 부분이 어떤 라우트를 처리하는지 정의하는 \"더 단순한 방법\"을 제공합니다. 이 부분을 유지하는 것이 승리라고 생각해요!\n\n저는 이러한 유형의 기능이 개발자 경험(DX)을 향상시키기 때문에 정말 좋아해요.\n\n물론, 이를 수행하는 것은 웹 프로젝트의 구조에 대해 많은 것을 결정할 수 있지만, 라우터를 직접 구축하는 경우에는 필요한 수정 사항을 정확히 필요한 대로 수행할 수도 있어요.\n\n그럼 이 구현이 어떻게 보이는지 살펴봅시다.\n\n<div class=\"content-ad\"></div>\n\n# 우리만의 라우터 구현하기\n\n이 예제에서는 ExpressJS를 사용할 것입니다. 하지만 ExpressJS의 기본 라우터를 사용하는 것은 말이 안 되기 때문에 사용하지 않을 것입니다. 대신에 웹 서버를 생성하는 과정을 좀 더 간단하게 추상화해주니까, 이것이 주된 목적은 아니지만 여러분들이 일을 더 쉽게 처리할 수 있게 해줄 겁니다.\n\n우리가 만들고자 하는 라우터는 다음과 같은 구조를 다룰 수 있도록 하는 것이 목표입니다:\n\n![라우터 구조](/assets/img/2024-05-27-DemystifyingRoutingCreatingYourOwnfile-path-basedRouter_1.png)\n\n<div class=\"content-ad\"></div>\n\n라우트 폴더의 내용을 확인해보세요:\n\n- 우리는 앱의 주요 라우트에 해당해야 할 index.js 파일이 있습니다.\n- 파일 이름을 사용하여 핸들러를 가질 수 있습니다. users.js는 /users로의 요청을 처리하고, books/index.js 파일은 /books로의 요청을 처리할 것입니다.\n- /books/addresses URL을 처리하는 books/addresses/index.js와 같이 더 깊게 중첩된 라우트도 있습니다.\n- 마지막으로, /books/[book].js 파일 덕분에 동적 라우트를 사용할 수도 있습니다. 해당 파일은 루트나 /books/addresses가 아닌 /books 내의 모든 경로를 처리할 것입니다.\n\n총평하자면 매우 완벽한 구조이며, 보다시피, 동적 라우트를 처리하는 유일한 복잡한 로직이 있고, 나머지는 상당히 간단합니다.\n\n## 파일 경로 기반 라우팅 추가하기\n\n<div class=\"content-ad\"></div>\n\n먼저 간단한 논리를 살펴봅시다: 동적 경로를 제외한 모든 것입니다.\n\nExpressJS를 설치한 후에는 all 메서드를 사용하여 catch-all 핸들러를 설정해 봅시다:\n\n이 코드는 모든 라우트를 캐치하고(all 메서드 덕분) 모든 HTTP 동사를 다루고(*/ 라우트 덕분) 있습니다.\n\n이 핸들러에 의해 요청이 캐치될 때마다 URL을 가져와 routes 폴더 안에 .js 확장자가 있는 파일이 있는지 확인합니다. 파일이 없으면 폴더로 가정하고 해당 폴더 안에 있는 index.js 파일을 찾습니다.\n\n<div class=\"content-ad\"></div>\n\n해결되었으니, executeRoute 함수를 호출하고 결과 값을 얻겠습니다. 만약 false이면, 파일을 찾을 수 없어 실행에 실패한 것으로 간주하겠습니다. 그래서 그 오류를 \"404 — 찾을 수 없음 응답\"으로 변환하겠습니다.\n\n이 퍼즐의 빠진 조각 executeRoute 함수는 다음과 같이 생겼습니다:\n\n동적 import 함수를 사용하고 있는데, 찾고 있는 파일이 존재한다면 계속 진행하고 요청에 사용된 HTTP 동사를 얻습니다. 이렇게 하면 catch-all 핸들러인 handler(네, 정말 좋은 이름!)를 정의하거나 동사의 이름을 메서드 이름으로 사용할 수 있습니다. 정의하면 코드가 대신 사용하겠죠.\n\n만약 import에 실패한다면(예외를 발생시킨다면) 그것은 가져올 파일이 없기 때문이며, 이는 해당 경로가 매핑되어 있지 않다는 의미입니다. 결과적으로 404 오류를 반환해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n믿든지 말든지, 파일 경로 라우팅을 달성하는 데 거의 필요한 것이 전부입니다.\n\n물론, 이전에 보여준 것처럼 동적 라우팅을 지원하려면 \"약간\"의 추가 코드가 필요합니다.\n\n# 동적 라우팅 지원 추가\n\n이제 URL에 따라 라우트 핸들러를 가져오는 방법을 알았으므로 executeRoute 함수가 false를 반환하는 시나리오에 대한 몇 가지 로직을 추가해야 합니다. 결국, 이는 라우트가 파일과 직접 매핑되지 않음을 의미하지만 \"와일드카드\" 파일을 지원하고자 하는 것이기도 합니다.\n\n<div class=\"content-ad\"></div>\n\n다음과 같은 로직을 추가하는 것이 좋습니다:\n\n- 직접 매핑된 파일이 없다면, 우선 동적 매개변수의 이름과 값을 추출합니다.\n- URL에서 이름을 제거하여 특수 파일이 있는 폴더를 이해합니다.\n- 동적 매개변수의 이름을 대괄호로 묶어 새 파일의 이름을 만듭니다.\n- 새 파일을 가져와서 executeRoute를 호출하려고 합니다.\n\n다음 예시를 상상해보세요:\n\n- 경로 /api/users/donald를 요청했습니다.\n- 파일 /routes/api/users/donald.js 또는 /routes/api/users/donald/index.js를 찾았지만 그 안에 아무 것도 없습니다.\n- 그래서 우리는 URL에서 \"donald\"를 제거하고, /routes/api/users 폴더 안에서 대괄호로 묶인 이름을 가진 파일을 찾아냅니다. 그리고 [user].js 파일을 찾습니다.\n- 이제 동적 매개변수가 \"user\"라는 것과 그 값이 \"donald\"인 것을 알게 되었습니다.\n- 이제 동적 파일을 가져와서 매개변수를 Request 객체에 추가하여 동적 핸들러가 그것을 사용할 수 있게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n상기는 우리 서버의 전체 코드입니다. 이미 알고 있는 부분은 무시하셔도 괜찮지만, 58번 줄에 추가한 false 절 내부의 논리를 살펴보세요. 이미 설명한 내용을 따르고 있습니다.\n\n파일 이름과 매개변수 이름의 결합을 “동적 핸들러”라고 부르며, URL에서 그것을 가져오는 함수를 작성했습니다.\n\n상기 코드 상단에 있는 getDynamicHandler 함수는 “특별” 파일이 있어야 하는 폴더를 탐색합니다. 그 폴더 안의 모든 파일을 읽고, 이름에 괄호가 있는 파일을 찾습니다. 루트 당 하나의 동적 핸들러만 갖는 것이 합리적이므로, 한 번 찾으면 더 이상 찾지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n정말 깔끔한 정규 표현식을 사용하면 파일 이름에서 대괄호로 둘러싸인 매개변수 이름도 캡처할 수 있어요.\n\n이제 매개변수의 이름과 핸들러 코드를 포함한 파일 이름을 모두 반환할 수 있어요. 기억해요, 매개변수의 실제 값은 URL에서 직접 가져와요.\n\n그게 바로 동적 라우터가 간단한 라우트, 다양한 HTTP 동사, 그리고 특별히 명명된 핸들러 파일을 사용하는 동적 라우트를 처리할 수 있게 한 거에요.\n\n이 모든 것이 100줄 미만의 단일 파일 안에 포함되어 있답니다.\n\n<div class=\"content-ad\"></div>\n\n물론, 제 비슷한 실험들과 마찬가지로, 이것은 학습용 연습입니다. 이 코드를 제품 환경으로 가져가려면 조금 정리하는 것과 아마도 동적 요청마다 디스크에서 읽는 것을 피하기 위해 캐싱을 추가하는 것을 권장합니다. 마지막으로 유닛 테스트를 추가하는 것이 좋습니다.\n\n그럼에도 불구하고, 이 글이 유용했기를 바라며, 이전에 자체 파일 경로 라우터를 작성한 경험이 있다면 댓글에 어떻게 했는지 알려주세요. 노트를 비교해보고 싶습니다!\n\n# 레고와 같은 재사용 가능한 구성 요소로 앱 만들기\n\n![이미지](/assets/img/2024-05-27-DemystifyingRoutingCreatingYourOwnfile-path-basedRouter_2.png)\n\n<div class=\"content-ad\"></div>\n\n비트의 오픈 소스 도구는 25만 명 이상의 개발자들이 컴포넌트로 앱을 빌드할 수 있도록 도와줍니다.\n\n어떤 UI, 기능 또는 페이지든 재사용 가능한 컴포넌트로 변환하고, 여러 애플리케이션들 간에 공유하세요. 협업하고 더 빠르게 빌드하는 것이 더 쉬워집니다.\n\n→ 더 알아보기\n\n앱을 컴포넌트로 분할하여 앱 개발을 더 쉽게 만들고, 원하는 워크플로에 대해 최상의 경험을 즐기세요:\n\n<div class=\"content-ad\"></div>\n\n## → 마이크로 프론트엔드\n\n## → 디자인 시스템\n\n## → 코드 공유 및 재사용\n\n## → 모노 레포(repository)\n\n<div class=\"content-ad\"></div>\n\n# 더 알아보기","ogImage":{"url":"/assets/img/2024-05-27-DemystifyingRoutingCreatingYourOwnfile-path-basedRouter_0.png"},"coverImage":"/assets/img/2024-05-27-DemystifyingRoutingCreatingYourOwnfile-path-basedRouter_0.png","tag":["Tech"],"readingTime":6}],"page":"59","totalPageCount":154,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}