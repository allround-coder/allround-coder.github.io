{"pageProps":{"post":{"title":"í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²•","description":"","date":"2024-05-16 04:22","slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing","content":"\n\ní† í°í™”ê°€ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? í˜„ì¬ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Models)ì€ ì–´ë–¤ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ìˆ˜í–‰í• ê¹Œìš”? í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•´ìš”!\n\n![ì´ë¯¸ì§€](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png)\n\nìì—°ì–´ ì²˜ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°œë°œ ì´í›„ ë§ì€ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•œ í›„ NLP ì‘ì—…ê³¼ ê´€ë ¨ëœ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” í† í°í™”ì…ë‹ˆë‹¤. ì²˜ìŒì˜ í™”ì´íŠ¸ìŠ¤í˜ì´ìŠ¤(whitespace) ë° êµ¬ë‘ì (tokenizer)ì„ êµ¬ì¶•í•œ ì´í›„ í˜„ì¬ì˜ ë¬¸ë§¥ì (contextual) ë° êµ¬ì¡°ì (tokenizers) í† í¬ë‚˜ì´ì €ë“¤ê¹Œì§€ ë§ì€ ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìš”ì¦˜ì—ëŠ” BERT ë° ê·¸ ë³€í˜•, ChatGPT, Claudeì™€ ê°™ì€ ìƒì„± ëª¨ë¸ì´ íŠ¹íˆ NLP ë¶„ì•¼ì—ì„œ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ì—ì„œëŠ” í…ìŠ¤íŠ¸ í† í°í™” ê³¼ì •ì´ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ë° ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.\n\n# í† í°í™” ê¸°ìˆ  ë°œì „ì˜ ì—¬ì •\n\n\n\ní† í°í™”ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ê¸°ë³¸ í† í°í™” ê¸°ìˆ ì—ëŠ” ê³µë°±, ë‹¨ì–´ ë° ë¬¸ì¥ í† í°í™”ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´íœ˜ í¬ê¸° ë° ì •ë³´ ì†ì‹¤, ë¬¸ë§¥ ë¶€ì¡± ë“±ê³¼ ê°™ì€ ì¼ë¶€ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ n-gram, BPE (Byte Pair Encoding), SentencePiece í† í°í™”ì™€ ê°™ì€ ê¸°ìˆ ì´ ì†Œê°œë˜ì—ˆìœ¼ë©° ê±°ì˜ ëª¨ë“  í•œê³„ë¥¼ í•´ì†Œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ í˜„ì¬ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ë©° ì„ë² ë”©ì—ì„œ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì´í•´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì´ì œ ê° ê¸°ìˆ ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤!\n\n## ê¸°ë³¸ í† í°í™” ê¸°ìˆ \n\nì´ëŸ¬í•œ ê¸°ìˆ ì€ ë°ì´í„°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë° ì£¼ë¡œ ì´ˆì ì„ ë§ì¶”ë©° ì–´ë–¤ ì²­í¬ê°€ ë‹¤ë¥¸ ì²­í¬ì™€ ì–´ë–»ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ í¬ê²Œ ì‹ ê²½ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ê¸°ìˆ ì´ ì‘ë™í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n1. ê³µë°± í† í°í™” - íƒ­, ê³µë°±, ìƒˆ ì¤„ ë“±ì˜ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë“  ë‹¨ì–´ê°€ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n   \n:warning: í•œê³„\n- ë¬¸ë§¥ì  ì˜ë¯¸ ì†ì‹¤: ë‹¨ì–´ë¥¼ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ì¢…ì¢… ë¬¸ì¥ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤.\n- ì–´íœ˜ í­ë°œ: ê° ê³ ìœ í•œ ë‹¨ì–´ê°€ í† í°ì´ ë˜ë¯€ë¡œ, ì–´ë– í•œ ì–¸ì–´ë„ ìˆ˜ì‹­ì–µ ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¢…ì¢… ë§¤ìš° í° í›ˆë ¨ ì–´íœ˜ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.\n- ì¡ìŒì´ ë§ì€ ë°ì´í„° ì²˜ë¦¬ ì–´ë ¤ì›€: ì´ëª¨ì§€, ê³¼ë„í•œ ë¬¸ì¥ ë¶€í˜¸ ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ì—¬ í† í°í™”ê°€ ë¶€ì •í™•í•´ì§‘ë‹ˆë‹¤.\n\n\n\n\n![word tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png)\n\n2. ë‹¨ì–´ í† í°í™” - ê³µë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ëœ ë¬¸ì¥ í† í°í™”ì—ì„œ ë¬¸ì¥ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ë‹¨ì–´ê°€ ë”°ë¡œ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„\n- ë‹¨ì–´ ì‚¬ì´ì˜ ìƒí™©ì  ì˜ë¯¸ ì†ì‹¤\n- ì–´íœ˜í­ë°œ\n\n![sentence tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png)\n\n3. ë¬¸ì¥ í† í°í™” - ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ ë“±ì˜ êµ¬ë‘ì  ë° ë‹¤ë¥¸ ì–¸ì–´ë³„ ê·œì¹™ì„ ì´í•´í•˜ì—¬ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„ - ê¸°ê³„ ë²ˆì—­ ë“±ì˜ ì‘ì—…ì— ìœ ìš©í•˜ì§€ë§Œ ì—¬ì „íˆ ë‹¨ì–´ ìˆ˜ì¤€ í† í°í™”ì— ì˜ì¡´í•˜ë©° ì´ë¡œ ì¸í•œ í•œê³„ë¥¼ ë¬¼ë ¤ë°›ìŠµë‹ˆë‹¤.\n\n\n\n\nğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤:\n\n```js\n# NLTK ì‚¬ìš©\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nnltk.download('punkt')\n\n# ì…ë ¥ ë¬¸ì¥\ntext = \"When I left the place, I didn't take the left turn.\"\n\n# ê³µë°± ê¸°ì¤€ í† í°í™”\nwhitespace_tokens = text.split()\n\n# ë‹¨ì–´ í† í°í™”\nword_tokens = word_tokenize(text)\n\n# ë¬¸ì¥ í† í°í™”\nsentence_tokens = sent_tokenize(text)\n\nprint(\"Whitespace Tokenization:\", whitespace_tokens)\nprint(\"Word Tokenization:\", word_tokens)\nprint(\"Sentence Tokenization:\", sentence_tokens)\n```\n\në˜í•œ SpaCy, Scikit-learn, Stanza ë“±ì˜ ë‹¤ë¥¸ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì´ëŸ¬í•œ í† í°í™” ê¸°ìˆ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n# ê³ ê¸‰ í† í°í™” ê¸°ìˆ \n\n\n\nê³ ê¸‰ ê¸°ìˆ ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ í•œê³„ë¥¼ ì™„í™”í•˜ë ¤ê³  ì‹œë„í•˜ê³ , ë‹¨ì–´ ê°„ ìƒí˜¸ ê´€ê³„ ë° ë¬¸ì¥ ë‚´ ë§¥ë½ì— ì´ˆì ì„ ë§ì¶”ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤:\n\nï¸1. N-ê·¸ë¨-\nâ–ª í…ìŠ¤íŠ¸ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì§€ì •ëœ N ê¸¸ì´ì˜ í† í°ì„ ë§Œë“­ë‹ˆë‹¤.\nâ–ª ì´ ë°©ë²•ì€ ì„œë¡œ ê°€ê¹ê²Œ ë°œìƒí•˜ëŠ” ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì¡ì•„ëƒ…ë‹ˆë‹¤.\nğŸ’¡ì´ ê¸°ìˆ ì€ ìŒì„± ì¸ì‹, í…ìŠ¤íŠ¸ ì™„ì„± ë“±ê³¼ ê°™ì€ ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ ê¸°ë³¸ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„ â€” ì—°ì†ëœ ë‹¨ì–´ì™€ì˜ ê´€ê³„ë§Œ íŒŒì•…í•©ë‹ˆë‹¤. ë” ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„  ë‹¤ì‹œ ë§¥ë½ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\n\n![image](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png)\n\n2. ë°”ì´íŠ¸ ìŒ ë¶€í˜¸í™”-\nâ–ª ì—¬ê¸°ì„œëŠ” í•™ìŠµ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ë¬¸ì/ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ì–´íœ˜ì§‘ì„ ë§Œë“­ë‹ˆë‹¤.\nâ–ª ì—°ì† ë°œìƒ ë¬¸ìì˜ ë¹ˆë„ìˆ˜ì— ê¸°ë°˜í•˜ì—¬ ì–´íœ˜ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\nâ–ª ì¤‘ì§€ ì¡°ê±´(ë˜ëŠ” ìµœëŒ€ ë³‘í•© ìˆ˜)ì´ ì¶©ì¡±ë˜ë©´ ì…ë ¥ í…ìŠ¤íŠ¸(í…ŒìŠ¤íŠ¸ ì…ë ¥)ëŠ” ì´ ìƒì„±ëœ ì–´íœ˜ì§‘ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ë©ë‹ˆë‹¤.\nâ–ª ì–´íœ˜ ì™¸ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ì–´íœ˜ í¬ê¸°ê°€ ë¬´ë„ˆì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\nğŸ’¡RoBERTa, GPT2ëŠ” ì´ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„-\nâ–ª í›ˆë ¨ ë‹¨ê³„ì—ì„œ ê°œë°œëœ ê³ ì •ëœ ì–´íœ˜ í¬ê¸°ë¡œ ì¸í•´ ë•Œë¡œëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ì— ë¬¸ì œê°€ ìƒê¸°ê¸°ë„ í•©ë‹ˆë‹¤.\nâ–ª ì´ ì•Œê³ ë¦¬ì¦˜ì€ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ì„ ëª¨ì•„ ì‚¬ìš©í•˜ë©°, ë¬¸ì¥ì˜ í˜•íƒœí•™ì  ë° ë¬¸ë§¥ì  ë³µì¡ì„±ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.\n\n\n\n<img src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\" />\n\n3. SentencePiece-  \n- SentencePieceëŠ” Unigramê³¼ Dynamic Programming ë˜ëŠ” BPE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì„œë¸Œì›Œë“œ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n- ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ Unicode ë¬¸ìë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ˆê¸° ë‹¨ì–´ í† í°í™”ê°€ í•„ìš”ì—†ìŠµë‹ˆë‹¤.\n- ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- ì²˜ìŒì— Unicode ë¬¸ì ìˆ˜ì¤€ í† í°ì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ì˜ í† í°í™” ë° ë””í† í°í™”ë¥¼ ëª¨ë‘ ë„ì™€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\nğŸ’¡BERT, XLNet, T5 ë“± ë§ì€ HuggingFace íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì˜ ìœ ì§€ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\nâš ï¸ ì œí•œ ì‚¬í•­-  \n- ì–¸ì–´ì— ë…ë¦½ì ì´ì§€ë§Œ ë‹¤ì–‘í•œ ì–¸ì–´ì— ëŒ€í•´ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- ë¬¸ë‹¨ì´ë‚˜ ì„¹ì…˜ê³¼ ê°™ì€ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì„¸ë¶€ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  í•˜ìœ„ ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ì „íˆ ì·¨ê¸‰í•©ë‹ˆë‹¤.\n\nğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ìˆ ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ:\n\n```js\n# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°\nimport sentencepiece as spm\nfrom tokenizers import ByteLevelBPETokenizer\nmodel_path = \"ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ\"\ntrain_text = \"í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ\"\n\n###############################\n# BPE êµ¬í˜„\n###############################\n\nBPE_tokenizer = ByteLevelBPETokenizer()\n\n# utf-8 ì¸ì½”ë”©ëœ ì½”í¼ìŠ¤ë¡œ í† í¬ë‚˜ì´ì € í›ˆë ¨ì‹œí‚¤ê¸°\nBPE_tokenizer.train(files=['í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ'], vocab_size=1000, min_frequency=2)\n\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ì €ì¥\nmodel_path = 'ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ'\nBPE_tokenizer.save_model(model_path)\n\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\nBPE_tokenizer = ByteLevelBPETokenizer.from_file(f\"{model_path}/vocab.json\", f\"{model_path}/merges.txt\")\n\n# í…ìŠ¤íŠ¸ í† í°í™”\ntext = \"I would love to see a lion!\"\nBPE_encoded_tokens = BPE_tokenizer.encode(text)\n\nprint(\"ì›ë³¸ í…ìŠ¤íŠ¸:\", text)\nprint(\"ì¸ì½”ë”©ëœ í† í°:\", BPE_encoded_tokens.tokens)\n\n\n###############################\n# SentencePiece êµ¬í˜„\n###############################\n\nspm.SentencePieceTrainer.train(input=train_text, model_prefix=model_path, vocab_size=1000, num_threads=4)\n\n# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\nsp_model = model_path + \".model\"\nsp = spm.SentencePieceProcessor(model_file=sp_model)\n\ntext = \"I would love to see a lion when we reach the zoo!\"\n\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° ë°˜í™˜\ntokens_subword = sp.encode_as_pieces(text)\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° ID ë°˜í™˜\ntokens_ids = sp.encode_as_ids(text)\n# ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í°í™” ë° ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í° ID ë°˜í™˜\ntokens_byte = sp.encode(text)\n\n# í† í°ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\ndecoded_text = sp.decode_pieces(tokens_subword)\n\nprint(\"ì›ë³¸ í…ìŠ¤íŠ¸:\", text)\nprint(\"í† í°í™”ëœ í…ìŠ¤íŠ¸:\", tokens_subword)\nprint(\"ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸:\", decoded_text)\n```\n\n\n\nì´ëŸ¬í•œ ê³ ê¸‰ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œí•œ í† í°ë“¤ì€ BERT, GPT ë“±ê³¼ ê°™ì€ ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì‘ì—…ì— í•„ìš”í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í°ë“¤ì€ ëª¨ë¸ë¡œ ì „ì†¡ë˜ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥ì  ë° êµ¬ì¡°ì  ì˜ë¯¸ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.","ogImage":{"url":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png"},"coverImage":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png","tag":["Tech"],"readingTime":6},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h1: \"h1\",\n    h2: \"h2\",\n    ol: \"ol\",\n    li: \"li\",\n    ul: \"ul\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"í† í°í™”ê°€ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? í˜„ì¬ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Models)ì€ ì–´ë–¤ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ìˆ˜í–‰í• ê¹Œìš”? í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•´ìš”!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png\",\n        alt: \"ì´ë¯¸ì§€\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ìì—°ì–´ ì²˜ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°œë°œ ì´í›„ ë§ì€ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•œ í›„ NLP ì‘ì—…ê³¼ ê´€ë ¨ëœ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” í† í°í™”ì…ë‹ˆë‹¤. ì²˜ìŒì˜ í™”ì´íŠ¸ìŠ¤í˜ì´ìŠ¤(whitespace) ë° êµ¬ë‘ì (tokenizer)ì„ êµ¬ì¶•í•œ ì´í›„ í˜„ì¬ì˜ ë¬¸ë§¥ì (contextual) ë° êµ¬ì¡°ì (tokenizers) í† í¬ë‚˜ì´ì €ë“¤ê¹Œì§€ ë§ì€ ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìš”ì¦˜ì—ëŠ” BERT ë° ê·¸ ë³€í˜•, ChatGPT, Claudeì™€ ê°™ì€ ìƒì„± ëª¨ë¸ì´ íŠ¹íˆ NLP ë¶„ì•¼ì—ì„œ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ì—ì„œëŠ” í…ìŠ¤íŠ¸ í† í°í™” ê³¼ì •ì´ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ë° ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"í† í°í™” ê¸°ìˆ  ë°œì „ì˜ ì—¬ì •\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"í† í°í™”ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ê¸°ë³¸ í† í°í™” ê¸°ìˆ ì—ëŠ” ê³µë°±, ë‹¨ì–´ ë° ë¬¸ì¥ í† í°í™”ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´íœ˜ í¬ê¸° ë° ì •ë³´ ì†ì‹¤, ë¬¸ë§¥ ë¶€ì¡± ë“±ê³¼ ê°™ì€ ì¼ë¶€ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ n-gram, BPE (Byte Pair Encoding), SentencePiece í† í°í™”ì™€ ê°™ì€ ê¸°ìˆ ì´ ì†Œê°œë˜ì—ˆìœ¼ë©° ê±°ì˜ ëª¨ë“  í•œê³„ë¥¼ í•´ì†Œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ í˜„ì¬ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ë©° ì„ë² ë”©ì—ì„œ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì´í•´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì´ì œ ê° ê¸°ìˆ ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤!\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"ê¸°ë³¸ í† í°í™” ê¸°ìˆ \"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ì´ëŸ¬í•œ ê¸°ìˆ ì€ ë°ì´í„°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë° ì£¼ë¡œ ì´ˆì ì„ ë§ì¶”ë©° ì–´ë–¤ ì²­í¬ê°€ ë‹¤ë¥¸ ì²­í¬ì™€ ì–´ë–»ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ í¬ê²Œ ì‹ ê²½ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ê¸°ìˆ ì´ ì‘ë™í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ê³µë°± í† í°í™” - íƒ­, ê³µë°±, ìƒˆ ì¤„ ë“±ì˜ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë“  ë‹¨ì–´ê°€ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \":warning: í•œê³„\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë¬¸ë§¥ì  ì˜ë¯¸ ì†ì‹¤: ë‹¨ì–´ë¥¼ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ì¢…ì¢… ë¬¸ì¥ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì–´íœ˜ í­ë°œ: ê° ê³ ìœ í•œ ë‹¨ì–´ê°€ í† í°ì´ ë˜ë¯€ë¡œ, ì–´ë– í•œ ì–¸ì–´ë„ ìˆ˜ì‹­ì–µ ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¢…ì¢… ë§¤ìš° í° í›ˆë ¨ ì–´íœ˜ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì¡ìŒì´ ë§ì€ ë°ì´í„° ì²˜ë¦¬ ì–´ë ¤ì›€: ì´ëª¨ì§€, ê³¼ë„í•œ ë¬¸ì¥ ë¶€í˜¸ ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ì—¬ í† í°í™”ê°€ ë¶€ì •í™•í•´ì§‘ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png\",\n        alt: \"word tokenization\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë‹¨ì–´ í† í°í™” - ê³µë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ëœ ë¬¸ì¥ í† í°í™”ì—ì„œ ë¬¸ì¥ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ë‹¨ì–´ê°€ ë”°ë¡œ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë‹¨ì–´ ì‚¬ì´ì˜ ìƒí™©ì  ì˜ë¯¸ ì†ì‹¤\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì–´íœ˜í­ë°œ\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png\",\n        alt: \"sentence tokenization\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë¬¸ì¥ í† í°í™” - ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ ë“±ì˜ êµ¬ë‘ì  ë° ë‹¤ë¥¸ ì–¸ì–´ë³„ ê·œì¹™ì„ ì´í•´í•˜ì—¬ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„ - ê¸°ê³„ ë²ˆì—­ ë“±ì˜ ì‘ì—…ì— ìœ ìš©í•˜ì§€ë§Œ ì—¬ì „íˆ ë‹¨ì–´ ìˆ˜ì¤€ í† í°í™”ì— ì˜ì¡´í•˜ë©° ì´ë¡œ ì¸í•œ í•œê³„ë¥¼ ë¬¼ë ¤ë°›ìŠµë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NLTK\"\n        }), \" ì‚¬ìš©\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" nltk\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" nltk.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"tokenize\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" word_tokenize, sent_tokenize\\n\\nnltk.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"download\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'punkt'\"\n        }), \")\\n\\n# ì…ë ¥ ë¬¸ì¥\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"When I left the place, I didn't take the left turn.\\\"\"\n        }), \"\\n\\n# ê³µë°± ê¸°ì¤€ í† í°í™”\\nwhitespace_tokens = text.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"split\"\n        }), \"()\\n\\n# ë‹¨ì–´ í† í°í™”\\nword_tokens = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"word_tokenize\"\n        }), \"(text)\\n\\n# ë¬¸ì¥ í† í°í™”\\nsentence_tokens = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"sent_tokenize\"\n        }), \"(text)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Whitespace Tokenization:\\\"\"\n        }), \", whitespace_tokens)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Word Tokenization:\\\"\"\n        }), \", word_tokens)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Sentence Tokenization:\\\"\"\n        }), \", sentence_tokens)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ë˜í•œ SpaCy, Scikit-learn, Stanza ë“±ì˜ ë‹¤ë¥¸ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì´ëŸ¬í•œ í† í°í™” ê¸°ìˆ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"ê³ ê¸‰ í† í°í™” ê¸°ìˆ \"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ê³ ê¸‰ ê¸°ìˆ ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ í•œê³„ë¥¼ ì™„í™”í•˜ë ¤ê³  ì‹œë„í•˜ê³ , ë‹¨ì–´ ê°„ ìƒí˜¸ ê´€ê³„ ë° ë¬¸ì¥ ë‚´ ë§¥ë½ì— ì´ˆì ì„ ë§ì¶”ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ï¸1. N-ê·¸ë¨-\\nâ–ª í…ìŠ¤íŠ¸ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì§€ì •ëœ N ê¸¸ì´ì˜ í† í°ì„ ë§Œë“­ë‹ˆë‹¤.\\nâ–ª ì´ ë°©ë²•ì€ ì„œë¡œ ê°€ê¹ê²Œ ë°œìƒí•˜ëŠ” ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì¡ì•„ëƒ…ë‹ˆë‹¤.\\nğŸ’¡ì´ ê¸°ìˆ ì€ ìŒì„± ì¸ì‹, í…ìŠ¤íŠ¸ ì™„ì„± ë“±ê³¼ ê°™ì€ ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ ê¸°ë³¸ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„ â€” ì—°ì†ëœ ë‹¨ì–´ì™€ì˜ ê´€ê³„ë§Œ íŒŒì•…í•©ë‹ˆë‹¤. ë” ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„  ë‹¤ì‹œ ë§¥ë½ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë°”ì´íŠ¸ ìŒ ë¶€í˜¸í™”-\\nâ–ª ì—¬ê¸°ì„œëŠ” í•™ìŠµ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ë¬¸ì/ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ì–´íœ˜ì§‘ì„ ë§Œë“­ë‹ˆë‹¤.\\nâ–ª ì—°ì† ë°œìƒ ë¬¸ìì˜ ë¹ˆë„ìˆ˜ì— ê¸°ë°˜í•˜ì—¬ ì–´íœ˜ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\\nâ–ª ì¤‘ì§€ ì¡°ê±´(ë˜ëŠ” ìµœëŒ€ ë³‘í•© ìˆ˜)ì´ ì¶©ì¡±ë˜ë©´ ì…ë ¥ í…ìŠ¤íŠ¸(í…ŒìŠ¤íŠ¸ ì…ë ¥)ëŠ” ì´ ìƒì„±ëœ ì–´íœ˜ì§‘ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ë©ë‹ˆë‹¤.\\nâ–ª ì–´íœ˜ ì™¸ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ì–´íœ˜ í¬ê¸°ê°€ ë¬´ë„ˆì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\\nğŸ’¡RoBERTa, GPT2ëŠ” ì´ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„-\\nâ–ª í›ˆë ¨ ë‹¨ê³„ì—ì„œ ê°œë°œëœ ê³ ì •ëœ ì–´íœ˜ í¬ê¸°ë¡œ ì¸í•´ ë•Œë¡œëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ì— ë¬¸ì œê°€ ìƒê¸°ê¸°ë„ í•©ë‹ˆë‹¤.\\nâ–ª ì´ ì•Œê³ ë¦¬ì¦˜ì€ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ì„ ëª¨ì•„ ì‚¬ìš©í•˜ë©°, ë¬¸ì¥ì˜ í˜•íƒœí•™ì  ë° ë¬¸ë§¥ì  ë³µì¡ì„±ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"SentencePiece-\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"SentencePieceëŠ” Unigramê³¼ Dynamic Programming ë˜ëŠ” BPE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì„œë¸Œì›Œë“œ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ Unicode ë¬¸ìë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ˆê¸° ë‹¨ì–´ í† í°í™”ê°€ í•„ìš”ì—†ìŠµë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì²˜ìŒì— Unicode ë¬¸ì ìˆ˜ì¤€ í† í°ì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ì˜ í† í°í™” ë° ë””í† í°í™”ë¥¼ ëª¨ë‘ ë„ì™€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\\nğŸ’¡BERT, XLNet, T5 ë“± ë§ì€ HuggingFace íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì˜ ìœ ì§€ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\\nâš ï¸ ì œí•œ ì‚¬í•­-\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì–¸ì–´ì— ë…ë¦½ì ì´ì§€ë§Œ ë‹¤ì–‘í•œ ì–¸ì–´ì— ëŒ€í•´ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ë¬¸ë‹¨ì´ë‚˜ ì„¹ì…˜ê³¼ ê°™ì€ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì„¸ë¶€ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  í•˜ìœ„ ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ì „íˆ ì·¨ê¸‰í•©ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ìˆ ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" sentencepiece \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" spm\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \"\\nmodel_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ\\\"\"\n        }), \"\\ntrain_text = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ\\\"\"\n        }), \"\\n\\n###############################\\n# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BPE\"\n        }), \" êµ¬í˜„\\n###############################\\n\\nBPE_tokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \"()\\n\\n# utf-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8\"\n        }), \" ì¸ì½”ë”©ëœ ì½”í¼ìŠ¤ë¡œ í† í¬ë‚˜ì´ì € í›ˆë ¨ì‹œí‚¤ê¸°\\nBPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(files=[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ'\"\n        }), \"], vocab_size=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1000\"\n        }), \", min_frequency=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \")\\n\\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ì €ì¥\\nmodel_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ'\"\n        }), \"\\nBPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"save_model\"\n        }), \"(model_path)\\n\\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\\nBPE_tokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_file\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"{model_path}/vocab.json\\\"\"\n        }), \", f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"{model_path}/merges.txt\\\"\"\n        }), \")\\n\\n# í…ìŠ¤íŠ¸ í† í°í™”\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"I would love to see a lion!\\\"\"\n        }), \"\\nBPE_encoded_tokens = BPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode\"\n        }), \"(text)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ì›ë³¸ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", text)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ì¸ì½”ë”©ëœ í† í°:\\\"\"\n        }), \", BPE_encoded_tokens.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"tokens\"\n        }), \")\\n\\n\\n###############################\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentencePiece\"\n        }), \" êµ¬í˜„\\n###############################\\n\\nspm.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"SentencePieceTrainer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(input=train_text, model_prefix=model_path, vocab_size=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1000\"\n        }), \", num_threads=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4\"\n        }), \")\\n\\n# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\\nsp_model = model_path + \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\".model\\\"\"\n        }), \"\\nsp = spm.\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentencePieceProcessor\"\n        }), \"(model_file=sp_model)\\n\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"I would love to see a lion when we reach the zoo!\\\"\"\n        }), \"\\n\\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° ë°˜í™˜\\ntokens_subword = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode_as_pieces\"\n        }), \"(text)\\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"ID\"\n        }), \" ë°˜í™˜\\ntokens_ids = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode_as_ids\"\n        }), \"(text)\\n# ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í°í™” ë° ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í° \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"ID\"\n        }), \" ë°˜í™˜\\ntokens_byte = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode\"\n        }), \"(text)\\n\\n# í† í°ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\\ndecoded_text = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"decode_pieces\"\n        }), \"(tokens_subword)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ì›ë³¸ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", text)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"í† í°í™”ëœ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", tokens_subword)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", decoded_text)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ì´ëŸ¬í•œ ê³ ê¸‰ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œí•œ í† í°ë“¤ì€ BERT, GPT ë“±ê³¼ ê°™ì€ ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì‘ì—…ì— í•„ìš”í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í°ë“¤ì€ ëª¨ë¸ë¡œ ì „ì†¡ë˜ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥ì  ë° êµ¬ì¡°ì  ì˜ë¯¸ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}