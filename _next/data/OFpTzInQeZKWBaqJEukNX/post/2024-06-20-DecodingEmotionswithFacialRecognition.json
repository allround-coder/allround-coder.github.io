{"pageProps":{"post":{"title":"얼굴 인식을 통해 감정 해독하기","description":"","date":"2024-06-20 05:00","slug":"2024-06-20-DecodingEmotionswithFacialRecognition","content":"\n\n<img src=\"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png\" />\n\n인간의 감정을 얼굴 표현을 통해 이해하는 것은 우리에게 자연스러운 기술이지만, 컴퓨터에게 같은 기술을 가르치는 것은 어렵게 느껴질 수 있습니다. 다행히도, 적절한 도구와 조금의 코딩을 통해 이를 실현할 수 있습니다. 이 글에서는 Python을 사용하여 감정 감지를 위한 얼굴 표현 인식을 구현하는 두 가지 방법을 탐구해 보겠습니다: DeepFace를 사용하는 방법과 Keras를 활용한 합성곱 신경망(CNN)을 사용하는 방법\n\n# 1.) Deepface 사용\n\nDeepface는 파이썬용 경량 얼굴 인식 및 얼굴 속성 분석(나이, 성별, 감정 및 인종) 프레임워크입니다. DeepFace를 몇 줄의 코드로 실행할 수 있지만, 그 뒤의 모든 과정에 대해 깊이 있는 지식을 습득할 필요가 없습니다. 사실, 라이브러리를 가져오고 정확한 이미지 경로를 입력으로 전달하기만 하면 됩니다; 그게 전부입니다!\n\n<div class=\"content-ad\"></div>\n\n1.) .py 파일로 필요한 모듈 가져오기\n\n```js\nfrom deepface import DeepFace\nimport cv2\n```\n\n2.) 얼굴 캐스케이드 분류기 로드\n\n이 명령은 전면 얼굴 검출 모델과 함께 CascadeClassifier 객체를 초기화합니다. 그 결과인 face_cascade 객체를 사용하여 이미지에서 얼굴을 감지할 수 있습니다. Haar Cascade는 파이썬의 OpenCV 라이브러리를 사용하여 쉽게 구현할 수 있는 얼굴 검출을 위한 인기 있는 알고리즘입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\r\n```\r\n\r\n3.) 비디오 스트림을 시작하고 분류기를 실행합니다.\r\n\r\n0은 기본 카메라를 나타냅니다. 외부 웹캠을 연결한 경우 1을 입력하세요.\r\n\r\n```js\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    # 성공 또는 실패 여부를 나타내는 부울 값인 ret 및 캡쳐된 프레임인 frame을 캡쳐합니다.\r\n    ret, frame = cap.read()\r\n\r\n    # 프레임을 그레이스케일로 변환합니다.\r\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\n    # 그레이스케일 프레임을 RGB 형식으로 변환합니다.\r\n    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\r\n\r\n    # 프레임에 얼굴을 감지합니다.\r\n    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=10, minSize=(30, 30))\r\n\r\n    for (x, y, w, h) in faces:\r\n        # RGB 프레임에서 y에서 y+h, x에서 x+w까지의 영역에서 얼굴 ROI(관심 영역)를 추출합니다.\r\n        face_roi = rgb_frame[y:y + h, x:x + w]\r\n\r\n        # DeepFace를 사용하여 얼굴 ROI에서 감정 분석을 수행합니다.\r\n        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\r\n\r\n        # 주요 감정을 결정합니다.\r\n        emotion = result[0]['dominant_emotion']\r\n\r\n        # 얼굴 주위에 직사각형을 그리고 예측된 감정과 함께 레이블을 붙입니다.\r\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\r\n        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\r\n\r\n    # 결과 프레임을 표시합니다.\r\n    cv2.imshow('실시간 감정 감지', frame)\r\n\r\n    # 종료하려면 'q'를 누르세요.\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\n# 캡처를 해제하고 모든 창을 닫습니다.\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n```\n\n<div class=\"content-ad\"></div>\n\n다음 링크에서 haarcascade 파일을 다운로드할 수 있어요 — https://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml\n\n카스케이드 분류기에 대해 더 읽어보고 싶다면 -https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\n\n.py 파일과 haarcascade_frontalface_default.xml 파일을 동일한 폴더에 넣고 .py 파일을 실행해주세요. 모두 잘 작동되면, 카메라 스트림이 보이는 외부 창에 감정이 표시될 거예요!\n\n# 2.) Keras를 이용한 합성곱 신경망\n\n<div class=\"content-ad\"></div>\n\n합성곱 신경망(Convolutional Neural Networks)은 이미지 처리에 사용되는 피드 포워드 네트워크의 일종입니다. 이러한 네트워크는 일반적인 완전 연결 레이어에 추가적인 합성곱(Convolutional) 및 풀링(Pooling) 레이어를 특징으로 합니다. 주로 그리드(grid) 형식의 데이터(이미지, 비디오)와 함께 작동합니다.\n\n- https://www.kaggle.com/datasets/msambare/fer2013 에서 FER-2013 데이터셋을 다운로드하세요. 훈련 및 테스트 디렉토리를 'data'라는 공통 폴더 아래에 넣으세요.\n- .py 파일에 필요한 모듈을 가져오세요\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import models, layers\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\nimport os\n```\n\n3. 동일한 파일에서 모델을 구축하고 훈련시키세요.\n\n<div class=\"content-ad\"></div>\n\n```js\ntrain_data_dir='data/train/'\nvalidation_data_dir='data/test/'\n\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=30,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode= 'nearest')\nvalidation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    color_mode='grayscale',\n    target_size=(48, 48),\n    batch_size=32,\n    class_mode='categorical' ,\n    shuffle=True)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_data_dir,\n    color_mode='grayscale',\n    target_size=(48, 48),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle=True)\n\nclass_labels=['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\nimg, label = train_generator.__next__()\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout (0.1))\nmodel.add (Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add (Dropout(0.1))\nmodel. add (Conv2D(256, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel. add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout (0.2))\nmodel. add(Dense(7, activation='softmax'))\n\nmodel.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())\n\ntrain_path = \"data/train\"\ntest_path = \"data/test\"\nnum_train_imgs = 0\nfor root, dirs, files in os.walk(train_path):\n    num_train_imgs += len(files)\nnum_test_imgs = 0\nfor root, dirs, files in os.walk(test_path):\n    num_test_imgs += len(files)\n\nprint(\"Number of training images: \", num_train_imgs)\nprint(\"Number of testing images: \", num_test_imgs)\n\nmodel.fit(train_generator, steps_per_epoch=num_train_imgs//32, epochs=50, validation_data=validation_generator, validation_steps=num_test_imgs//32)\n\nmodel.save('model.h5')  \n```\n\n모델.h5 파일이 현재 디렉토리에 저장됩니다.\n\n4. 테스트\n\n```js\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nmodel=tf.keras.models.load_model('model.h5')\n\nfaceDetect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\nvideo=cv2.VideoCapture(0)\n\nlabels_dict={0:'Angry',1:'Disgust', 2:'Fear', 3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}\n\nwhile True:\n    ret,frame=video.read()\n    gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces= faceDetect.detectMultiScale(gray, 1.3, 3)\n    for x,y,w,h in faces:\n        sub_face_img=gray[y:y+h, x:x+w]\n        resized=cv2.resize(sub_face_img,(48,48))\n        normalize=resized/255.0\n        reshaped=np.reshape(normalize, (1, 48, 48, 1))\n        result=model.predict(reshaped)\n        label=np.argmax(result, axis=1)[0]\n        print(label)\n        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 1)\n        cv2.rectangle(frame,(x,y),(x+w,y+h),(50,50,255),2)\n        cv2.rectangle(frame,(x,y-40),(x+w,y),(50,50,255),-1)\n        cv2.putText(frame, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n        \n    cv2.imshow(\"실시간 감정 인식\",frame)\n    k=cv2.waitKey(1)\n    if k==ord('q'):\n        break\n\nvideo.release()\ncv2.destroyAllWindows()\n```\n\n<div class=\"content-ad\"></div>\n\n해당 파이썬 스크립트를 실행해보세요. 코드가 동작할 것을 기대합니다!\n\n# 개선 사항\n\n이 코드에 주의를 집중시키기 위해 Spatial Transformer의 추가를 활용할 수도 있습니다. 해당 내용은 논문에 언급되어 있습니다.\n\nDeep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network- Shervin Minaee, Amirali Abdolrashidi, Expedia Group\nUniversity of California, Riverside","ogImage":{"url":"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png"},"coverImage":"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png","tag":["Tech"],"readingTime":8},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<img src=\"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png\">\n<p>인간의 감정을 얼굴 표현을 통해 이해하는 것은 우리에게 자연스러운 기술이지만, 컴퓨터에게 같은 기술을 가르치는 것은 어렵게 느껴질 수 있습니다. 다행히도, 적절한 도구와 조금의 코딩을 통해 이를 실현할 수 있습니다. 이 글에서는 Python을 사용하여 감정 감지를 위한 얼굴 표현 인식을 구현하는 두 가지 방법을 탐구해 보겠습니다: DeepFace를 사용하는 방법과 Keras를 활용한 합성곱 신경망(CNN)을 사용하는 방법</p>\n<h1>1.) Deepface 사용</h1>\n<p>Deepface는 파이썬용 경량 얼굴 인식 및 얼굴 속성 분석(나이, 성별, 감정 및 인종) 프레임워크입니다. DeepFace를 몇 줄의 코드로 실행할 수 있지만, 그 뒤의 모든 과정에 대해 깊이 있는 지식을 습득할 필요가 없습니다. 사실, 라이브러리를 가져오고 정확한 이미지 경로를 입력으로 전달하기만 하면 됩니다; 그게 전부입니다!</p>\n<div class=\"content-ad\"></div>\n<p>1.) .py 파일로 필요한 모듈 가져오기</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> deepface <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">DeepFace</span>\n<span class=\"hljs-keyword\">import</span> cv2\n</code></pre>\n<p>2.) 얼굴 캐스케이드 분류기 로드</p>\n<p>이 명령은 전면 얼굴 검출 모델과 함께 CascadeClassifier 객체를 초기화합니다. 그 결과인 face_cascade 객체를 사용하여 이미지에서 얼굴을 감지할 수 있습니다. Haar Cascade는 파이썬의 OpenCV 라이브러리를 사용하여 쉽게 구현할 수 있는 얼굴 검출을 위한 인기 있는 알고리즘입니다.</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">face_cascade = cv2.<span class=\"hljs-title class_\">CascadeClassifier</span>(cv2.<span class=\"hljs-property\">data</span>.<span class=\"hljs-property\">haarcascades</span> + <span class=\"hljs-string\">'haarcascade_frontalface_default.xml'</span>)\n</code></pre>\n<p>3.) 비디오 스트림을 시작하고 분류기를 실행합니다.</p>\n<p>0은 기본 카메라를 나타냅니다. 외부 웹캠을 연결한 경우 1을 입력하세요.</p>\n<pre><code class=\"hljs language-js\">cap = cv2.<span class=\"hljs-title class_\">VideoCapture</span>(<span class=\"hljs-number\">0</span>)\r\n\r\n<span class=\"hljs-keyword\">while</span> <span class=\"hljs-title class_\">True</span>:\r\n    # 성공 또는 실패 여부를 나타내는 부울 값인 ret 및 캡쳐된 프레임인 frame을 캡쳐합니다.\r\n    ret, frame = cap.<span class=\"hljs-title function_\">read</span>()\r\n\r\n    # 프레임을 그레이스케일로 변환합니다.\r\n    gray_frame = cv2.<span class=\"hljs-title function_\">cvtColor</span>(frame, cv2.<span class=\"hljs-property\">COLOR_BGR2GRAY</span>)\r\n\r\n    # 그레이스케일 프레임을 <span class=\"hljs-variable constant_\">RGB</span> 형식으로 변환합니다.\r\n    rgb_frame = cv2.<span class=\"hljs-title function_\">cvtColor</span>(gray_frame, cv2.<span class=\"hljs-property\">COLOR_GRAY2RGB</span>)\r\n\r\n    # 프레임에 얼굴을 감지합니다.\r\n    faces = face_cascade.<span class=\"hljs-title function_\">detectMultiScale</span>(gray_frame, scaleFactor=<span class=\"hljs-number\">1.1</span>, minNeighbors=<span class=\"hljs-number\">10</span>, minSize=(<span class=\"hljs-number\">30</span>, <span class=\"hljs-number\">30</span>))\r\n\r\n    <span class=\"hljs-keyword\">for</span> (x, y, w, h) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">faces</span>:\r\n        # <span class=\"hljs-variable constant_\">RGB</span> 프레임에서 y에서 y+h, x에서 x+w까지의 영역에서 얼굴 <span class=\"hljs-title function_\">ROI</span>(관심 영역)를 추출합니다.\r\n        face_roi = rgb_frame[<span class=\"hljs-attr\">y</span>:y + h, <span class=\"hljs-attr\">x</span>:x + w]\r\n\r\n        # <span class=\"hljs-title class_\">DeepFace</span>를 사용하여 얼굴 <span class=\"hljs-variable constant_\">ROI</span>에서 감정 분석을 수행합니다.\r\n        result = <span class=\"hljs-title class_\">DeepFace</span>.<span class=\"hljs-title function_\">analyze</span>(face_roi, actions=[<span class=\"hljs-string\">'emotion'</span>], enforce_detection=<span class=\"hljs-title class_\">False</span>)\r\n\r\n        # 주요 감정을 결정합니다.\r\n        emotion = result[<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'dominant_emotion'</span>]\r\n\r\n        # 얼굴 주위에 직사각형을 그리고 예측된 감정과 함께 레이블을 붙입니다.\r\n        cv2.<span class=\"hljs-title function_\">rectangle</span>(frame, (x, y), (x + w, y + h), (<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">255</span>), <span class=\"hljs-number\">2</span>)\r\n        cv2.<span class=\"hljs-title function_\">putText</span>(frame, emotion, (x, y - <span class=\"hljs-number\">10</span>), cv2.<span class=\"hljs-property\">FONT_HERSHEY_SIMPLEX</span>, <span class=\"hljs-number\">0.9</span>, (<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">255</span>), <span class=\"hljs-number\">2</span>)\r\n\r\n    # 결과 프레임을 표시합니다.\r\n    cv2.<span class=\"hljs-title function_\">imshow</span>(<span class=\"hljs-string\">'실시간 감정 감지'</span>, frame)\r\n\r\n    # 종료하려면 <span class=\"hljs-string\">'q'</span>를 누르세요.\r\n    <span class=\"hljs-keyword\">if</span> cv2.<span class=\"hljs-title function_\">waitKey</span>(<span class=\"hljs-number\">1</span>) &#x26; <span class=\"hljs-number\">0xFF</span> == <span class=\"hljs-title function_\">ord</span>(<span class=\"hljs-string\">'q'</span>):\r\n        <span class=\"hljs-keyword\">break</span>\r\n\r\n# 캡처를 해제하고 모든 창을 닫습니다.\r\ncap.<span class=\"hljs-title function_\">release</span>()\r\ncv2.<span class=\"hljs-title function_\">destroyAllWindows</span>()\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>다음 링크에서 haarcascade 파일을 다운로드할 수 있어요 — <a href=\"https://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml\" rel=\"nofollow\" target=\"_blank\">https://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml</a></p>\n<p>카스케이드 분류기에 대해 더 읽어보고 싶다면 -<a href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\" rel=\"nofollow\" target=\"_blank\">https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html</a></p>\n<p>.py 파일과 haarcascade_frontalface_default.xml 파일을 동일한 폴더에 넣고 .py 파일을 실행해주세요. 모두 잘 작동되면, 카메라 스트림이 보이는 외부 창에 감정이 표시될 거예요!</p>\n<h1>2.) Keras를 이용한 합성곱 신경망</h1>\n<div class=\"content-ad\"></div>\n<p>합성곱 신경망(Convolutional Neural Networks)은 이미지 처리에 사용되는 피드 포워드 네트워크의 일종입니다. 이러한 네트워크는 일반적인 완전 연결 레이어에 추가적인 합성곱(Convolutional) 및 풀링(Pooling) 레이어를 특징으로 합니다. 주로 그리드(grid) 형식의 데이터(이미지, 비디오)와 함께 작동합니다.</p>\n<ul>\n<li><a href=\"https://www.kaggle.com/datasets/msambare/fer2013\" rel=\"nofollow\" target=\"_blank\">https://www.kaggle.com/datasets/msambare/fer2013</a> 에서 FER-2013 데이터셋을 다운로드하세요. 훈련 및 테스트 디렉토리를 'data'라는 공통 폴더 아래에 넣으세요.</li>\n<li>.py 파일에 필요한 모듈을 가져오세요</li>\n</ul>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> tensorflow <span class=\"hljs-keyword\">as</span> tf\n<span class=\"hljs-keyword\">from</span> tensorflow <span class=\"hljs-keyword\">import</span> keras\n<span class=\"hljs-keyword\">from</span> keras <span class=\"hljs-keyword\">import</span> models, layers\n<span class=\"hljs-keyword\">from</span> keras.models <span class=\"hljs-keyword\">import</span> Sequential\n<span class=\"hljs-keyword\">from</span> keras.layers <span class=\"hljs-keyword\">import</span> Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n<span class=\"hljs-keyword\">import</span> os\n</code></pre>\n<ol start=\"3\">\n<li>동일한 파일에서 모델을 구축하고 훈련시키세요.</li>\n</ol>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-js\">train_data_dir=<span class=\"hljs-string\">'data/train/'</span>\nvalidation_data_dir=<span class=\"hljs-string\">'data/test/'</span>\n\ntrain_datagen = tf.<span class=\"hljs-property\">keras</span>.<span class=\"hljs-property\">preprocessing</span>.<span class=\"hljs-property\">image</span>.<span class=\"hljs-title class_\">ImageDataGenerator</span>(\n    rescale=<span class=\"hljs-number\">1.</span>/<span class=\"hljs-number\">255</span>,\n    rotation_range=<span class=\"hljs-number\">30</span>,\n    shear_range=<span class=\"hljs-number\">0.3</span>,\n    zoom_range=<span class=\"hljs-number\">0.3</span>,\n    horizontal_flip=<span class=\"hljs-title class_\">True</span>,\n    fill_mode= <span class=\"hljs-string\">'nearest'</span>)\nvalidation_datagen = tf.<span class=\"hljs-property\">keras</span>.<span class=\"hljs-property\">preprocessing</span>.<span class=\"hljs-property\">image</span>.<span class=\"hljs-title class_\">ImageDataGenerator</span>(rescale=<span class=\"hljs-number\">1.</span>/<span class=\"hljs-number\">255</span>)\n\n\ntrain_generator = train_datagen.<span class=\"hljs-title function_\">flow_from_directory</span>(\n    train_data_dir,\n    color_mode=<span class=\"hljs-string\">'grayscale'</span>,\n    target_size=(<span class=\"hljs-number\">48</span>, <span class=\"hljs-number\">48</span>),\n    batch_size=<span class=\"hljs-number\">32</span>,\n    class_mode=<span class=\"hljs-string\">'categorical'</span> ,\n    shuffle=<span class=\"hljs-title class_\">True</span>)\n\nvalidation_generator = validation_datagen.<span class=\"hljs-title function_\">flow_from_directory</span>(\n    validation_data_dir,\n    color_mode=<span class=\"hljs-string\">'grayscale'</span>,\n    target_size=(<span class=\"hljs-number\">48</span>, <span class=\"hljs-number\">48</span>),\n    batch_size=<span class=\"hljs-number\">32</span>,\n    class_mode=<span class=\"hljs-string\">'categorical'</span>,\n    shuffle=<span class=\"hljs-title class_\">True</span>)\n\nclass_labels=[<span class=\"hljs-string\">'Angry'</span>, <span class=\"hljs-string\">'Disgust'</span>, <span class=\"hljs-string\">'Fear'</span>, <span class=\"hljs-string\">'Happy'</span>, <span class=\"hljs-string\">'Neutral'</span>, <span class=\"hljs-string\">'Sad'</span>, <span class=\"hljs-string\">'Surprise'</span>]\nimg, label = train_generator.<span class=\"hljs-title function_\">__next__</span>()\n\n\nmodel = <span class=\"hljs-title class_\">Sequential</span>()\n\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Conv2D</span>(<span class=\"hljs-number\">32</span>, kernel_size=(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">3</span>), activation=<span class=\"hljs-string\">'relu'</span>, input_shape=(<span class=\"hljs-number\">48</span>,<span class=\"hljs-number\">48</span>,<span class=\"hljs-number\">1</span>)))\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Conv2D</span>(<span class=\"hljs-number\">64</span>, kernel_size=(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">3</span>), activation=<span class=\"hljs-string\">'relu'</span>))\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">MaxPooling2D</span>(pool_size=(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>)))\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Dropout</span> (<span class=\"hljs-number\">0.1</span>))\nmodel.<span class=\"hljs-property\">add</span> (<span class=\"hljs-title class_\">Conv2D</span>(<span class=\"hljs-number\">128</span>, kernel_size=(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">3</span>), activation=<span class=\"hljs-string\">'relu'</span>))\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">MaxPooling2D</span>(pool_size=(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>)))\nmodel.<span class=\"hljs-property\">add</span> (<span class=\"hljs-title class_\">Dropout</span>(<span class=\"hljs-number\">0.1</span>))\nmodel. add (<span class=\"hljs-title class_\">Conv2D</span>(<span class=\"hljs-number\">256</span>, kernel_size=(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">3</span>), activation=<span class=\"hljs-string\">'relu'</span>))\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">MaxPooling2D</span>(pool_size=(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>)))\nmodel. <span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Dropout</span>(<span class=\"hljs-number\">0.1</span>))\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Flatten</span>())\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Dense</span>(<span class=\"hljs-number\">512</span>, activation=<span class=\"hljs-string\">'relu'</span>))\nmodel.<span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Dropout</span> (<span class=\"hljs-number\">0.2</span>))\nmodel. <span class=\"hljs-title function_\">add</span>(<span class=\"hljs-title class_\">Dense</span>(<span class=\"hljs-number\">7</span>, activation=<span class=\"hljs-string\">'softmax'</span>))\n\nmodel.<span class=\"hljs-title function_\">compile</span>(optimizer = <span class=\"hljs-string\">'adam'</span>, loss=<span class=\"hljs-string\">'categorical_crossentropy'</span>, metrics=[<span class=\"hljs-string\">'accuracy'</span>])\n<span class=\"hljs-title function_\">print</span>(model.<span class=\"hljs-title function_\">summary</span>())\n\ntrain_path = <span class=\"hljs-string\">\"data/train\"</span>\ntest_path = <span class=\"hljs-string\">\"data/test\"</span>\nnum_train_imgs = <span class=\"hljs-number\">0</span>\n<span class=\"hljs-keyword\">for</span> root, dirs, files <span class=\"hljs-keyword\">in</span> os.<span class=\"hljs-title function_\">walk</span>(train_path):\n    num_train_imgs += <span class=\"hljs-title function_\">len</span>(files)\nnum_test_imgs = <span class=\"hljs-number\">0</span>\n<span class=\"hljs-keyword\">for</span> root, dirs, files <span class=\"hljs-keyword\">in</span> os.<span class=\"hljs-title function_\">walk</span>(test_path):\n    num_test_imgs += <span class=\"hljs-title function_\">len</span>(files)\n\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"Number of training images: \"</span>, num_train_imgs)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"Number of testing images: \"</span>, num_test_imgs)\n\nmodel.<span class=\"hljs-title function_\">fit</span>(train_generator, steps_per_epoch=num_train_imgs<span class=\"hljs-comment\">//32, epochs=50, validation_data=validation_generator, validation_steps=num_test_imgs//32)</span>\n\nmodel.<span class=\"hljs-title function_\">save</span>(<span class=\"hljs-string\">'model.h5'</span>)  \n</code></pre>\n<p>모델.h5 파일이 현재 디렉토리에 저장됩니다.</p>\n<ol start=\"4\">\n<li>테스트</li>\n</ol>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> cv2\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">import</span> tensorflow <span class=\"hljs-keyword\">as</span> tf\n<span class=\"hljs-keyword\">import</span> matplotlib.<span class=\"hljs-property\">pyplot</span> <span class=\"hljs-keyword\">as</span> plt\n\nmodel=tf.<span class=\"hljs-property\">keras</span>.<span class=\"hljs-property\">models</span>.<span class=\"hljs-title function_\">load_model</span>(<span class=\"hljs-string\">'model.h5'</span>)\n\nfaceDetect=cv2.<span class=\"hljs-title class_\">CascadeClassifier</span>(<span class=\"hljs-string\">'haarcascade_frontalface_default.xml'</span>)\n\nvideo=cv2.<span class=\"hljs-title class_\">VideoCapture</span>(<span class=\"hljs-number\">0</span>)\n\nlabels_dict={<span class=\"hljs-number\">0</span>:<span class=\"hljs-string\">'Angry'</span>,<span class=\"hljs-number\">1</span>:<span class=\"hljs-string\">'Disgust'</span>, <span class=\"hljs-number\">2</span>:<span class=\"hljs-string\">'Fear'</span>, <span class=\"hljs-number\">3</span>:<span class=\"hljs-string\">'Happy'</span>,<span class=\"hljs-number\">4</span>:<span class=\"hljs-string\">'Neutral'</span>,<span class=\"hljs-number\">5</span>:<span class=\"hljs-string\">'Sad'</span>,<span class=\"hljs-number\">6</span>:<span class=\"hljs-string\">'Surprise'</span>}\n\n<span class=\"hljs-keyword\">while</span> <span class=\"hljs-title class_\">True</span>:\n    ret,frame=video.<span class=\"hljs-title function_\">read</span>()\n    gray=cv2.<span class=\"hljs-title function_\">cvtColor</span>(frame, cv2.<span class=\"hljs-property\">COLOR_BGR2GRAY</span>)\n    faces= faceDetect.<span class=\"hljs-title function_\">detectMultiScale</span>(gray, <span class=\"hljs-number\">1.3</span>, <span class=\"hljs-number\">3</span>)\n    <span class=\"hljs-keyword\">for</span> x,y,w,h <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">faces</span>:\n        sub_face_img=gray[<span class=\"hljs-attr\">y</span>:y+h, <span class=\"hljs-attr\">x</span>:x+w]\n        resized=cv2.<span class=\"hljs-title function_\">resize</span>(sub_face_img,(<span class=\"hljs-number\">48</span>,<span class=\"hljs-number\">48</span>))\n        normalize=resized/<span class=\"hljs-number\">255.0</span>\n        reshaped=np.<span class=\"hljs-title function_\">reshape</span>(normalize, (<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">48</span>, <span class=\"hljs-number\">48</span>, <span class=\"hljs-number\">1</span>))\n        result=model.<span class=\"hljs-title function_\">predict</span>(reshaped)\n        label=np.<span class=\"hljs-title function_\">argmax</span>(result, axis=<span class=\"hljs-number\">1</span>)[<span class=\"hljs-number\">0</span>]\n        <span class=\"hljs-title function_\">print</span>(label)\n        cv2.<span class=\"hljs-title function_\">rectangle</span>(frame, (x,y), (x+w, y+h), (<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">255</span>), <span class=\"hljs-number\">1</span>)\n        cv2.<span class=\"hljs-title function_\">rectangle</span>(frame,(x,y),(x+w,y+h),(<span class=\"hljs-number\">50</span>,<span class=\"hljs-number\">50</span>,<span class=\"hljs-number\">255</span>),<span class=\"hljs-number\">2</span>)\n        cv2.<span class=\"hljs-title function_\">rectangle</span>(frame,(x,y-<span class=\"hljs-number\">40</span>),(x+w,y),(<span class=\"hljs-number\">50</span>,<span class=\"hljs-number\">50</span>,<span class=\"hljs-number\">255</span>),-<span class=\"hljs-number\">1</span>)\n        cv2.<span class=\"hljs-title function_\">putText</span>(frame, labels_dict[label], (x, y-<span class=\"hljs-number\">10</span>),cv2.<span class=\"hljs-property\">FONT_HERSHEY_SIMPLEX</span>,<span class=\"hljs-number\">0.8</span>,(<span class=\"hljs-number\">255</span>,<span class=\"hljs-number\">255</span>,<span class=\"hljs-number\">255</span>),<span class=\"hljs-number\">2</span>)\n        \n    cv2.<span class=\"hljs-title function_\">imshow</span>(<span class=\"hljs-string\">\"실시간 감정 인식\"</span>,frame)\n    k=cv2.<span class=\"hljs-title function_\">waitKey</span>(<span class=\"hljs-number\">1</span>)\n    <span class=\"hljs-keyword\">if</span> k==<span class=\"hljs-title function_\">ord</span>(<span class=\"hljs-string\">'q'</span>):\n        <span class=\"hljs-keyword\">break</span>\n\nvideo.<span class=\"hljs-title function_\">release</span>()\ncv2.<span class=\"hljs-title function_\">destroyAllWindows</span>()\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>해당 파이썬 스크립트를 실행해보세요. 코드가 동작할 것을 기대합니다!</p>\n<h1>개선 사항</h1>\n<p>이 코드에 주의를 집중시키기 위해 Spatial Transformer의 추가를 활용할 수도 있습니다. 해당 내용은 논문에 언급되어 있습니다.</p>\n<p>Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network- Shervin Minaee, Amirali Abdolrashidi, Expedia Group\nUniversity of California, Riverside</p>\n</body>\n</html>\n"},"__N_SSG":true}