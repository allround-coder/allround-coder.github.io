{"pageProps":{"posts":[{"title":"리액트 시작하는 개발자가 봐야하는 글","description":"","date":"2024-05-20 22:08","slug":"2024-05-20-GettingStartedwithReactYourFunandEasyGuide","content":"\n\n- HTML: 웹페이지의 기본 구조를 이해합니다.\n- CSS: 스타일을 적용하여 웹페이지를 멋지게 만듭니다.\n- JavaScript: 웹페이지를 인터랙티브하게 만드는 코딩 언어를 배웁니다.\n- DOM 조작: 웹페이지를 동적으로 변경하는 방법에 익숙해집니다.\n\n- TODO 앱 만들기: 간단한 할 일 목록을 만들면서 배운 것을 연습해보세요!\n\n```js\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <title>나의 TODO 앱</title>\n  <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n  <div class=\"todo-container\">\n    <input type=\"text\" id=\"todo-input\" placeholder=\"새로운 할 일 추가\">\n    <button id=\"add-btn\">추가</button>\n    <ul id=\"todo-list\"></ul>\n  </div>\n  <script src=\"script.js\"></script>\n</body>\n</html>\n```\n\n```css\n/* styles.css */\n.todo-container {\n  max-width: 400px;\n  margin: 0 auto;\n}\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n// script.js\nconst todoInput = document.getElementById('todo-input');\nconst addBtn = document.getElementById('add-btn');\nconst todoList = document.getElementById('todo-list');\n\naddBtn.addEventListener('click', () => {\n  const todoText = todoInput.value.trim();\n  if (todoText !== '') {\n    const todoItem = document.createElement('li');\n    todoItem.textContent = todoText;\n    todoList.appendChild(todoItem);\n    todoInput.value = '';\n  }\n});\n```\n\n- Components: 요소들을 웹페이지 구성의 빌딩 블록으로 생각해보세요.\n- Props: 이것들은 요소들이 서로 대화할 때 사용하는 메시지와 같습니다.\n- UseState: 웹페이지에서 무슨 일이 일어나고 있는지 추적하는 데 도움을 줍니다.\n- UseEffect: 이것은 웹페이지가 서로 다른 액션에 대응하는 방법을 관리합니다.\n\n- Create Components: 할 일 목록을 재사용할 수 있는 작은 부분들로 나누세요.\n- Use Props: 할 일 목록의 다른 부분들 사이에 정보를 전달하세요.\n- Manage State: useState로 할 일 목록에서 무슨 일이 일어나고 있는지 추적하세요.\n- Handle Side Effects: useEffect를 사용하여 새로운 할 일 항목을 가져오거나 무언가 변경될 때 페이지를 업데이트하는 등의 작업을 수행하세요.\n\n```js\n// script.js\nconst TodoApp = () => {\n  const [todos, setTodos] = React.useState([]);\n  const [todoText, setTodoText] = React.useState('');\n\n  const handleAddTodo = () => {\n    if (todoText.trim() !== '') {\n      setTodos([...todos, todoText]);\n      setTodoText('');\n    }\n  };\n\n  return (\n    <div className=\"todo-container\">\n      <input\n        type=\"text\"\n        value={todoText}\n        onChange={(e) => setTodoText(e.target.value)}\n        placeholder=\"Add a new todo\"\n      />\n      <button onClick={handleAddTodo}>Add</button>\n      <ul>\n        {todos.map((todo, index) => (\n          <li key={index}>{todo}</li>\n        ))}\n      </ul>\n    </div>\n  );\n};\n\nReactDOM.render(<TodoApp />, document.getElementById('root'));\n```\n\n<div class=\"content-ad\"></div>\n\n## 이제 React에 익숙해졌으니 창의력을 발휘해보세요! 무엇이든 시작해보고 어떤 놀라운 것들을 만들어낼 수 있는지 확인해보세요. 기억하세요, 모든 위대한 프로젝트는 간단한 아이디어로 시작됩니다. 여러분은 할 수 있어요!","ogImage":{"url":"/assets/img/2024-05-20-GettingStartedwithReactYourFunandEasyGuide_0.png"},"coverImage":"/assets/img/2024-05-20-GettingStartedwithReactYourFunandEasyGuide_0.png","tag":["Tech"],"readingTime":3},{"title":"문서에서 LLM 세부 조정을 위한 지시 생성 자동화","description":"","date":"2024-05-20 22:04","slug":"2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning","content":"\n\n\n![Automating Instruction Generation](/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_0.png)\n\n큰 언어 모델 (LLM)은 뛰어난 생성 능력으로 다양한 제품에 진입하고 있으며, 우리는 새로운 응용 분야가 버섯처럼 생겨나고 있다는 것을 발견하고 있습니다. 이러한 모델들은 일반적인 도구이며 종종 도메인 특화 지식이 부족하여 그 영향력이 다소 줄어들 수 있습니다. 이러한 유용한 도메인 지식은 분산된 기업 리포지토리에 숨겨져 있을 수 있습니다.\n\n귀하의 도메인 데이터로 사용자 정의 LLM을 세밀 조정하면 이 간극을 좁히는 데 도움이 될 수 있습니다. 이 과정으로 나아가는 데 중요한 단계 중 하나가 데이터 준비입니다. 이는 데이터의 품질이 세밀 조정된 모델의 성능에 중대한 영향을 미칠 것이기 때문에 중요한 단계입니다. 이러한 데이터 세트를 수동으로 정비하려고 하면 매우 비용이 많이 들고 시간이 많이 소요되는 작업일 수 있습니다.\n\n이 기사에서는 Mistral 7B Instruct 모델을 사용하여 내부 문서에서 지침 및 교육 데이터 세트를 자동으로 생성하는 비용 효율적인 대안을 탐색할 것입니다. 우리는 귀하의 도메인을 포괄적으로 다룰 수 있는 지침 생성의 새로운 접근법을 취할 것입니다. Mistral 7B는 또한 학습 데이터 세트 생성을 위해 검색 보조 생성 (RAG) 설정에서 사용됩니다. 한번 훈련 데이터 세트를 확보하면 이 데이터 세트를 사용하여 Mistral 7B를 실제로 세밀히 조정하여 지역 도메인 지식으로보갰습니다.MLX 프레임워크 라이브러리를 호출합니다.\n\n\n<div class=\"content-ad\"></div>\n\n지시 생성부터 모델 세밀 조정까지의 종단 간 워크플로우를 탐색할 예정이에요. 여기에서 다뤄야 할 내용이 많아요. 시작해 볼까요!\n\n## 목차\n\n1.0 주요 활성화 기술 개요\n2.0 설계 및 구현\n2.1 지시 생성\n2.2 훈련 데이터셋 생성\n2.3 Function main\n3.0 지시 생성 실행\n4.0 훈련 데이터셋 생성 실행\n5.0 MLX를 사용한 세밀 조정\n6.0 모델 유효성 검사\n7.0 최종 생각들\n\n# 1.0 주요 활성화 기술 개요\n\n<div class=\"content-ad\"></div>\n\n이 작업은 RAM이 8GB인 MacBook Air M1에서 진행될 예정입니다. 상대적으로 제한된 컴퓨팅 및 메모리 리소스 때문에 Mistral 7B Instruct v0.1 모델의 4비트 양자화 버전을 채택하고 있습니다. GGUF 형식의 이러한 양자화된 모델을 로드하기 위해 llama-cpp-python 라이브러리를 사용할 것입니다. 이 라이브러리는 llama.cpp 라이브러리의 파이썬 바인딩입니다.\n\nfaiss-cpu는 CPU를 사용하여 밀집 벡터의 효율적인 유사성 검색 및 클러스터링을 위한 라이브러리입니다. 교육 데이터 생성을 위해 RAG 기술을 채택할 것입니다. RAG 애플리케이션에는 FAISS 벡터스토어에서 관련 문서 스니펫을 검색하는 리트리버 시스템과 검색된 스니펫을 컨텍스트로 사용하여 응답을 생성하는 LLM이 포함됩니다. 이전 연구에서 앙상블 리트리버가 적합하다는 것을 보여준 바 있습니다. 그 결과물을 근거로 선택한 리트리버 목록에서 Reciprocal Rank Fusion 알고리즘을 사용하여 결과를 앙상블하고 재정렬합니다. 우리는 앙상블을 위해 BM25 리트리버와 FAISS 리트리버를 0.3:0.7의 비율로 결합할 것입니다.\n\n마지막으로 중요한 기술 부분은 세밀한 조정과 관련이 있습니다. llama.cpp 및 MLX 프레임워크 라이브러리는 세밀한 조정을 지원하기 위한 도구를 제공합니다. 후자는 Apple 실리콘을 활용하여 하드웨어 가속을 제공하여 맥에서 세밀한 조정이 매우 간편해지도록 하는 것입니다. 따라서 우리는 여기서 MLX를 채택할 것입니다.\n\n이제 개발 환경을 준비할 준비가 되었습니다. 이 프로젝트를 관리하기 위해 가상 환경을 생성합시다. 환경을 생성하고 활성화하려면 다음을 실행합시다:\n\n<div class=\"content-ad\"></div>\n\n\npython3.10 -m venv llm_tuning\nsource llm_tuning/bin/activate\n\n\n다음으로 필요한 모든 라이브러리를 설치합니다:\n\n\npip install langchain faiss-cpu sentence-transformers flask-sqlalchemy psutil unstructured pdf2image unstructured_inference pillow_heif opencv-python pikepdf pypdf\npip install mlx\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n\n\n위 마지막 줄은 M1 프로세서에서 하드웨어 가속을 사용하여 Mistral 7B를 양자화한 llama-cpp-python 라이브러리를 설치하는 과정을 포함합니다. Metal을 사용하면 계산이 GPU에서 실행됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n환경이 준비되었으니, 시스템 설계와 구현을 살펴봅시다.\n\n# 2.0 설계 및 구현\n\n그림 1에 설명된대로 데이터셋 생성 시스템에는 두 개의 모듈이 있습니다.\n\n<img src=\"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_1.png\" />\n\n<div class=\"content-ad\"></div>\n\nLoadVectorize 모듈은 최근에 출시된 (2023 년 12 월) 440 페이지의 IT 벤더 배포 가이드를 로드하는 작업을 포함합니다. 또한 문서 분할 및 벡터화를 처리하며, BM25 검색기의 인스턴스화도 처리합니다. 이 모듈은 이전 작업에서 소개되었고 여기서 그대로 사용되었습니다 [1].\n\n두 번째 모듈에는 두 가지 주요 기능이 포함되어 있습니다. 첫 번째 기능은 지시 생성을 다룹니다. 이는 QA 체인을 사용하여 문서 청크 목록의 맥락에서 지시 생성을 수행하는 작업입니다. 두 번째 기능은 앙상블 검색기의 인스턴스화를 수행한 다음 앙상블 검색기의 맥락에서 지시 목록을 대상으로 QA 체인을 생성하는 작업을 합니다.\n\n이제 두 번째 모듈을 깊이 있는 살펴보겠습니다.\n\n## 2.1 지시 생성\n\n<div class=\"content-ad\"></div>\n\n이 종단 간 워크플로우에서는 Riverbed SteelHead에 대한 샘플 400페이지 이상의 PDF 문서를 도메인 지식으로 사용하고 있습니다. Riverbed SteelHead는 응용 프로그램 가속 솔루션입니다. 첫 번째 단계로 Mistral 7B를 언어 모델로 사용하여 이 문서와 관련된 지침(또는 프롬프트)를 생성할 것입니다.\n\n여기서 주요 설계 과제는 LLM이 아직 익숙하지 않은 영역에 대해 어떤 지시를 생성해야 하는지를 어떻게 판단할 것인가입니다. 이는 모든 내부 문서에 일반적으로 적용될 수 있는 과제입니다. 벡터화 단계의 일환으로, FAISS vectorstore는 문서 청크에 대한 참조를 갖고 있습니다. 이 청크들은 총체적으로 도메인 지식을 형성합니다. 이 지시 생성 함수의 주요 아이디어는 각 청크를 개별적인 컨텍스트로 사용하여 LLM이 지시를 생성하게 하는 것입니다. 각 청크가 가진 지식을 포괄적으로 다룰 수 있는 지침을 제공하기 위해 모든 문서 청크에 대해 이상적으로는 지침을 생성해야 합니다. 생성된 지시의 수는 채택된 문서 청크 크기에 비례해야 합니다. 시간과 플랫폼 리소스 제한으로 인해 이번 데모에서는 100개의 임의의 문서 청크에 대해 두 가지 질문을 생성할 것입니다.\n\n문서 청크에 액세스하려면 FAISS 객체에서 docstore 객체를 가져와야 하며, 모든 문서 청크를 나타내는 docstore_id 목록을 가져오십시오. 각 반복에서 관련 문서 청크를 찾아 이를 질의 체인을 위한 컨텍스트로 사용합니다.\n\n이 지시 생성을 위한 프롬프트는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n선택된 각 문서 청크를 반복하면서 해당 청크를 컨텍스트로 하고 위 프롬프트를 사용하여 QA 체인을 호출합니다. 생성된 지시 사항은 진행 상황을 나타내며, 소요된 시간과 함께 콘솔에 표시됩니다. 생성된 지시 사항은 instructions.txt 파일에 저장됩니다. 생성 진행 상황을 나타내기 위해 각 반복마다 현재 질문 번호와 소요된 시간이 표시됩니다. 이해를 돕기 위해 다음 목록은 generate_instructions 함수의 코드를 보여줍니다.\n\n```js\ndef generate_instructions(db,QA_PROMPT,llm) -> None:\n    output_parser = StrOutputParser()\n    # Custom QA Chain\n    chain = (\n        {\"context\": RunnablePassthrough() , \"question\": RunnablePassthrough()}\n        | QA_PROMPT\n        | llm\n        | output_parser\n        )\n\n    # access docstore and docstore id for 100 random chunks\n    vs = db.__dict__.get(\"docstore\")\n    docstore_id_list = list(db.__dict__.get(\"index_to_docstore_id\").values())\n    rand_doc_id_list = random.choices(docstore_id_list, k=200)\n\n    query = '''\n    제공된 컨텍스트를 기반으로 SteelHead에 대한 두 가지 질문을 생성하세요. 질문은 SteelHead WAN 가속 및 관련 개념에 관한 것이어야 합니다. 질문은 다음 중 하나로 시작해야 합니다: \"What\", \"How', \"Is there a\", \"What are the\", \"How do I\", \"When is it\", \"Does SteelHead have\", \"How to\", \"What is the difference\", \"Which\", \"List\". 각 질문에 대한 답변이나 범주를 제공할 필요는 없습니다.\n    '''\n    qfile = open(\"instructions.txt\", \"w\")\n    start_gen = timeit.default_timer()\n    for i,doc_id in enumerate(rand_doc_id_list):\n        start = timeit.default_timer()\n        a_doc = vs.search(doc_id)\n        result = chain.invoke({\"question\": query, \"context\": a_doc.page_content})\n        resp_time = timeit.default_timer() - start # seconds\n        print(f'{\"-\"*50}\\nQ #{i}: {result}\\nTime: {resp_time}\\n{\"-\"*50}\\n')\n        qfile.write(result)\n    qfile.close()\n    # total time for generation\n    gen_time = timeit.default_timer() - start_gen # seconds\n    print(f'Total generation time => {timedelta(seconds=gen_time)}')\n```\n\n이제 이 모듈의 두 번째 주요 함수를 살펴봅시다.\n\n## 2.2 Training Dataset Generation\n\n<div class=\"content-ad\"></div>\n\n지시 사항이 준비되었으면 이제 훈련 데이터 세트 생성을 진행할 수 있습니다. 이전과 마찬가지로 Mistral 7B를 LLM으로 사용하며, 이번에는 RAG 설정을 사용합니다. 우리는 FAISS 최대 여유도(MMR) 및 BM25 검색기의 EnsembleRetriever를 사용할 것입니다. 이전에 언급한 바와 같이 이러한 검색기 목록에 대해 0.3:0.7 비율이 최상의 정확도 성능을 달성했음을 보여주었습니다.\n\n지시 사항을 반복하면 LLM에 대한 쿼리를 실행하여 해당 답변을 사용하여 다음 형식의 JSON 문자열을 생성합니다:\n\n`s`[INST] 'instruction'[/INST] 'answer'`/s`\n\n훈련 데이터 세트를 준비하면 이 데이터 세트의 80%가 훈련에 사용되어 train.jsonl에 저장됩니다. 남은 20%의 데이터 세트는 검증에 사용되어 valid.jsonl로 저장됩니다. 아래 목록은 위 절차를 generate_training 함수로 캡처한 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```python\ndef generate_training(db, bm25_r, QA_PROMPT, llm) -> None:\n    # retriever 생성\n    faiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 3}, max_tokens_limit=1000)\n    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_r, faiss_retriever], weights=[0.3, 0.7])\n    output_parser = StrOutputParser()\n    # 사용자 지정 QA Chain\n    chain = (\n        {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n        | QA_PROMPT\n        | llm\n        | output_parser\n    )\n    with open('instructions.txt') as tfile:\n        instructions = tfile.readlines()\n    start_t_gen = timeit.default_timer()\n    train_lines = list()\n    for i, instruction in enumerate(instructions, start=1):\n        print(f\"처리 중 ({i}/{len(instructions)}):\")\n        start = timeit.default_timer()\n        try:\n            answer = chain.invoke(instruction)\n        except Exception as e:\n            # LLM으로 답변할 수 없는 질문 건너뛰기\n            print(f'답변 실패 => {e}')\n            continue\n        resp_time = timeit.default_timer() - start # 초\n        print(f'{\"-\"*50}\\n질문 #{i}: {instruction}\\n답변: {answer}\\n소요 시간: {resp_time}\\n{\"-\"*50}\\n')\n        result = json.dumps({\n            'text': f'<s>[INST] {instruction}[/INST] {answer}</s>'\n        }) + \"\\n\"\n        # 임시 파일에 작성\n        with open('train_valid.jsonl', 'a') as file:\n            file.write(result)\n        train_lines.append(result)\n    gen_time = timeit.default_timer() - start_t_gen # 초\n    with open('train.jsonl', 'w') as file:\n        file.writelines(train_lines[:int(len(train_lines) * 0.2)])\n    with open('valid.jsonl', 'w') as file:\n        file.writelines(train_lines[int(len(train_lines) * 0.2):])\n    print(f'총 학습 생성 시간 => {timedelta(seconds=gen_time)}')\n```\n\n위의 주요 함수 중 하나를 호출하려면 다음에 설명된 대로 main 함수를 사용합니다.\n\n## 2.3 main 함수\n\n이 함수에서 두 함수에서 사용하는 여러 개의 공통 개체가 인스턴스화됩니다. 먼저 프롬프트 템플릿이 정의됩니다. 그런 다음 LlamaCpp를 사용하여 4비트 Mistral 7B Instruct 모델을 GGUF 형식으로 로드합니다. 그런 다음 pdf 문서를 벡터화하고 해당 FAISS 객체에 대한 참조 및 BM25 검색기를 얻습니다.\n\n<div class=\"content-ad\"></div>\n\n두 가지 생성 함수 중 어느 것이든 쉽게 호출할 수 있도록 명령줄 옵션을 사용해 보겠습니다. 'main' 함수는 최대 두 개의 부울 인수를 받아들이게 됩니다. 이는 제공된 명령줄 옵션에 의해 제어될 것입니다. 명령줄 옵션을 통해 발표나 훈련 데이터셋 생성 작업 중 어떤 것을 실행할지 결정하기 위해 라이브러리 argparse를 활용하겠습니다.\n\n아래 코드는 이러한 명령줄 옵션 처리 및 'main' 함수를 포함하고 있습니다.\n\n```python\ndef main(is_gen_instruct=False, is_gen_training=False):\n    # 프롬프트 템플릿\n    qa_template = \"\"\"<s>[INST] 이 신종은 도움이 되는 조수입니다.\n    아래 컨텍스트를 사용하여 이하의 질문에 정확하고 간결하게 답하세요:\n    {context}\n    [/INST] </s>{question}\n    \"\"\"\n\n    # 프롬프트 인스턴스 생성\n    QA_PROMPT = PromptTemplate.from_template(qa_template)\n\n    llm = LlamaCpp(\n        model_path=\"./models/mistral_7b_gguf/mistral-7b-instruct-v0.1.Q2_K.gguf\",\n        temperature=0.01,\n        max_tokens=2000,\n        top_p=1,\n        verbose=False,\n        n_ctx=3000\n    )\n    db, bm25_r = LoadVectorize.load_db()\n    if is_gen_instruct:\n        generate_instructions(db, QA_PROMPT, llm) \n    elif is_gen_training:\n        generate_training(db, bm25_r, QA_PROMPT, llm) \n\nif __name__ == \"__main__\":\n    # 파서 초기화\n    parser = argparse.ArgumentParser(\"LLM 미세 조정을 위한 명령어 생성 스크립트\")\n    group = parser.add_mutually_exclusive_group()\n\n    # 선택적 상호배제 인수 추가\n    group.add_argument(\"-i\", \"--instructions\", action='store_true', help = \"지시사항 생성\")\n    group.add_argument(\"-t\", \"--training\", action='store_true', help = \"훈련 및 검증 데이터 생성\")\n\n    # 명령줄에서 인수 읽기\n    args = parser.parse_args()\n    if args.instructions:\n        main(is_gen_instruct=args.instructions)\n    elif args.training:\n        main(is_gen_training=args.training)  \n```\n\n이로써 데이터 생성 시스템 구현이 완료되었습니다. 이 시스템에 대한 전체 코드는 다음 GitHub 저장소에서 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n자, 이제 한 번 시도해 봅시다!\n\n# 3.0 지시 생성 실행\n\n이 연습에서는 코드를 -i 명령행 옵션과 함께 실행하여 지시 생성 프로세스를 시작할 것입니다. 다음 콘솔 출력 추출은 200개의 질문을 생성하기 위한 실행을 나타냅니다. 이 과정 전체는 제 맥에서 2시간 넘게 소요되었습니다.\n\n```js\n$ python main.py -i\n\n--------------------------------------------------\nQ #0: \n1. SteelHead에서 QoS 설정을 어디서 찾을 수 있을까요?\n2. MX-TCP와 TCP 간에 패킷 손실 처리 측면에서 차이가 있나요?\n시간: 57.88847145799991\n--------------------------------------------------\n\n--------------------------------------------------\nQ #1: \n1. SteelHead에서 SSL 구성 정보를 어디서 찾을 수 있을까요?\n2. SSL 구성을 위해 클라이언트 가속기 간에 신뢰 관계가 필요한가요?\n시간: 47.30005858300001\n--------------------------------------------------\n\n--------------------------------------------------\nQ #2: \n1. 클러스터 내의 SteelHead 간 연결 전달을 활성화하는 구성은 어디에 있나요?\n2. 동일 SteelHead에서 다중 인터페이스를 사용하는 것과 ITD 고가용성 배포를 위해 여러 SteelHead를 사용하는 것 사이에 차이가 있나요?\n시간: 70.70811329100025\n--------------------------------------------------\n\n--------------------------------------------------\nQ #3: \n1. PBR 배포에 사용되는 SteelHead에서 CDP를 어디서 활성화할까요?\n2. SteelHead에서 CDP를 활성화하기 위해 사용해야 하는 특정 명령이 있나요?\n시간: 68.81058954199989\n--------------------------------------------------\n...\n\nQ #99: \n1. SteelHead WAN 가속화의 정확한 주소 할당은 무엇인가요?\n2. 정확한 주소 할당은 SteelHead에서 연결 풀 가속화를 어떻게 가능하게 할까요?\n시간: 63.51242004099913\n--------------------------------------------------\n\n총 생성 시간 => 2:06:10.565294\n```\n\n<div class=\"content-ad\"></div>\n\n생성된 지침을 검토한 결과, 많은 좋은 질문이 나왔어요. 그런데 \"어디서 찾을 수 있나요\"와 같은 질문들이 많았는데, 이는 도메인 지식을 얻는 데 도움이 되지 않는다고 생각해서 목록에서 제외했어요. 또한, 일부 문서 청크에 대한 질문들이 거의 동일한 경우가 많았고, 이러한 중복들은 제거했어요. 마지막으로, 부정확하거나 의미 없는 질문들이 몇 개 있었어요. 이 모든 정제 작업을 거친 뒤에 좋은 질문이 150개 남았어요. 또한, 질문들이 번호 매겨지고 예상치 못한 서식이 있어서 조정해야 했어요. 이것은 다음 작업을 위해 데이터 품질을 보장하기 위한 인간의 개입이 필요함을 명확히 보여줍니다.\n\n지침이 준비되었으니, 이제 훈련 데이터 집합 생성을 진행합시다.\n\n# 4.0 훈련 데이터 집합 생성 실행\n\n이제 동일한 스크립트를 -t 옵션을 사용하여 실행하여 훈련 및 검증 데이터 집합 생성을 시작합니다. 제 리소스가 제한된 기기에서는 시간이 많이 걸렸어요. 다행히 콘솔 출력을 통해 진행 상황을 잘 파악할 수 있었어요. 이 실행 중에 발생하는 열의 양 때문에 Mac을 일부러 공중에 두어 냉각 효과를 향상시켰어요. 아래는 이 실행의 콘솔 출력 일부입니다:\n\n<div class=\"content-ad\"></div>\n\n\n```js\n$ python main.py -t\n\n처리 중 (1/150):\n--------------------------------------------------\nQ #1: MX-TCP와 TCP 간의 데이터 손실 처리 방식에는 차이가 있나요?\nA:\n네, MX-TCP와 TCP 간에는 데이터 손실 처리에 차이가 있습니다. MX-TCP는 쓰루풋 감소 없이 데이터 손실을 처리하기 위해 설계되었으며, TCP는 일반적으로 데이터 손실 시 쓰루풋이 감소합니다. MX-TCP는 WAN을 통해 전방 오류 수정을 통해 데이터 손실을 효과적으로 처리합니다.\n시간: 152.08327770899996\n\n처리 중 (2/150):\n--------------------------------------------------\nQ #2: 클러스터 내의 SteelHeads 간 연결 전달을 활성화하기 위한 구성은 어디에 있나요?\nA:\n클러스터 내의 SteelHeads 간 연결 전달을 활성화하려면 각 SteelHead의 CLI에서 두 SteelHeads의 in-path0_0 IP 주소를 이웃으로 구성해야 합니다. 그런 다음 다음 명령을 각 SteelHead의 CLI에서 입력할 수 있습니다:\n\nenable\nconfigure terminal\nSteelHead communication enable\nSteelHead communication multi-interface enable\nSteelHead name <SteelHead name> main-ip <SteelHead IP address>\n\n연결 전달을 활성화한 후, ITD 배포에서 더 큰 탄력성과 중복성을 제공하기 위해 fail-to-block 및 allow-failure 명령을 구성할 수 있습니다.\n시간: 215.70585895799923\n\n처리 중 (3/150):\n--------------------------------------------------\nQ #3: 동일한 SteelHead에서 여러 인터페이스를 사용하는 것과 ITD 고가용성 배포를 위해 여러 SteelHeads를 사용하는 것 사이에는 차이가 있나요?\nA:\n네, 동일한 SteelHead에서 여러 인터페이스를 사용하는 것과 ITD 고가용성 배포를 위해 여러 SteelHeads를 사용하는 것 사이에 차이가 있습니다. 동일한 SteelHead에서 여러 인터페이스를 사용하면 하나의 SteelHead 이상의 가속 쓰루풋 용량을 제공할 수 있지만, 여러 SteelHeads를 사용하는 것보다 동일한 수준의 중복성이나 탄력성을 제공하지 않을 수 있습니다. 반면, 여러 SteelHeads를 사용하면 더 큰 중복성과 탄력성을 제공할 수 있지만, 동일한 SteelHead에서 여러 인터페이스를 사용하는 것만큼의 가속 쓰루풋 용량을 제공하지 않을 수 있습니다.\n시간: 179.73986179200074\n\n...\n\n처리 중 (150/150):\n--------------------------------------------------\nQ #150: SteelHead에서 올바른 주소 지정은 연결 풀 가속을 어떻게 가능하게 합니까?\n\nA:\n\n올바른 주소 지정을 통해 SteelHead에서 연결 풀 가속을 가능하게 함으로써 미리 서로간에 다수의 TCP 연결을 생성할 수 있습니다. 이는 올바른 주소 지정이 TCP/IP 패킷 헤더의 특정 값을 사용하므로 SteelHeads가 필요한 클라이언트 및 서버 IP 주소 및 포트 유형을 감지할 수 있기 때문입니다. 투명 주소 지정이 활성화된 경우 SteelHeads는 클라이언트 및 서버 IP 주소 및 포트 유형을 감지할 수 없기 때문에 미리 TCP 연결을 생성할 수 없습니다. 가속하려는 연결 수가 SteelHead 모델의 한계를 초과하는 경우, 초과된 연결은 SteelHead에 의해 가속되지 않고 통과됩니다.\n시간: 159.28865737500018\n--------------------------------------------------\n\n\n총 교육 세대 생성 시간 => 9:20:06.321521\n```\n\n이 교육 데이터세트 생성에는 9시간 이상이 소요되었습니다! 문서의 일부에 대해 언어 모델이 답변을 생성하지 못한 경우도 몇 가지 발생했습니다. 1,200개 이상의 청크가 포함된 선택된 문서를 종합적으로 다루려면 2,000개 이상의 지시어가 필요할 수 있으며, 이는 실행 기간 동안 내 Mac이 지속적인 고온을 견딜 수 있을 때에만 가능할 것입니다! 그래도 전체적인 프로세스를 보여주기 위해 우리는 다음 제한된 교육 데이터세트로 세밀한 조정을 진행할 것입니다.\n\n# 5.0 MLX를 사용한 세밀 조정\n\nMLX는 Apple 실리콘 기반의 머신 러닝 연구를 위한 배열 프레임워크입니다 [2]. Llama, Mistral 및 TinyLlama와 같은 LLM에 대한 텍스트 생성 및 세밀 조정에 사용될 수 있습니다. 세밀 조정을 위해 모델은 MLX에서 인식하는 형식이어야 하므로 이전에 사용했던 GGUF 버전을 사용할 수 없습니다. MLX는 mlx-examples Github 저장소의 스크립트를 제공하여 전체 워크플로우를 지원합니다. 아래와 같이 생성 시스템의 디렉터리 내에서 MLX 예제 저장소를 클론해 보겠습니다:\n\n\n<div class=\"content-ad\"></div>\n\n```bash\n$ git clone https://github.com/ml-explore/mlx-examples.git\n```\n\nHuggingFace에서 Mistral 7B를 다운로드하고 4비트 모델로 양자화하려면 convert.py 스크립트를 사용할 수 있습니다. 이 스크립트는 기본적으로 입력으로 HuggingFace repo를 취하고 결과를 디렉토리 mlx_model에 출력합니다. 다음은 샘플 실행 출력입니다:\n\n```bash \n$ python mlx-examples/lora/convert.py --hf-path mistralai/Mistral-7B-Instruct-v0.1 -q\n[INFO] Loading\nmodel-00003-of-00003.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54G/4.54G [33:52<00:00, 2.23MB/s]\nmodel-00001-of-00003.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.94G/4.94G [36:15<00:00, 2.27MB/s]\nmodel-00002-of-00003.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████\n```\n\n<div class=\"content-ad\"></div>\n\n이전 섹션에서의 훈련 데이터셋으로 모델을 세밀하게 조정할 준비가 되었습니다. MLX는 파라미터 효율적 세밀조정(PEFT)을 LoRA를 통해 지원합니다. LoRA는 모델의 일부 파라미터를 업데이트하는 데 중점을 둡니다. 종종 특정 레이어나 모델의 일부를 동결하는 것을 포함합니다. 이 방법을 사용하면 세밀조정이 빨라집니다. 또한 MLX는 양자화된 모델에서 QLoRA를 사용합니다.\n\n이제 lora.py 스크립트가 도움이 될 것입니다. 모델이 양자화되었음을 감지하면 이 스크립트는 자동으로 QLoRA를 사용하도록 전환합니다. --data 옵션은 훈련 및 검증 데이터셋이 포함된 디렉터리를 지정하는 데 사용됩니다. --lora-layers 옵션은 세밀 조정할 레이어 수를 설정하는 데 사용됩니다. 그리고 --iters 옵션은 훈련 반복 횟수를 지정합니다. 학습률 및 샘플링 온도와 같은 다른 선택적 설정도 있으며, 세밀 조정을 제어하는 데 사용할 수 있습니다. 모든 도움말 목록을 보려면 단순히 -h 옵션을 사용하여 스크립트를 실행하십시오.\n\n다음 콘솔 출력은 저희 모델에 대한 세밀 조정 실행을 보여주며, 저의 Mac에서 약 40분이 소요되었습니다:\n\n```js\n$ python mlx-examples/lora/lora.py \\\n  --train \\\n  --model ./mlx_model \\\n  --data ./ \\\n  --batch-size 1 \\\n  --lora-layers 2 \\\n  --iters 1000\n미리 훈련된 모델 로딩 중\n총 파라미터 1242.550M\n조정 가능한 파라미터 0.213M\n데이터셋 로딩 중\n훈련 중\n반복 1: 검증 손실 3.565, 소요 시간 32.649초\n반복 10: 훈련 손실 3.008, Iter/sec 0.401, 토큰/sec 80.419\n...\n반복 1000: 훈련 손실 1.511, Iter/sec 0.361, 토큰/sec 74.861\n반복 1000: 검증 손실 1.777, 소요 시간 31.679초\n반복 1000: adapter 가중치를 adapters.npz에 저장했습니다.\n```\n\n<div class=\"content-ad\"></div>\n\n트레이닝 시작 시 3.008의 손실이 있었고, 마지막 반복 중에는 1.511까지 떨어졌어요. 기본적으로 모델은 매 100번의 반복마다 저장됩니다. 제 컴퓨터의 자원 한정 때문에 lora 레이어를 두 개 이상 사용하면 시스템이 메모리 부족으로 작동을 멈춥니다. 그러나 귀하의 컴퓨터에 더 많은 RAM이 있다면, 레이어 수를 더 많이 실험해보세요.\n\n미세 조정이 완료된 후, 결과 모델은 현재 디렉토리에 adapters.npz로 저장됩니다. 변경 사항을 기본 모델에 병합하려면 MLX 스크립트 fuse.py를 사용할 수 있습니다. 결과로 얻은 결합된 모델을 로컬 디스크에 저장하거나 선택한 HuggingFace 저장소에 푸시할 수 있습니다. 아래 fuse.py 실행은 모델을 로컬 디렉토리 ./models/mistral7b에 저장할 겁니다:\n\n```js\n$ python mlx-examples/lora/fuse.py --model ./mlx_model  --adapter-file ./adapters.npz --save-path ./models/mistral7b\n미세 조정된 모델 로딩 중\n$\n```\n\n저희의 트레이닝 데이터셋이 상당히 한정적이므로, 병합을 포기하기로 결정했어요. 이제 모델 검증을 준비할 차례입니다.\n\n<div class=\"content-ad\"></div>\n\n# 6.0 모델 유효성 검사\n\n모델의 생성 능력을 테스트하려면 여전히 스크립트 lora.py를 사용할 수 있습니다. 아래는 생성을 위한 기본 사용 방법입니다:\n\n```js\n$ python mlx-examples/lora/lora.py --model ./mlx_model \\\n    --max-tokens 1000 \\\n    --prompt '스틸헤드 경로 선택의 목적은 무엇입니까?'\n```\n\nFine-tuning이 LLM에 도움이 되었는지 확인하기 위해 기본 및 fine-tuned 모델의 생성 테스트를 실행할 수 있습니다. 아래는 두 모델의 생성을 보여줍니다:\n\n<div class=\"content-ad\"></div>\n\n\n### 기본 모델 테스트\n\n$ python mlx-examples/lora/lora.py --model ./mlx_model --max-tokens 1000 --prompt 'SteelHead 경로 선택의 목적은 무엇입니까?'   \n사전 훈련된 모델 로드 중\n총 매개변수 1244.041M\n훈련 가능한 매개변수 1.704M\n데이터셋 로드 중\n생성 중\nSteelHead 경로 선택의 목적은 무엇입니까?\n\nWAN 최적화 솔루션의 경로 선택 기능은 네트워크를 통해 전송되는 데이터 양을 줄이고 데이터 소스와 클라이언트 사이의 왕복 횟수를 최소화하여 단일 왕복이 소요되는 데이터 전송을 최소화하여 시간 소요와 추가적인 네트워크 트래픽을 줄이는 것입니다...\n==========\n\n### 세부 조정된 모델\n\n$ python mlx-examples/lora/lora.py --model ./mlx_model --adapter-file ./adapters.npz --max-tokens 1000 --prompt 'SteelHead 경로 선택의 목적은 무엇입니까?'\n사전 훈련된 모델 로드 중\n총 매개변수 1244.041M\n훈련 가능한 매개변수 1.704M\n데이터셋 로드 중\n생성 중\nSteelHead 경로 선택의 목적은 무엇입니까?\n\n네트워크 경로 선택 기능의 목적은 SaaS 응용 프로그램 트래픽을 기반으로 두 위치 사이에서 가장 효율적인 경로를 선택하는 것입니다. SteelHead 네트워크 경로 선택은 네트워크 트래픽을 특정 응용 프로그램에 액세스하기 위해 특정 경로(네트워크 경로 또는 WAN 경로 연결)를 사용하고 네트워크 성능을 보장하기 위해 트래픽을 우선순위로 처리할 수 있습니다. 이 기능은 여러 경로(다중 WAN 링크, MPLS 연결 및 LAN 등)이 있는 환경에서 특히 유용합니다.\n\n\n세부 조정된 모델 테스트는 lora.py를 --adapter-file 옵션과 어댑터 파일명을 함께 사용하여 실행하는 것을 포함합니다. 그 이외에는 모든 것이 동일합니다.\n\n비교를 쉽게하기 위해 기본 및 세부 조정된 모델 간의 두 실행 결과를 비교한 결과가 Table 1에 나타납니다. 두 쿼리에 대한 기본 모델의 답변은 모두 잘못되었습니다. 세부 조정된 모델의 답변은 거의 정확하지만 일부 오류가 있습니다. 그럼에도 불구하고 비교적 작은 교육 데이터 집합을 사용하여 세세하게 조정한 모델은 여전히 비교적 잘 학습할 수 있습니다.\n\n<img src=\"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_2.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n# 7.0 최종 소견\n\nLLM은 공공 도메인의 정보가 풍부한 영역에서 그들의 생성 능력으로 뛰어납니다. 그러나 이산 기관 저장소에 숨겨진 많은 도메인 지식이 탭되기를 기다리고 있습니다.\n\n본문에서는 문서 조각마다 Mistral 7B를 사용하여 일정 수의 지침을 생성하여 문서의 종합적인 커버리지를 보장하는 지침 생성 접근 방법을 소개했습니다. 교육 데이터 셋 생성을 위해 우리는 다시 앙상블 검색기를 활용한 Mistral 7B를 RAG 설정에서 사용했습니다. 그런 다음 이 데이터 셋을 사용하여 Apple 실리콘에 최적화된 라이브러리 MLX와 QLoRA 기술을 사용하여 파인튜닝을 수행했습니다. 상대적으로 작은 교육 데이터 셋으로도 이 지식을 파악하는 능력이 향상된 것을 보는 것은 유망한 것입니다.\n\n읽어 주셔서 감사합니다!\n\n<div class=\"content-ad\"></div>\n\n참고 문헌\n- Querying Internal Documents using Mistral 7B with Context from an Ensemble Retriever\n- [GitHub 링크](https://github.com/ml-explore/mlx)","ogImage":{"url":"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_0.png"},"coverImage":"/assets/img/2024-05-20-AutomatingInstructionGenerationoffanyDocumentforLLMFine-Tuning_0.png","tag":["Tech"],"readingTime":20},{"title":"파이썬만 활용해 경험 없이 아름다운 웹 앱 만드는 방법","description":"","date":"2024-05-20 22:02","slug":"2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience","content":"\n\n## FastAPI, Jinja2 및 DaisyUI 사용하기.\n\n또 다른 주말, 또 다른 가려운 부분이 생겼네요. 개인적으로 저는 하드웨어든 소프트웨어든 심지어 기계 작업 프로젝트든 매체에 상관없이 무언가를 만드는 것을 좋아합니다. 저와 같은 사람들에게는 한 가지나 한 가지 기술에 집중하는 것이 어려운 도전이죠. 무언가를 만드는 욕구가 만만치 않아 빨리 만들고 싶어해요 (자랑할 만한 것은 아닐지도 모르죠). 내 생각에는 웹 앱을 만드는 것은 상당한 인내심과 지속적인 적응과 학습 의지가 필요한 일입니다. 제가 가장 좋아하는 일은 아닙니다. 아마 당신도 공감할 수 있을 겁니다.\n\n지난 블로그 글에서 읽었겠지만, 매주 새로운 기술을 탐험하기 위해 떠나는 나의 여정 중 하나로, 파이썬 웹 프레임워크인 FastAPI에 몰두해보기로 했습니다. 또 다른 신기술에 뛰어드는 것 대신에, FastAPI를 사용하여 순수하게 Python만으로 (JavaScript 사용 안 함) 기능적인 웹 앱을 개발하는 데 초점을 맞추기로 했습니다. 이 글을 쓰기 전에 Vue.js에도 조금 손을 대봤는데, 흥미로웠지만 복잡성을 더하고, 제 JavaScript 능력은 그리 높지 않다는 것을 깨달았습니다. 처음부터 다시 시작했죠.\n\n이 글에서는 주변 청크를 검색하는 기존 FastAPI 프로젝트를 확장해 나갈 것입니다. 더 많은 내용을 알고 싶으시면 이전 글을 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n기술 탐구 내역:\n\n- FastAPI — 파이썬 웹 프레임워크\n- Jinja2 — 템플릿 엔진\n- DaisyUI — Tailwind CSS용 구성 요소 라이브러리\n\n경험이 풍부한 웹 앱 개발자나 프로그래밍 전문가라면, 본 문서가 새로운 통찰을 제공하지는 않을지도 모릅니다. 그러나 Python으로 웹 앱을 구축하는 세부 사항에 대해 궁금해하는 분이나 Streamlit과 같은 것보다 더 많은 제어권을 제공하는 Python 웹 개발자 채용 공고에 대해 궁금해하는 분이라면, 여기가 바로 당신이 찾던 곳입니다. 함께 과정을 탐험해보겠습니다.\n\n솔직히 말씀드리자면, FastAPI를 사용해 프론트엔드를 구축하는 것은 쉬웠지만, 앱과 tailwindcss (DaisyUI)를 통합하는 것이 저에게 가장 시간이 많이 걸렸습니다. 그 부분을 건너뛸 수도 있었지만, 웹 앱을 구축하는 유일한 이유는 나에게 창의력을 펼칠 수 있는 자유가 있다는 점이라는 사실을 인정해야 합니다. 누가 멋진 사용자 인터페이스를 좋아하지 않겠습니까? 그러니 더 이상 말이 필요 없으니, 함께 확인해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n# DaisyUI란 무엇인가요?\n\n**모든 사람들이 멋진 사용자 인터페이스를 좋아합니다**, 하지만 말하는 것이 맞다고 여겨지며, CSS 스타일을 작성하는 일은 종종 사람들이 웹 앱의 시각적으로 매력적인 프론트엔드를 만드는 것을 꺼리게 합니다. CSS에 대해서 말할 때 **마치 녹음된 음반이라도 듣는 것 같은데**, 제가 CSS에 관해서는 좀게 놀기 싫은 타입이라서 대개 제 할 일 목록의 제일 뒷부분에 두게 되죠. 학교에서 미술 수업? 그냥 턱걸이로 벗어난다고 말해야 할 것 같아요. CSS의 픽셀 완벽한 스타일링 세부사항에 뛰어드는 것이 나를 즐겁게 하진 않습니다.\n\n저와 같은 사람들을 위해(그렇습니다, Tailwind는 프로덕션 급 앱에서도 사용됩니다), Tailwind는 생명을 구원해줍니다. **HTML 컴포넌트를 위한 미리 정의된 스타일과 템플릿을 제공하여**, 특정 클래스를 추가함으로써 쉽게 커스터마이즈할 수 있습니다.\n\nTailwind는 엄청난 인기를 얻었으며, 이제는 다양한 분야의 애플리케이션에서 상용품으로 자리 잡았습니다. 만약 잘 안다면, **앱이 순정 Tailwind CSS를 사용하고 있는지 종종 알아낼 수 있습니다**. Tailwind 클래스를 세심하게 조정할 수도 있고, 아니면 Tailwind에 맞게 제작된 컴포넌트 라이브러리를 사용하는 손쉬운 방법을 선택할 수도 있습니다. 마치 Tailwind라는 Tailwind를 사용하는 것과 같은 이치인 거죠, ㅋㅋ. 여러 가지 컴포넌트 라이브러리가 있고, **DaisyUI가 그중 하나입니다**.\n\n<div class=\"content-ad\"></div>\n\nDaisyUI를 사용하는 것에는 수많은 기능과 이점이 있습니다. 더 자세히 알아보려면 그들의 웹사이트를 방문하시면 됩니다. 제가 제일 좋아하는 기능 중 하나는 테마 선택 기능입니다. 다양한 테마 중에서 선택할 수 있어서 완벽한 색상 구성을 찾는 데 어려움을 겪지 않을 것입니다.\n\n# 웹 앱의 파일 구조\n\n```js\nroot\n|-app\n  |- chroma_db\n  |- functions.py\n  |- main.py\n  |- models.py\n  |- chroma_db\n  |- static\n    |- css\n      |- app.css\n  |- styles\n    |- app.css\n  |- templates\n    |- index.html\n  |- files\n    |- samples.pdf\n```\n\nchroma_db 폴더, models.py, functions.py는 이전에 만든 프로젝트에서 이어지게 될 것입니다. 이전에 FastAPI에 익숙하지 않으셨다면, 이 기사를 읽을 것을 권해드립니다.\n\n<div class=\"content-ad\"></div>\n\n# 사용자 인터페이스\n\n모든 기술적인 내용에 지루해하기 전에, 여기 사용자 인터페이스의 일부를 엿볼 수 있는 이미지입니다. 힘내세요!\n\n![Interface Image 0](/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_0.png)\n\n![Interface Image 1](/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_1.png)\n\n<div class=\"content-ad\"></div>\n\n비록 Behance에서 자랑할 만한 것은 아니지만, 정말 멋지게 보이지 않나요? 개인적으로 나는 기본 Times New Roman, 흑백 테마보다 훨씬 좋아하는 편이에요. 그 테마는 정말 웹 개발을 싫어하게 만들어요.\n\n만약 이 작업이 많은 노력을 필요로 할 것이라고 생각한다면, 안심하세요. 나는 오랜만에 CSS에 손 대지 않았고, 아주 빠르게 할 수 있었어요. 당신도 할 수 있다는 걸 함께 확인해봐요!\n\n# 앱을 위한 Tailwind 설정\n\nDaisyUI를 위한 CDN을 사용하거나 Tailwind 플러그인으로 설치할 수 있어요. 프로덕션 환경에서는 권장되지 않지만, 과정을 훨씬 간단하게 만드는 CDN을 사용할 수 있어요. 하지만 권장하는 방법으로, Tailwind 플러그인으로 설치할 거에요. 이 방법은 약간 더 어려울 수 있고 일부 설정 문제가 있을 수 있지만, 누가 좋은 도전을 싫어하겠어요?\n\n<div class=\"content-ad\"></div>\n\nTailwind를 설치하려면 JavaScript의 패키지 관리자 인 NPM이 설치되어 있는지 확인하십시오. NPM이 설치되어 있지 않은 경우 문서를 참조하십시오.\n\n프로젝트의 루트 디렉토리 터미널에서 다음 명령을 실행해야 합니다:\n\n```js\nnpm install -D tailwindcss\nnpx tailwindcss init\nnpm i -D daisyui@latest\n```\n\n성공적으로 실행되면 프로젝트의 루트 디렉토리에 tailwind.config.js라는 구성 파일이 생성됩니다.\n\n<div class=\"content-ad\"></div>\n\ntailwind.config.js 파일에서는 DaisyUI를 플러그인으로 추가해야 합니다. 파일은 다음과 같이 보일 것입니다:\n\n```js\nconst { default: daisyui } = require('daisyui');\n\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  content: [\"./app/templates/*.html\"],\n  theme: {\n    extend: {},\n  },\n  plugins: [\n    require(\"daisyui\")\n  ],\n  daisyui: {\n    themes: [\"light\", \"dim\", \"acid\"],\n  },\n}\n```\n\n스타일이 적용되지 않는 이유를 찾느라 시간을 많이 소비했습니다. 30분 이상을 쓴 뒤에야 HTML 파일 경로가 content 섹션에 잘못 지정되어 있어서 그랬다는 것을 발견했습니다. 비슷한 문제를 겪는다면 먼저 이 부분을 확인해 보세요. DaisyUI 테마도 여기서 설정할 수 있습니다.\n\n이제 절차를 모두 마쳤으므로 tailwind CSS 파일을 생성할 시간입니다. 주로 두 가지 파일이 생성될 것입니다: styles/app.css (입력 파일)와 static/css/app.css (클래스별 스타일이 적용된 생성된 CSS 파일).\n\n<div class=\"content-ad\"></div>\n\nstyles/app.css 파일에서 다음 tailwind 지시문을 정의하세요:\n\n```js\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\ntailwind CSS 파일을 생성하려면, 다음 명령을 실행하세요:\n\n```js\nnpx tailwindcss -i ./styles/app.css -o ./static/css/app.css --watch\n```  \n\n<div class=\"content-ad\"></div>\n\n그만입니다. Tailwind와 DaisyUI를 성공적으로 설치했습니다.\n\n# Jinja2를 사용하여 템플릿 생성하기\n\nJinja2는 Python용 템플릿 엔진으로, 애플리케이션에서 모듈식이고 동적인 HTML 콘텐츠를 만드는 데 사용됩니다. 이는 우리의 웹 애플리케이션에서 전혀 JavaScript를 사용하지 않게 될 것이므로 매우 도움이 됩니다. API에서 가져온 정보를 HTML 템플릿에 매핑하고, 그런 다음 Jinja2를 사용하여 렌더링할 것입니다. Jinja2를 사용하면 HTML 내부에 반복문, 조건문, 필터, 변수 등을 직접 사용할 수 있는 표현식을 사용할 수 있습니다. 이러한 표현식에 대해 더 알아보려면 Jinja2 문서의 관련 문서를 읽어보세요.\n\n```js\n<div>\n    <!-- 결과가 존재하는지 확인 -->\n    { if not results }\n    <h2>결과 없음</h2>\n    { endif }\n    { if results }\n    <h2>가장 가까운 이웃</h2>\n    <div id=\"results\">\n        <!-- 가장 가까운 이웃이 여기에 표시됩니다 -->\n        { for result in results }\n        <div>\n            <input type=\"checkbox\" />\n            <div>\n                <p>{ result.page_content[:25]|safe }...</p>\n            </div>\n            <div>\n                <p>{ result.page_content }</p>\n            </div>\n        </div>\n        { endfor }\n    </div>\n    { endif }\n</div>\n```\n\n<div class=\"content-ad\"></div>\n\n이 템플릿은 결과의 존재를 확인하기 위해 조건식을 사용합니다. 결과가 발견되면 각각에 대한 모달을 생성하기 위해 반복합니다. Python 프로그래밍에 익숙하다면, 이를 이해하는 데 어렵지 않을 것입니다.\n\n## HTML에 Tailwind CSS 파일 링크하기\n\nTailwind 스타일을 활성화하려면 다음 코드 줄을 HTML의 `head` 태그에 추가하여 링크해야 합니다.\n\n```js\n<link rel=\"stylesheet\" href=\"{url_for('static',path='css/app.css')}\">\n```\n\n<div class=\"content-ad\"></div>\n\n## 템플릿에 Tailwind 클래스 추가하기\n\n이 글을 길게 만들기보다는 Tailwind 클래스에 너무 깊게 파고들지 않을 거에요. 여기에서 다양한 유틸리티 클래스에 대해 읽을 수 있어요. Tailwind 클래스를 사용하면 위의 HTML이 다음과 같이 보일 거에요:\n\n```js\n<div class=\"max-w-md mx-auto\">\n    <!-- 결과가 있는지 확인하기 -->\n    { if not results }\n    <h2 class=\"text-lg font-semibold mb-2 text-info\">결과를 찾을 수 없습니다</h2>\n    { endif }\n    { if results }\n    <h2 class=\"text-lg font-semibold mb-2 text-info\">가장 가까운 이웃들</h2>\n    <div id=\"results\">\n        <!-- 가장 가까운 이웃들이 여기에 표시됩니다 -->\n        { for result in results }\n        <div class=\"collapse bg-base-200 mb-4\">\n            <input type=\"checkbox\" />\n            <div class=\"collapse-title text-xl font-medium text-primary\">\n                <p>{ result.page_content[:25]|safe }...</p>\n            </div>\n            <div class=\"collapse-content\">\n                <p>{ result.page_content }</p>\n            </div>\n        </div>\n        { endfor }\n    </div>\n    { endif }\n</div>\n```\n\n요약하자면, 일부 Tailwind 유틸리티 클래스는 다음을 나타냅니다:\n\n<div class=\"content-ad\"></div>\n\n- text-lg: 큰 글꼴 크기\n- mb-2: 2단계의 하단 여백\n- bg-base-200: 베이스 클래스의 배경 색상. 이것은 선택한 테마나 static/css/app.css에서 설정한 스타일에 따라 달라집니다.\n\n전체 HTML 또는 특정 섹션에 테마를 선택하려면 data-theme 속성을 사용하세요.\n\n예를 들어.\n\n```js\n<html data-theme=\"cupcake\"></html>\n```\n\n<div class=\"content-ad\"></div>\n\nOR\n\n```js\n<html data-theme=\"dark\">\n  <div data-theme=\"light\">\n    이 div는 항상 밝은 테마를 사용합니다.\n    <span data-theme=\"retro\">이 span은 항상 레트로 테마를 사용합니다!</span>\n  </div>\n</html>\n```\n\n# API를 사용하여 데이터 가져오기\n\nAPI가 모두 설정되었으므로, 이전 튜토리얼에서 생성한 대로, 이제 HTML에 데이터를 가져와야 합니다. 이를 위해 main.py 파일을 약간 수정해야 하고, 그럼 준비됩니다. 그러나 그에 앞서, 데이터를 가져오는 트리거 역할을 할 HTML 폼을 만들어 봅시다.\n\n<div class=\"content-ad\"></div>\n\n## HTML 폼\n\n```js\n<div class=\"max-w-md p-8 mx-auto mb-8 rounded-md shadow-md bg-neutral\">\n        <form id=\"query-form\" method=\"post\" action=\"/neighbours/\" class=\"flex flex-col mb-6\">\n            <div class=\"mb-4\">\n                <input type=\"text\" placeholder=\"Query\" id=\"query\" name=\"query\" required\n                    class=\"input input-ghost w-full max-w-xs\" />\n            </div>\n            <div class=\"mb-4\">\n                <input type=\"range\" min=\"1\" max=\"5\" value=\"3\" class=\"range\" step=\"1\" name=\"neighbours\" id=\"neighbours\" />\n                <div class=\"w-full flex justify-between text-xs px-2\">\n                    <span>|</span>\n                    <span>|</span>\n                    <span>|</span>\n                    <span>|</span>\n                    <span>|</span>\n                </div>\n            </div>\n            <button type=\"submit\" class=\"btn btn-accent\">제출</button>\n        </form>\n  </div>\n```\n\n대부분이 DaisyUI 구성 요소로 구성되어 있습니다. 중요한 점은 form의 action 속성이 우리 API의 /neighbours 엔드포인트를 가리키고 POST 메소드를 사용해야 한다는 것입니다.\n\n## FastAPI 앱\n\n<div class=\"content-ad\"></div>\n\nJinja2 템플릿을 렌더링하려면 fastapi.templating 모듈을 사용해야 합니다.\n\n```js\nfrom fastapi.templating import Jinja2Templates\n\ntemplates = Jinja2Templates(directory=\"templates\")\n```\n\nHTML 템플릿이 동적으로 렌더링되어야 하므로 CSS 스타일 또는 다른 정적 리소스를 포함하는 폴더를 마운트해야 합니다. 이를 통해 템플릿 내에서 폴더에 액세스할 수 있습니다.\n\n```js\nfrom fastapi.staticfiles import StaticFiles\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n```\n\n<div class=\"content-ad\"></div>\n\n엔드포인트 함수에 필요한 유일한 조정은 응답 클래스를 HTMLResponse로 변경하는 것뿐입니다. JSON 객체 대신 웹페이지를 렌더링할 것이기 때문입니다.\n\n```js\n@app.get(\"/\", response_class=HTMLResponse)\nasync def main(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n# 이웃 가져오기\n@app.post(\"/neighbours/\", response_class=HTMLResponse)\nasync def fetch_item(request: Request, query: str=Form(...), neighbours: int=Form(...)):\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n    results = db.similarity_search(query, k=neighbours)\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"results\": results})\n```\n\nFastAPI 폼은 Pydantic 모델을 지원하지 않습니다. 대신 Form 메서드를 사용하여 폼에서 게시된 데이터를 구문 분석합니다. 함수 매개변수에서 query: str = Form(...)는 엔드포인트가 문자열인 query이름의 폼 필드를 예상한다는 것을 나타냅니다. 함수가 호출될 때 TemplateResponse를 반환하고 결과를 템플릿에 전달합니다.\n\n다 됐어요! 완료했습니다. 이것은 Streamlit 없이 Python으로 웹 앱을 만드는 가장 쉬운 방법 중 하나였어요. Tailwind나 DaisyUI CSS 없이 기본적인 'Hello World' 애플리케이션을 선택했더라도 더 쉽게할 수 있었겠지만, 그렇게 한 것에 재미가 어디 있나요?\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_2.png\" />\n\n# 결론\n\n나는 가능한 한 철저하게 노력하여 파이썬을 사용하여 FastAPI를 이용해 웹 앱을 구축하는 데 필요한 모든 중요한 측면을 자세히 설명했다. 프론트엔드를 구축하는 동안 문제가 발생하면 언제든지 ChatGPT에 의지하고, 그것에게 프론트엔드를 만들도록 요청할 수 있다. 유용한 팁 중 하나는 Tailwind 문서에서 샘플 코드를 제공하고 앱에 통합하도록 요청하는 것이다. 튼튼한 보일러플레이트가 있으면 이를 역공학하고 원하는 대로 사용자 정의할 수 있다. 이 방식은 프로세스를 간단하게 만들며 문서 전체를 읽는 것보다 더 매력적으로 만든다.\n\n다음 주말에는 완전한 RAG 애플리케이션을 Llama3을 사용하여 구축하거나 이것에 파일 업로드 기능을 추가할 예정이다. 나는 처음부터 파일 업로드를 구현하고 처리하는 데 경험이 없기 때문에 그것이 흥미로울 것이다.\n\n\n<div class=\"content-ad\"></div>\n\n저희 튜토리얼을 즐기셨기를 바랍니다. 경험이 부족한 사람이 작성하여 이해하기 쉬울 거예요. 안녕히 계세요!\n\n## Github Repo","ogImage":{"url":"/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_0.png"},"coverImage":"/assets/img/2024-05-20-HowIBuiltABeautifulWebAppPurelyinPythonwithZeroExperience_0.png","tag":["Tech"],"readingTime":11},{"title":"AI 에이전트들을 해킹했어요 이제 모두 무료로 이용할 수 있어요","description":"","date":"2024-05-20 21:59","slug":"2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree","content":"\n\n![image](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_0.png)\n\n몇 주 전, 무료로 Gradio API 호출을 사용할 수 있는 비밀 핵을 발견했습니다(여기 및 여기에서 더 읽을 수 있어요). Ben Auffarth가 지은 멋진 책인 랭체인에 관한 연구를 완료했는데요... 그리고 영감을 받았어요.\n\nLangchain과 함께 Gradio API를 사용하여 무한한 AI 에이전트의 가능성을 무료로 테스트할 수 있는 방법이 있는지 궁금했습니다. 당신도 당신을 위해 무료로 작동하는 에이전트의 기초를 마련할 준비가 되어 있나요?\n\n자리에 튼튼히 잡히세요. 이 기사에서도 여러분이 같은 일을 할 수 있는 방법을 설명하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 왜 AI 에이전트?\n\n내게 있어서 AI 에이전트에 관한 이야기는 지금까지 배경 속에 있던 것이라고 말해야겠어요. 첫째로, 나는 오픈 소스 모델만을 사용하여 작업하고 싶다는 생각을 가지고 있어서입니다. 둘째로, AI 에이전트 분야로 이동하기 위해 작고 정확한 모델을 찾고 있었기 때문입니다. GPU가 없기 때문에 모델의 선택과 크기가 항상 우선 순위입니다.\n\n## 그런데... 이 에이전트들이 뭐죠?\n\n저는 공정 제어 산업 자동화 엔지니어이기 때문에 에이전트가 무엇인지 설명하는 것은 쉽게 이해되어요: 에이전트들은 의사 결정 과정의 주도 역할을 합니다.\n\n<div class=\"content-ad\"></div>\n\n- 환경과 상호작용하며 선택을 내리고 특정 목표를 달성하기 위해 설계된 컴퓨터 프로그램 또는 시스템입니다.\n- 인간에 의해 직접적으로 제어되지 않고, 자율적인 개체로 독립적으로 작동하여 유연한 문제 해결 능력을 발휘합니다.\n\n에이전트는 반응적(Reactive)이거나 적극적(Proactive)인 성격, 환경의 안정성(고정 또는 동적), 그리고 다중 에이전트 시스템에 참여하는 정도와 같은 독특한 특성에 따라 분류될 수 있습니다.\n\n- 반응적 에이전트는 환경 자극에 신속히 반응하고 이러한 입력에 기반하여 행동을 취합니다.\n- 적극적 에이전트는 목표를 달성하기 위해 적극적으로 계획을 세우고 행동합니다.\n\n여러 에이전트가 협력할 때, 그들은 다중 에이전트 시스템을 형성하며 각각이 공통 목표에 기여합니다. 효과적인 조정과 소통을 보장하기 위해 이러한 에이전트들은 행동을 동기화하고 서로 상호작용해야 합니다.\n\n<div class=\"content-ad\"></div>\n\nLangchain은 내장된 기능을 갖춘 강력한 프레임워크로, 모든 종류의 AI 에이전트를 조직화하고 조정하는 데 사용할 수 있습니다. 다음 글에서 그에 대해 자세히 배워보겠습니다.\n\n여기서는 에이전트 애플리케이션을 위한 기본 도구를 만들어보겠습니다.\n\n# 문서는 퍼즐입니다\n\n<div class=\"content-ad\"></div>\n\n아직 예제가 없습니다. Gradio와 Langchain의 문서는 꽤 좋지만 주로 OpenAI 예제에 초점을 맞추고 있습니다. 오픈 소스 도구와 AI를 사용할 때마다 가장 큰 숙제가 바로 이겁니다.\n\n그래서 저는 스스로 만들기로 결심했습니다. 1주일간의 고군분투 끝에 가능하다는 것을 깨달았어요.\n\n동시에 좋은 문서는 해결책의 원천입니다. 우리는 해결해야 할 문제를 알고 있습니다: Gradio API 호출을 Langchain의 LLM 인스턴스로 결합시키기입니다.\n\n두 프레임워크의 문서를 훑어 보면서 몇 가지 영감을 얻었습니다:\n\n<div class=\"content-ad\"></div>\n\nGradio Python client: Gradio Python client을 사용하면 어떤 Gradio 앱이든 API로 쉽게 사용할 수 있습니다. 예를 들어, 마이크로부터 녹음된 오디오 파일을 전사하는 Hugging Face Space를 고려해보세요. 아래에 예시가 있습니다.\n\n![Gradio Example](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_2.png)\n\ngradio_client 라이브러리를 사용하면 프로그래밍 방식으로 오디오 파일을 전사하는 API로 Gradio를 쉽게 사용할 수 있습니다. 작동 방식을 이해하려면 이전 글 \"Chatbot Cheat Code: Build Your AI Assistant Running A HUGE LLM Without Spending A Penny — Part 1/Part 2\"를 참조해주세요.\n\nLangchain Gradio component: Hugging Face Spaces에는 수천 개의 Gradio 앱이 있습니다. 이 라이브러리는 이러한 앱들을 LLM(Large Language Model)의 손끝에 두는 데 도움이 됩니다. 구체적으로, gradio-tools는 Gradio 앱을 도구로 변환하는 Python 라이브러리로, 이를 이용해 큰 언어 모델(LLM) 기반 에이전트가 작업을 완료하는 데 활용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, LLM은 온라인에서 찾은 음성 녹음을 전사하고 그 내용을 요약하는 Gradio 도구를 사용할 수 있습니다. 또는 Google 드라이브의 문서에 OCR을 적용한 다음 해당 내용에 대한 질문에 답변하는 다른 Gradio 도구를 사용할 수도 있습니다.\n\nLangchain의 블로그에 따르면, 사전에 구축된 도구 중 하나가 아닌 공간을 사용하려면 쉽게 자체 도구를 만들 수 있습니다. 본 기사를 통해 프로세스가 실제로 쉬운지 여부를 직접 판단하게 될 것입니다...\n\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_3.png)\n\n# 사용자 지정 래퍼를 만들어야 합니다\n\n\n<div class=\"content-ad\"></div>\n\nLangchain은 거대한 통합 모음을 보유하고 있어요: 기본적으로 언어 모델, 문서 로더, 데이터베이스 등을 모듈식이고 쉽게 연결할 수 있어요.\n\n그들은 우리 모두의 도구 세트와 함께 사용될 수 있는 사용자 정의 LLM 클래스를 생성할 수 있는 가능성을 열어 두었어요. \n\n여기서는 LangChain에서 Llama-3-8b에 연결하는 방법을 배웠어요. 하지만 이 과정은 다른 툴을 사용하고 싶거나 LangChain에서 지원하는 것과 다른 래퍼를 사용하고 싶을 때에도 동일해요. \n\n그럼 시작해봐요. 이 예제에서는 Langchain을 Llama-3-8b에 연결할 거에요. 하지만 그레디오 API와 허깅페이스 허브 데모 애플리케이션에 대해서도 (작은 트릭들이 있긴 하지만) 동일한 프로세스가 적용돼요.\n\n<div class=\"content-ad\"></div>\n\n무료 구글 Colab 노트북을 열어보자. CPU만 있는 것으로 충분하다. Google Colab를 처음 사용하거나 무료로 얻는 방법을 모르는 경우 여기 지침을 읽어보세요:\n\n우리가 필요한 라이브러리를 먼저 설치합시다.\n\n```js\n%pip install --upgrade --quiet gradio_tools huggingface_hub langchain\n```\n\n이 노트북은 HuggingFace 토큰이 없어도 작동합니다. 그러나 강력히 권장하긴 하지만요: 여기 기사에서 지시 사항을 따라 해보세요.\n\n<div class=\"content-ad\"></div>\n\n## Gradio 클라이언트 인스턴스화\n\n이것이 첫 번째 단계입니다. 기본적으로 gradio 도구를 사용하여 Gradio 데모 애플리케이션을 호스팅하고 있는 HuggingFace Space에 API 호출을 설정합니다. 추론은 그 곳에서 이루어지며, LLM으로부터 응답을 받게 될 것입니다.\n\n```js\nfrom gradio_client import Client\n\nclient = Client(\"ysharma/Chat_with_Meta_llama3_8b\")\n\n# 이 부분은 연결을 테스트하기 위함입니다\nresult = client.predict(\n  message=\"Hello!!\",\n  request=0.95,\n  param_3=512,\n  api_name=\"/chat\"\n)\nprint(result)\n```\n\n노트북 셀을 실행하면 데모 엔드포인트와 결과에 대한 연결이 표시됩니다. HF_token을 전달하지 않으면 경고 메시지가 표시됩니다.\n\n<div class=\"content-ad\"></div>\n\n해당 부분을 수정하려면 다음과 같이 하세요:\n\n```js\nfrom gradio_client import Client\n\nyourHFtoken = \"hf_xxxxxxxxxxxxxxxxxxxx\" # 여기에 HF 토큰 입력\nclient = Client(\"ysharma/Chat_with_Meta_llama3_8b\", hf_token=yourHFtoken)\n```\n\n이제 Gradio 클라이언트가 작동하는 것을 알았습니다. Gradio와 Langchain을 연결하기 위해 Langchain에 새로운 LLM 래퍼를 생성해야 합니다.\n\n전체 노트북은 해당 GitHub 저장소에서 찾을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n## 사용자 정의 LLM 래퍼\n\n사용자 정의 LLM이 구현해야 하는 필수 사항은 두 가지뿐입니다.\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_4.png)\n\n위 문서 페이지에 따르면 새로운 클래스에서 최소한으로 _call 및 _llm_type 매개변수부터 시작합니다.\n\n<div class=\"content-ad\"></div>\n\n알림: 위의 Python 코드의 80%는 Langchain 설명서에서 직접 가져온 것입니다 😅 걱정하지 마세요, Colab 노트북 링크를 올릴 테니 그 전에 단계별로 설명해드리겠습니다.\n\n```js\nfrom typing import Any, List, Mapping, Optional\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\nclass GradioClientChat(LLM):\n    \"\"\"\n    Gradio API 호출을 기반으로 한 사용자 지정 LLM 클래스입니다.\n    \"\"\"\n    from gradio_client import Client\n    chatbot: Any = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # ChatBot 클래스를 인스턴스화합니다.\n        self.chatbot = Client(\"ysharma/Chat_with_Meta_llama3_8b\")\n\n    @property\n    def _llm_type(self) -> str:\n        return \"Gradio API client Meta_llama3_8b\"\n\n    def _call(\n            self,\n            prompt: str,\n            stop: Optional[List[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            chatbot=None,\n            request: float = 0.95,\n            param: float = 512,\n    ) -> str:\n        \"\"\"\n        지정된 프롬프트를 사용하여 Gradio API 클라이언트 Meta_llama3_8b에 API 호출을 실행하고 응답을 반환합니다.\n        \"\"\"\n        if chatbot is None:\n            chatbot = self.chatbot\n\n        if stop is not None:\n            raise ValueError(\"stop kwargs are not permitted.\")\n\n        # API에서 응답 반환\n        result = chatbot.predict(   #.submit for streaming effect / .predict for normal output\n              message=prompt,\n                request=request,\n                param_3=param,\n                api_name=\"/chat\"\n        )\n        return str(result)\n```\n\n처음 접하신 분들을 위해, 여기에서는 함수를 만드는 게 아니라 클래스를 생성하고 있습니다. 파이썬 클래스는 비슷한 객체 그룹이 공유할 수 있는 메서드(함수)와 속성(변수) 세트를 정의하는 청사진 또는 템플릿입니다. 객체를 생성하는 데 사용되는 청사진 역할을 하며, 건물의 설계와 구조를 개요로 나타내는 건축 청사진과 유사합니다. 클래스는 코드를 구조화하고 코드 재사용성을 제공하여 대규모 프로그램을 쉽게 작성하고 유지할 수 있게 해줍니다.\n\n우리의 클래스로 돌아가면: 필요한 모든 langchain 라이브러리를 가져온 후, GradioClientChat(LLM)라는 새로운 클래스를 생성합니다. 여기서 Gradio 클라이언트를 챗봇으로 사용합니다. 클래스는 LLM Langchain 클래스의 속성을 상속합니다. 이러한 이유로 _call 및 _llm_type 같은 몇 가지 속성과 메서드가 기본 사용자 정의 객체에서 필수적인 것입니다.\n\n<div class=\"content-ad\"></div>\n\n첫 번째 부분은 객체의 초기화와 _llm_type에 대한 부분이에요:\n\n```js\nclass GradioClientChat(LLM):\n    \"\"\"\n    Gradio API 호출을 기반으로 한 사용자 지정 LLM 클래스입니다.\n    \"\"\"\n    from gradio_client import Client\n    chatbot: Any = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # ChatBot 클래스의 인스턴스 생성\n        self.chatbot = Client(\"ysharma/Chat_with_Meta_llama3_8b\")\n\n    @property\n    def _llm_type(self) -> str:\n        return \"Gradio API client Meta_llama3_8b\"\n```\n\n그 다음으로, _call 메서드를 생성해요. 이 메서드는 가장 중요한 부분인데, 특정 Gradio API에 구성된 모델 매개변수(프롬프트 및 모델 매개변수)를 수락하여 추론을 실행하는 메서드입니다.\n\n```js\ndef _call(\n            self,\n            prompt: str,\n            stop: Optional[List[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            chatbot=None,\n            request: float = 0.95,\n            param: float = 512,\n    ) -> str:\n        \"\"\"\n        지정된 프롬프트를 사용하여 Gradio API client Meta_llama3_8b에 API 호출을 수행하고 응답을 반환합니다.\n        \"\"\"\n        if chatbot is None:\n            chatbot = self.chatbot\n\n        if stop is not None:\n            raise ValueError(\"stop kwargs are not permitted.\")\n\n        # API에서 응답 반환\n        result = chatbot.predict(   #.submit for streaming effect / .predict for normal output\n              message=prompt,\n                request=request,\n                param_3=param,\n                api_name=\"/chat\"\n        )\n        return str(result)\n```\n\n<div class=\"content-ad\"></div>\n\n입력 매개변수에는 변수 이름과 함께 기본값으로 request: float = 0.95, param: float = 512와 같은 값을 설정합니다. 이 값들은 구체적인 Gradio API를 반영해야 합니다. 우리의 경우 API 문서에서 ysharma/Chat_with_Meta_llama3_8b를 확인해주세요.\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_5.png)\n\n그런 다음, Gradio API 호출을 통해 추론을 실행하고 텍스트를 반환합니다.\n\n```js\n# API로부터의 응답 반환\nresult = chatbot.predict(   #.submit은 스트리밍 효과, .predict은 일반 출력용\n      message=prompt,\n      request=request,\n      param_3=param,\n      api_name=\"/chat\"\n)\nreturn str(result)\n```\n\n<div class=\"content-ad\"></div>\n\n\"새로 만든 클래스를 사용하여 셀을 실행해주세요.\n\n![image](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_6.png)\n\n# 우리의 사용자 정의 LLM을 인스턴스화하고 실행해봅시다\n\n이제 HuggingFace 데모 공간으로 둘러싸인 Langchain LLM를 실행할 준비가 되었습니다.\"\n\n<div class=\"content-ad\"></div>\n\n```js\n# llm을 인스턴스화하세요\nllm = GradioClientChat()\n\n# _call 메서드를 invoke와 함께 실행하세요\nresult = llm.invoke(\"인공 지능이란 무엇인가요?\")\nprint(result)\n```\n\n몇 초 안에 실행하면 응답을 받을 수 있어요!\n\n# 보너스 트랙: 스트리밍 효과\n\n한 번 더 단계를 추가하는 것은 문제가 되지 않겠죠? 몇 가지 추가적인 메서드를 생성할 수 있음을 보았습니다. 그 중 하나인 _stream은 생성될 때 토큰을 하나씩 반환하는 역할을 합니다. 이를 위해 이전 클래스 끝에 추가 메서드를 추가할 수 있어요.  \n\n\n<div class=\"content-ad\"></div>\n\n```python\ndef _stream(\n    self,\n    prompt: str,\n    stop: Optional[List[str]] = None,\n    run_manager: Optional[CallbackManagerForLLMRun] = None,\n    chatbot=None,\n    request: float = 0.95,\n    param: float = 512,\n    **kwargs: Any,\n) -> Iterator[GenerationChunk]:\n    \"\"\"주어진 프롬프트에서 LLM을 스트리밍합니다. \n\n    이 메서드는 스트리밍을 지원하는 하위 클래스에 의해 재정의되어야 합니다. \n\n    구현되지 않은 경우, stream에 대한 호출의 기본 동작은 모델의 비스트리밍 버전으로 \n    대체하여 출력을 단일 청크로 반환하는 것입니다. \n\n    Args: \n        prompt: 생성할 프롬프트. \n        stop: 생성 시 사용할 정지 단어입니다. 모델 출력은 이러한 하위 문자열 중 \n            하나가 처음 발생하는 곳에서 잘립니다. \n        run_manager: 실행을 위한 콜백 매니저. \n        **kwargs: 임의의 추가 키워드 인수입니다. 일반적으로 모델 공급자 API \n            호출에 전달됩니다. \n\n    Returns: \n        GenerationChunks의 이터레이터.\n    \"\"\"\n    if chatbot is None:\n        chatbot = self.chatbot\n\n    if stop is not None:\n        raise ValueError(\"stop kwargs are not permitted.\")\n\n    # API에서 응답 반환\n    for char in chatbot.submit(   #.submit for streaming effect / .predict for normal output\n          message=prompt,\n            request=request,\n            param_3=param,\n            api_name=\"/chat\"\n            ):\n        chunk = GenerationChunk(text=char)\n        if run_manager:\n            run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n\n        yield chunk\n```\n\n첫 부분은 기본적으로 동일하다는 점을 알아두세요. 변경 사항은 2곳에서 발생합니다:\n\n- 처음부분에서 메소드의 출력이 문자열이 아닌 Iterator 객체임을 선언합니다.\n- 끝부분에서 for 루프를 시작하고, predict 대신 submit() 메서드를 호출합니다. 차이에 대해 제2부에서 설명했습니다.\n\n이제 새로운 추가와 print에서 작은 변경을 한 Class를 다시 실행할 수 있습니다. Google Colab에서도 텍스트가 생성 중에 스트리밍되는 것을 확인할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nllm = GradioClientChat()\n# 텍스트 인터페이스를 위한 코드 - Steramlit에서는 필요하지 않습니다\nfinal = ''\nfor token in llm.stream(\"what is science?\"):\n        if final == '':\n            final=token\n            print(token, end=\"\", flush=True)\n        else:\n            try:\n                print(token.replace(final,''), end=\"\", flush=True)\n                final = token\n            except:\n                pass\n```\n\nStreamlit을 사용하는 경우, 위의 코드는 필요하지 않습니다. Iterator 객체는 점진적으로 진행되기 때문에 모든 토큰을 하나씩 쌓아둡니다. 하지만 Google Colab에서는 그럴 여유가 없습니다. 단어별로 인쇄해야 하므로, 이미 생성된 내용을 뺀 새 Iterator 스트림으로 인쇄 기능을 조정해야 합니다.\n\n# 결론... 지금까지\n\n우리는 어디에서든 실행할 수 있는 AI 에이전트들의 자유 무리의 기초를 단순하게 놓았습니다. 솔직히 말해서, Gradio API 주변에 새롭게 만든 래퍼를 사용하여 Langchain 튜토리얼을 자유롭게 시도해볼 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n큰 모델인 Qwen100b와 같은 모델을 약간 조정하여 실행해 볼 수도 있어요. 성공하면 알려주세요.\n\n무엇을 기다리고 있나요?\n\n이 기사를 즐겁게 읽었기를 바라요. 이 기사에 대한 노트북도 있답니다.\n\n이 이야기가 가치 있는 정보를 제공했고 조금이라도 지원하고 싶다면, 다음을 해볼 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n- 이 이야기에 대해 많이 박수를 쳐주세요.\n- 기억하기 좀 더 유용한 부분을 강조합니다 (나중에 그것들을 더 쉽게 찾을 수 있고, 나는 더 나은 기사를 쓸 수 있습니다.)\n- 자신의 AI를 시작하는 방법을 배우세요. 이 무료 eBook을 다운로드하세요.\n- 내 링크를 사용하여 Medium 멤버십에 가입하십시오 — (무제한 Medium 이야기를 읽으려면 매월 $5)\n- Medium에서 나를 팔로우하세요.\n- 나의 최신 기사를 읽어보세요: https://medium.com/@fabio.matricardi\n\n더 많은 내용을 읽고 싶다면 여기 몇 가지 아이디어가 있습니다:\n\nYoussef Hosni의 이 기사로 직접 시도해 볼 수 있습니다.\n\n학습 자료:\n\n<div class=\"content-ad\"></div>\n\n파이썬 클래스의 예시:\n\n간단한 은행 계좌를 나타내는 프로그램을 만들고 싶다고 상상해보세요. \"BankAccount\"라는 클래스를 정의할 수 있습니다. 이 클래스에는 계좌 번호를 저장하는 \"account_number\" 및 현재 잔액을 저장하는 \"balance\"라는 두 가지 속성이 있을 수 있습니다. 또한 계좌에 돈을 입금하는 \"deposit()\" 메소드와 계좌에서 돈을 인출하는 \"withdraw()\" 메소드가 있을 것입니다.\n\n이 클래스의 인스턴스를 생성하려면 다음과 같이 클래스를 호출하는 방식으로 인스턴스화하면 됩니다:\n\n```js\naccount1 = BankAccount(\"12345\", 1000)\n```\n\n<div class=\"content-ad\"></div>\n\n지금, account1은 BankAccount 클래스의 객체이며, 계좌 번호가 \"12345\"이며 시작 잔고는 1000원입니다. 그런 다음 account1에서 deposit(500)과 같은 메서드를 호출하여 잔고에 500을 추가하거나 withdraw(200)를 호출하여 200을 빼는 등의 작업을 할 수 있습니다.\n\n클래스는 객체 지향 프로그래밍의 기본 개념으로, 관련된 데이터와 함수를 재사용 가능한 구성요소로 구성하여 복잡한 프로그램을 작성할 수 있게 해줍니다.\n\n![이미지](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_7.png)\n\n이 이야기는 Generative AI 출판물의 일환으로 발행되었습니다.\n\n<div class=\"content-ad\"></div>\n\n우리와 함께 최신 AI 이야기 속에 머무르기 위해 Substack, LinkedIn 및 Zeniteq에서 연락을 유지해보세요. 함께 AI의 미래를 만들어 봅시다!\n\n![Image](/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_8.png)","ogImage":{"url":"/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_0.png"},"coverImage":"/assets/img/2024-05-20-IhackedtheAIagentsNowyoucanhavethemallforfree_0.png","tag":["Tech"],"readingTime":14},{"title":"파이썬으로 인과 관계 탐색하기 차이인차이법","description":"","date":"2024-05-20 21:55","slug":"2024-05-20-ExploringcausalitywithPythonDifference-in-differences","content":"\n\n<img src=\"/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_0.png\" />\n\n인과 관계를 확립하는 것은 현대 분석에서 가장 중요하면서 종종 간과되는 영역 중 하나입니다. 저는 다가오는 시리즈의 기사에서 우리의 인과 추론 워크샵에서 가장 많이 사용되는 도구를 설명하고 강조하고 싶습니다.\n\n# 인과 추론 101\n\n인과 추론을 정의하는 것부터 시작해봅시다. 저는 Scott Cunningham의 \"믹스테잎\" 책에서 가져온 정의를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그는 그것을 특정 결과에 대한 사건과 선택의 영향을 추정하는 연구로 정의합니다. 우리는 변수 간의 인과 관계를 수립하려고 노력합니다 (우리는 이들을 처리와 효과라고 부를 수 있습니다). 이는 비즈니스부터 공공 정책 설정까지 다양한 영역에서 널리 발생하는 문제입니다.\n\n일반적으로 인과성 파악 프레임워크의 설정은 상대적으로 간단하며 다음으로 구성됩니다:\n\n- 처리군 — 처리를 받는 그룹\n- 대조군 — 처리 효과를 평가하기 위한 기준으로 삼으려는 그룹\n- 처리 — 분석하고자 하는 처리에 직접적으로 관련된 모든 활동\n- 관심 결과\n\n이 설정은 이론적인 개념뿐만 아니라 광범위한 실제 시나리오에 적용할 수 있는 실용적인 도구입니다. 웹사이트 최적화부터 A/B 테스트, 약물 임상 실험부터 개발 프로그램의 효과 추정에 이르기까지, 인과 추론의 응용 분야는 광범위하고 다양합니다.\n\n<div class=\"content-ad\"></div>\n\n우리가 인과 효과를 확인하기 위해 충족시켜야 하는 조건을 고려해 봅시다. 먼저, 우리는 처리 그룹과 대조 그룹이 비교 가능하다고 가정해야 합니다. 두 그룹은 처리를 받았을 때와 받지 않았을 때 동일하게 행동해야 합니다. 예를 들어, 처리 그룹의 객체는 처리를 받지 않았을 경우 대조 그룹의 객체와 동일하게 행동해야 합니다.\n\n그 반대도 마찬가지입니다. 대조 그룹의 객체는 처리를 받았을 경우 처리 그룹의 객체와 동일하게 행동해야 합니다. 그러므로 두 그룹 간의 유일한 차이점은 처리에서만 나와야 합니다. 처리 그룹의 결과를 대조 그룹의 결과와 비교하여 우리는 처리 효과를 확인할 수 있습니다.\n\n대조 그룹은 비교뿐만 아니라 처리 그룹의 대체불능을 제시합니다. 이것은 주어진 처리에 노출되지 않았을 때 전자가 어떻게 행동했을지를 보여줍니다. 이것은 인과 효과를 확인하는 데 대조 그룹의 중요한 역할을 강조합니다.\n\n두 그룹이 비슷하다는 가정은 강력하며 가용 데이터와 연구 설계에 따라 달라집니다. 이러한 비교 가능성을 달성하는 것이 인과 추론의 중요한 과제입니다.\n\n<div class=\"content-ad\"></div>\n\n# 가짜 — 실험\n\n어떻게 그러한 조건들을 얻을 수 있을까요? 인과 관계 주제를 다루는 대부분의 논문은 무작위 실험이 인과 관계를 확립하는 데 있어 황금 표준이라는 개념으로 시작합니다. 그러나 이러한 실험은 종종 실현 가능하거나 실용적으로 수행하기 어려울 수 있습니다.\n\n그래서 우리는 계속해서 우리가 인과 관계를 찾는 데 도움이 되는 도구를 찾고 있습니다. 이 문제에 대처하는 연구 방법을 가짜 실험이라고 부릅니다.\n\n본 문서의 나머지 부분에서는 가장 중요하고 자주 사용되는 가짜 실험적 방법 중 하나인 차이 차이 방법에 초점을 맞출 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 최저 임금 연구\n\n이 방법을 고전적인 응용 분야에서 설명하겠습니다. 이 접근 방식을 이해하기 위해 Card와 Kruger의 유명한 최저 임금 연구를 살펴보겠습니다.\n\n최저 임금이 고용에 미치는 영향은 경제학과 공공 정책 분야에서 가장 뜨거운 논쟁 중 하나입니다. 이 연구의 저자들은 이 질문에 대한 답을 찾으려고 했습니다. 이러한 유형의 문제는 무작위 실험을 사용하여 설명할 수 없는 사례의 완벽한 예입니다. 특정 그룹이나 지역을 서로 다른 최저 임금 수준에 무작위로 할당하는 것은 사실상 불가능합니다.\n\n1992년, 뉴저지는 최저 임금을 시간당 4.25달러에서 5.05달러로 인상했습니다. Card와 Kruger는 뉴저지를 비교할 기준을 찾고 있었습니다.\n\n<div class=\"content-ad\"></div>\n\n연구자들은 뉴저지와 펜실베이니아의 고용 수준을 비교하기로 결정했습니다. 전자 주가 대조군 역할을 하는 것으로 선택되었습니다. 뉴저지와 지리적, 경제적 조건 면에서 유사한 펜실베이니아가 선택되었습니다.\n\n연구자들은 1992년 이전과 이후에 두 주의 패스트푸드 레스토랑을 조사하여 종업원 수를 확인했습니다. 고용량 연구 자료를 사용한 이유는 패스트푸드 업계가 최저임금 변화에 빠르게 대응할 수 있기 때문입니다.\n\n## 데이터 세트\n\n이제 데이터를 자세히 살펴보는 적절한 시기입니다. 필요한 데이터 변환을 거친 후 (교육 목적을 위해 간소화된 내용), 다음 데이터 구조가 사용 가능합니다. David Card 웹사이트(https://davidcard.berkeley.edu/data_sets.html)에서 데이터 세트를 사용했습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_1.png\" />\n\n각 행을 식당 설문조사 결과로 취급할 수 있습니다. 중요한 정보는 주 이름, 총 고용 인원, 그리고 주어진 레코드가 최저임금 변경 전이나 후 기간인지를 나타내는 플래그입니다. 최저임금의 변경을 분석 대상의 처리 변수로 취급할 것입니다.\n\n기술적으로, 차트 작성을 쉽게 하기 위해 시간별 및 주별 평균을 데이터 프레임에 저장할 것입니다:\n\n## 직관적인 접근\n\n<div class=\"content-ad\"></div>\n\n최저임금 인상의 영향을 직관적으로 알아보려면 어떻게 해야 할까요?\n\n가장 직관적인 방법은 처리 후 두 주(State)의 평균 고용량을 비교하는 것입니다.\n\n![그림](/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_2.png)\n\n차트를 통해 뉴저지의 평균 고용량이 펜실베이니아보다 약간 낮았음을 알 수 있습니다. 최저임금에 반대하는 사람들은 모두 크게 기뻐할 것이고 이 경제 정책 도구가 제대로 작동하지 않는다고 결론을 내릴 수 있습니다. 또는 아직 결론을 내기에는 너무 이르지 않을까요?\n\n<div class=\"content-ad\"></div>\n\n안타깝게도, 이 방법은 올바르지 않습니다. 이는 두 주 간의 사전 처리 차이에 대한 중요한 정보를 빼먹고 있습니다. 우리가 가지고 있는 정보는 무작위 실험 이외의 것에서 나온 것이기 때문에 두 주 간의 격차를 설명할 수 있는 다양한 요인을 식별할 수 없게 됩니다.\n\n이 두 주는 거기서 일하는 사람들의 수와 그들의 경제 상태 면에서 매우 다를 수 있습니다. 이들을 처리 후 비교하는 것은 최저임금의 영향에 대한 것을 밝혀 내지 않을 뿐만 아니라 부정확한 결론에 이를 수도 있습니다. 저는 거의 모든 경우에 이러한 유형의 비교를 피하는 것이 좋다고 생각합니다.\n\n## 처리 전/후 비교\n\n우리는 처리 후 두 주를 비교하여 결론을 내릴 수 없습니다. 어떤가요, 최저임금 변경에 영향을 받은 주만 살펴볼까요? 이 프로그램의 영향을 평가하는 다른 방법은 뉴저지의 최저임금 변경 전후 고용을 비교하는 것입니다. 아래 코드 블록은 정확히 이를 수행합니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_3.png\" />\n\n이전/이후 비교는 다른 결과를 보여줍니다. 최저 임금을 인상한 후, 뉴져지 주의 패스트푸드 레스토랑의 평균 고용량이 증가했습니다.\n\n유감스럽게도, 이러한 결론은 결정적이지 않습니다. 이 간단한 비교에는 많은 결함이 있습니다. 처리 전후를 비교할 때 강력한 가정 하나를 보여줍니다: 최저 임금이 인상되지 않았다면 뉴저지의 고용 수준은 변경 전과 동일하게 유지되었을 것이라는 것입니다.\n\n직관적으로, 이는 그럴듯한 시나리오로 보이지는 않습니다. 이 기간 동안 일반 경제 활동이 증가할 가능성이 있었고, 정부 프로그램이 고용을 보조할 수도 있었으며, 레스토랑 업계가 수요 증가로 큰 폭으로 경험할 수도 있었습니다. 이러한 시나리오 중 일부는 고용 수준에 영향을 미칠 수 있습니다. 간단히 선천과 후천을 비교하여 처리의 인과 관계적 영향을 확립하는 데는 보통 충분하지 않습니다.\n\n\n<div class=\"content-ad\"></div>\n\n이와 같은 비교는 다양한 상황에서 매우 일반적입니다. 이전에 논의한 방법보다 더 신뢰할 수 있다고 생각하더라도 결과를 비교할 때는 항상 신중해야 합니다.\n\n# 차이 차이법\n\n마침내 이제 우리는 공연의 주인공인 차이 차이법을 소개할 준비가 다 되어 있습니다. 우리가 처리 이후 두 그룹을 비교해서 인과 효과가 있는지 확인하는 것만으로는 부족하다는 것을 발견했습니다. 처리 전후에 처리된 그룹을 비교하는 것만으로도 충분하지 않습니다. 이 두 방법을 결합해 볼까요?\n\n차이 차이 분석을 통해 선택한 그룹 간의 결과 변수 변화를 시간에 따라 비교할 수 있습니다. 시간은 매우 중요한 요소로, 우리는 처리가 시작된 이후에 무엇이 어떻게 변했는지 비교할 수 있습니다. 이 방법의 단순함은 놀랍지만, 모든 인과적 접근 방법과 마찬가지로 가정에 의존합니다.\n\n<div class=\"content-ad\"></div>\n\n나중에 다양한 유의점에 대해 다룰 예정이에요. 우선, 이 평가 연구를 수행하는 데 필요한 구성 요소에 대해 시작해보죠. DiD 연구는 적어도 두 개의 서로 다른 시기에 두 개의 그룹이 필요해요. 한 그룹은 치료를 받고, 다른 하나는 비교 그룹으로 사용돼요. 언제 그룹을 비교할지 알아야 해요. 이 작업을 위해 필요한 항목은 무엇이 있을까요?\n\n- 통제 그룹의 전 치료 시점 결과 변수 값\n- '치료받는' 그룹의 전 치료 시점 결과 변수 값\n- 통제 그룹의 후 치료 시점 결과 변수 값\n- '치료받는' 그룹의 후 치료 시점 결과 변수 값\n\n다음 단계로 진행하여 다음 메트릭을 계산해야 합니다:\n\n- 치료받는 그룹과 통제 그룹 사이의 결과 변수 차이(치료 전 기간)\n- 치료받는 그룹과 통제 그룹 사이의 결과 변수 차이(치료 후)\n\n<div class=\"content-ad\"></div>\n\n다음으로 할 일은 무엇인가요? 우리는 마침내 차이-차이(differece-in-differences)를 계산합니다. 이는 전 처리와 후 처리 사이의 차이를 의미합니다. 이 측정은 평균 처리 효과의 추정을 제공합니다.\n\n이 전략의 이유를 쉽게 이해할 수 있어요. 무작위화 실험에서의 데이터 부족으로 인해 그룹 간의 차이를 비교할 수 없습니다. 그러나 그룹 간의 차이를 측정하는 것은 가능합니다. 치료 이전과 이후 기간을 비교하여 치료 효과를 나타내는 결과 변수의 차이를 측정할 수 있습니다.\n\n왜 그럴까요? 치료가 시작되기 전에 두 그룹 모두 결과 변수에 대한 기준 값을 갖고 있었습니다. 아무 일도 일어나지 않았다면 두 그룹에서 무엇이든 동일하게 유지될 것으로 가정합니다. 그러나 치료가 진행되었습니다.\n\n치료는 한 그룹에만 영향을 미쳤습니다. 따라서, 결과 변수에서의 어떠한 변화는 '치료된' 그룹에서만 발생해야 합니다. 치료 그룹에서의 어떠한 변화는 결과 변수를 통제 그룹과 비교했을 때 결과 변수를 바꿀 것입니다. 이 변화는 치료의 효과입니다.\n\n<div class=\"content-ad\"></div>\n\n제어 그룹의 성과와 추이가 치료 전과 동일할 것으로 가정합니다. 또한, 치료가 발생하지 않았다면 치료 그룹의 개인들이 이전 활동을 유지했을 것으로 가정해야 합니다. 한 그룹에서의 치료 발생은 상황을 변화시키고 치료 효과를 제공합니다.\n\n## 응용\n\n새로운 도구를 사용하여 최저임금의 영향을 조사할 수 있습니다. 최저임금 예시로 돌아가서, 우리가 가지고 있는 정보를 토대로 다음 숫자를 알아낼 수 있습니다:\n\n- 뉴저지의 최저임금 인상 전 고용 상황\n- 펜실베이니아의 최저임금 인상 전 고용 상황\n- 뉴저지의 최저임금 인상 후 고용 상황\n- 펜실베이니아의 최저임금 인상 후 고용 상황\n\n<div class=\"content-ad\"></div>\n\n최저임금이 인상되기 전에는 펜실베이니아의 패스트푸드 레스토랑 평균 고용량이 더 많았습니다. 그러나 인상 이후에는 상황이 변했고, 두 주 간의 평균 고용 차이가 훨씬 줄었습니다.\n\n아래 코드는 최저임금 인상 이전과 이후의 고용 차이를 계산합니다 (nj_difference 및 penn_difference). 또한 두 차이를 빼는 것으로 차이 차이 추정치를 계산합니다.\n\n아래 코드는 차이를 플롯하여 시각적인 비교를 제공합니다. 추가로 대조사실선을 추가하고 있습니다. 기술적으로, 패스트푸드 레스토랑 업종이 펜실베이니아의 추세를 따른다면 뉴저지의 사후처리 고용량을 추정하는 것입니다. 차이-차이 이해에 중요한 역할을 하는 다음 단락에서 이 대조사실선에 대해 논의할 것입니다.\n\n차트에서 보듯이 뉴저지의 평균 고용량이 0.59 증가했고, 펜실베이니아에서는 감소했습니다. 이를 계산하여 차이를 측정하면 처리 효과를 2.75로 얻을 수 있습니다. 최저임금 인상은 평균 고용량 증가로 이어졌는데, 이는 놀라운 결과입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Screenshot](/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_4.png)\n\n한번 이 결과를 초래한 것에 대해 고려해 보겠습니다. 뉴저지의 고용은 크게 증가하지 않았습니다. 그러나 펜실베이니아의 평균 고용률은 감소했습니다.\n\n최저 시급이 오르지 않았다면, 뉴저지의 평균 고용은 펜실베이니아에서 관측된 추세를 따라가는 것으로 예상됩니다. 최저 시급이 오르지 않았다면, 평균 고용은 더 낮았을 것입니다.\n\n차트에서 보면, 뉴저지의 추세가 펜실베이니아에서 관측된 추세를 따르는 대역상실한 선으로 표시되어 있습니다. 대역상실한 선과 뉴저지에서 관측된 실제 값과의 차이는 2.75의 치료 효과를 나타냅니다.\n\n\n<div class=\"content-ad\"></div>\n\n치료의 도입으로 이러한 추세가 변하고 뉴저지의 고용이 해당 값을 유지하고 약간 증가할 수 있습니다. 이러한 유형의 분석에서 중요한 것은 처리 그룹의 변화가 대조 그룹에서 관찰된 변화와 비교하여 어떻게 되는지입니다.\n\n아래 표는 대개 DiD 분석에서 만나는 형식으로 계산 내용을 요약합니다. 치료 및 대조 그룹은 열에 나타내고, 기간은 행에 나타내며, 결과 변수의 측정값은 셀에 나타냅니다.\n\n오른쪽 하단 모서리에는 차이를 계산한 후 최종 추정치가 표시됩니다.\n\n![표](/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_5.png)\n\n<div class=\"content-ad\"></div>\n\n## 선형 회귀를 사용한 차이 차이 방법\n\n몇 가지 평균을 간단히 계산해 보았습니다. 차이 차이 모델의 계산의 간단함은 그 장점 중 하나입니다.\n\n그 결과를 얻는 다른 방법도 있습니다. 좋고 오래된 선형 회귀를 사용하여 동일한 결론에 도달할 수 있습니다. 이 모델을 여러 기간 및 그룹으로 확장하는 것이 유익할 것입니다.\n\n차이 차이 모델의 주요 장점 중 하나는 그 간단함입니다. 이 모델을 실행하기 위해 소수의 변수만 필요로 하며, 이를 간단하고 쉽게 사용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 결과 변수: 총 고용 (Y)\n- 기간: 처리 이전에는 0이고 처리 기간에는 1인 더미 변수 (T)\n- 그룹: 대조군에는 0이고 치료군에는 1인 더미 변수 (G)\n\n모델은 다음과 같은 형태를 가지고 있습니다:\n\n![식](/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_6.png)\n\n이 모델을 어떻게 해석할까요? B1은 처리 기간이 시작될 때 결과 변수 값의 증가를 나타냅니다. 우리의 예제는 처리 전후의 대조군 평균 고용 차이를 보여줍니다. 우리는 최저 임금이 증가하지 않는 상황에서 이 변화가 발생할 것으로 기대합니다.\n\n<div class=\"content-ad\"></div>\n\nB2는 통제 그룹에서 처리 그룹으로 결과 변수가 변경된 것을 나타냅니다. 이는 치료 전 세계의 두 그룹 간의 기준 차이입니다.\n\n치료 기간과 그룹 간의 상호 작용 용어(T*G)는 처리 기간과 처리 그룹이 모두 활성화 되었을 때 결과 변수의 변화를 보여줍니다. 처리된 그룹의 경우 처리된 기간에 대해 0이 아닌 값이 있습니다.\n\nDiD 분석에서 우리는 처리된 그룹에서 처리 기간 동안의 결과 변수 변경을 통제 그룹과 비교하는 것을 원합니다.\n\n이 모델의 결과를 Python에서 계산하는 많은 방법이 있습니다. 이 예제에서는 statsmodels 라이브러리를 사용할 것입니다. 우리 예제에서의 선형 모델의 명세는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n회귀 출력 결과를 보면, (노란색으로 표시된) 치료 효과가 바로 위에서 계산한 값과 동일함을 확인할 수 있습니다. 모든 계수가 이전에 계산한 값과 일치하는지 확인할 수 있어요.\n\n![같이 보기](/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_7.png)\n\n간단한 평균 계산에 회귀 분석을 사용하는 것이 지나치다고 느껴질 수 있지만, 여러 이점이 있답니다.\n\n먼저, 모든 그룹에 대한 평균을 계산하는 것보다 계산이 더 간단해요. 또한 모델을 확장하여 여러 비교 그룹과 기간을 포함할 때 회귀의 이점을 볼 수 있을 거예요.\n\n<div class=\"content-ad\"></div>\n\n핵심적인 가정들입니다.\n\n<div class=\"content-ad\"></div>\n\n회귀 분석은 차이 차이 모델의 적용을 결론 지었습니다. 이 시연은 이 방법이 얼마나 강력한지를 보여줍니다. 마무리하기 전에 이 모델의 잠재적인 한계에 대해 생각해 봅시다.\n\n원인 추론의 대부분에 대해, 모델은 우리가 그에 대해 하는 가정만큼 좋습니다. 이 방법에서 올바른 비교 그룹을 찾는 것은 필수적이며 도메인 전문 지식이 필요합니다.\n\n차이 차이에 대해 읽을 때 항상 평행한 추세 가정을 만나게 됩니다. 이는 치료 전에 두 그룹이 결과 변수의 일관된 추세를 가졌다는 것을 의미합니다. 또한 이 모델은 해당 추세가 시간이 지남에 따라 지속되고 그 차이가 치료가 없을 때 결과 변수에서 두 그룹 간에 동일하게 유지되도록 요구합니다.\n\n우리의 예에서는 두 주에서 패스트푸드 음식점의 평균 고용 변화가 시간에 따라 동일하게 변경된다고 가정합니다. 이 가정이 충족되지 않는 경우 차이 차이 분석은 편향됩니다.\n\n<div class=\"content-ad\"></div>\n\n역사는 하나의 것이지만, 우리는 또한 이 추세가 시간이 지나도 계속될 것으로 가정하며, 이것은 우리가 결코 알지 못하고 테스트할 수 없는 것입니다.\n\n이 가정은 부분적으로만 테스트할 수 있습니다. 우리는 역사적인 추세를 살펴보아 비슷한지 평가할 수 있습니다. 이를 위해 더 많은 역사적 데이터가 필요합니다—시간을 경과하며 추세를 그래프로 나타내면 이 가정의 좋은 지표가 됩니다.\n\n이것은 부분적으로 테스트할 수 있으며, 우리는 치료를 받은 그룹의 행동만 평가할 수 있습니다. 병든 그룹이 우리의 통제 그룹과 동일한 행동을 보였다고 가정하지만, 우리는 100% 확신할 수 없습니다. 우리가 평가할 수 있는 세상은 단 하나뿐입니다. 이것이 인과 추론의 근본적인 문제입니다.\n\n차이-차이(Difference-in-Differences)는 두 그룹의 구조가 시간이 지나도 동일하게 유지되어야 합니다. 치료 전에 두 그룹이 동일한 구성을 가져야 합니다. 그들은 치료에 노출되는 것을 제외하고는 동일한 특성을 가져야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 요약\n\n첫눈에는 모든 가정들이 이 방법을 덜 매력적으로 만들 수 있습니다. 그렇지만 가정 없이 통계적이고 분석적인 접근이 있을까요? 가능한 한 품질 좋은 데이터를 확보하고, 민감도 분석을 수행하며, 도메인 지식을 활용해야 합니다. 그러면 차이-차이 방법을 사용하여 흥미로운 통찰을 발견할 수 있습니다.\n\n위 게시물은 한 가지 단점이 있습니다 (그 외에도 많을 수 있습니다 — 알려주시면 감사하겠습니다). 상대적으로 간단한 시나리오인 두 그룹과 두 기간을 다루고 있습니다. 다가오는 게시물에서는 동일한 기술을 좀 더 복잡한 환경에서 사용하여 이 그림을 더 복잡하게 만들 것입니다.\n\n차이-차이 방법에 관한 설명이 도움이 되었으면 좋겠습니다. 이 글은 인과 추론에 대한 내 학습을 공유하기 위한 첫걸음입니다. 더 많은 내용이 이어질 예정입니다.\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\nCard, David & Krueger, Alan B, 1994. “Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania,” American Economic Review, American Economic Association, vol. 84(4), pages 772–793, September\n\nhttps://davidcard.berkeley.edu/data_sets.html\n\nImpact Evaluation in Practice — Second Edition https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation-in-practice\n\n<div class=\"content-ad\"></div>\n\nhttps://mixtape.scunning.com/09-difference_in_differences","ogImage":{"url":"/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_0.png"},"coverImage":"/assets/img/2024-05-20-ExploringcausalitywithPythonDifference-in-differences_0.png","tag":["Tech"],"readingTime":11},{"title":"파이썬에서 데이터 정제하기","description":"","date":"2024-05-20 21:54","slug":"2024-05-20-DATACleaninginPython","content":"\n\n요구 사항: - Python3, 판다 라이브러리\n\n파이썬에서 데이터 클린업을 위한 몇 가지 미리 정의된 메서드가 있어요\n\n예: — dropna(), fillna(), duplicated(), loc()\n\n1. Dropna : 빈 행을 제거하는 데 사용돼요\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-20-DATACleaninginPython_0.png)\n\n제가 데이터 세트를 가지고 있는데 빈 셀을 제거하고 싶어요 (제 경우 (5. 2015)) 빈 셀이 포함된 행을 제거할 수 있어요.\n\n![image](/assets/img/2024-05-20-DATACleaninginPython_1.png)\n\n2. Fillna() : 빈 값을 fillna 메서드로 대체할 수 있어요.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-20-DATACleaninginPython_2.png\" />\n\n이제 빈 값을 숫자 90으로 바꿀 수 있어요!\n\n<img src=\"/assets/img/2024-05-20-DATACleaninginPython_3.png\" />\n\n그리고 빈 값을 평균, 최빈값, 중앙값으로 대체할 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-20-DATACleaninginPython_4.png)\n\n![Image 2](/assets/img/2024-05-20-DATACleaninginPython_5.png)\n\n3. Duplicate(): it can remove duplicate rows from our dataset\n\n![Image 3](/assets/img/2024-05-20-DATACleaninginPython_6.png)\n\n\n<div class=\"content-ad\"></div>\n\n4. `loc()`: `loc`은 location을 의미하며 이 함수를 사용하여 값을 바꿀 수 있습니다. 예를 들어, 데이터 세트에 잘못된 값이 들어가 있을 때 이 값을 변경하고 싶다면 `loc()`를 사용할 수 있습니다 :\n\n![파일](/assets/img/2024-05-20-DATACleaninginPython_7.png)\n\n이제 5번째 행의 값을 98로 바꾸고 싶다면 :\n\n![파일](/assets/img/2024-05-20-DATACleaninginPython_8.png)\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-20-DATACleaninginPython_9.png)\n","ogImage":{"url":"/assets/img/2024-05-20-DATACleaninginPython_0.png"},"coverImage":"/assets/img/2024-05-20-DATACleaninginPython_0.png","tag":["Tech"],"readingTime":2},{"title":"JavaScript, Rust, 그리고 GPT-3로 60초 안에 400개 이상의 탭 정렬하기","description":"","date":"2024-05-20 21:47","slug":"2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3","content":"\n\n## 모든 재미를 위해 GPT-3 사용 중.\n\n![테이블 이미지](/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_0.png)\n\n저는 탭 중독자입니다. 솔직하게 말하자면요.\n\n현재 5개의 브레이브 창에서 약 460개의 탭을 열어 두고 있습니다. 북마크는 신경쓰지 말아요.\n\n<div class=\"content-ad\"></div>\n\n응, 난 정보 햄스터 같아. 모든 탭을 호아딩해놓고 충분한 시간을 내어 모든 걸 읽으려고 노력할 때까지 새로운 탭을 더 열어. 알 수 있듯이, 너무 많은 탭을 가지고 있다는 건 상당히 압도적일 수 있어. 탭 바의 경계 너머로 사라져 버렸을 때 뭔가를 찾아야 하는 경우거나, 화면을 바라보며 \"할 일이 너무 많다\"는 불안한 느낌이 드는 경우 등이 있지. 그냥 어떤 작업도 해야 할 일이 없는 데도 말이야.\n\n그래서 게을러버리고 있는 해커 본성으로, 그들을 실제로 정리하거나 청소하거나 *이런* 단순히 모두 닫지 않고 기계가 할 수 있는 방법이 없을까 궁금해졌어. 내 괴로움의 1클릭 솔루션을 갖출 수 있을까?\n\n내 내면의 과다 집착자를 코드를 사용해 마리 콘도의 노예로 바꿀 수 있을까?\n\n다행히도, 우리에게는 수십억 달러의 가치가 있을 모델 언어가 있어 열심히 일을 할 준비만 하고 기다리고 있지.\n\n<div class=\"content-ad\"></div>\n\n아이디어는 간단해요: GPT-3에 항목 목록을 제공하고 해당 항목이 속하는 카테고리 목록을 요청하세요. 그것을 Chrome 확장 프로그램으로 묶어서 마법이 일어나게 해 보세요.\n\n그래서, 손가락을 털어놓고 코딩해 봅시다.. 아니면.. 음... 기다려 봐요..\n\n# 복잡성의 달콤한 맛\n\n조금 되돌아가 봅시다. 우리의 계획은 충분히 간단해 보이죠. 그러나 소프트웨어에서 보통 그렇듯, 우리는 제대로 생각하지 않으면 범위와 예산을 폭발시킬 중요한 세부사항을 놓쳤습니다.\n\n<div class=\"content-ad\"></div>\n\n코드에 처음 뛰어들기 전에 후회의 세계에 빠지지 않도록 고려해야 할 중요한 문제 몇 가지가 있습니다:\n\n- 프롬프트 토큰 한도 - OpenAI의 언어 모델은 토큰 한도를 가지고 있습니다 - 2048 또는 4096 토큰. 각 토큰이 약 4개의 문자이기 때문에 우리의 프롬프트와 응답 크기는 각각 8192/16384 문자로 제한됩니다. 이 문제를 해결하는 몇 가지 방법이 있습니다(모두 다루겠습니다): 프롬프트를 소모 가능한 청크로 나누기, 토큰 수를 줄이기 위해 보내는 데이터 최적화, 우리의 작업에 대한 모델을 세밀하게 조정하기\n- API 키 보안 - OpenAI API는 사용된 토큰으로 API 호출을 청구하므로 API 키를 안전한 곳에 숨겨야 합니다. 확장 프로그램에 하드코딩하는 것은 권장되지 않습니다 - 우리의 키를 스크랩하려는 지루한 스크립트 키디로 인해 OpenAI에 백만 달러의 청구서를 지불하길 원한다면 제외합니다.\n- 사용자 프라이버시 - 탭 제목과 URL은 민감한 정보를 드러낼 수 있습니다 - 개인 문서, 링크, 세션 ID 및 개인에 대한 많은 데이터. 사용자가 확장 프로그램을 신뢰할 수 있도록 하려면 오픈 소스로 공개하고, 해당 소스에서 빌드 및 배포하고 다른 사람들에게 쉽게 배포할 수 있도록 해야 합니다.\n- 업데이트 용이성 - LLMs는 응답에 소질이 있고 OpenAI API는 단순한 실수로 인해 엄청난 사용 비용이 발생할 수 있으므로 사용자가 자유롭게 업데이트하게 두는 대신 업데이트를 통제하고 싶습니다. 이는 가장 중요한 코드를 확장 프로그램에 두지 않고 제어하고자 함을 의미합니다.\n\n이러한 문제를 어떻게 해결할까요?\n\n간단한 방법을 채택할 것입니다 - 확장 프로그램 자체에 모든 로직을 작성하는 대신 API 뒤에 숨깁니다 - 탭 데이터를 전달할 간단한 백엔드 서비스를 구축하여 프롬프트를 나누고 OpenAI API와 통신하여 데이터를 단일 응답으로 줄입니다. 이를 통해 키를 안전하게 보호하고 업데이트를 제어하며 시크릿 토큰을 노출하지 않고도 확장 프로그램을 오픈 소스로 공개할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이를 위해 Rust를 사용할 것입니다. 백엔드 프레임워크로 Axum을, 배포 플랫폼으로 Shuttle을, 그리고 CI로 GitHub Actions를 사용할 것입니다.\n\n그래서 코드 작성에 앞서, 무엇을 구축하고 있는지 개략적으로 파악하기 위해 냅킨 스케치를 몇 장 그려봅시다:\n\n<img src=\"/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_1.png\" />\n\n## 단계 1: 확장 프로그램 구축하기\n\n<div class=\"content-ad\"></div>\n\n크로미움 확장 프로그램을 만드는 것은 매우 간단해요. 그냥 브라우저 안에 살아있는 작은 웹페이지들이에요. (적절한 권한으로) 브라우저의 API에 접근할 수 있어요.\n\n우리는 Chrome API를 사용할 거예요. 이 API는 구글 크롬이 사용하는 API인데, 많은 크로미움 기반 브라우저들이 노출시켜요 (예를 들어, 저는 Brave를 사용하고 있어요. 물론 Edge도 사용해요, 다만 다른 이름공간을 사용해요).\n\n다른 브라우저들은 크로미움 프로젝트 기반으로 만들어지지 않았지만, 비슷한 확장 프로그램 API를 제공해요. 만약 두 API 간 차이에 대해 더 알고 싶다면, MDN 기사를 추천해요.\n\n구체적으로, 이 두 API에 초점을 맞출 거에요:\n\n<div class=\"content-ad\"></div>\n\n- chrome.tabs - 현재 사용자가 열어둔 탭을 조회할 수 있도록 합니다.\n- chrome.tabGroups - 기존 그룹을 조회하고 새 그룹을 만들며 그 안에 탭을 이동할 수 있도록 합니다.\n\n그럼 이제 빌딩 과정으로 넘어가 봅시다.\n\n확장 프로그램을 시작하기 위해 Chrome 확장 프로그램 CLI를 사용할 것인데요 — 이를 통해 필요한 초기 프로젝트 구조를 생성합니다.\n\n그러니 터미널을 열고 다음을 입력해 보세요:\n\n<div class=\"content-ad\"></div>\n\n\nnpm install -g chrome-extension-cli\nchrome-extension-cli bookie-js\ncd bookie-js\n\n\n마지막에 나오는 안내에 따라 빌드 폴더를 확장 프로그램으로 로드하세요. 이를 통해 확장 프로그램을 핫 리로드를 통해로드 및 테스트할 수 있어서 변경 사항이 즉시 보입니다.\n\n지금 생성된 구조 안을 엿보세요. 대부분이 자명합니다.\n\n\n├── README.md\n├── config\n│   ├── paths.js\n│   ├── webpack.common.js\n│   └── webpack.config.js\n├── node_modules\n├── package-lock.json\n├── package.json\n├── pbcopy\n├── public\n│   ├── icons\n│   ├── manifest.json\n│   └── popup.html\n└── src\n    ├── background.js\n    ├── contentScript.js\n    ├── popup.css\n    └── popup.js\n\n\n<div class=\"content-ad\"></div>\n\n우리는 현재 주로 세 개의 파일에 관심이 있어요:\n\npublic/manifest.json — manifest는 당신의 확장 프로그램에 대한 정보를 브라우저에 제공하는 JSON 파일입니다. 이 파일에는 이름, 기능, 시작 방법, 어떤 파일을 표시할지, 페이지에서 실행할 스크립트 등이 포함되어 있어요. 우리가 주목해야 할 몇 가지 필드는 다음과 같아요:\n\n- default_popup - 확장 프로그램 아이콘이 클릭될 때 나타낼 HTML 파일\n- permissions - 특정 Chrome API의 일부에 액세스하기 위해 필요한 권한\n- host_permissions - 당신의 확장 프로그램이 액세스할 수 있는 URL 패턴들의 집합\n\n지금은 이 모든 것을 그대로 두고, 나중에 다시 돌아와서 확인할게요.\n\n<div class=\"content-ad\"></div>\n\nsrc/popup.html은 UI의 시작점입니다. 웹 브라우저의 확장 기능 버튼을 클릭하면 이 HTML이 팝업됩니다. 여기에 간단한 인터페이스를 구축하는 데 사용할 것입니다.\n\nAPI의 /sort 엔드포인트를 호출하고 결과를 반환하는 '정렬' 버튼, 로딩 표시줄 및 문제가 발생할 경우 간단한 오류 상자가 있을 것입니다.\n디버깅을 위해 '탭 표시' 버튼을 추가하여 모든 탭의 목록을 표시할 수도 있습니다. 이제 몇 줄의 간단한 HTML을 작성해 봅시다.\n\n```js\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <title>Bookie JS</title>\n    <link rel=\"stylesheet\" href=\"popup.css\" />\n  </head>\n  <body>\n    <div class=\"app\">\n      <div class=\"button-container\">\n        <!-- 이 버튼을 클릭하면 API가 호출됩니다 -->\n        <button id=\"sortBtn\" class=\"button\">정렬하기</button>\n        <div id=\"loading\" class=\"loading\"></div>\n        <div id=\"error\" class=\"error\"></div>\n      </div>\n    </div>\n    <script src=\"popup.js\"></script>\n  </body>\n</html>\n```\n\nsrc/popup.js 파일은 JS가 위치하는 곳입니다. 우린 적당한 Vanilla JS 만 사용할 것이니 예쁘고 견고한 사이버네틱 SSSR JavaScript 프레임워크는 사용하지 않을 거에요. UI를 업데이트하기 위해 DOM 요소를 조작하는 간단한 render(state) 함수에 의존하며, 요소.style.display를 block/none으로 변경하여 간단한 표시 및 숨김 함수를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이제 함수로 우리의 사고 과정을 작성해 봅시다.\n\n```javascript\n'use strict';\nimport './popup.css';\n\n(function () {\nconst SORT_BTN = 'sortBtn';\nconst LOADING = 'loading';\nconst ERROR = 'error';\n    \n// API에서 탭 및 그룹을 가져옵니다\nasync function getTabsAndGroups(){};\n// 데이터로 백엔드 호출\nasync function callBackendToSort(tabsAndGroups){};\n// 브라우저에 결과를 적용합니다\nasync function applySort(sortedCategories){};\n// 앱 실행\nasync function run(){\n// 탭을 가져옵니다\nlet tabsAndGroups = await getTabsAndGroups();\nrender({loading: false, error: null}\nlet btn = document.getElementById('sortBtn')\n// 클릭하면 API를 호출하고 로딩을 표시하고 결과를 적용합니다\n btn.addEventListener('click',async ()=> {\n     render({loading: true, error: null}\n      try {\n        let result = await callBackendToSort(tabsAndGroups)\n        await applySort(result)\n        render({loading: false, error: undefined})\n      }catch (e){\n        render({loading: false, error: e})\n      }\n })\n}\n// 콘텐츠가로드 될 때 run 함수를로드합니다\ndocument.addEventListener('DOMContentLoaded', run);\n    \n})();\n```\n\n첫 번째 단계는 Chrome API에서 탭과 그룹을 조회하는 것입니다. 문서에 따르면 chrome.tabs.query를 사용하여 이 작업을 수행할 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_2.png)\n\n<div class=\"content-ad\"></div>\n\n그래, 한번 해보죠:\n\n```js\nasync function getTabsAndGroups() {\n    let chromeTabs = await chrome.tabs.query({})\n    console.log(chromeTabs)\n}\n```\n\n작동하지 않나요? 이제, 그 public/manifest.json 파일을 기억하시나요? 그리고 permissions 객체를요?\n\n그래서, 탭들, 제목들, 그리고 그룹들에 접근하려면, 그에 맞는 권한들을 추가해야 합니다. 그래서 manifest.json을 열어서 permissions 아래에 \"tabs\", \"tabGroups\"를 추가합니다. 그러면 설치시, Chrome이 확장 기능의 권한을 확인하고 사용자에게 어떤 정보에 접근하는지 알려줄 수 있습니다.\n하지만, 탭 API에 접근하려면, host-permissions라는 다른 특별한 권한이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n확장 프로그램이 실행되는 웹 사이트를 사용자에게 알려줍니다. 따라서 모든 탭에서 이를 사용하려면 적절한 URL 패턴을 추가해야 합니다. 그래서 manifest.json에 \"host_permissions\"라는 새 속성을 추가하여 모든 URL과 일치하도록 패턴을 설정하세요. 예를 들어 \"host_permissions\": [\"*://*/*\"]입니다. 마지막으로 이제 사용자의 모든 탭과 그룹에 액세스할 수 있습니다.\n\n이제 작동 중이므로 chrome.tabs.query 메서드가 반환하는 데이터에는 필요한 몇 가지 항목이 포함됩니다: id, title 및 groupId. 우리는 id와 title을 정렬에 사용하고, groupId를 쿼리하는 데 사용할 것이므로 먼저 반환된 객체를 필요한 속성만 사용하여 간소화된 버전으로 매핑할 것입니다.\n\n그룹에 대한 더 많은 데이터를 얻으려면 tabsForGroups 함수를 만들어야 합니다. 이 함수는 모든 고유 그룹을 찾고 각 그룹의 제목을 가져오기 위해 chrome.tabGroups.get(id)를 사용하여 Chrome API를 쿼리할 것입니다.\n\n```js\nasync function tabsToGroups(tabs){\n  //탭에서 모든 기존 그룹Id 가져오기\n  let groupIds = tabs\n      .map( (it)=>it.groupId)\n      .filter((it)=>it!==null && it!==undefined && it!==-1);\n  \n  //고유한 것만 가져올 수 있도록 세트에 넣기\n  let groups = new Set(groupIds)\n  \n  //각 탭 그룹에 대한 데이터를 가져오기 위해 chrome API에 쿼리\n  return await Promise.all([...groups]\n      .map(async (it) => {\n      let item = await chrome.tabGroups.get(it)\n        return {\n          id: item.id,\n          title: item.title\n        }\n    }));\n  }\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n// 이제 함수가 모든 탭과 그룹을 반환할 수 있습니다.\nasync function getTabsAndGroups() {\n    let chromeTabs = await chrome.tabs.query({});\n    let tabs = await mapTabs(chromeTabs);\n    let tabsWithGroups = await tabsToGroups(tabs);\n    let groups =  tabsWithGroups.filter((it)=>it.title.length !== 0);\n    return {\n      items: tabs,\n      categories: groups\n    };\n}\n```\n\n와아, 몇 가지 간단한 단계로 기존 그룹과 탭 목록을 가져왔어요.\n\n또한 API 호출 함수도 매우 간단합니다. 아직 API가 없기 때문에 로컬호스트로 일반 POST 요청을 작성할 거에요:\n\n```js\nasync function callBackendToSort(data) {    \n return await fetch('http://127.0.0.1:8000/sort',{\n      method: 'POST',\n      headers: {'Content-Type': 'application/json'},\n      body: JSON.stringify({\n        items: data.items,\n        categories: data.categories\n      })\n    });\n}\n```\n\n<div class=\"content-ad\"></div>\n\n저희 render 함수도 상당히 간단해요. 상태를 확인하고 UI를 그에 맞게 변경하는 것뿐이죠.\n\n```js\nfunction render(state){\n    if(state.loading){\n      show(LOADING)\n      hide(SORT_BTN)\n      hide(ERROR)\n    }else{\n      hide(LOADING)\n      show(SORT_BTN,true)\n    }\n    if(state.loading!==true &&\n      (state.error!==undefined && state.error!=null)){\n      show(ERROR)\n      showError(state.error)\n    }else\n      hide(ERROR)\n}\n```\n\n이제 할 일은 브라우저에 새로운 카테고리를 적용하는 applySort 함수를 구현하는 것 뿐입니다.\n\n아이디어는:\n\n<div class=\"content-ad\"></div>\n\n- 그룹이 존재하는지 확인하세요.\n- 그룹이 없다면, 새로 만드세요.\n- 탭 목록과 제목을 업데이트하세요.\n\n이를 위해 API 조사를 좀 해야 합니다. 이 부분을 다루는 문서가 조금 혼란스러울 수 있어요. \nchrome.tabGroups.create 또는 chrome.tabGroups.update와 같은 것이 있을 것으로 예상할 수 있지만... 그런 건 순진한 생각이죠.\n\n그룹을 만들기 위해 사용하는 API 호출은 chrome.tabs.group으로, chrome.tabs.group에 groupId를 전달하지 않습니다. 그럼 그룹이 생성되고 새로운 groupId가 반환됩니다. 이것은 chrome 팀에 의해 조금 이상한 호출입니다 - 탭이 그룹에 대해 알고 있고 제어해야하는 이유는 무엇인가요?\n\n그룹은 그룹 API를 통해 생성하고 관리되어야 할 텐데요?\n\n<div class=\"content-ad\"></div>\n\n아, 또한 그룹에 탭을 추가하려면 동일한 호출을 사용하여 tabIds를 통해 탭 배열을 전달합니다. \"이 API 호출을 통해 이미 객체를 생성하고 업데이트하고 있기 때문에 제목도 전달할 수 있을까요?\" 아니요, 그렇게 하려면 chrome.tabGroups.update API 호출을 사용해야 합니다.\n\n나는 이 이상한 구문이 그룹이 chrome에 추가 기능이었기 때문에 지원이 탭 API 자체로 후방 적용되었다고 가정했습니다. 그래서 이 가정을 테스트해 봅시다. Tabs API에 그룹을 추가한 커밋을 살펴보면 같은 논의가 댓글에서 나타나며, Tab Group API 제안으로 이어집니다. 팀은 탭 관리와 그룹 관리 사이의 역할을 분할하기로 결정한 것으로 보입니다. 탭을 이동시키는 것은 탭 관리이므로 해당 역할은 Tabs API에 속합니다.\n\n대체 제안도 논의되었었으며(TabGroups API에 해당 역할을 넣는 것), 그것의 장단점도 함께 논의되었습니다:\n\n![이미지](/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_3.png)\n\n<div class=\"content-ad\"></div>\n\n저의 관점에서(API 사용자로서) 단점 목록은 그렇게 나쁘지 않아 보입니다. 탭은 그룹에 대해 알 필요가 없을 것이며, 사용자 보안이 강화될 것입니다(확장 프로그램은 tabGroups 권한만 필요로 하며, 악성 확장 프로그램에 의한 잠재적인 남용 가능성이 줄어들 것입니다) 그리고 이는 추상화가 가지는 의미인 직관적 API로 구현 세부 정보를 숨겨주며 대체할 것입니다. 그래도 이상한 결정일 수 있어요.\n\n하지만 이따가 이런 걸 더 많이 이야기하고 있지 않고, 코드를 쓰도록 해봅시다.\n\n```js\nfunction applySort(sortedCategories){\n/* 우리가 원하는 응답 객체는 다음과 같습니다: \n{ categories: [\n { category_id: int, category_title: string, items: [int] }\n    ] }\n*/\nfor (i = 0; i < sortedCategories.categories.length; i++) {\n     let category = sortedCategories.categories[i]\n     let categoryId = category.category_id\n     // ID가 있는 그룹을 확인합니다\n     let groupExists = await chrome.tabGroups.get(categoryId)\n          .catch((e)=>undefined);\n      let groupId;\n      if(groupExists === undefined)\n         // ID가 없다면, chrome.tabs.group가 우리에게 ID를 반환합니다\n         groupId = await chrome.tabs.group({ tabIds: category.items });\n      else {\n          \n        // ID가 있다면, 기존 것을 사용합니다\n        groupId = groupExists.id\n        await chrome.tabs.group({groupId: groupId,\n                                tabIds: category.items});\n      }\n// 모든 그룹의 제목을 설정하고 그룹을 축소합니다\n      await chrome.tabGroups.update(groupId, {\n        collapsed: true,\n        title: category.title\n      });\n})\n}\n```\n\n이로써, JS 익스텐션 MVP가 완료되었습니다.\n\n<div class=\"content-ad\"></div>\n\n- 탭과 그룹을 수집합니다.\n- 그것들을 API로 보냅니다.\n- 반환된 정렬을 적용합니다.\n\n지금은 아직 API가 없으니 어떻게 테스트할까요?\n\n일부 단위 테스트를 작성해야하지만, 그건 다른 날로 미루겠습니다 (정말로요 — 약간 밑으로 내려가면 Jest를 사용하여 Chrome 확장 프로그램을 테스트하는 것을 살펴볼 거에요).\n\n지금은 callBackendToSort 함수의 반환을 조작하여 몇 가지 카테고리와 몇 가지 탭 ID를 포함시킬 수 있습니다 - 당신의 탭 ID와 함께 이런 식으로 (하지만 당신의 탭 ID로):\n\n<div class=\"content-ad\"></div>\n\n```json\n{\n \"categories\": [{\n  \"category_id\": 837293848,\n  \"category_name\": \"Hacker News\",\n  \"items\": [1322973609, 1322973620]\n }, {\n  \"category_id\": 837293850,\n  \"category_name\": \"Science\",\n  \"items\": [1322973618, 1322973617, 1322973608]\n }, {\n  \"category_id\": 837293851,\n  \"category_name\": \"GitHub\",\n  \"items\": [1322973619]\n }, {\n  \"category_id\": 837293852,\n  \"category_name\": \"Web Development\",\n  \"items\": [1322973612, 1322973613, 1322973615, 1322973616]\n }, {\n  \"category_id\": 837293853,\n  \"category_name\": \"Web APIs\",\n  \"items\": [1322973646]\n }]\n}\n```\n\n자, 이제 재미있는 부분으로 넘어갑시다 — API를 구축하기, 프롬프트 최적화, GPT 타임아웃 및 우리가 미래와 과거에서 범할 실수 수정하기.\n\n아, 그리고 우리는 기능의 복잡성과 크립트를 추가할 것이지만, 그것에 대해서는 나중에 더 설명하겠습니다.\n\n이제, 현재 가장 핫한 언어인 Rust로 다가가 봅시다.\n\n<div class=\"content-ad\"></div>\n\n\n![Sorting 400 Tabs in 60 Seconds With JavaScript, Rust, and GPT-3](/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_4.png)\n\n루스트에 대해 설명할 필요가 없을 것 같아요. 돌 아래에 살았더라도 루스트에 대해 들어봤을 거에요. 개발 커뮤니티는 루스트를 하늘 높이 칭찬하고 있어요. C 언어의 속도를 가지고 있으면서 자바의 안전성과 AH-64 아파치 공격 헬리콥터처럼 완벽한 헬리콥터 부모 역할을 하는 빌림 시스템을 가지고 있어요.\n\n하지만 문법이 깔끔하고, 성능이 놀라우며, 매크로도 멋있고, 대부분 메모리에 엄격하지만 여전히 원시 포인터에 액세스할 수 있고 !unsafe해도 돼요.\n\n그래서 이 언어를 익히는 느낌을 받아볼까요? 좀 재미있게 해보죠.\n\n\n<div class=\"content-ad\"></div>\n\n간단한 서비스를 만들 예정이에요. 우리는 탭 컬렉션을 가져와서 조금 단순화하고 OpenAI API와 대화할 거예요. 희망컨대 환각 없이 응답을 우리 익스텐션에서 사용할 수 있는 형태로 파싱할 거예요.\n\n이 과정에서 몇 가지 장애물이 있을 거에요. 탭이 너무 많아 돈을 낭비하는 문제부터 실리콘밸리가 일어나 OpenAI API를 무너뜨리기 전까지 말이에요.\n\n우리 서비스는 꽤 간단할 거예요. 우리는 탭과 기존 카테고리를 POST할 /sort 메소드를 노출할 거에요. 이를 구축하기 위해 Axum 프레임워크를 활용할 거에요. 이를 통해 /sort 엔드포인트가 있는 서버를 쉽게 시작할 수 있어요. 그리고 배포를 위해 셔틀을 사용할 거에요. AWS 구성 파일 물어다님으로 싸움하지 않고 Rust 서버를 쉽게 가동할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n프로젝트를 구성하는 데 사용할 것이기 때문에 설치부터 시작해봅시다.\n먼저, 러스트 패키지 관리자인 cargo가 필요합니다. 만약 설치되어 있지 않다면 여기에 있는 단계를 따라주세요. 둘째, 셔틀 계정이 필요합니다. 걱정하지 마세요. GitHub으로 1클릭 가입하면 됩니다. 양식을 작성할 필요 없어요.\n\n이제 올드 터미널을 열고 `cargo install cargo-shuttle && cargo shuttle login`을 입력한 다음 인증을 완료한 후 `cargo shuttle init`를 실행하세요.\n\n프로젝트 이름 및 위치를 설정하고 메뉴에서 axum을 프레임워크로 선택하세요. 이렇게 하면 라이브러리로 새로운 axum 프로젝트가 구성되며 셔틀이 종속성으로 설정됩니다.\n\n우리 폴더는 이제 이와 같이 보일 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n├── Cargo.lock\n├── Cargo.toml\n└── src\n    └── lib.rs\n\n\n간단한 구조에요 — `Cargo.toml` 파일이 있습니다. 이는 `manifest.json`이나 `package.json`의 러스트 버전입니다. 이 파일에는 패키지, 의존성, 컴파일 특징 등에 대한 메타데이터가 포함되어 있어요. `Cargo.lock`은 환경 간 일관된 빌드를 보장하기 위해 지정된 의존성 목록을 담은 파일입니다.\n\n메인 서버 코드는 `src/lib.rs` 파일 안에 있어요. 아직 신선하고 아름다운 상태에서 코드를 살펴보려고 합니다:\n\n\nuse axum::{routing::get, Router};\nuse sync_wrapper::SyncWrapper;\nasync fn hello_world() -> &'static str {\n    \"Hello, world!\"\n}\n#[shuttle_service::main]\nasync fn axum() -> shuttle_service::ShuttleAxum {\n    let router = Router::new().route(\"/hello\", get(hello_world));\n    let sync_wrapper = SyncWrapper::new(router);\n    Ok(sync_wrapper)\n}\n\n\n\n<div class=\"content-ad\"></div>\n\n여기서 주목할 점이 몇 가지 있어요:\n\n- 메인 메서드가 없어요 — 이 프로젝트들이 [lib]rary로 표시되어 있기 때문에 미리 정의된 진입점이 필요하지 않아요.\n- 라우터 — 당신의 Axum 서비스에 대한 \"진입점\". 요청은 여기를 통해 라우팅되며 코드는 매우 자명합니다 - 경로와 처리 함수를 매치시켜 요청을 처리해요. 즉, 우리의 supercoolservice.com/hello는 간단한 \"Hello, world!\" 텍스트를 반환할 거에요.\n- SyncWrapper — 우리의 라우터 객체를 감싸서 서로 다른 스레드에서 안전하게 액세스할 수 있도록 해요.\n- #[shuttle_service::main] - 이것은 러스트 매크로에요 - 알고 있다면 주석의 강력한 버전으로 생각해봐요. 코드를 작성하는 코드를 작성할 수 있도록 해주는데, 이게 게을러진 설명이에요. 음.. 여기서 빠른 탈선이 필요한 것 같아요.\n\n## 매크로의 마법 같은 영역으로 빠른 탈선\n\n<img src=\"/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n이제, 매크로에 들어가기 전에 경고로 처음부터 말씀드리겠습니다. 이 글은 [여기에 즐겨 사용하는 언어를 삽입]에서의 매크로에 대한 100% 설명이 아닙니다. 이에 대해 수백 권의 책, 안내서 및 기사들이 존재합니다.\n\n하지만 여기에 우연히 들어온 독자들 중에서 \"모네드는 엔도펑터 범주 내에서 모노이드이다\" 스타일의 매크로 설명 기사를 읽기 싫어하는 분들을 위해, 매크로의 아름다운 토끼굴로 빠르게 들어가보도록 하겠습니다.\n\n그래서 상상해 보죠. 우리가 상상의 언어 '버스트'에서 작업 중이라고요.\n\n'버스트'는 이제 트위터 전체에서 소문이 나고 있는 멋진 새로운 언어이며, 메타버스 AI 웹4 앱의 언어가 될 것으로 말하고 있습니다. 그러나 새로운 언어이므로 아직 초기 단계이며 많은 라이브러리가 없습니다. 예를 들어, 아직 JSON 직렬화 라이브러리가 없기 때문에 직렬화 코드를 아직 수동으로 작성해야 합니다. 따라서 구조체를 만들 때마다 직렬화 코드를 작성해주어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimpl ToJson for ReallyBigModel {\n   fn toJson() -> String {\n       return mapOf { \"id\" to id, \n             \"name\" to name,\n             \"isReal\" to isReal,\n             ..., \n             \"stuff\" to stuff.toJson())\n         }.toJson() \n   }\n}\n```\n\n조금 귀찮죠? 매일 이렇게 많은 보일러플레이트를 작성하고 싶지 않으시죠.\n\n<div class=\"content-ad\"></div>\n\n하지만 어느 날, 최신 변경 로그를 읽는 중에 새롭게 추가된 매크로(macros)라는 새로운 기능을 발견했다. 매크로는 여러 종류가 있지만, Bust에서의 매크로는 두 가지로 구성된 특별한 메서드로 정의할 수 있는 것이야:\n\n- 매크로 속성(attribute)\n- 매크로 함수\n\n속성은 다른 코드에 적용할 수 있는 표식 같은 거야.\n\n클래스나 메서드 위에 큼지막한 빨간 X 표시가 있는 걸 상상해봐. 그래서 컴파일러가 컴파일 중에 함수를 만나면, 그 함수 머리 위에 큼지막한 빨간 X 표시가 있다면, 너의 매크로 함수를 호출해야 한다는 걸 알 수 있어.\n\n<div class=\"content-ad\"></div>\n\n매크로 함수는 속성으로 표시된 코드를 받아 처리한 후, 새로운 코드를 컴파일러에 반환하여 해당 기능이 있는 곳에 통합합니다.\n\n예를 들어, 우리가 toJson 매크로를 만들었다면, 어떤 구조체 위에 toJson 속성을 추가할 수 있으며, 이를 대신 코드로 작성해줄 것입니다. 따라서 위의 코드는 다음과 같이 변환될 것입니다:\n\n```js\n#[toJson]\nstruct ReallyBigModel {\n   id: String,\n   name: String,\n   isReal: Bool,\n   ...\n   stuff: AnotherBigModel\n}\n```\n\n그리고 매크로는 어떻게 생겼을까요?\n\n<div class=\"content-ad\"></div>\n\n위에 나와 있는 코드(토큰으로 표시됨)를 가져와서 이를 대체할 새로운 코드를 반환하는 함수입니다.\n\n```js\n#[toJson]\n#[toJson] fn addToJsonTrait(input: TokenStream) -> TokenStream { \n  let tree = parseIntoAST(input) \n  let nodes = ast.data.asStruct();\n  let name = tree.identity\n   // Get all the children that are properties\n   // Map them into format: $name to name \n  let properties = nodes\n    .filter((child)=>child.isProperty)\n    .map((property) => \"\\\"${property.name}\\\" to ${property.name}\")\n    .joinToString(\",\\n\") \n  // Write the toJson trait body\n  let body = quote! { //this is also a kind of macro!\n     impl ToJson for #name { \n      fn toJson() -> String { mapOf { properties }.toJson()}; \n    }\n   }\n   return body.intoTree().intoStream() \n}\n```\n\n참고: 이것은 가상 언어인 Bust입니다. 모든 언어에는 자체 매크로 구현이 있으며, 이것은 단순화된 표현일 뿐이므로 글이 지나치게 길어지지 않도록 한 것입니다.\n\n이제 우리 컴파일러가 #[toJson]로 표시된 클래스에 도착하면 addToJsonTrait 메서드를 호출하여 클래스 코드를 전달하고, 새 코드를 반환할 때까지 컴파일을 계속하기 전에 기다릴 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그렇게 해서, 우리는 매크로 함수를 사용하여 시간을 많이 절약했고 항상 되고 싶어했던 생산적인 Bust 개발자가 될 수 있게 되었어요!\n\n하지만 너무 흥분하지는 마세요 - 이것은 단지 상상 속의 구현입니다. 매크로에 대해 알아야 할 것이 많이 있고, 매크로에 대해 깊이 파고들어 보길 권하고 싶어요. 러스트 자체에는 몇 가지 다른 유형의 매크로가 있습니다. 이것이 사람들이 Lisp를 너무 좋아하는 이유 중 하나로, 적절한(syntactic hygiene) 및 비적절한 비적절한 비적절한(expansion) 매크로, 다양한 확장 유형 및 더 많은 마법이 숨겨져 있어요.\n\n그래서 이제 그 문제를 해결했으니, 다시 API를 구축해 봅시다.\n\n## The POST office\n\n<div class=\"content-ad\"></div>\n\n우리의 서비스의 간단한 매력을 /sort POST 방식 뒤에 숨기겠습니다. 따라서 그 \"hello world\"를 삭제하고 /sort 요청을 처리하는 라우터로 교체할 거에요 - Router::new().route(\"/sort\", post(sort_items)) 그리고 요청을 처리할 sort_items 메서드:\n\n```js\nasync fn sort_items(Json(payload): Json<SortRequestPayload>)\n                                       -> impl IntoResponse {\n (StatusCode::OK, Json(\"ok\")).into_response()\n}\n```\n\n이 메서드는 요청 구조의 Json 래퍼를 받아 우리 서버가 처리할 수 있는 IntoResponse 트레이트의 구현을 반환할 거에요.\n\n구체적으로, 우리는 이를 StatusCode, T 튜플 형식으로 반환할 거에요. 이것은 서버가 적절한 응답으로 변환하는 방법을 알고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n한 가지 더 구현해야 할 것이 있습니다. 바로 요청 데이터 구조입니다. 이제 같은 파일에 만들어 둘 대신에 src 폴더에 models.rs라는 새 파일을 열어 기본 정의를 만들어 봅시다.\n\n우리는 받게 될 래퍼인 SortRequestPayload가 필요합니다. 이는 카테고리와 항목의 목록을 포함해야 합니다. 따라서 이를 위한 구조도 필요합니다 - Category와 Item이라는 구조를 추가해 봅시다.\n\n또한 카테고리와 항목을 가진 목록이 필요합니다. 이를 반환할 수 있는 카테고리에 속한 항목을 가지고 있는 구조와 이를 위한 래퍼도 만들어야 합니다. 또한 문제가 발생한 곳을 알 수 있도록 ErrorResponse를 추가합니다.\n\n```rust\n//in models.rs\npub(crate) struct SortRequestPayload {\n    pub(crate) categories: Vec<Category>,\n    pub(crate) items: Vec<Item>,\n}\n\npub(crate) struct Category {\n    pub(crate) id: usize,\n    pub(crate) title: String,\n}\n\npub(crate) struct Item {\n    pub(crate) id: usize,\n    pub(crate) title: String,\n}\n\npub(crate) struct CategoryWithItems {\n    pub category_id: usize,\n    pub category_name: String,\n    pub items: Vec<usize>\n}\n\npub(crate) struct Categories {\n    pub categories: Vec<CategoryWithItems>\n}\n\npub(crate) struct ErrorResponse {\n    pub message: String,\n}\n```\n\n<div class=\"content-ad\"></div>\n\n하지만, 한 가지 문제가 발생했습니다 — 우리는 Serde 라이브러리를 사용하여 JSON과 (디)시리얼화가 쉽게 가능하도록 하려고 합니다. 이를 위해 앞서 구성한 매크로와 비슷한 매크로를 사용할 것입니다. 그래서 cargo.toml 파일을 열어서 serde와 serde_json을 종속성으로 추가해주세요:\n\n```js\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\n```\n\n이제, serde의 #[derive(Deserialize)] 매크로를 사용하여 우리의 구조체를 표시할 수 있으므로 프레임워크가 수신된 JSON을 구조체로 역직렬화하는 방법을 알 수 있습니다.\n\n```js\n// models.rs 파일 안에\n#[derive(Deserialize)]\npub(crate) struct SortRequestPayload {\n    pub(crate) categories: Vec<Category>,\n    pub(crate) items: Vec<Item>,\n}\n\n#[derive(Deserialize)]\npub(crate) struct Category {\n    pub(crate) id: usize,\n    pub(crate) title: String,\n}\n\n#[derive(Deserialize)]\npub(crate) struct Item {\n    pub(crate) id: usize,\n    pub(crate) title: String,\n}\n\n#[derive(Deserialize)]\n#[derive(Serialize)]\npub(crate) struct CategoryWithItems {\n    pub category_id: usize,\n    pub category_name: String,\n    pub items: Vec<usize>\n}\n\n#[derive(Deserialize)]\n#[derive(Serialize)]\npub(crate) struct Categories {\n    pub categories: Vec<CategoryWithItems>\n}\n\n#[derive(Serialize)]\npub(crate) struct ErrorResponse {\n    pub message: String,\n}\n```\n\n<div class=\"content-ad\"></div>\n\n위 작업을 완료했으니, 이제 코드 작업에 다시 몰두해 볼까요?\n\n우리의 계획을 살펴보겠습니다:\n\n```js\n1. 아이템 가져오기\n2. 아이템을 카테고리에 할당하기\n3. 프롬프트를 청크로 나누기\n4. 재귀적 정렬:\n    4.1. 기존 카테고리와 청크를 입력으로 받아 프롬프트로 변환\n    4.2. OpenAI에 정렬 요청하기\n    4.3. 응답 역직렬화하기\n    4.4. 기존 카테고리에 추가하기\n    4.5. 청크가 남아있는 동안, 다시 4.1로 돌아감\n5. 결과 반환하기\n```\n\n그리고 메소드로 구조화해 보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```rust\n//in lib.rs\n...\nfn create_chunks_for_prompting(items: Vec<Item>) -> Vec<Vec<Item>>\n```\n\n```rust\nfn sort_recursively(sorted_categories: Vec<CategoryWithItems>,\n                    remaining: Vec<Vec<Item>>) -> Result<Categories, Error>\nfn build_prompt(items: Vec<Item>, categories: Vec<CategoryWithItems>) -> String\nfn prompt_open_ai(prompt: String) -> Result<String, String>\n```\n\n우리는 프롬프트가 필요할 것이라고 생각해요. 아래와 같이 시도해봅시다. - 우리는 GPT3에게 항목 목록을 받을 것이라고 알려주고 형식을 제시한 후 목록을 포함해야 합니다.\n\n그런 다음, 반환할 유효한 JSON 형식을 설명하고 기존 카테고리를 전달하세요. 마지막으로, 유효한 JSON 형식으로 반환하도록 요청하세요. JSON 형식을 준수하고 이를 해석하지 않기를 희망하지만 후속 게시물에서 그 부분을 미세 조정할 거에요.\n\n<div class=\"content-ad\"></div>\n\n지금은 프롬프트의 끝 부분에 유효한 JSON 형식을 명시하고 \"유효한 JSON 형식\"을 언급하는 것이 꽤 적절해 보입니다.\n\n```js\n항목 목록을 [제목, id] 형식으로 받게 됩니다.\n제목과 URL을 기반으로 범주로 분류하며,\n기존 범주를 사용하거나 새 범주를 만들어 사용합니다.\n탭은:\n[$tabName, $tabId].\n반환할 유효한 JSON 형식은 다음과 같습니다:\n{ \"categories\": [ { \n    \"category_id\":\"여기에 id 입력\",\n    \"category_name\": \"여기에 이름 입력\", \n    \"items\":[여기에 tab_id 입력] } \n]}.\n기존 범주는: \n$categories\n보다 자세한 새 범주 목록(기존 및 새 범주 포함)과 항목을 유효한 JSON 형식으로 제시합니다.\n```\n\n그게 좋아요!\n\n이를 우리 코드 내에서 사용할 상수로 분리해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n```rust\nconst PROMPT_TEXT_START: &str = \"아이템 목록을 제목과 id의 형태로 받게 됩니다. 제목과 URL을 기반으로 기존 카테고리를 사용하거나 새로 만들어 아이템을 분류해주세요.\";\nconst PROMPT_TEXT_MIDDLE: &str = \"\\n반환할 유효한 JSON 형식은:\n{ \\\"categories\\\": [ { \\\"category_id\\\":\\\"여기에 id 입력\\\", \\\"category_name\\\": \\\"이름 입력\\\", \\\"items\\\":[tab_id 여기에] } ]}.\n기존 카테고리는:\";\nconst PROMPT_TEXT_ENDING: &str = \"탭을 사용하여 더 상세한 카테고리 목록(기존 및 새로 만든 항목)을 유효한 JSON 형식으로 제공합니다:\";\n```\n\n마지막으로 `sort_items` 메서드로 들어가서 모든 내용을 작성할 수 있습니다. 먼저 데이터를 소유권을 얻고 청크(조각)로 분할합니다:\n\n```rust\nlet items = payload.items;\nlet categories = payload.categories.iter().map(|it| {\n    CategoryWithItems {\n        category_id: it.id,\n        category_name: it.title.to_owned(),\n        items: Vec::new(),\n    }\n}).collect();\n```\n\n```rust\nlet prompt_slices = create_chunks_for_prompting(items_with_indexes);\n```\n\n<div class=\"content-ad\"></div>\n\n왜 청크를 사용하는 걸까요?\n\n모든 항목을 한 번에 추가하면 우리의 프롬프트 크기가 4096 토큰 이상이 될 수 있습니다. 사용할 모델에서 프롬프트와 완성을 위한 최대 길이로 지원하는 값이기 때문이죠.\n\n따라서 우리는 적절한 크기로 분할하고 완성을 위한 여분 공간도 확보해야 합니다. 여분 공간으로 50%를 남기고 2048 크기의 프롬프트를 유지할 것입니다.\n\n이를 달성하기 위해 create_chunks_for_prompting 함수는 두 가지 작업을 수행해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n- 우리 기본 프롬프트의 토큰 수를 세어 보세요.\n- API로 전송하는 데이터의 토큰 수를 세어 보세요.\n- 총 토큰 수의 크기를 2048로 나누고 하드코딩된 프롬프트 크기를 뺀 값을 기준으로 필요한 청크 수를 계산하세요.\n\nOpenAI 문서에 따르면, 토큰 하나는 대략 4개의 문자 크기로 보실 수 있습니다.\n\n이제, 토큰을 세는 다양한 방법이 있지만, 올바르게 하려면 길이를 4로 나누는 것 이상의 작업을 해야 할 것입니다 - Rust 토크나이저 크레이트와 그들의 GPT2 토크나이저를 사용하는 게 최선일 것입니다.\n\n하지만, 그런 접근은 다른 문제로 이어지기 때문에, 이번에는 건너뛰고 간단한 방법을 사용하겠습니다 - split_whitespace 메서드를 사용해 토큰 길이의 근사치를 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```rust\nfn create_chunks_for_prompting(items: Vec<Item>) -> Vec<Vec<Item>> {\n   \n  // 데이터 내의 토큰\n  let json_size = serde_json::to_string(&items).unwrap()\n      .split_whitespace()\n      .collect_vec()\n      .len();\n  \n  // 하드코딩된 프롬프트의 크기 구하기\n  let hardcoded_prompt = format!(\"{a}{b}{c}\",\n                                 a =String::from(PROMPT_TEXT),\n                                 b = String::from(PROMPT_TEXT_APPEND),\n                                 c= String::from(PROMPT_TEXT_ENDING));\n  \n  let hardcoded_prompt_size = hardcoded_prompt\n      .split_whitespace()\n      .len();\n  \n  // 아이템을 나눌 청크의 수 계산\n  let chunks_to_make = json_size / (2048 - hardcoded_prompt_size);\n  \n  // 벡터를 N개의 청크로 나누기\n  let chunk_size = items.chunks(items.len() /\n                                    (if chunks_to_make > 0 {\n                                    chunks_to_make\n                                    } else { 1 }));\n                                    \n  // 청크 목록 반환\n  return chunk_size.map(|s| s.into()).collect();\n}\n```\n\n이제 build_prompt 함수에 대해 이야기해 보겠습니다.\n\n프롬프트를 구성하기 위해 정렬해야 할 항목 목록과 기존의 카테고리가 필요합니다.\n\n항목 목록을 [제목, ID] 형식의 문자열로 변환한 다음, 카테고리를 JSON으로 변환하여 이를 모두 결합하는 형식으로 하나의 프롬프트를 생성할 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```rust\nfn build_prompt(items: Vec<Item>,\n                categories: Vec<CategoryWithItems>) -> String {\n  // items을 [title,id] 형태로 매핑한 후 모두 문자열로 결합합니다\n    let items_joined = items.iter().map(|item| format!(\n                                        \"[{title},{id}]\",\n                                        title = item.title,\n                                        id = item.id))\n                                .collect()\n                                .join(\",\");\n    let categories_json = serde_json::to_string(&categories).unwrap();\n    \n    format!(\"{prompt}\\n{tabs}{middle}{categories}\\n{ending}\",\n            prompt = String::from(PROMPT_TEXT_START),\n            tabs = items_joined,\n            middle = String::from(PROMPT_TEXT_MIDDLE),\n            categories = categories_json,\n            ending = String::from(PROMPT_TEXT_ENDING))\n}\n```\n\n다음으로, 이 프롬프트를 실제로 OpenAI에 전송하려면 HTTP 클라이언트가 필요합니다.\n\n이를 위해 reqwest 크레이트를 사용할 것입니다. 이 크레이트는 간단한 비동기 함수를 사용하여 OpenAI API와 통신할 수 있도록 해주는 고수준 HTTP 클라이언트를 제공하며, 쉬운 직렬화/역직렬화를 위한 JSON 기능도 제공합니다.\n\n그러니 우리의 Cargo.toml 파일에 이를 추가해 봅시다:\n\n<div class=\"content-ad\"></div>\n\n\n[의존성]\n...\nreqwest = { version = \"0.11\", features = [\"json\"] }\n\n\n여기서 우리는 오래된 좋은 빌더 패턴을 통해 HTTP 클라이언트를 구축할 수 있습니다.\n\n```rust\nlet client = Client::builder()\n    .http2_keep_alive_timeout(Duration::from_secs(120))\n    .timeout(Duration::from_secs(120))\n    .build()\n    .unwrap();\n```\n\n그러나 만약 우리가 prompt_open_ai 함수 내에서 클라이언트를 구축한다면, 우리는 각 요청마다 Client 인스턴스를 생성하게 될 것이므로, 대신 종속성을 만들어 sort_items 함수에 클라이언트 코드를 추가한 다음, 이를 sort_recursively 함수와 prompt_open_ai 함수에 전달하도록 하겠습니다.\n\n<div class=\"content-ad\"></div>\n\nHTTP 클라이언트를 한 번만 사용하여 /sort 호출당 한 번의 인스턴스만 사용할 수 있습니다. 그리고 prompt_open_ai 함수는 실제 API를 호출하고 결과를 받는 데 중점을 둘 수 있습니다.\n\n그래서 간단한 POST 호출을 구축하고 그 결과를 받는 방법을 살펴보겠습니다.\n\n코드를 깔끔하게 유지하기 위해 우리의 구조 안에 별도의 모듈을 만들 것입니다. 모듈은 코드를 저장하는 컨테이너이며 (패키지와 유사한) 코드의 서로 다른 영역 간에 분리를 만들 수 있도록 해줍니다.\n\nopenai라는 새 폴더를 만들고 그 안에 두 개의 새 파일을 만들어 주세요:\n\n<div class=\"content-ad\"></div>\n\n- 우리 코드를 위한 mod.rs 파일\n- 우리 모델을 위한 models.rs 파일을 열어보세요.\n\nmodels.rs를 열어서 OpenAI Completion API와 통신하기 위해 필요한 구조체를 추가해보세요:\n\n```js\nuse serde::{Deserialize, Serialize};\n```\n\n```js\n#[derive(Serialize)]\npub(crate) struct AskGPT {\n    pub prompt: String,\n    pub model: String,\n    pub max_tokens: usize,\n    pub stream: bool,\n    pub temperature: usize,\n    pub top_p: usize,\n    pub n: usize,\n}\n#[derive(Deserialize)]\npub(crate) struct Completion {\n    pub model: String,\n    pub choices: Vec<Choices>,\n}\n#[derive(Deserialize)]\npub(crate) struct Choices {\n    pub text: String,\n    pub index: usize,\n}\n```\n\n<div class=\"content-ad\"></div>\n\nmod.rs 파일에서 prompt_open_ai 메서드를 구축할 수 있습니다. 이 메서드는 새로 생성된 AskGPT 모델을 OpenAI의 /completions 엔드포인트에 보낼 POST 요청을 함께 합니다.\n\n이 곳에는 몇 가지 중요한 필드가 있습니다. 자명한 prompt 필드, 완성을 담당할 모델을 선택할 수 있는 model 필드 (작성 시점에 가장 성능이 좋은 것은 text-davinci-003 이며), 우리가 4096으로 설정할 max_tokens 필드 (최대치, 당연한 얘기), 응답 개수를 제어하는 n 필드 및 어떤 확률을 고려할지 알려주는 temperature 필드가 있습니다. 이 값이 높을수록 완성이 더 무작위로 보일 수 있지만, 여기서는 0을 사용하여 출력이 덜 무작위로 나오도록 합니다.\n\n참고: 이 부분에서는 OpenAI API 키가 필요합니다. 여기서 찾을 수 있습니다.\n\n```rust\nasync fn prompt_open_ai(prompt_txt: String,\n                        client: &Client) -> Result<String, String> {\n    let token = String::from(\"여기에_귀하의_API_키_입력\")\n    let auth_header = format!(\"Bearer {}\", token);\n    let req = client.post(\"https://api.openai.com/v1/completions\")\n        .header(\"Authorization\", auth_header)\n        .json(&AskGPT {\n            prompt: prompt_txt,\n            model: String::from(\"text-davinci-003\"),\n            max_tokens: 4096,\n            n: 1,\n            stream: false,\n            temperature: 0,\n        }).send().await;\n}\n```\n\n<div class=\"content-ad\"></div>\n\n결과가 나왔네요!\n\n하지만 그 결과로 뭘 할까요?\n\n우리는 그냥 await 뒤에 ?를 추가해도 되지만, 재미가 없죠. 그래서 제가 가장 좋아하는 러스트의 기능 중 하나인 유명한 match를 사용할 거에요.\n\nmatch 문은 러스트 개발 경험의 핵심이며, 강력한 패턴 매칭 기능을 제공하여 코드가 따라가는 모든 경로를 확실하게 다룰 수 있도록 도와줍니다.\n\n<div class=\"content-ad\"></div>\n\nIan씨, 이건 무슨 특별한 점이 있는 걸까요? 이게 그냥 더 강력한 if/else일 뿐인 거 아닌가요?\n\n아니에요, 이건 그 이상입니다.\n\nif/else나 switch 문들의 집합과 달리, match는 코드가 갈 수 있는 모든 가능성을 확인하도록 강요합니다. 이는 당신이 코드가 행복한 길과 슬픈 길 둘 다 다루도록 확실하게 해줍니다.\n\n이게 왜 이렇게 초능력이라고 할까요?\n\n<div class=\"content-ad\"></div>\n\n버그가 처리되지 않은 케이스로 인한 가능성을 줄이고 모든 가능한 케이스를 다루도록 강제하기 때문에 코드를 즉시 개선할 수 있습니다. 이는 코드의 가독성을 향상시키고 버그를 해결하며 유지보수성을 높이는 희귀한 도구 중 하나입니다.\n\n그러니 이를 시도해 봅시다. 구문은 간단합니다. 왼쪽에는 일치시키려는 패턴이 있고, 오른쪽에는 실행할 코드 블록이 있습니다.\n\n먼저, 우리는 실제 요청이 발생했는지 여부를 확인해볼 것입니다. 이를 통해 우리가 받은 결과(Result)를 확인합니다.\n\n```js\nmatch req {\n    Ok(response) => {\n        // 요청이 실제로 발생했습니다. 안전하게 응답에 접근할 수 있습니다.\n    }\n    Err(error) => {\n        // 오류 처리 작업\n    }\n}\n```\n\n<div class=\"content-ad\"></div>\n\n이제 우리 Ok 브랜치에서는 안심하고 응답 객체에 접근할 수 있습니다. 에러 케이스에 대비했기 때문에 런타임 중에 충돌을 일으키지 않을 것입니다.\n\n이제 우리는 요청이 실제로 성공적이었는지를 확인할 단계로 넘어갈 수 있습니다. 단순히 상태 코드가 200 OK인지 확인하여 성공한 요청인지 확인할 수 있습니다.\n\n```js\nmatch response.status() {\n    StatusCode::OK => {\n      // 흥미진진한 성공 \n    }\n    other => {\n      // TODO 에러 처리\n    }\n}\n```\n\n마지막으로, 본격적인 단계로 진행합니다 — 요청이 성공했다면, 응답을 담은 Completion 구조체로 본문을 역직렬화해야 합니다. 그러나 이 역시 실패할 수 있으므로 이 부분에서도 빠르게 매칭을 수행하고 완료 객체에서 응답을 추출해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n```rust\nmatch response.json::<Completion>().await {\n    Ok(parsed) => {\n        // 우리의 요청 매개변수 n==1 때문에 choices에 항상 적어도 1개의 항목이 있다는 것을 알고 있습니다.\n        // 그러므로 우리는 단순하게 언랩을 해서 사용할 겁니다.\n        let choices = parsed.choices.first().unwrap();\n        let json: &str = choices.text.borrow();\n        Ok(String::from(json))\n    }\n    Err(err) => {\n            return Err(Parsing);\n        }\n}\n```\n\n이제 에러를 처리하는 방법을 알아봅시다 — 여러분, 저는 이 세 유형의 에러로 모든 가능한 에러를 압축할 것입니다. 어떤 문제가 생길까요.. — 연결 에러(connection error), 서버 응답 에러(server response error) 및 파싱 에러(parsing error)를 표시하는 열거형(enum)을 추가합시다. models.rs로 올라가서 다음과 같이 추가해보세요:\n\n```rust\n#[derive(Debug)]\npub(crate) enum OpenAiError {\n    Connection,\n    Parsing,\n    Server,\n}\n\nmatch req {\n    Ok(response) => {\n        match response.status() {\n            StatusCode::OK => {\n                match response.json::<Completion>().await {\n                    Ok(parsed) => {\n                        // 우리의 요청 때문에 최소한 1개의 항목이 항상 있습니다.\n                        let choices = parsed.choices.first().unwrap();\n                        let json: &str = choices.text.borrow();\n                        Ok(String::from(json))\n                    }\n                    Err(err) => Err(Parsing);\n                }\n            }\n            other => Err(Server)          \n        }\n    }\n    Err(err) => Err(Connection)\n}\n```\n\n축하합니다! 우리는 안전하게 요청을 보내고 이 과정에서 모든 안좋은 상황과 기쁜 상황을 모두 다루었습니다. \n\n\n<div class=\"content-ad\"></div>\n\n우리 요청이 증가하면서, 이제 드디어 재귀적으로 정렬하는 함수인 sort_recursively 함수에 대해 작업을 시작할 수 있게 되었어요. 여기서 왜 재귀일까요? 왜냐하면 우리는 GPT3가 우리의 축소 함수로 작용하면서 리스트를 자기 자신에게 축소하고 있거든.\n\n우리가 여기서 루프를 사용하고 이 방법을 n번 호출할 수는 있지만, 이렇게 하면 루프 외부의 변수(우리의 카테고리를 포함하는 변수)도 변경해야 하기 때문에 조금 어수선해집니다. 그러니까, 더럽게 하기보다는 재귀를 통해 깔끔하고 기능적인 방식으로 해결할 거에요.\n\n그러니까 이제 main.rs 파일을 열어서 sort_recursively 함수에 들어가 보도록 해요.\n\n먼저 우리의 프롬프트를 구축한 다음, prompt_open_ai로 보내고 응답을 역직렬화하려고 해요. 성공하면 기존의 카테고리들과 연결하고 남은 청크를 가지고 다시 sort_recursively에 전달합니다. 이를 계속해서 반복하여 남은 청크가 없을 때까지 반복합니다.\n\n<div class=\"content-ad\"></div>\n\n```rust\nasync fn sort_recursively(\n    sorted_categories: Vec<CategoryWithItems>,\n    remaining: Vec<Vec<Item>>,\n    client: Client) -> Result<Categories, Error> {\n    let prompt = build_prompt(remaining.first().unwrap().to_vec(), sorted_categories);\n    let ai_response = prompt_open_ai(prompt, &client).await.unwrap();\n    let json = ai_response.as_str();\n    \n    let generated = serde_json::from_str::<Categories>(json);\n    \n    let result = generated.map_err(|err| err.to_string()).and_then(|res| {\n        res.map_err(|err| err.to_string()).and_then(|wrapper| {\n            let mut new_categories = wrapper.categories.to_owned();\n            let mut next_slice = remaining.to_owned();\n            next_slice.remove(0);\n            next_categories.append(&mut new_categories);\n            \n            if next_slice.len() != 0 {\n                sort_recursively(next_categories, next_slice, client).await\n            } else {\n                Ok(Categories { categories: next_categories })\n            }\n        })\n    });\n\n    result\n}\n```\n\nWith all these matches, our code is starting to look quite messy. One way to avoid nested match hell is to use map, map_err, and and_then extensions — they operate on either the left (map) or the right (map_err) side of the Result, enabling us to avoid nesting hell by simply chaining them into a more readable, concise version of it.\n\nThe data will pass only through the corresponding operands so we can safely map our data and errors to the proper format.\n\nWe’ll use it to reduce the first set of nested matches and we’ll leave the last one as a match. Why? Because async closures still aren’t stable in Rust it seems. We’ll map all the errors into an `Err(String)` format so we can return it properly.\n\n\n<div class=\"content-ad\"></div>\n\n```rust\nasync fn sort_recursively(sorted_categories: Vec<CategoryWithItems>,\n                          remaining: Vec<Vec<Item>>,\n                          client: Client) -> Result<Categories, String> {\n    let mut next_categories = Vec::from(sorted_categories.deref());\n    let prompt = build_prompt(remaining.first().unwrap().to_vec(),\n                              sorted_categories);\n    let ai_response_result = prompt_open_ai(prompt, &client).await;\n    let res = ai_response_result\n        .map_err(|e|\n                format!(\"OpenAI와 통신 중 오류 발생 - {:?}\", e))\n        .and_then(|ai_response|\n            serde_json::from_str::<Categories>(ai_response.as_str())\n                .map_err(|_| \"응답 파싱 오류\".to_string()));\n    match res {\n        Ok(wrapper) => {\n            let mut new_categories = wrapper.categories.to_owned();\n            // 처리된 청크 제거\n            let mut next_slice = remaining.to_owned();\n            next_slice.remove(0);\n            // 카테고리 합치기\n            next_categories.append(&mut new_categories);\n            // 아직 끝나지 않았다면 재귀 호출\n            if next_slice.len() != 0 {\n                sort_recursively(next_categories, \n                                next_slice,\n                                client).await\n                    .map_err(|e| \n                        format!(\"정렬 실패, 이유: {}\", e))\n            } else {\n                Ok(Categories { categories: next_categories })\n            }\n        }\n        Err(msg) => Err(msg)\n    }\n}\n```\n\n여기 있습니다 — 우리는 API를 안전하고 오류없이 호출했는데요...\n\n컴파일되지 않네요.\n\n음, 한 가지 생각하지 못한 것은 비동기 재귀입니다.\n\n\n<div class=\"content-ad\"></div>\n\n이게 왜 문제가 됐는지 궁금하시죠?\n\nRust (그리고 다른 많은 언어들)에서 async/await가 어떻게 구현되는지 때문에, 내부적으로는 해당 메서드 안에 있는 모든 futures를 포함하는 상태 머신 타입을 생성합니다.\n\n그런데 이제 여기에 재귀를 추가하면, 생성된 타입이 자기 자신을 참조하기 시작합니다 — 그래서 내부적으로 잠재적으로 무한히 재귀적인 타입으로 폭발하고 컴파일러는 타입의 크기를 결정할 수 없게 됩니다.\n\n은 폭발하는 것을 막기 위해, 재귀를 수정하여 Box에 포장된 Future를 반환해야 하는데, 그러면 힙에 대한 포인터만 얻게 되어 전체 객체가 아닌 포인터를 제공하여 내부적으로 무한 자기 참조를 방지할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제에 대해 좀 더 읽어보고 더 깊이 파고들어보시는 것을 추천드립니다. 많은 언어에서 나타나는 언어 디자인 질문과 개념들을 다루고 있습니다. 하지만 지금은 async_recursion 크레이트를 사용할 것이니, Cargo.toml로 이동해서 다음과 같이 추가해주세요:\n\n```js\n[dependencies]\n..\nasync-recursion = \"1.0.2\"\n```\n\n그리고 함수에 #[async_recursion] 매크로를 붙여서 Box 처리할 수 있도록 해주세요.\n\n이제 이 문제를 해결했으므로, 원래의 sort_items 메서드로 돌아가서 마침내 API 요청에 응답할 차례입니다. 마지막으로 그곳에 Client 인스턴스를 추가한 채로, 그 아래로 내려가서 sort_recursively 메서드를 호출하고, map_err를 사용하여 오류를 ErrorResponse 구조체로 매핑하고, JSON으로 래핑하여 응답으로 반환하고, map을 사용하여 Ok 결과를 적절한 응답으로 바꿉니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nsort_recursively(categories, prompt_slices, client).await\n    .map_err(|e| \n        (StatusCode::INTERNAL_SERVER_ERROR, \n        Json(ErrorResponse { message: e })).into_response())\n    .map(|wrapper| {\n        let new_categories = wrapper.categories.iter().map(|item| {\n            CategoryWithItems {\n                category_id: item.category_id.to_owned(),\n                category_name: item.category_name.to_owned(),\n                items: item.items.to_owned(),\n            }\n        }).collect::<Vec<CategoryWithItems>>();\n        (StatusCode::OK, Json(Categories {\n            categories: new_categories\n        })).into_response()\n    })\n```\n\n그리고 이렇게 하면, 이제 우리의 서비스가 완료되었습니다!\n\n우리는 응답을 가져와서 형식을 지정하고 사용자에게 돌려줍니다. 우리의 계획은 안전하고 제대로 되어 있습니다. 배포해야 할 일만 남았는데, 인스턴스 프로비저닝, 보안 그룹 설정 또는 도커파일 작성에 대해 걱정할 필요가 없습니다. 셔틀을 통해 서비스를 만들었기 때문에 단순히 터미널을 사용하여 쉽게 배포할 수 있습니다.\n\n쉘에서 프로젝트 폴더를 열고 다음을 입력하세요:\n\n<div class=\"content-ad\"></div>\n\n카고 셔틀을 배포했습니다.\n\n이제 일어서서 몇 번 숨 쉬고, 커피 한 모금을 한 후, 새로운 서버가 이미 https://projectname.shuttleapp.rs/ 에서 구동되고 있다는 것을 알게 될 거에요.\n\n자, 그런데... 이 작업을 왜 했더라?\n\n아, 맞다, JS 확장 프로그램을 작성 중이었죠. 서버가 올라와서 거의 다 완성됐어요. 확장 프로그램으로 이동해서 localhost 엔드포인트를 방금 셔틀로부터 받은 실제 엔드포인트로 교체하기만 하면 돼요.\n\n<div class=\"content-ad\"></div>\n\n이제 확장 프로그램을 테스트하기 위해 작은 창에 로드하세요. 정렬 버튼을 누르고 잠시 기다리면 — 바로 그렇습니다! 탭이 마법처럼 올바른 그룹으로 정리될 것입니다! 마침내!\n\n이제 본격적으로 하나의 창에서 해보죠 — 탭이 이미 600개가 가까워진 창이죠. 그러니까 정렬 버튼을 누르고 — 기다려보세요...\n\n...기다려보세요...\n\n.....조금 더 기다려보세요...\n\n<div class=\"content-ad\"></div>\n\n잠시만 기다려주세요...\n\n60초보다 훨씬 오래 걸리고 있는 것 같네요...\n\n아, 기다려봐요...\n\n에러가 발생한 것 같네요?\n\n<div class=\"content-ad\"></div>\n\n앗— 토큰 한도에 도달했네요!\n\n왜 그럴까요? 우린 전체 덩어리를 맞게 만들기 위해 청킹 작업을 했는데 말이죠?\n\n음, 많이 보니까 프롬프트 크기 계산이 좀 더 정확히 필요해 보여요.\n\n그리고 재귀가 문제를 일으키고 있는 것 같아요— 각 프롬프트에 이전 카테고리를 모두 추가하는 방식으로 크기가 폭발적으로 증가하고 전체 체인을 완료하는 데 정말 매우 오랜 시간이 걸리고요— 60초보다 훨씬 더 말이에요.\n\n<div class=\"content-ad\"></div>\n\n그리고 마지막으로, 카테고리가 조금... 별로네요.\n\n다음 반복에 할 일이 더 많아져서 좋네요. 이 재귀를 제거하는 방법, GPT 토크나이저 사용법, 딕셔너리 파일을 이진 파일에 넣는 방법, 그리고 시간을 낭비하지 않고 셔틀의 정적 폴더 서비스를 사용하는 방법 등을 알아볼 거에요.\n\n또한 모델을 미세 조정해서 토큰을 줄이면서 더 나은 결과를 얻을 예정이에요. 그리고 우리가 게으르다는 것을 감안해, 훈련 데이터 생성은 GPT 자체를 사용할 거예요.\n\n지금까지 읽어 주셔서 감사합니다. 다음 시리즈에서 계속해서 발전하고 잠재적 문제를 해결할 예정이니 걱정 마시고 '인간 대 기계'의 다음 에피소드에서 만나요.\n\n<div class=\"content-ad\"></div>\n\n`<img src=\"/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_7.png\" />`","ogImage":{"url":"/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_0.png"},"coverImage":"/assets/img/2024-05-20-Sorting400Tabsin60SecondsWithJavaScriptRustandGPT-3_0.png","tag":["Tech"],"readingTime":39},{"title":"React Conf 2024에서 새롭게 소개된 내용들","description":"","date":"2024-05-20 21:45","slug":"2024-05-20-WhatsNewatReactConf2024","content":"\n\n## 빠르게 따라잡는 빠른 안내서\n\n![React Conf 2024](/assets/img/2024-05-20-WhatsNewatReactConf2024_0.png)\n\n내 무료 뉴스레터에서 원본으로 작성되었습니다.\n\nReact 개발자들은 모두 2024년 5월 15일에 공식적으로 시작된 React Conf 2024에 집중하고 있습니다. 여기 8시간의 비디오 다시보기: [React Conf 2024 Video](링크).\n\n<div class=\"content-ad\"></div>\n\n아래는 제가 강조해야 한다고 생각하는 몇 가지 주요 사항을 빠르게 요약한 글입니다. 이를 통해 빠르게 최신 정보를 파악할 수 있습니다.\n\n# React Router와 Remix의 통합\n\nRemix가 React Router와 통합을 발표했습니다. 다가오는 React Router v7에서는 모든 Remix 기능이 포함될 것입니다. Remix 사용자는 import 문을 변경하면 됩니다:\n\n```js\n- import { Link } from `@remix-run/react`\n+ import { Link } from `react-router`\n```\n\n<div class=\"content-ad\"></div>\n\n리액트 라우터 사용자들을 위해, 이제 SSR, prefetching 또는 Vite 플러그인을 포함한 Remix 기능을 React 프로젝트에서 직접 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-WhatsNewatReactConf2024_1.png)\n\nRemix는 항상 React Router 위에 있는 레이어에 불과했으며 시간이 지남에 따라 이 레이어가 줄어들고 있습니다. 이제 그 크기가 매우 작아져서 예정된 Remix v3 릴리스가 이제 React Router v7로 출시될 예정입니다.\n\n사실 이렇게 말해도 될 것 같습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-20-WhatsNewatReactConf2024_2.png\" />\n\n# React 19에서 새로운 기능 소개\n\n여기에 빠르게 코드 중심의 요약이 있고, 좀 더 간략한 내용은 여기에 있습니다:\n\n## Actions 기능\n\n<div class=\"content-ad\"></div>\n\n개선된 비동기 작업 및 상태 업데이트 처리를 위한 useTransition 및 useOptimistic과 같은 훅을 사용하여 suspense, 오류 처리 및 낙관적 업데이트의 관리를 간편화했습니다.\n\n## 서버 구성 요소:\n\n- 서버 구성 요소: React 19에서 공식적으로 서버 구성 요소 통합을 지원하며, 빌드 시간 이전에 구성 요소를 사전 렌더링하도록 허용하며, 빌드 시간 실행 및 실시간 요청 처리 두 가지 모드가 있습니다.\n- 서버 액션: 클라이언트 측 구성 요소가 “use server” 지시어를 사용하여 서버에서 비동기 함수를 호출하고 실행할 수 있으며, 프레임워크가 서버 함수에 대한 참조를 생성합니다.\n\n## 기능 최적화:\n\n<div class=\"content-ad\"></div>\n\n- Ref 로 속성 전달: ref를 함수 컴포넌트 인수로 직접 전달할 수 있어서 forwardRef가 필요하지 않게 되었습니다.\n- 향상된 수분 공급 오류 보고: 클라이언트 측 렌더링이 서버 측 렌더링된 콘텐츠와 일치하지 않을 때 개선된 오류 보고로 더 명확한 오류 메시지를 제공합니다.\n- 공급자 최적화: 기존의 'Context.Provider'가 필요하지 않도록 'Context'를 직접 공급자로 사용합니다.\n- Ref 정리 함수: 컴포넌트가 언마운트될 때 정리를 처리하기 위해 ref 콜백 함수에서 정리 함수를 반환하는 지원이 추가되었습니다.\n- useDeferredValue의 초기 값: 컴포넌트의 초기 렌더링에 값을 지정할 수 있습니다.\n- 문서 메타데이터 지원: React를 사용하여 컴포넌트 내에서 'title', 'link', 'meta' 태그를 직접 정의하고 자동으로 이를 문서 'head'로 승격시켜 줍니다.\n- 스타일시트 지원: 컴포넌트 트리 내에서 스타일시트를 관리하는 기능이 내장되어 있으며, 로딩 순서를 자동으로 처리합니다.\n- 비동기 스크립트 지원: 컴포넌트 트리 어디에서든 비동기 스크립트를 렌더링하여 스크립트 관리를 간소화합니다.\n- 리소스 프리로딩 지원: prefetchDNS, preconnect, preload, preinit과 같은 사전로드 API를 도입하여 리소스 로딩을 최적화합니다.\n- 타사 스크립트 및 브라우저 익스텐션 호환성: 타사 스크립트 및 브라우저 익스텐션과의 호환성이 개선되었습니다.\n- 더 나은 오류 보고: 오류 처리를 향상시키기 위한 다양한 옵션이 추가되었습니다.\n- 사용자 정의 요소 지원 (웹 컴포넌트): 사용자 정의 요소에 대한 지원이 개선되었습니다.\n\n# React 컴파일러\n\nReact 컴파일러인 Forget으로도 알려진 React 컴파일러가 이제 오픈 소스입니다. 여기에서 찾아볼 수 있습니다. Rust로 작성되었으며 React 19 베타 또는 온라인 Playground에서 사용해 볼 수 있습니다:\n\n<img src=\"/assets/img/2024-05-20-WhatsNewatReactConf2024_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n개발자들에 미치는 영향은 useMemo, useCallback, React.memo API를 사용하여 수동으로 최적화를 할 필요가 없어졌다는 점입니다. 이는 현재 이에 한정되어 있으며 useEffect와 같은 의존성 규칙에는 영향을 미치지 않습니다. 현재도 여전히 React 훅 규칙 (예: 훅을 최상위 레벨에서만 호출하는 것과 같은)을 따라야 합니다.\n\n컴파일러에 의해 최적화된 컴포넌트는 React Devtools (v5.0+)에서 \"Memo ✨\" 배지가 표시됩니다:\n\n![Memo Badge](/assets/img/2024-05-20-WhatsNewatReactConf2024_4.png)\n\n# 두 대의 컴퓨터를 위한 리액트\n\n<div class=\"content-ad\"></div>\n\nDan Abramov가 React 클라이언트 컴포넌트와 서버 컴포넌트의 각각의 장점을 소개했고, 어떻게 선택해야 하는지에 대해 설명했습니다. 여기에 간략하게 정리해보겠습니다:\n\n## 서버 사이드 컴포넌트의 장점\n\n- 데이터 접근: 서버 사이드 컴포넌트는 서버의 데이터와 파일에 접근할 수 있어 데이터 집약적인 애플리케이션에 유용합니다.\n- 데이터 전처리: 서버 사이드 컴포넌트는 데이터를 읽고 전처리한 후 클라이언트로 보내는데 유용합니다.\n- 빌드 시 렌더링: 서버 사이드 컴포넌트는 빌드 시 실행하여 정적 UI를 생성할 수 있어 SEO 및 초기 로드 성능에 도움이 됩니다.\n- 클라이언트 단순화: 서버에서 복잡한 데이터 로직을 처리함으로써 (UI = f(데이터)), 클라이언트 부담을 줄일 수 있으며, 클라이언트는 필요한 UI 데이터만 받아 보여줍니다.\n\n## 클라이언트 사이드 컴포넌트의 장점:\n\n<div class=\"content-ad\"></div>\n\n- 즉시 피드백: 사용자가 UI와 상호 작용할 때(예: 버튼 클릭), 서버 응답을 기다리지 않고 즉시 피드백을 받을 수 있습니다.\n- 서버 폴링 없음: 사용자 작업 중 일부(슬라이더 드래깅 또는 버튼 클릭 등)에 대해 서버로부터 추가 요청이나 데이터 다운로드가 필요하지 않습니다.\n- 더 나은 사용자 경험: 직접적인 상호 작용 반응은 사용자 경험을 향상시켜 응용 프로그램이 더 반응적이고 부드러워지도록 합니다.\n- 클라이언트 측 상태 사용: 컴포넌트는 클라이언트 측 상태(UI = f(상태))를 사용하여 매우 상호 작용적이고 반응성이 좋은 사용자 인터페이스를 구축할 수 있습니다.\n\n# React Server Components in Expo Router\n\nExpo Router는 React Native 및 웹 애플리케이션용 파일 기반 라우터입니다. 이를 통해 응용 프로그램의 화면 간 탐색을 관리하고 Android, iOS 및 웹과 같은 여러 플랫폼에서 사용자가 응용 프로그램 UI의 다른 부분을 매끄럽게 이동할 수 있도록 합니다.\n\n서버 구성 요소의 장점은 클라이언트에 완전히 상호 작용적인 동적 UI를 전송할 수 있다는 것입니다. 이는 응용 프로그램이 다른 사용자 작업에 기반한 복잡한 UI 요소를 제공할 수 있음을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n# React 규칙 어기기\n\nReact에는 몇 가지 규칙이 있어요:\n\n![React Rules](/assets/img/2024-05-20-WhatsNewatReactConf2024_5.png)\n\nCharlotte은 이러한 규칙에 대한 이유를 논의하여 React의 내부 메커니즘을 더 깊이 이해하려고 합니다.\n\n<div class=\"content-ad\"></div>\n\n최근에 React 아래의 내부를 더 깊이 이해하기 위해 기사를 썼어요. 간소화된 Fiber 아키텍처와 동시 모드를 사용하여 렌더링 중에 메인 스레드를 차단하지 않도록 했죠. 여기서 이 지침을 어기면 안 되는 이유도 이해할 수 있어요.\n\n# React 서버 구성 요소로 RedwoodJS\n\nRedwoodJS는 포함된 배터리를 갖춘 또 다른 풀스택 JavaScript 애플리케이션 프레임워크에요. 주로 스타트업을 대상으로 하고 있습니다.\n\n높은 수준에서, 이는 사용자 정의 GraphQL API와 통신하는 React 프런트엔드입니다. API는 데이터베이스와 상호 작용하기 위해 Prisma를 사용합니다. 기본 설정으로는 테스트에 Jest, 로깅에 Pino, UI 구성 요소 목록에 Storybook을 사용할 수 있어요. 인증(Auth0 같은) 또는 CSS 프레임워크(Tailwind CSS 같은) 설정은 명령줄 호출만으로도 간단하게 할 수 있어요. 게다가 Redwood의 아키텍처를 통해 서버리스 공급자(Netlify, Vercel 등) 또는 전통적인 서버 및 컨테이너 공급자(AWS, Render 등)로 배포할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이것은 주로 웹 개발에 중점을 둔 Day 1에서의 최신 정보이다. Day 2는 React Native에 관한 것입니다.\n\n가장 기대되는 것은 React 컴파일러입니다. 아직 실험 중이지만, 현재 제품에서 시도해보고 싶다면 피드백을 제공하는 것을 도와주기 위해 작업 그룹에 참여할 수 있습니다.\n\n만약 이 글이 도움이 된다면, 더 많은 웹 개발 통찰력을 위해 구독을 고려해 주세요. 읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-05-20-WhatsNewatReactConf2024_0.png"},"coverImage":"/assets/img/2024-05-20-WhatsNewatReactConf2024_0.png","tag":["Tech"],"readingTime":6},{"title":"대용량 JSON 파일을 효율적으로 전송하는 방법","description":"","date":"2024-05-20 21:43","slug":"2024-05-20-HowToTransferLargeJSONFilesEfficiently","content":"\n\n<img src=\"/assets/img/2024-05-20-HowToTransferLargeJSONFilesEfficiently_0.png\" />\n\n대규모 JSON 데이터를 전송할 때, 기존 방식을 사용하면 데이터 처리를 시작하기 전에 완전한 JSON 데이터를 수신해야 하므로 사용자 경험에 영향을 줄 수 있습니다. 이 문제를 해결하기 위해 기존의 JSON 스트림 구문 분석 라이브러리를 사용할 수 있습니다. 예를 들어 내부적으로 TextDecoder API를 사용하는 @streamparser/json이 있습니다.\n\nTextDecoder API는 이진 데이터 (일반적으로 ArrayBuffer 또는 TypedArray)를 문자열로 디코딩하기 위한 JavaScript API입니다. 이는 Web 플랫폼의 일부이며 텍스트 인코딩의 디코딩을 처리하는 데 주로 사용됩니다. 예를 들어, 서버로부터 수신한 스트리밍 데이터, 파일 데이터 등을 처리하는 데 사용됩니다.\n\n# TextDecoder API 사용 이유\n\n<div class=\"content-ad\"></div>\n\n웹 애플리케이션에서 이진 데이터를 다룰 때, 이 데이터를 읽을 수 있는 문자열 형식으로 변환해야 하는 경우가 종종 있습니다. TextDecoder를 사용하면 이를 효율적이고 편리하게 할 수 있습니다.\n\nTextDecoder API에는 다음과 같은 기능이 있습니다:\n\n- 효율적: 수동으로 바이트 단위로 처리하는 것보다 효율적이며, 문자열로 직접 디코딩이 가능합니다.\n- 여러 인코딩 지원: 다양한 텍스트 인코딩(예: UTF-8, UTF-16, ISO-8859-1 등)을 지원합니다.\n- 스트리밍 처리 지원: 데이터를 조각조각으로 처리할 수 있어 대용량 데이터 스트림 또는 실시간 처리가 필요한 데이터에 적합합니다.\n\n# TextDecoder API 사용 방법\n\n<div class=\"content-ad\"></div>\n\n다음으로, TextDecoder API를 사용하는 네 가지 시나리오를 소개하겠습니다:\n\n- 서로 다른 인코딩된 바이너리 데이터 해독\n- 스트리밍 JSON 데이터 디코딩\n- 대용량 JSON 파일의 데이터 청크 디코딩\n\n## 1. 서로 다른 인코딩된 바이너리 데이터 해독\n\n```js\n// 다른 인코딩으로 TextDecoder 인스턴스 생성\nconst utf16Decoder = new TextDecoder('utf-16');\nconst iso88591Decoder = new TextDecoder('iso-8859-1');\n\nconst utf16Array = new Uint16Array([0x0048, 0x0065, 0x006C, 0x006C, 0x006F]);\nconst iso88591Array = new Uint8Array([72, 101, 108, 108, 111]);\n\n// 문자열로 디코딩\nconst utf16String = utf16Decoder.decode(utf16Array);\nconst iso88591String = iso88591Decoder.decode(iso88591Array);\n\nconsole.log(utf16String); // 출력：\"Hello\"\nconsole.log(iso88591String); // 출력：\"Hello\"\n```\n\n<div class=\"content-ad\"></div>\n\n## 2. 스트리밍 JSON 데이터 디코딩\n\n먼저, 결과를 살펴봅시다:\n\n![이미지](/assets/img/2024-05-20-HowToTransferLargeJSONFilesEfficiently_1.png)\n\n위의 예제에서는 Node.js의 http 모듈을 사용하여 로컬 SSE (서버 전송 이벤트) 서버를 빠르게 구축합니다.\n\n<div class=\"content-ad\"></div>\n\nserver.js\n\n```js\nconst http = require(\"http\");\n\nconst PORT = 3000;\n\nconst server = http.createServer((req, res) => {\n  if (req.url === \"/sse\") {\n    res.writeHead(200, {\n      \"Content-Type\": \"text/event-stream\",\n      \"Cache-Control\": \"no-cache\",\n      Connection: \"keep-alive\",\n      \"Access-Control-Allow-Origin\": \"*\",\n      \"Access-Control-Allow-Headers\":\n        \"Origin, X-Requested-With, Content-Type, Accept\",\n    });\n\n    let id = 1;\n    const interval = setInterval(() => {\n      const data = {\n        id: id,\n        message: `This is message ${id}`,\n        timestamp: +new Date(),\n      };\n      res.write(`data: ${JSON.stringify(data)}\\n\\n`);\n\n      if (id == 5) {\n        res.write(\"event: end\\n\");\n        res.write(\"data: End of stream\\n\\n\");\n        clearInterval(interval);\n        res.end();\n      }\n\n      id++;\n    }, 1000);\n\n    req.on(\"close\", () => {\n      clearInterval(interval);\n    });\n  } else {\n    res.writeHead(404, { \"Content-Type\": \"text/plain\" });\n    res.end(\"404 Not Found\");\n  }\n});\n\nserver.listen(PORT, () => {\n  console.log(`Server is running on http://localhost:${PORT}`);\n});\n```\n\nsse 핸들러에서는 Content-Type 응답 헤더의 유형을 \"text/event-stream\"으로 설정하여 클라이언트에게 스트리밍 데이터를 반환한다는 것을 알려줍니다.\n\nindex.html\n\n<div class=\"content-ad\"></div>\n\n```js\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>SSE & TextDecoder</title>\n</head>\n<body>\n    <h1>서버-전송 이벤트 JSON 스트림 데이터 디코딩</h1>\n    <div id=\"messages\"></div>\n    <script src=\"client.js\"></script>\n</body>\n</html>\n```\n\nclient.js\n\n```js\ndocument.addEventListener(\"DOMContentLoaded\", () => {\n  const messagesDiv = document.querySelector(\"#messages\");\n  const textDecoder = new TextDecoder(\"utf-8\");\n\n  fetch(\"http://localhost:3000/sse\").then((response) => {\n    const reader = response.body.getReader();\n    return new ReadableStream({\n      start(controller) {\n        function push() {\n          reader.read().then(({ done, value }) => {\n            if (done) {\n              controller.close();\n              return;\n            }\n\n            const chunk = textDecoder.decode(value, { stream: true });\n            const lines = chunk.split(\"\\n\");\n\n            for (const line of lines) {\n              if (line.startsWith(\"data: \")) {\n                const json = line.slice(6);\n                const data = JSON.parse(json);\n                const p = document.createElement(\"p\");\n                p.textContent = `ID: ${data.id}, Message: ${data.message}, Timestamp: ${data.timestamp}`;\n                messagesDiv.appendChild(p);\n              } else if (line.startsWith(\"event: end\")) {\n                const p = document.createElement(\"p\");\n                p.textContent = \"스트림의 끝\";\n                messagesDiv.appendChild(p);\n                return;\n              }\n            }\n            push();\n          });\n        }\n        push();\n      },\n    });\n  });\n});\n```\n\nSSE 이벤트 스트림은 간단한 텍스트 데이터 스트림이며 해당 텍스트는 UTF-8 형식을 사용하여 인코딩됩니다. 따라서 textDecoder 객체를 생성할 때 인코딩을 utf-8로 설정해야 합니다. 텍스트Decoder 객체가 준비되면 해당 객체가 제공하는 decode 메소드를 호출하여 디코딩할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 3. 대용량 JSON 파일에서 데이터 청크 디코딩하기\n\n다시 한번, 먼저 결과를 살펴봅시다:\n\n![이미지](/assets/img/2024-05-20-HowToTransferLargeJSONFilesEfficiently_2.png)\n\n위 그림에서 JSON 데이터 출력은 다음 large.json 파일에서 나온 것입니다. 우리는 파일을 0.5KB씩 잘라서 500ms마다 다음 청크를 보내는 방식으로 처리합니다. @streamparser/json 라이브러리를 사용하여 JSON 청크를 파싱할 수 있는 기능을 구현했습니다.\n\n<div class=\"content-ad\"></div>\n\n\nlarge.json\n\n```js\n[\n  {},\n  {\n    \"image\": [\n      {\n        \"shape\": \"rect\",\n        \"fill\": \"#333\",\n        \"stroke\": \"#999\",\n        \"x\": 0.5e1,\n        \"y\": 0.5,\n        \"z\": 0.8,\n        \"w\": 0.5e5,\n        \"u\": 2e10,\n        \"foo\": 2e1,\n        \"bar\": 2,\n        \"width\": 47,\n        \"height\": 47\n      }\n    ],\n    \"corners\": { \"1\": true, \"3\": true, \"7\": true, \"9\": true }\n  },\n ...\n]\n```\n\njson-server.js\n\n```js\nconst http = require(\"http\");\nconst { join } = require(\"path\");\nconst { readFileSync } = require(\"fs\");\n\nconst PORT = 3000;\n\nconst largeJson = readFileSync(join(__dirname, \"large.json\")).toString();\n\nconst server = http.createServer((req, res) => {\n  if (req.url === \"/stream-json\") {\n    res.writeHead(200, {\n      \"Content-Type\": \"application/json\",\n      \"Cache-Control\": \"no-cache\",\n      Connection: \"keep-alive\",\n      \"Access-Control-Allow-Origin\": \"*\",\n      \"Access-Control-Allow-Headers\":\n        \"Origin, X-Requested-With, Content-Type, Accept\",\n    });\n\n    const CHUNK_SIZE = 512;\n    let position = 0;\n\n    const interval = setInterval(() => {\n      const chunk = largeJson.slice(position, position + CHUNK_SIZE);\n      res.write(chunk);\n      position += CHUNK_SIZE;\n\n      if (position >= largeJson.length) {\n        clearInterval(interval);\n        res.end();\n      }\n    }, 500);\n\n    req.on(\"close\", () => {\n      clearInterval(interval);\n    });\n  } else {\n    res.writeHead(404, { \"Content-Type\": \"text/plain\" });\n    res.end(\"404 Not Found\");\n  }\n});\n\nserver.listen(PORT, () => {\n  console.log(`Server is running on http://localhost:${PORT}`);\n});\n``` \n\n\n<div class=\"content-ad\"></div>\n\nstream.html\n\n```js\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Stream JSON</title>\n  </head>\n  <body>\n    <h1>Stream JSON</h1>\n    <div id=\"messages\"></div>\n    <script type=\"module\">\n      import { JSONParser } from \"https://cdn.jsdelivr.net/npm/@streamparser/json-whatwg@0.0.21/+esm\";\n      const messagesDiv = document.querySelector(\"#messages\");\n\n      document.addEventListener(\"DOMContentLoaded\", async () => {\n        const parser = new JSONParser();\n\n        const response = await fetch(\"http://localhost:3000/stream-json\");\n\n        const reader = response.body.pipeThrough(parser).getReader();\n        while (true) {\n          const { done, value: parsedElementInfo } = await reader.read();\n          if (done) break;\n\n          const { value, key, parent, stack, partial } = parsedElementInfo;\n          if (partial) {\n            console.log(`Parsing value: ${value}... (still parsing)`);\n          } else {\n            const p = document.createElement(\"p\");\n            p.textContent = `${JSON.stringify(value)}`;\n            messagesDiv.appendChild(p);\n            console.log(`Value parsed: ${JSON.stringify(value)}`);\n          }\n        }\n      });\n    </script>\n  </body>\n</html>\n```\n\n@streamparser/json 라이브러리는 다른 용도도 있어요. 관심이 있다면 사용 설명서를 살펴보세요. TextDecoder API에 대한 다른 유용한 사용 사례가 있으면 댓글을 남겨주세요.\n\nTypeScript는 정말 멋지고 배울 가치가 있어요. TypeScript를 배우고 싶다면 Medium이나 Twitter에서 저를 팔로우해서 TS와 JS에 관한 더 많은 내용을 읽을 수 있어요!","ogImage":{"url":"/assets/img/2024-05-20-HowToTransferLargeJSONFilesEfficiently_0.png"},"coverImage":"/assets/img/2024-05-20-HowToTransferLargeJSONFilesEfficiently_0.png","tag":["Tech"],"readingTime":9},{"title":"러스트 트레이트 TypeScript 인터페이스의 강력한 대안","description":"","date":"2024-05-20 21:42","slug":"2024-05-20-RustTraitAPowerfulAlternativeToTypeScriptInterface","content":"\n\n\n![RustTraitAPowerfulAlternativeToTypeScriptInterface](/assets/img/2024-05-20-RustTraitAPowerfulAlternativeToTypeScriptInterface_0.png)\n\nRust은 인터페이스 개념을 갖고 있지만, 다른 프로그래밍 언어들과는 다르게 클래스와 함수의 동작을 지정하기 위해 인터페이스 키워드를 사용하지 않습니다. 대신, Rust의 가장 가까운 추상화 패턴은 트레이트입니다. 이러한 개념들은 많은 차이가 있지만, 둘 다 다중 가능한 구현을 다루는 문제를 해결합니다.\n\n이 블로그 포스트에서는 TypeScript 코드 조각과 잠재적인 Rust 동등 코드를 비교하여 간단하고 유연하며 조립 가능한 코드를 어떻게 구현하는지를 보여줄 것입니다.\n\n# 선언\n\n\n<div class=\"content-ad\"></div>\n\n이는 데이터베이스에 문서 및 이미지를 저장하고 나열하는 프로젝트를 상상해 봅시다. 두 유형의 파일이 동일한 저장소에 저장되고 공통 특성을 공유하기 때문에 공통 정보를 공유하기 위해 인터페이스를 사용할 수 있습니다.\n\n인터페이스를 사용하면 공통 속성 및 메소드를 정의하여 어느 유형의 파일과도 작업할 수 있는 코드를 쉽게 작성할 수 있습니다.\n\nTypeScript에서는 이러한 인터페이스를 다음과 같이 정의할 수 있습니다:\n\n```js\ninterface Entity {\n    id: string;\n    timestamp: number;\n}\n\ninterface Document extends Entity {\n    revised: boolean;\n}\n\ninterface Image extends Entity {\n    type: string;\n}\n```\n\n<div class=\"content-ad\"></div>\n\nRust에서는 상속이 없기 때문에 가장 간단한 대응 구현은 타입을 복제해야 한다.\n\n```js\nstruct Document {\n    id: String,\n    timestamp: u64,\n    revised: bool,\n}\n\nstruct Image {\n    id: String,\n    timestamp: u64,\n    mime_type: String,\n}\n```\n\n# 상속 및 제네릭\n\n이제 특정 문서 또는 이미지를 찾고 싶은 시나리오를 고려해보겠습니다. TypeScript에서는 다음과 같은 코드로 이를 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nconst getDocument = (\n id: string,\n documents: Document[]\n): Document | undefined =>\n documents.find(({ id: docId }) => docId === id);\n\nconst getImages = (\n id: string,\n images: Image[]\n): Image | undefined =>\n images.find(({ id: imageId }) => imageId === id);\n```\n\n하지만 두 함수가 동일한 인터페이스를 구현하기 때문에 중복을 피할 수 있습니다. 제네릭 함수를 추출하여 코드 중복을 피는 것이 좋습니다:\n\n```js\nconst get = <T extends Entity>(\n id: string,\n elements: T[]\n): T | undefined =>\n elements.find(({ id: elementId }) => elementId === id);\n\nconst getDocument = (\n id: string,\n documents: Document[]\n): Document | undefined => get<Document>(id, documents);\n\nconst getImages = (\n id: string,\n images: Image[]\n): Image | undefined => get<Image>(id, images);\n```\n\nRust에서 동일한 기능을 구현하는 경우 초기에는 코드를 중복해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n```rs\nfn get_document(id: String, documents: Vec<Document>) -> Option<Document> {\n    documents.into_iter().find(|document| document.id == id)\n}\n\nfn get_image(id: String, images: Vec<Image>) -> Option<Image> {\n    images.into_iter().find(|image| image.id == id)\n}\n```\n\n위에서 보듯이 Rust 코드는 TypeScript 구현과 매우 유사합니다. 그러나 Rust에는 상속이나 인터페이스 키워드가 없기 때문에 중복을 피하기 위해 위의 패턴을 정확히 복제할 수 없습니다. 여기서 트레잇이 나옵니다.\n\n이 특정 예에서 문서 및 이미지 두 객체 모두 공유하는 공통 특성은 ID를 사용하여 비교할 수 있다는 것입니다. 이것이 우리가 이러한 특성을 트레잇으로 선언하고 각 구조체에 대한 해당 구현을 제공할 수 있는 이유입니다.\n\n```rs\ntrait Compare {\n    fn compare(&self, id: &str) -> bool;\n}\n\nimpl Compare for Document {\n    fn compare(&self, id: &str) -> bool {\n        self.id == id\n    }\n}\n\nimpl Compare for Image {\n    fn compare(&self, id: &str) -> bool {\n        self.id == id\n    }\n}\n```\n\n<div class=\"content-ad\"></div>\n\n마침내 러스트에서 공통 코드를 일반 함수로 추출할 수 있게 되었습니다. 이전에 TypeScript에서 했던 것처럼요.\n\n```js\nfn get<T: Compare>(id: String, elements: Vec<T>) -> Option<T> {\n    elements.into_iter().find(|element| element.compare(&id))\n}\n\nfn get_document(id: String, documents: Vec<Document>) -> Option<Document> {\n    get(id, documents)\n}\n\nfn get_image(id: String, images: Vec<Image>) -> Option<Image> {\n    get(id, images)\n}\n```\n\n또한 러스트에서는 트레이트를 “+” 기호로 결합하여 여러 공통 특성을 정의할 수 있습니다. 예를 들어:\n\n```js\nfn get<T: Compare + OtherTrait>(id: String, elements: Vec<T>) -> Option<T> {\n    elements\n        .into_iter()\n        .find(|element| element.compare(&id) && element.other_trait(&id))\n}\n```\n\n<div class=\"content-ad\"></div>\n\n이러한 패턴은 두 개의 매개변수가 동일한 구조체와 관련될 수 있기 때문에 객체를 비교하는 구현도 흥미로울 수 있습니다.\n\n```rust\ntrait Compare {\n    fn sort(&self, other: &Self) -> Ordering;\n}\n\nimpl Compare for Document {\n    fn sort(&self, other: &Self) -> Ordering {\n        self.timestamp.cmp(&other.timestamp)\n    }\n}\n```\n\n# 결론\n\n우리는 트레이트가 제공할 수 있는 강력함의 일부만 살펴봤지만, 저와 같이 러스트를 탐색하고 있는 자바스크립트 개발자들에게 이 간단한 튜토리얼이 유용할 것이라고 희망합니다.\n\n<div class=\"content-ad\"></div>\n\n무한한 여정이 시작됩니다\n데이비드\n\n더 많은 모험을 원하시면 트위터에서 제 계정을 팔로우해주세요.","ogImage":{"url":"/assets/img/2024-05-20-RustTraitAPowerfulAlternativeToTypeScriptInterface_0.png"},"coverImage":"/assets/img/2024-05-20-RustTraitAPowerfulAlternativeToTypeScriptInterface_0.png","tag":["Tech"],"readingTime":4}],"page":"66","totalPageCount":156,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}