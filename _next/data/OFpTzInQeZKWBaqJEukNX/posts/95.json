{"pageProps":{"posts":[{"title":"플러터 프로젝트에 조금의 창조적 예술 추가하기","description":"","date":"2024-05-15 11:44","slug":"2024-05-15-AddingabitofGenerativeArttoaFlutterproject","content":"\n여러분 안녕하세요, 저는 최근에 pub.dev에 제 첫 번째 패키지를 게시했어요. 이 패키지는 여러분의 프로젝트를 좀 더 흥미롭게 만들어줄 수 있는 기능을 제공해요. 이 글에서는 이 패키지를 만드는 과정과 활용 방법에 대해 알려드릴게요.\n\nFlutter Animated Generative Art Backgrounds collection (gen_art_bg)은 플러터 앱에 흥미로운 애니메이션 배경을 추가하거나 로딩 화면으로 사용하는 것을 쉽게 만들어줍니다.\n\n![image](/assets/img/2024-05-15-AddingabitofGenerativeArttoaFlutterproject_0.png)\n\n모든 것은 flutter 지식과 기술을 향상시키기 위해 개발 중인 사이드 프로젝트로 시작했어요. 간단한 게임을 만들기 시작했고 정적 애플리케이션에 애니메이션을 추가하고 심지어 미니멀한 디자인을 유지하는 것에 도전했죠.\n\n저는 Flutter에서 애니메이션 그리드 배경에 대해 쓴 글을 정확히 이렇게 썼어요.\n\n한 예를 만들어서 앱에 통합시켰는데 결과물이 정말 마음에 들었어요. 더 발전시키고 몇 가지 더 예제를 추가하고 싶었어요.\n\n몇몇 작업에서 영감받아 flutter와 비슷한 것을 구현해보기로 결정했어요.\n\nflutter_spinkit에 영감을 받아요.\n\n그리고 플러터에서 화제인 생성 예술 주제의 멋진 기사 및 저장소를 소개합니다.\n\n- 플러터에서의 생성 예술\n- funvas\n- Flutter-Artbook\n- 아트 프로세싱 플레이그라운드\n- GenArtCanvas\n\n## p5.js 제작자\n\n- Patt Vira\n- mattdesl\n\n## 작성자 정보\n\n- 로니 카우프만\n\n만약 이 주제에 대한 다른 자료를 알고 계시다면 꼭 알려주세요!\n\n# 그래서 왜 막바지에 패키지를 만들게 되었을까요?\n\n위에는 두 가지 분명한 이유가 있습니다:\n\n- 진짜 작은 프로젝트 만들기\n\n네, 패키지도 작은 프로젝트예요(일부 패키지는 앱만큼 유용할 수 있어요). 프로젝트 개발 및 배포의 모든 과정을 직접 경험하고 싶었습니다.\n\n- 커뮤니티 기여\n\np5.js를 사용하면 이러한 애니메이션을 쉽게 구현할 수 있습니다. 플러터는 다른 작업이 필요합니다. 원하는 배경의 이름을 호출하는 것이 코드를 처음부터 작성하는 것보다 더 쉽다고 판단했습니다.\n\n15개의 예제가 준비되자마자 패키지 개발을 시작했습니다. 인터넷에는 프로젝트를 만드는 방법에 대한 많은 안내서가 있으므로 여기서는 몇 가지 포인트만 언급하겠습니다.\n\n위젯 속성을 조정할 수 있는 기능을 추가하여 생성자를 통해 전달하고 패키지를 쉽게 사용할 수 있도록 README.md를 형식화했습니다.\n\n# 패키지 소개\n\n설치는 pub.dev의 모든 패키지와 동일한 방식으로 진행됩니다:\n\n```yaml\ndependencies:\n  gen_art_bg: ^0.0.2\n```\n\n```js\nimport \"package:gen_art_bg/gen_art_bg.dart\";\n```\n\n또는 Flutter의 경우:\n\n```js\nflutter pub add gen_art_bg\n```\n\n그 다음으로 진행할 내용은:\n\n```js\nvoid main() {\n  runApp(const MaterialApp(\n    debugShowCheckedModeBanner: false,\n    home: Scaffold(\n      body: WaveLineGrid(\n        columns: 15, // 열의 수를 변경하려면 이 값을 변경하세요\n        rows: 25, // 행의 수를 변경하려면 이 값을 변경하세요\n        locationConstant: 100, // 위치를 변경하려면 이 값을 변경하세요\n        animationDuration: Duration(seconds: 5), // 애니메이션 지속 시간을 변경하려면 이 값을 변경하세요\n      )\n    ),\n  ));\n}\n```\n\n완성!\n\n다음으로, 우리는 각 예제를 개별적으로 살펴볼 것입니다.\n\n# 쇼케이스\n\n## WaveLineGrid\n\n![WaveLineGrid](https://miro.medium.com/v2/resize:fit:324/1*Ya_bFaYvfthWV7aCbIKAdA.gif)\n\n```js\nWaveLineGrid(\n        columns: 15, // 그리드의 열 수\n        rows: 25, // 그리드의 행 수\n        locationConstant: 100, // 그리드 위치 조정 상수\n        animationDuration:  Duration(seconds: 5), // 애니메이션의 지속 시간\n      ),\n```\n\n## PerlinNoise\n\n<img src=\"https://miro.medium.com/v2/resize:fit:324/1*MfXXrfKaEwLfOxmFLUQjOA.gif\" />\n\n```js\nPerlinNoise(\n        width: 40, // 폭\n        height: 40, // 높이\n        frequency: 5, // 주파수\n      ),\n```\n\n## 랜덤스퀘어\n\n![이미지](https://miro.medium.com/v2/resize:fit:312/1*eROYDX56LY7L-MO0S90-hQ.gif)\n\n```js\nRandomSquare(\n        gridSize: 10, // 그리드 크기를 변경하려면 이 값을 수정하세요\n        updateInterval: Duration(seconds: 1), // 업데이트 간격을 변경하려면 이 값을 수정하세요\n      ),\n```\n\n## 스파이럴 웨이브\n\n<img src=\"https://miro.medium.com/v2/resize:fit:312/1*nk6mjIB32974wRuJm5FzQA.gif\" />\n\n```js\nSpiralWave(\n        size: 10, // 각 원의 크기\n        k: 20, // 파도 효과 제어 상수\n      ),\n```\n\n## GridOfLines\n\nmattdesl에 영감을 받음\n\nmd\n![GridOfLines animation](https://miro.medium.com/v2/resize:fit:324/1*u010xIK6bJ1u3P0gbcX9SQ.gif)\n\n```js\nGridOfLines(\n        animationDuration: 5, // Animation duration in seconds\n        gridSize: 10, // Number of lines in the grid\n        strokeWidth: 0.015, // Stroke width of the lines\n        color: Colors.black, // Color of the lines\n      ),\n```\n\n## AnimatedBWSquares and AnimatedColoredSquares\n\nRoni Kaufman의 영감을 받아 만들어졌습니다.\n\n![image](https://miro.medium.com/v2/resize:fit:324/1*M8eaiZY1slAFRz_C3KU0-g.gif)\n\n```js\nAnimatedBWSquares(\n        squareCount: 40, // Number of squares\n        animationDuration: 10, // Duration of the animation\n        margin: 0, // Margin around the canvas\n        strokeWidth: 1.5, // Stroke width of the squares\n      ),\n```\n\n## AnimatedLines\n\n![image](https://miro.medium.com/v2/resize:fit:324/1*zLmbd3nXmeePU0aS4Iul0w.gif)\n\n```js\nAnimatedLines(\n        numberOfLines: 30, // 라인 수\n        lineLength: 200, // 각 라인의 길이\n        lineColor: Colors.black, // 각 라인의 색상\n        strokeWidth: 3, // 각 라인의 스트로크 너비\n        animationDuration: 10, // 애니메이션 지속 시간\n      ),\n```\n\n## AnimatedLinesGradient\n\n<img src=\"https://miro.medium.com/v2/resize:fit:324/1*Noj2EpPkomlHwaBhsxBnwA.gif\" />\n\n```js\nAnimatedLinesGradient(\n        animationDuration: 5, // 애니메이션 지속 시간\n      ),\n```\n\n## 랜덤노이즈\n\n![랜덤노이즈](https://miro.medium.com/v2/resize:fit:324/1*G5SU8F9du_k4jwfceK0n-w.gif)\n\n```js\nRandomNoise(\n        duration: Duration(seconds: 10), // 애니메이션 지속 시간\n        dotSize: 13, // 점의 크기\n        dotSpacing: 11, // 점 사이의 간격\n      ),\n```\n\n## 몰나르아트\n\nRoaa Khaddam의 영감을 받아서\n\n<img src=\"https://miro.medium.com/v2/resize:fit:324/1*CJHl7YWPQSw5zdFCxPPH4g.gif\" />\n\n```js\nMolnarArt(\n        rows: 8, // 행 수\n        cols: 8, // 열 수\n        n: 12, // 코드\n        colSeq: [\n          Color(0xFFC4951B),\n          Color(0xFF9E3C52),\n          Color(0xFF1D6383),\n          Color(0xFF19315B),\n          Color(0xFF0D1280),\n          Color(0xFFADD27D),\n          Color(0xFFBD1528),\n          Color(0xFF0D4D89),\n          Color(0xFFAC4075),\n          Color(0xFFAB933C),\n          Color(0xFF7EB741),\n          Color(0xFF1C2266),\n        ],\n      ),\n```\n\nMolnarArt 함수의 매개변수 n은 각 그리드 셀에 생성된 이진 코드의 비트 수를 맡습니다. 이 이진 코드는 각 셀의 패턴 구조를 정의하는 데 사용됩니다. 좀 더 구체적으로, 이 이진 코드의 각 비트는 특정 패턴 레이어가 매핑되어야 하는지를 나타냅니다. 예를 들어, n이 12이면 각 그리드 셀에 대해 무작위 12비트 이진 코드가 생성됩니다. 이 코드의 각 비트는 다른 패턴 레이어를 나타냅니다. 비트가 1로 설정되어 있으면 해당 패턴 레이어가 해당 셀에 표시되고, 비트가 0이면 레이어가 표시되지 않습니다.\n\n## ConicGradient\n\n<img src=\"https://miro.medium.com/v2/resize:fit:324/1*oy61Nehr-KMyZ5tnT6zvsw.gif\" />\n\n```js\nConicGradient(\n        durationSeconds: 10, // 애니메이션의 지속 시간(초)\n        maxDiameter: 1.2, // 그라데이션의 최대 지름\n        steps: 10, // 그라데이션의 단계 수\n      ),\n```\n\n## PulsedCircleGrid\n\nInspired by Roni Kaufman\n\n![Image](https://miro.medium.com/v2/resize:fit:324/1*I5wdqJfYMHm3mpI7oyF_Cg.gif)\n\n```js\nPulsedCircleGrid(\n        cellSize: 36, // Size of each grid cell\n        marginSize: 72, // Margin around the grid\n        circleDiameter: 27, // Diameter of circles\n        animationDuration: Duration(seconds: 5), // Animation duration\n        numberOfRowsColumns: 12, // Number of rows and columns in the grid\n      ),\n```\n\n## WaveDotGrid\n\nWaveLineGrid를 사용한 동일한 예시이지만 점 사이에 선이 없는 버전입니다.\n\n![WaveDotGrid](https://miro.medium.com/v2/resize:fit:324/1*-lIFdHQ6m3s4mzeF21mPNg.gif)\n\n```js\nWaveDotGrid(\n        columns: 15, // 열의 수\n        rows: 25, // 행의 수\n        locationConstant: 100, // 위치 상수\n      ),\n```\n\n여기서 각 예시가 모든 기기 크기에서 동일하게 동작하지는 않는다는 점을 강조해야 합니다. 만약 문제가 발생하는 경우 문제를 열어 알려주세요. 라이브러리에 기능이 누락된 것 같다면 Github에서 티켓을 올려주시면 살펴보겠습니다. PR도 환영합니다.\n\n# 다음은 무엇인가요?\n\n더 많은 예제를 추가하여 이 패키지를 개발하고 싶습니다. 이런 종류의 개발은 처음이라서 피드백을 공유해 주시면 감사하겠습니다.\n또한 패키지의 개발에 참여해 주시면 더욱 기쁠 것입니다.\n\n## 링크\n\n- 패키지\n- GitHub\n\n이 기사를 즐겨 보셨길 바랍니다. 의견을 댓글로 공유해 주세요 ❤️\n\n참, 패키지 개발 권장 사항에 대해 Eugenia님에게 많은 감사를 드립니다\n","ogImage":{"url":"/assets/img/2024-05-15-AddingabitofGenerativeArttoaFlutterproject_0.png"},"coverImage":"/assets/img/2024-05-15-AddingabitofGenerativeArttoaFlutterproject_0.png","tag":["Tech"],"readingTime":7},{"title":"대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1","description":"","date":"2024-05-15 11:42","slug":"2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1","content":"\n\n<img src=\"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png\" />\n\n# 소개\n\nBERT, RoBERTa 또는 T5와 같은 Transformer 기반 모델은 자연어 처리에서 맞춤 문제에 대한 최신 솔루션을 제공합니다. 제품에서 모델을 제공하는 보편적인 방법은 모델에 대한 API를 제공하는 Docker 이미지를 빌드하는 것입니다. 이미지는 필요한 종속성, 모델 자체 및 모델로 입력 데이터를 처리하는 코드를 캡슐화합니다. 큰 생성 모델 (GenAI)과 비교하면, 이러한 모델은 상대적으로 작아서 0.5~2GB 정도입니다. 그러나 모델을 Docker 이미지로 배포하는 간단한 방법을 따를 때, 이미지 크기가 8GB에 이를 수 있음에 놀랐을 수도 있습니다. 대상 이미지가 왜 그렇게 큰지, 그리고 이미지 크기를 줄일 수 있는 방법이 있는지 궁금했던 적이 있나요? 이 이야기에서는 Docker 이미지가 왜 그렇게 큰지 설명하고 그 크기를 줄이는 방법에 대해 논의하겠습니다.\n\n이 이야기에서 사용된 Python 스크립트 및 Docker 파일의 예시는 [1]에서도 확인할 수 있습니다:\n\n\n\n# 기본 도커 이미지\n\n언어 감지 모델을 위한 간단한 도커 이미지를 만들어 봅시다. 모델을 구축하기 위한 몇 가지 전제사항은 다음과 같습니다:\n\n- 훈련된 모델을 사용할 것입니다: papluca/xlm-roberta-base-language-detection [2].\n- 가능한 최상의 성능을 얻기 위해 GPU를 활용할 것입니다.\n- 단일 텍스트를 처리하는 간단한 엔드포인트를 제공하기 위해 FastAPI를 사용할 것입니다.\n\n다음은 이미지를 빌드하기 위한 Dockerfile입니다:\n\n\n\n모델을 로드하고 추론을 수행하는 데 사용된 코드는 다음과 같습니다:\n\n다음은 이미지를 빌드하는 데 사용된 명령어입니다:\n\n```js\ndocker build -t language_detection_cuda . -f Dockerfile_cuda\n```\n\n... 그리고 이미지를 실행하세요.\n\n\n\n```js\n도커 실행 --gpus 0 -p 8000:8000 language_detection_cuda\n```\n\n... 엔드포인트를 테스트해 보겠습니다:\n\n```js\n시간 curl -X 'POST'   'http://localhost:8000/process'   -H 'accept: application/json'   -H 'Content-Type: application/json'   -d '{\n  \"text\": \"Certo ci sono stati dei problemi - problemi che dovremo risolvere in vista, per esempio, dell'\\''ampliamento - ma a volte ne esageriamo il lato negativo.\"\n}'\n```\n\n다음 출력을 받았습니다:\n\n\n\n```js\n\"it\"\n```\n\n지금까지 특별한 것은 없어요. 엔드포인트가 할 일을 잘 수행하고 있어요.\n\n모델의 크기는 1.11GB입니다 (model.safetensors 파일), 토크나이저를 위한 추가 10MB가 있어요. 이제 도커 이미지의 크기를 보겠습니다:\n\n```js\ndocker images | grep language_detection_cuda\n```\n\n\n\n… 출력물은:\n\n```js\nlanguage_detection_cuda    최신 버전   47f4c1c0de2d   33분 전   7.05GB\n```\n\n도커 이미지의 총 용량은 7.05GB입니다. 와우, 상당히 많죠? 하지만 왜 이미지가 그렇게 큰 걸까요? 이제 컨테이너로 들어가서 내부를 확인해 보겠습니다.\n\n```js\ndocker run -it --gpus 0 -p 8000:8000 --entrypoint \"/bin/bash\"  language_detection_cuda\n```\n\n\n\n이미지 크기를 분석하기 위해 du명령어를 사용하고 가장 큰 폴더를 추적할 것입니다.\n\n```shell\ndu -h --max-depth 1 /\n```\n\n루트 디렉토리의 출력을 포함하여 가장 큰 폴더들:\n\n```shell\n5.9G    /usr\n1.1G    /workspace\n ...\n```\n\n\n\nWorkspace 폴더에는 모델과 Python 스크립트가 포함되어 있으며, 주로 model.safetensors 파일의 크기입니다. 여기서 놀라운 점은 없어요.\n\nusr 폴더에는 Python 코드를 실행하는 데 필요한 종속성이 포함되어 있어요. 폴더 안에 무엇이 있는지 살펴보겠습니다.\n\n\n- 5.3G /usr/local/lib/python3.9/dist-packages/\n- 2.9G /usr/local/lib/python3.9/dist-packages/nvidia\n- 1.6G /usr/local/lib/python3.9/dist-packages/torch\n- 439M /usr/local/lib/python3.9/dist-packages/triton\n- 77M /usr/local/lib/python3.9/dist-packages/transformers\n- 53M /usr/local/lib/python3.9/dist-packages/sympy\n- ...\n\n\n5.9G 중 5.3G는 Python 모듈을 위한 것입니다. 가장 큰 패키지는 다음과 같습니다:\n\n\n\n- 3.0 GB — nvidia (cuda, cudnn, cublas, 등)\n- 1.6 GB — torch\n- 0.4 GB — triton\n\nnvidia와 triton 모듈은 torch에 종속됩니다. GPU에서 추론을 실행하려면 nvidia 모듈이 필요합니다. 다시 말해, transformers 모듈을 실행하기 위해서 torch 모듈이 필요합니다. 아래 다이어그램은 언급된 모듈이 전체 이미지에 기여하는 방식을 보여줍니다.\n\n![Diagram](/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_1.png)\n\nGPU에서 추론을 실행하려면 이미지 크기를 크게 줄일 수 있는 방법은 없습니다. 그러나 GPU 추론 대신 ONNX [4] 및 양자화를 사용하여 Docker 이미지 크기를 최대 10배 줄일 수 있습니다.\n\n\n\n# ONNX 모델을 포함한 Docker 이미지\n\nint8 양자화가 적용된 ONNX는 성능 손실이 거의 없는 채 모델 크기를 4배로 축소할 수 있습니다 [5]. 다른 장점은 Docker 이미지의 크기를 최대 10배 줄일 수 있다는 것입니다. 이것이 어떻게 가능한 걸까요? ONNX 모델의 Docker 이미지를 빌드하는 데 필요한 작업을 살펴보겠습니다:\n\n다음은 onnxruntime을 사용하여 추론을 실행하는 Python 코드입니다:\n\n먼저, 이미지를 빌드하고 크기를 비교해 보겠습니다. 그런 다음, 이와 이전 이미지 간의 차이를 분석하겠습니다.\n\n\n\n```js\n도커 빌드 -t language_detection_onnx . -f Dockerfile_onnx\n```\n\n… 그리고 이미지를 실행하세요:\n\n```js\n도커 실행 -p 8000:8000 language_detection_onnx\n```\n\n이미지의 크기를 비교해봅시다:\n\n\n\n```js\n도커 이미지 | grep language_detection\n```\n\n출력:\n\n```js\nlanguage_detection_cuda    latest   47f4c1c0de2d   33분 전   7.05GB\nlanguage_detection_onnx    latest   3086089bd994   9시간 전    699MB\n```\n\n7.05 GB 대 699 MB — 이것은 정말로 10배 작은 도커 이미지입니다. 이게 어떻게 가능했을까요?\n\n\n\n세 이미지 사이에는 세 가지 주요 차이점이 있습니다.\n\n## 1. 베이스 도커 이미지\n\n대신 nvidia/cuda:11.8.0-base-ubuntu22.04를 사용하는 대신에 훨씬 작은 베이스 도커 이미지 python:3.9-slim을 사용했습니다. 첫 번째 이미지에는 GPU에서 추론을 실행하는 데 필요한 모든 Nvidia 라이브러리가 포함되어 있습니다 (CUDA, cuDNN, cuBLAS). ONNX 및 양자화된 모델로 추론을 실행하기 위해서는 GPU가 필요하지 않습니다. 따라서 Nvidia 라이브러리가 필요하지 않습니다.\n\n## 2. Python 모듈\n\n\n\n토치 대신 NVIDIA와 Triton 모듈이 필요 없는 ONNX Runtime을 사용했습니다. 이렇게 하면 세 개의 큰 Python 모듈을 제거할 수 있었어요.\n\n## 3. ONNX 형식의 양자화된 모델\n\n마지막으로 중요한 차이점은 ONNX 형식으로 변환된 양자화된 모델을 사용했다는 것입니다 [3]. model_quantized.onnx 파일은 279 MB로, 원본 모델 크기의 4분의 1 크기입니다.\n\n# 결론\n\n\n\n양자화된 ONNX를 사용하면 제품 이미지의 크기를 최대 10배까지 줄일 수 있어요.\n\n어떤 경우에는 제품 모델의 크기가 과도한 모델 성능보다 중요할 수 있어요. 그런 경우에는 모델 양자화와 ONNX 형식으로 전환하는 것이 도움이 될 수 있어요. 양자화는 Docker 이미지의 크기를 줄일 뿐만 아니라 CPU를 사용하는 인스턴스보다 GPU를 사용하는 인스턴스보다 비용을 줄일 수 있어요. 그럼에도 결정은 여러 요인에 기반해야 해요 - 해결해야 하는 문제, 예상 성능, 예상 추론 시간, 양자화된 모델에 대한 성능 손실, 그리고 제품 환경의 구성 등을 고려해야 해요.\n\n# 문제 해결\n\n도커 이미지를 --gpus 매개변수와 함께 실행하는 중 문제가 발생하면 다음을 확인해보세요:\n\n\n\n- Nvidia 컨테이너 툴킷 설치\n\n```js\nsudo apt install nvidia-container-toolkit\n```\n\n2. 도커 서비스 재시작\n\n```js\nsudo systemctl restart docker\n```\n\n\n\n다음 명령은 GPU에 관한 정보를 출력해야 합니다:\n\n```js\ndocker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\n```\n\n# 참조\n\n[1] https://github.com/CodeNLP/codenlp-docker-ml\n\n\n\n[2] [papluca/xlm-roberta-base-language-detection](https://huggingface.co/papluca/xlm-roberta-base-language-detection)\n\n[3] [protectai/xlm-roberta-base-language-detection-onnx](https://huggingface.co/protectai/xlm-roberta-base-language-detection-onnx)\n\n[4] [ONNX](https://onnx.ai/)\n\n[5] [Reducing Inference Time of T5 Models](https://medium.com/codenlp/reducing-inference-time-of-t5-models-76e996523fb2?sk=f02379f5a8363d2de73a332fcef55f78)","ogImage":{"url":"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png"},"coverImage":"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png","tag":["Tech"],"readingTime":6},{"title":"Kolmogorov-Arnold Networks KANs를 사용한 시계열 예측","description":"","date":"2024-05-15 11:39","slug":"2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting","content":"\n\n\n![링크](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_0.png)\n\n다층 퍼셉트론(MLP)은 딥러닝 모델의 기본적인 구조 중 하나입니다. 이는 N-BEATS, NHiTS 및 TSMixer와 같은 최신 예측 모델의 기본 구성 요소도 됩니다.\n\n2024년 4월 30일에 KAN: Kolmogorov-Arnold Network 논문이 발표되었으며, 많은 딥러닝 분야의 전문가들의 주목을 끌었습니다. 여기서 저자들은 MLP의 대안으로 콜모고로프-아놀드 네트워크 또는 KAN을 제안합니다.\n\n가중치와 고정 활성화 함수를 사용하는 대신, KAN은 스플라인으로 매개변수화 된 학습 가능한 함수를 사용합니다. 연구자들은 KAN이 MLP보다 더 적은 학습 가능한 매개변수로 더 정확할 수 있다고 제안합니다.\n\n\n\n\n이 글에서는 우리가 KAN의 아키텍쳐와 주요 요소를 이해하는 데 도움이 되는 스플라인에 대해 먼저 살펴보겠습니다. 그런 다음, 우리는 KAN의 내부 작동 방식을 자세히 살펴보겠습니다. 마지막으로, 우리는 KAN을 시계열 예측에 적용하고 표준 MLP 및 N-BEATS 모델과의 성능을 평가할 것입니다.\n\n더 자세한 내용은 KAN에 대한 원본 논문을 읽어보세요.\n\n시작해봅시다!\n\n# 스플라인 방문\n\n\n\n스플라인은 콜모고로프-아놀드 네트워크의 핵심이기 때문에 이를 이해하는 데 시간을 투자해봅시다.\n\n스플라인은 다항식으로 조각조각 나누어진 함수로 간단하게 정의됩니다. 이를 통해 우리는 고차 다항식을 사용하지 않고도 많은 데이터 포인트를 횡단하는 부드러운 선을 구축할 수 있습니다. 고차 다항식은 진동이 심하여 피하는 것이 좋습니다.\n\n다음 예시를 살펴보세요.\n\n![image](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_1.png)\n\n\n\n위 그림에서 볼 수 있듯이, 네 개의 데이터 포인트가 있습니다. 이들을 통해 선을 맞추고 싶다면 3차 다항식을 사용할 수 있습니다. 차수가 n인 다항식은 n+1개의 계수를 갖는다는 것을 기억해야 합니다. 따라서 좋은 적합을 위해 최소한 n+1개의 데이터 포인트가 필요합니다.\n\n우리는 이를 Excel에서 시도해볼 수 있습니다. 데이터에 트렌드 라인을 추가하고 3차 다항식을 지정해 보세요.\n\n![그림](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_2.png)\n\n보시다시피, 이 경우에 다항식을 적합시키는 것이 잘 작동합니다. 부드러운 곡선을 얻을 수 있습니다.\n\n\n\n그러나 더 많은 데이터 포인트가 있을 때 어떻게 될까요?\n\n이 경우, Excel은 6차 다항식으로 제한됩니다. 그래서 아래에 표시된 것처럼 일곱 개의 데이터 포인트를 통과하는 선을 맞추어 보겠습니다.\n\n![이미지](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_3.png)\n\n위 그림에서 처음 몇 점을 통해 맞는 것은 합리적이지만, 오른쪽 끝의 마지막 두 점에서 큰 진동이 발생합니다. 이것이 고차 다항식을 사용하는 문제입니다.\n\n\n\n현실에서 우리는 매우 큰 데이터셋을 다루기 때문에 점차적으로 커지는 다항식을 사용하는 것은 의미가 없어요. \n\n대신, 우리는 저차수 다항식을 각 부분 데이터 집합에 맞추어 데이터셋을 나눌 수 있어요.\n\n이 경우에는 7개의 데이터 포인트를 통과하는 단일 선을 맞추는 대신, 첫 네 개의 포인트에 3차 다항식을 맞추고, 마지막 네 개의 포인트에 다른 3차 다항식을 맞출 수 있어요. 각 세트가 최종적인 맞춤에 공백이 없도록 하기 위해 하나의 데이터 포인트를 공유한다는 점에 유의해주시면 좋겠어요.\n\n<img src=\"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_4.png\" />\n\n\n\n위의 그림에서, 우리는 적은 진동을 얻는 것을 볼 수 있습니다. 그러나 네 번째 데이터 포인트를 자세히 살펴보세요. 맞춘 선에 이상한 단절이 나타납니다. 이로 인해 부드러운 적합이 아닙니다.\n\n단절이 발생하는 지점을 노트라고 합니다. 곡선의 부자연스러움을 해결하기 위해 각 다항식의 도함수가 노트에서 동일해야 한다는 조건을 추가합니다.\n\n이것은 모든 데이터 포인트에서 부드러운 적합 곡선을 보장합니다. 더불어 임의의 데이터 포인트를 사용할 수 있도록 하기 위해 각 다항식의 두 번째 도함수에도 제약 조건을 설정하여 노트에서 두 번째 도함수도 동일하게 만듭니다.\n\n데이터를 하위 시퀀스로 분할하고 각 시퀀스에 낮은 차수의 다항식을 적합하며 노트에서 제약 조건이 준수되도록 하는 결과는 스플라인입니다. 만약 스플라인이 많은 3차 다항식으로 구성된다면, cubic 스플라인을 얻게 됩니다.\n\n\n\n위의 그림에서는 스플라인을 사용하여 데이터 포인트를 맞추는 결과를 볼 수 있습니다. 선이 부드럽고 이상한 진동이 없는 것을 볼 수 있으며 원하는만큼 많은 포인트를 사용할 수 있습니다.\n\nKAN의 경우, 기저 스플라인 또는 B-스플라인을 사용합니다.\n\n아이디어는 임의의 스플라인 함수가 B-스플라인의 선형 조합으로 표현될 수 있다는 것입니다. 더 중요한 것은 각 스플라인 함수가 B-스플라인의 고유한 조합을 가지고 있다는 것입니다.\n\n\n\n그 모든 것을 염두에 두고, KAN 아키텍처를 더 자세히 살펴보겠습니다.\n\n# 콜모고로프-아놀드 네트워크 탐색\n\n지금은 스플라인에 대한 보다 깊은 이해를 갖게 되었으니, 콜모고로프-아놀드 네트워크 아키텍처에 통합된 방식을 살펴보겠습니다.\n\n먼저, 말할 것도 없이 KAN은 콜모고로프-아놀드 표현 정리에 기초합니다. 이는 다변수 연속 함수를 단변수 함수와 덧셈 연산의 유한 조합으로 표현할 수 있다는 것을 확립합니다.\n\n\n\n간단히 말해, 다변수 함수는 여러 단변수 함수를 결합하는 것으로 요약됩니다.\n\n이 정리는 단변수 함수가 부드럽고 학습 가능한 경우에만 실용적인 가치를 갖습니다. 만약 그것들이 비부드러운 함수이거나 프랙탈 함수라면 학습할 수 없게 되어서 KAN이 쓸모가 없어집니다.\n\n다행히 대부분의 사용 사례는 부드러운 함수를 포함하고 있기 때문에, KAN은 MLP 대안으로 제안되고 있습니다.\n\n## KAN의 아키텍처\n\n\n\n그 후 연구자들은 다변수 함수를 표현하기 위해 단변수 함수를 학습하는 신경망을 구축했습니다.\n\n![KAN Architecture](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_6.png)\n\n위 그림에서 우리는 KAN의 구조와 MLP와 비교하는 방법을 볼 수 있습니다.\n\nKAN의 엣지(선으로 표시된)는 학습 가능한 단변수 함수로, B-스플라인으로 매개변수화됩니다. 그런 다음, 노드(점으로 표시된)에서는 합산이 수행됩니다.\n\n\n\nKolmogorov-Arnold 표현 정리가 신경망에서 작동 중이라는 것을 인식하는 데 시간을 갖는 것이 좋습니다. 여러 단변량 함수가 학습되고 결합되어 최종적으로 다른 프로세스를 표현하도록 합니다.\n\n또한, MLP 아키텍처와 대조되는 방법을 볼 수 있습니다. MLP에서 노드는 고정된 활성화 함수로 설정되어 있으며 일반적으로 ReLU와 같은 비선형 함수입니다. 그런 다음, MLP는 가중치를 학습할 수 있는 엣지를 가지고 있습니다.\n\n따라서 KAN과 MLP 간의 주요 차이점은 KAN에서 비선형 함수가 학습 가능하고 MLP에서는 고정되어 있다는 것입니다. 따라서 KAN은 입력 데이터로부터 더 적은 매개변수를 사용하여 학습할 수 있으며 함수가 입력 데이터에 따라 학습되고 조정되기 때문에 기술적으로 더 나은 결과를 얻을 수 있습니다.\n\n물론 더 크고 깊은 KAN을 사용하면 근사 및 일반화 능력이 향상되어 네트워크가 임의의 함수를 학습할 수 있습니다.\n\n\n\n이제 KAN을 더 깊게 만들면서도 매개변수를 효과적으로 유지하는 비결은 그리드 확장에 있습니다.\n\n## KAN에서의 그리드 확장\n\n그리드 확장은 KAN에서 각 B-스플라인 함수에 대해 모델의 정확도와 효율성을 높이는 방법으로 사용됩니다.\n\n![그리드 확장](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_7.png)\n\n\n\n위의 그림에서 KAN (왼쪽)에서 활성화 흐름과 각 스플라인에 적용된 그리드 확장 기술 (오른쪽)을 볼 수 있습니다.\n\n그리드 확장을 통해 스플라인 그리드의 세분화를 증가시켜 복잡한 함수의 더 나은 근사치를 얻을 수 있습니다. 이는 같은 영역 내에서 간격의 수를 증가시킴으로써 이루어집니다. 그림에서는 G1 = 5에서 G2 = 10으로 변화함으로써 이를 설명하고 있습니다.\n\n간격의 수를 증가시킴으로써 최종 스플라인 함수를 구성하는 조각의 수도 증가합니다. 이는 데이터에서 보다 세부적인 행동을 배울 수 있게 합니다.\n\n이 확장 기능은 MLP의 경우와 대조적입니다. MLP의 경우 더 복잡한 함수를 학습하기 위해 모델이 더 깊어져야 합니다. 그러나 KAN에서는 각 스플라인마다 이 작업을 수행하므로 더 복잡한 함수를 학습하기 위해서는 기술적으로 더 적은 층이 필요합니다.\n\n\n\n이제 우리는 KAN과 그 내부 작동에 대해 잘 이해했으니 파이썬을 사용하여 시계열 예측에 적용하는 방법을 살펴보겠습니다.\n\n# KAN으로 예측하기\n\n이 섹션에서는 파이썬을 사용하여 예측 작업에 KAN 아키텍처를 테스트합니다.\n\n본 문서 작성 시점에서 KAN은 매우 새로운 기술이므로, 내가 좋아하는 예측 라이브러리 neuralforecast를 이 Pytorch 기반의 KAN 구현으로 확장하였고 이를 시계열 예측에 적용했습니다.\n\n\n\n또한, KAN 모델이 neuralforecast의 안정적인 릴리스에서 사용할 수 없을 수도 있습니다. 결과를 재현하려면 다음을 수행할 수 있습니다:\n\n- neuralforecast 저장소를 복제하고 이 브랜치에서 작업\n- 또는 브랜치가 병합된 경우 다음을 실행할 수 있습니다.\n\n```js\npip install git+https://github.com/Nixtla/neuralforecast.git\n```\n\n지금 이 실험에서는 Creative Commons Attribution 4.0 라이선스를 통해 제공된 월간 M3 데이터셋에 KAN 모델을 테스트합니다.\n\n\n\n이 데이터셋은 다양한 도메인에서 나온 월별 주기를 가진 1428개의 고유한 시계열을 포함하고 있습니다.\n\nKAN의 성능은 간단한 MLP와 N-BEATS 모델과 비교될 것입니다.\n\n이러한 결과를 재현하는 모든 코드는 GitHub에서 확인할 수 있습니다.\n\n시작해봅시다!\n\n\n\n## 초기 설정\n\n우리는 이 실험에 필요한 패키지들을 import하여 시작합니다.\n\n```js\nimport pandas as pd\n\nfrom datasetsforecast.m3 import M3\n\nfrom utilsforecast.losses import mae, smape\nfrom utilsforecast.evaluation import evaluate\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import KAN, MLP, NBEATS\n```\n\n우리는 neuralforecast와 호환성 있는 형식으로 월간 M3 데이터셋을 불러오기 위해 datasetsforecast 라이브러리를 사용합니다.\n\n\n\n```js\nY_df, *_ = M3.load(\"./data\", \"Monthly\")\n```\n\n그런 다음 데이터셋 사양에 지정된 대로 18의 예측 기간을 사용합니다. 따라서 마지막 18 개의 시간 단계를 테스트 세트로 예약하고 나머지 데이터를 학습에 사용합니다.\n\n```js\nhorizon = 18\n\ntest_df = Y_df.groupby('unique_id').tail(horizon)\ntrain_df = Y_df.drop(test_df.index).reset_index(drop=True)\n```\n\n좋아요! 이 단계에서는 학습 및 테스트 세트가 준비되었으므로 모델을 적합할 준비가 되었습니다.\n\n\n\n## 모델 맞추기\n\n모델을 맞추기 위해, 우리는 훈련시키고자 하는 모든 모델들의 목록을 간단히 정의합니다. 여기서는 기본 설정을 유지합니다. 기본 KAN과 MLP는 입력 레이어, 은닉 레이어, 출력 레이어로 이루어진 세 개의 레이어만 가지고 있음을 유의해 주세요.\n\n또한 1000번의 훈련 단계로 설정하고, 인내심을 3으로 설정했습니다. 이는 검증 손실이 세 번의 확인 후에도 개선되지 않으면 모델이 훈련을 멈출 것임을 의미합니다.\n\n```js\nmodels = [\n    KAN(input_size=2*horizon,\n        h=horizon,\n        scaler_type='robust',\n        max_steps=1000,\n        early_stop_patience_steps=3),\n    MLP(input_size=2*horizon,\n        h=horizon,\n        scaler_type='robust',\n        max_steps=1000,\n        early_stop_patience_steps=3),\n    NBEATS(input_size=2*horizon,\n           h=horizon,\n           scaler_type='robust',\n           max_steps=1000,\n           early_stop_patience_steps=3)\n]\n```\n\n\n\n대박! 이제 데이터를 처리하고 모델을 맞추는 NeuralForecast의 인스턴스를 만듭니다. 그런 다음 fit 메소드를 호출하여 각 모델을 훈련시킵니다.\n\n```js\nnf = NeuralForecast(models=models, freq='M')\n\nnf.fit(train_df, val_size=horizon)\n```\n\n훈련이 완료되면 예측을 수행하고 각 모델의 성능을 평가할 수 있습니다.\n\n## 평가\n\n\n\n이제 적합된 모델을 활용하여 예측을 수행할 수 있고, 해당 값들을 테스트 세트에 저장된 실제 값과 비교할 수 있습니다.\n\n```js\npreds = nf.predict()\n\npreds = preds.reset_index()\n\ntest_df = pd.merge(test_df, preds, 'left', ['ds', 'unique_id'])\n```\n\n우리의 모델을 평가하기 위해 평균 절대 오차(MAE)와 대칭 평균 절대 백분율 오차(sMAPE)를 사용합니다.\n\n이 부분에서는 예측 모델을 평가하는 많은 메트릭과 유틸리티 함수를 편리하게 제공하는 utilsforecast 라이브러리를 사용합니다.\n\n\n\n```js\n평가 = evaluate(\n    test_df,\n    metrics=[mae, smape],\n    models=[\"KAN\", \"MLP\", \"NBEATS\"],\n    target_col=\"y\",\n)\n\n평가 = 평가.drop(['unique_id'], axis=1).groupby('metric').mean().reset_index()\n평가\n```\n\n위의 코드 블록은 아래에 표시된 결과를 출력합니다.\n\n<img src=\"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_8.png\" />\n\n위의 표에서 KAN이 아주 간단한 MLP과 비교하여 최악의 성능을 달성하는 것을 볼 수 있습니다. 예상대로, N-BEATS는 MAE가 637, sMAPE가 7.1%로 최상의 성능을 달성했습니다.\n\n\n\nKAN의 성능이 그리 눈에 띄지는 않지만, 알아두세요. KAN은 학습 가능한 매개변수가 272k로 MLP(1.1M)와 N-BEATS(2.2M)에 비해 적습니다. MLP에 비해 매개변수 수를 75% 줄인 것이죠. 그럼에도 불구하고, 이 시나리오에서 그 성능은 실망스럽습니다.\n\n## 시계열 예측을 위한 KAN의 벤치마킹\n\n위 실험은 전체 M3 데이터셋뿐만 아니라 M4 데이터셋에도 손쉽게 확장할 수 있습니다. 두 데이터셋 모두 Creative Commons Attribution 4.0 라이선스로 제공됩니다.\n\n아래에서 벤치마킹 결과가 표시됩니다. 최고의 성능은 굵게, 두 번째로 좋은 성능은 밑줄로 표시되어 있습니다.\n\n\n\n<img src=\"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_9.png\" />\n\n위 표를 보면 KAN이 종종 최악의 예측 모델이라는 것을 알 수 있습니다. 일반적으로 간단한 MLP보다 성능이 낮습니다. KAN은 주간 M4 데이터셋에서 MLP보다 우수한 성과를 보이며, 시간당 M4 데이터셋에서 가장 우수한 성과를 내고 있음을 알 수 있습니다. 또한 KAN이 벤치마크에서 가장 느린 모델임을 주목해 주세요.\n\n모든 예측 작업에 대해 KAN은 실제로 MLP 또는 N-BEATS보다 매개변수 효율적이지만 성능은 실망스럽습니다.\n\n# KAN에 대한 내 의견\n\n\n\nKAN 아키텍처는 많은 관심을 끌고 있고 해당 모델을 중심으로 큰 홍보가 진행되고 있습니다. 그러나 예측 작업에 적용할 경우 상당히 부정적인 결과가 나왔습니다.\n\nMLP도 예측 모델로는 그리 좋지 않다는 점은 예상했습니다. KAN은 MLP의 대체제로 제안되었기 때문입니다.\n\n논문의 저자들은 MLP 대비 성능이 향상된다고 주장하지만, 시계열 예측에 적용했을 때는 그러한 성과가 나타나지 않았습니다.\n\n제 생각에 진정한 잠재력은 MLP 유닛을 N-BEATS나 NHiTS와 같이 고급 시계열 예측 모델에서 KAN으로 대체하는 데 있다고 생각합니다. 결국 시계열 예측은 어려운 작업이며, KAN이나 MLP와 같은 모델은 스스로만으로는 성능이 충분하지 않습니다.\n\n\n\n그러나 KAN 기반의 N-BEATS 또는 NHiTS를 사용하면 가벼워지고 더 빠르며 더 나은 예측이 가능할 수 있습니다. 이를 곧 테스트되길 희망합니다.\n\n# 결론\n\nKolmogorov-Arnold 네트워크(KAN)는 딥러닝에서 근본적인 다층 퍼셉트론(MLP) 대체로 제시됩니다.\n\n이는 Kolmogorov-Arnold 표현 정리를 적용하여 다변수 함수가 일변수 함수의 조합으로 표현될 수 있다고 설명하고 있습니다.\n\n\n\nKAN에서는 단변량 함수들이 B-스플라인으로 학습되고 매개변수화됩니다. MLP가 고정된 비선형 활성화 함수를 사용하는 것과는 달리, 스플라인은 학습되어 훈련 데이터에 맞게 조정될 수 있는 비선형 함수입니다.\n\n이는 KAN이 MLP보다 매개변수를 효율적으로 사용할 수 있게 하며 이론적으로 더 나은 성능을 달성할 수 있다는 것을 의미합니다.\n\n그러나 KAN을 시계열 예측에 적용한 결과, 모델이 매우 간단한 MLP보다 성능이 부족한 경우가 많았습니다.\n\n그 잠재력은 아마 MLP를 N-BEATS나 NHiTS와 같은 더 정립된 예측 모델로 대체하는 데 있을 것으로 예상됩니다.\n\n\n\nKAN은 아주 새로운 기술이지만, 여전히 딥 러닝 분야에서 흥미로운 진전을 보여줍니다.\n\n읽어 주셔서 감사합니다! 즐겁게 읽으셨기를 바라며 무언가 새로운 것을 배우셨으면 좋겠어요!\n\n건배 🍻\n\n# 제게 응원을 해주세요\n\n\n\n제 일에 만족하고 계신가요? 지지를 표현해보세요. 'Buy me a coffee'는 저를 격려하는 간단한 방법입니다. 한 잔의 커피를 마실 수 있고, 여러분도 응원할 수 있어요! 응원하고 싶다면 아래 버튼을 클릭해주세요 👇\n\n![KAN: Kolmogorov–Arnold Networks by Ziming Liu1, Yixuan Wang Sachin Vaidya Fabian Ruehle, James Halverson, Marin Soljačić, Thomas Y. Hou, Max Tegmark](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_10.png)\n\n# 참고문헌\n\nKAN: Kolmogorov–Arnold Networks 저자: Ziming Liu1, Yixuan Wang Sachin Vaidya Fabian Ruehle, James Halverson, Marin Soljačić, Thomas Y. Hou, Max Tegmark\n\n\n\nGitHub에 Kolmogorov-Arnold network의 효율적인 구현이 있어요!","ogImage":{"url":"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_0.png"},"coverImage":"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksKANsforTimeSeriesForecasting_0.png","tag":["Tech"],"readingTime":11},{"title":"GPT-4 대 GPT-4 대 Gemini 15   성능 분석","description":"","date":"2024-05-15 11:37","slug":"2024-05-15-GPT-4ovsGPT-4vsGemini15PerformanceAnalysis","content":"\n\n## 오픈에이아이(OpenAI)의 새로운 프래그십 모델의 영어 언어 이해 능력 측정\n\n![이미지](/assets/img/2024-05-15-GPT-4ovsGPT-4vsGemini15PerformanceAnalysis_0.png)\n\n오픈에이아이의 GPT-4o 최근 공개로 인공지능 언어 모델과 그들과의 상호작용에 새로운 시대가 열렸습니다.\n\n가장 인상적인 부분은 대화 중단과 함께 ChatGPT와의 실시간 상호작용을 지원하는 것이었습니다.\n\n\n\n실시간 데모 중 일부 키크는 사건이 있었지만, 팀이 이룬 성과에 놀랍지 않을 수가 없어요.\n\n더 좋은 소식은, 데모 직후 OpenAI가 GPT-4o API에 접속 권한을 부여했어요.\n\n본 기사에서는, 제가 만든 영어 데이터셋을 사용해 GPT-4o 대 GPT-4 대 Google의 Gemini 및 Unicorn 모델의 분류 능력을 측정한 독립적인 분석을 제시할 거에요.\n\n이 모델 중 어떤 것이 영어 이해력에서 가장 강한지 알아볼까요?\n\n\n\n![image](/assets/img/2024-05-15-GPT-4ovsGPT-4vsGemini15PerformanceAnalysis_1.png)\n\n# GPT-4o에 대한 새로운 소식\n\n제일 먼저 소개하는 것은 OmnI 모델 개념으로, 텍스트, 오디오, 비디오를 매끄럽게 이해하고 처리하도록 설계되었습니다.\n\nOpenAI의 초점은 GPT-4 수준의 지능을 대중들에게 민주화 하는 방향으로 바뀌어, GPT-4 수준의 언어 모델 지능을 무료 사용자에게도 접근 가능하게 만드는 것을 중심으로 이루어지는 것으로 보입니다.\n\n\n\nOpenAI가 GPT-4o에 향상된 품질과 속도로 50개 이상의 언어에 대해 더 포괄적이고 전 세계적으로 접근 가능한 AI 경험을 제공한다고 발표했습니다. 더 저렴한 가격으로!\n\n그들은 또한 유료 구독자들이 비유료 사용자들과 비교하여 5배 용량을 제공받게 될 것이라고 언급했습니다.\n\n게다가 대중을 위해 오디오, 비전, 텍스트 인터페이스를 통해 실시간 추론을 용이하게 하는 ChatGPT의 데스크톱 버전을 출시할 예정입니다.\n\n# GPT-4o API 사용 방법\n\n\n\n새로운 GPT-4o 모델은 OpenAI의 기존 채팅 완성 API를 따르며, 역호환성을 유지하고 사용하기 간단합니다.\n\n```js\nfrom openai import AsyncOpenAI\n\n\nOPENAI_API_KEY = \"<your-api-key>\"\n\n\ndef openai_chat_resolve(response: dict, strip_tokens = None) -> str:\n    if strip_tokens is None:\n        strip_tokens = []\n    if response and response.choices and len(response.choices) > 0:\n        content = response.choices[0].message.content.strip()\n        if content is not None or content != '':\n            if strip_tokens:\n                for token in strip_tokens:\n                    content = content.replace(token, '')\n            return content\n    raise Exception(f'응답을 해결할 수 없습니다: {response}')\n\n\nasync def openai_chat_request(prompt: str, model_nane: str, temperature=0.0):\n    message = {'role': 'user', 'content': prompt}\n    client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n    return await client.chat.completions.create(\n        model=model_nane,\n        messages=[message],\n        temperature=temperature,\n    )\n\n\nopenai_chat_request(prompt=\"안녕하세요!\", model_nane=\"gpt-4o-2024–05–13\")\n```\n\nGPT-4o는 ChatGPT 인터페이스를 통해도 이용 가능합니다:\n\n<img src=\"/assets/img/2024-05-15-GPT-4ovsGPT-4vsGemini15PerformanceAnalysis_2.png\" />\n\n\n\n# 공식 평가\n\nOpenAI의 블로그 게시물에는 MMLU 및 HumanEval과 같은 알려진 데이터셋의 평가 점수가 포함되어 있습니다.\n\n![그래프](/assets/img/2024-05-15-GPT-4ovsGPT-4vsGemini15PerformanceAnalysis_3.png)\n\n그래프에서 확인할 수 있듯이, GPT-4o의 성능은 이 분야에서 최첨단으로 분류될 수 있으며 — 새로운 모델이 더 저렴하고 빠르다는 것을 고려하면 매우 유망하게 들립니다.\n\n\n\n지난 해 동안 여러 모델들을 보았는데, State-of-the-art 언어 성능을 주장하는 모델들이 많았어요. 하지만 실제로는 이러한 모델들 중 일부가 이러한 공개 데이터셋에서 부분적으로 학습되었거나 (또는 오버핏팅)하여 리더보드에서 현실적이지 않은 점수를 보여주기도 했어요.\n\n그러므로, 이러한 모델들의 성능을 독립적으로 분석하고, 제가 만든 데이터셋과 같은 잘 알려지지 않은 데이터셋을 사용하여 성능을 평가하는 것이 중요합니다 😄\n\n# 제 평가 데이터셋 🔢\n\n이전 글에서 설명했듯이, 저는 다양한 LLMs를 통해 분류 성능을 측정할 수 있는 토픽 데이터셋을 만들었어요.\n\n\n\n데이터셋은 50가지 주제로 분류된 200개의 문장으로 구성되어 있습니다. 일부는 분류 작업을 더 어렵게 만들기 위해 밀접하게 관련되어 있습니다.\n\n전체 데이터셋은 저가 수작업으로 영어로 작성하고 레이블을 지정했습니다.\n\n그런 다음 GPT4 (gpt-4-0613)를 사용하여 데이터셋을 여러 언어로 번역했습니다.\n\n그러나 이 평가 중에는 데이터셋의 영어 버전만 평가할 것이며, 데이터셋 생성과 주제 예측에 동일한 언어 모델을 사용함으로 인해 발생할 수 있는 잠재적인 편향으로 인해 결과에 영향을 미치지 않아야 합니다.\n\n\n\n지금 당장 데이터셋을 확인해보세요: 주제 데이터셋.\n\n# 성능 결과 📊\n\n다음 모델들을 평가하기로 결정했어요:\n\n- GPT-4o: gpt-4o-2024–05–13\n- GPT-4: gpt-4–0613\n- GPT-4-Turbo: gpt-4-turbo-2024–04–09\n- Gemini 1.5 Pro: gemini-1.5-pro-preview-0409\n- Gemini 1.0: gemini-1.0-pro-002\n- Palm 2 Unicorn: text-unicorn@001\n\n\n\n언어 모델에 주어진 작업은 데이터셋의 각 문장을 올바른 주제와 일치시키는 것입니다. 이를 통해 각 언어와 각 모델의 정확도 점수 및 오류율을 계산할 수 있습니다.\n\n대부분의 모델이 올바르게 분류되기 때문에 각 모델의 오류율을 그래프로 플로팅하고 있습니다.\n\n낮은 오류율은 더 나은 모델 성능을 나타냅니다.\n\n그래프에서 볼 수 있듯이, GPT-4o는 모든 모델 중에서 가장 낮은 오류율을 보여 2개의 실수만 발생했습니다.\n\n\n\nGPT-4, Gemini 1.5, and Palm 2 Unicorn는 GPT-4o보다 한 가지 더 실수가 있었음을 알 수도 있습니다. 이들은 강력한 성능을 보여주고 있습니다. 흥미로운 점은 GPT-4 Turbo가 GPT-4-0613보다 약간 성능이 떨어진다는 것인데, 이는 OpenAI가 모델 페이지에 작성한 내용과는 다른 결과입니다.\n\n마지막으로, Gemini 1.0은 가격대를 고려하면 예상대로 다소 뒤처지고 있습니다.\n\n# 결론 💡\n\n이 독특한 영어 데이터셋을 활용한 이 분석은 이러한 고급 언어 모델의 최첨단 능력에 대한 통찰을 제공합니다.\n\n\n\nGPT-4, OpenAI의 최신 모델은 테스트된 모델 중에서 가장 낮은 오류율로 놀랍습니다. 이는 OpenAI가 성능에 관한 주장을 확증합니다.\n\n인공지능 커뮤니티와 사용자들은 서로 독립적인 평가를 계속해야 합니다. 이를 통해 표준화된 벤치마킹만으로는 실용적인 효과를 제공하는 모델에 대해 더 명확한 그림을 제시할 수 있습니다.\n\n데이터셋이 상당히 작기 때문에 결과는 데이터셋에 따라 달라질 수 있습니다. 성능은 영어 데이터셋만을 사용했으며, 다국어 비교는 다음 기회를 기다려야 할 것입니다.\n\n읽어 주셔서 감사합니다!\n\n\n\n향후 유사한 콘텐츠를 받으려면 팔로우하세요!\n\n문의 사항이 있으시면 언제든지 연락해주세요!","ogImage":{"url":"/assets/img/2024-05-15-GPT-4ovsGPT-4vsGemini15PerformanceAnalysis_0.png"},"coverImage":"/assets/img/2024-05-15-GPT-4ovsGPT-4vsGemini15PerformanceAnalysis_0.png","tag":["Tech"],"readingTime":5},{"title":"ChatGPT-4o, 오픈AI의 새로운 주력 모델 전체 리뷰","description":"","date":"2024-05-15 11:35","slug":"2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview","content":"\n\n일년 만에 OpenAI의 새로운 모델, 그들의 트랜스포머 패밀리의 최신 버전인 GPT-4o (\"omnimodal\")가 드디어 나왔어요.\n\n이 모델은 텍스트, 오디오, 이미지, 비디오 처리 및 이미지 생성에서 엄청나게 빠르며, 코딩 및 멀티모달 추론 개선을 보여주고 있어요. 또한 3D 렌더링과 같은 새로운 형태의 모달리티를 가능하게 합니다.\n\n그리고 lmsys.org의 챗봇 아레나에 따르면, 약 두 주 전에 우리가 논의한 유명한 gpt2-chatbot의 프록시 모델로부터 얻은 결과를 기반으로 이미 최고의 올라운드 모델이라고 해요.\n\n하지만 이번에 모델을 공개한 이유는 Sam Altman이 말한 대로 \"무지의 가리개를 밀어내는 것\"이 아니라, 최첨단 인공지능을 수십억 명의 손에 무료로 전달하는 것에 있어요.\n\n\n\n여기 ChatGPT-4o에 대해 알아야 할 모든 정보가 있어요.\n\n# 다중성의 저주\n\n다중 모닥과 대형 언어 모델, 즉 MLLM은 상당한 시간 동안 존재해 왔지만, GPT-4o는 오디오, 비디오, 이미지 및 텍스트 네 가지 다른 모드에 걸친 진정한 다중 모달리티를 보여주는 첫 번째 모델로 보입니다.\n\n- 네, Gemini 1.5와 같은 모델은 후자 세 가지에 대해 진정으로 다중 모단에 보였지만, 오디오에 대해서는 아니었어요.\n- 실제로, GPT-4V는 오디오 처리/생성 및 이미지 생성을 허용했지만, Whisper, OpenAI TTO 및 Dall-e3와 별도의 모델들과 통합하여 해당 기능들을 가능하게 했어요.\n\n\n\n안녕하세요! ChatGPT-4o는 모든 모달리티에 기본적으로 작동하는 하나의 단일 모델을 의미하는 올인원 모델입니다.\n\n그런데 이게 무슨 의미일까요?\n\n## 다중 모달리티 입력, 다중 모달리티 출력\n\n이에 대해 더 자세히 이야기할 것이며, 이번 주 목요일에 무료 뉴스레터를 통해 더 많은 내용을 알려드리겠습니다 (위 참조).\n\nChatGPT-4o는 이제 더 이상 \"그저 큰 언어 모델\"이 아니라는 아이디어입니다.\n\n\n\n대형 언어 모델(LLMs)은 시퀀스 대 시퀀스 모델입니다(입력과 출력이 모두 시퀀스인 모델). 보통 텍스트를 입력으로 받아 다른 텍스트를 출력합니다.\n\n이 이미지 인코더와 같은 구성 요소와 결합하면 이미지도 처리할 수 있으며, 다른 모달리티에도 동일하게 적용됩니다.\n\n그러나 많은 경우에 이러한 구성 요소는 내적 요소가 아닙니다. 따라서 LLM은 입력을 사용하여 다른 데이터 유형을 처리할 수는 있지만 cross-modal 추론을 수행할 수는 없습니다.\n\n그렇다면 그것이 무슨 의미일까요?\n\n\n\n민아 무라티가 공식 발표에서 강조한 대로, 말은 단어 이상의 것을 포함합니다. 톤, 감정, 일시정지 및 다른 여러 단서들도 포함되어 발언자가 무엇을 전달하려는지에 대한 추가 정보를 전달합니다.\n\n예를 들어, \"널 죽일 거야!\"라는 문장은 말하는 사람이 명백한 의도를 보여주거나 문장 중간에 웃음을 보여주는 경우 매우 다른 해석을 가질 수 있습니다.\n\n그러나 지금까지 ChatGPT의 이전 버전이 실제로 수신한 것은 단지 음성 전사뿐이었기 때문에 다른 모든 단서들이 손실되었습니다. 따라서 모델은 음성을 해석할 때 매우 제한되었으며 이전의 두 가지 예제는 모두 그것과 동일했습니다.\n\n그러나 이제 ChatGPT-4o는 텍스트, 이미지, 오디오 및 비디오 (비디오 생성 제외)를 처리하고 생성하는 데 필요한 모든 구성 요소를 포함하고 있습니다. 다시 말해, GPT-4o는 첫 번째 모델로, 인간처럼 모든 모달리티와 그 이유를 결합합니다.\n\n\n\n그것을 알았다면, 어제 새롭고 흥미로운 능력이 제시되었나요?\n\n## 다재다능한 \"야수\"\n\n두 시간반의 짧은 발표에도 불구하고, 언급할 가치가 있는 많은 것이 보여졌습니다.\n\n사실, ChatGPT-4o에는 수십억 명이 사용하는 제품에서 수십억 명이 사용하는 제품으로 변화시키는 데 필요한 많은 특성이 있습니다.\n\n\n\n## 인상적인 쇼케이스\n\n첫 번째로, 내가 본 중에서 가장 인상적인 두 가지 중 하나인 것은 ChatGPT가 실시간 비디오 인식을 수행한다는 것입니다. 구글은 Gemini이 그랬지만 실제로 그렇지 않았습니다.\n\n다른 비디오에서, OpenAI의 X 관객 중 한 사람이 실시간 번역을 제안했고, ChatGPT-4o는 다른 큰 개선 사항이 있어서 완벽하게 실행되었습니다: 인간 수준의 대기 시간.\n\nChatGPT-4o와 같은 음성 비서가 사회에 미칠 수 있는 흥미로운 사용 사례 중 하나는 교육입니다. 항상 인내심 있는 AI 모델은 학생들이 복잡한 작업을 배울 때 도움이 될 수 있습니다.\n\n\n\n메모리는 비디오 시연 중에 감지되지 않은 매우 흥미로운 기능이었습니다. 아래 비디오에서 OpenAI의 대표이사 그렉 브록먼이 모델이 처음에는 무시하는 비디오 프레임의 '침입자'를 가지고 있습니다.\n\n그러나 그렉이 이에 반응하도록 모델에 요청하자, 모델은 이전에 발생한 정확한 상호 작용을 다시 호출합니다. 이는 두 가지를 의미합니다:\n\n- 모델은 이전 이벤트를 기억할 수 있는 것으로 보입니다.\n- 모델이 특정 작업에 집중하고 나머지를 무시하는 메커니즘이 있는 것으로 보입니다. 이는 OpenAI가 계층화된 주의를 가진 매우 능률적인 비디오 인코딩 메커니즘을 개발했을 수 있다는 것을 의미할 수 있습니다.\n\n물론, 이 모든 것에 대해 X는 열광했고, 매우 흥미로운 스레드가 나타났습니다. 아마도 가장 인상적으로 느꼈던 것 중 하나는 OpenAI의 윌 데퓨가 제작한 것인데, 이것은 많은 예제를 보여주어 GPT-4o의 기본 다중 모달성을 증명했습니다.\n\n\n\n모델은 제어 넷 유형 이미지 조건화가 없어도 여러 세대에 걸쳐 문자 일관성을 유지하는 것으로 보입니다:\n\n![이미지1](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_0.png)\n\n모델은 사진을 가져와 대체 3D 뷰를 생성하고 이를 실제 3D 렌더링으로 조립할 수도 있습니다.\n\n![이미지2](https://miro.medium.com/v2/resize:fit:1000/1*KTEVeb4ty--ihrJ-HiGnYg.gif)\n\n\n\n시위와 관계없이, 해당 모델은 특히 새로운 벤치마킹의 왕이 되었습니다.\n\n## 더 똑똑해졌지만 AGI는 아님\n\n이전부터 의심되었던 바와 같이, lmsys.org의 X 페이지와 OpenAI 연구원들의 확인에 따르면, 'im-also-a-good-gpt2-chatbot'인 'gpt2-chatbot' 라인업의 일원이 사실 ChatGPT-4o였습니다.\n\n전자가 공유한 이미지에서 gpt2-chatbots 또는 GPT-4o 챗봇은 GPT-4 및 Claude 3 Opus 모델에 비해 전반적인 ELO(품질 측정) 면에서 앞서 있습니다.\n\n\n\n![image](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_1.png)\n\n또 다른 뚜렷한 개선 사항은 코딩에서 볼 수 있습니다. 개선 정도가 미친 듯이 100 ELO 점수가 상승했어요. 참고로, 두 모델 간의 100 점 차이는 패배 모델이 선호되는 경우가 1/3만 된다는 걸 의미합니다.\n\n특히 코딩에 대해 이야기하자면, 가장 주목할 만 한 공지 사항 중 하나는 ChatGPT 데스크톱 앱이었어요. 이 앱은 디버깅과 같은 작업에서 모델을 완전한 노트북 화면에서 활용할 수 있게 도와줄 겁니다. 이 비디오에서 확인할 수 있어요.\n\n그리고 이 공지에는 강력한 언어 개선도 포함되어 있었습니다.\n\n\n\n## 전 세계 인구의 97%가 서비스되었습니다\n\n제 모델의 토크나이저를 크게 개선했을 것 같아요, 특히 비영어권 언어를 고려할 때 (전 세계 인구의 97%까지 제공할 수 있다고 주장합니다. 상당한 주장이네요).\n\n이를 증명하기 위해, 해당 모델이 언어당 토큰을 상당히 줄였다고 주장하는 테이블을 공개했습니다.\n\n![이미지](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_2.png)\n\n\n\n압축이 왜 중요한지 궁금하신가요? 그들이 주장하는 바는 빠르고 더 효율적인 버전 뿐만 아니라 보다 뛰어난 \"언어 지능\"을 보여줍니다. 간단히 말해, 언어가 가지는 토큰이 적을수록 모델이 언어를 생성하는 방법을 더 잘 알게 됩니다.\n\n하지만 ChatGPT-4가 정말 그만큼 우수하고 지능적인 발전이라고 할 수 있을까요? \n\n음, 아닙니다.\n\n\n\n## 차분해지세요, AGI가 아닙니다\n\nOpenAI가 공유한 그래프에 따르면, 이 모델은 현재 가장 우수한 것으로 분명하지만, 나머지에 비해 지능적인 개선은 미미합니다.\n\n![이미지](/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_3.png)\n\n'지능' 개선을 보면, 본 릴리스는 소박해 보일 수 있습니다. 그러나 저는 완전히 동의하지 않습니다. 왜냐하면 이 릴리스는 다음 중요한 새로운 영역에 대한 것이 아니라 다른 어떤 것에 대한 것이었기 때문입니다.\n\n\n\n그러나 모델이 더 똑똑해지지 않았다면 그 의미가 뭡니까?\n\n# OpenAI의 진짜 의도\n\n내가 보기에, 이 릴리스에는 세 가지 구성 요소가 있습니다:\n\n- 다음 프론티어인 이른바 'GPT-5'의 대규모 릴리스를 위한 시간 확보\n- 오늘 열리는 Google의 I/O 컨퍼런스를 사전에 저질러놓기\n- Apple 승리\n\n\n\n하나씩 해보자.\n\n## 다가오는 다음 단계는 가깝지만 완전하게는 아니다\n\nOpenAI의 CTO 미나 무라티는 공개적으로 이 점에 대해 언급했습니다. GPT-4o는 지능적인 크게 발전이 있는 것이 아니며, 사실 그들은 명시적으로 \"GPT-4 수준의 지능\"이라고 밝혔습니다.\n\n또한 그들은 곧 '다가오는' 다음 단계에 대한 소식과 업데이트를 받게 될 것이며, 그것에 어떤 이름을 붙일지에 대해 언급했습니다.\n\n\n\n## 구글의 최악의 악몽\n\n이 시점에서 OpenAI가 무엇을 공개할지 예측하고 싶다면, 그냥 구글이 무엇을 할지 보세요.\n\n예를 들어, 구글이 Gemini 1.5를 위해 백만 컨텍스트 윈도우를 출시하면서 MLLM이 한 번에 처리할 수 있는 데이터 양을 크게 늘린 것을 보면, OpenAI는 전혀 다른 이야기로 넘어가서 Sora라는 비디오 생성 모델을 출시했습니다.\n\n오늘 고대로 예상되는 Google I/O 컨퍼런스가 온라인으로 열리기 전에, OpenAI는 그보다 하루 전에 자체 컨퍼런스를 열어 전자에 대한 분석가들에 대한 기대치를 매우 높였습니다.\n\n\n\n간단히 말하면, 이제 구글이 새로운 AI 기능을 제공하는 것이 아닙니다. OpenAI의 발표에 기반해 구글이 어떻게 응답하는지 확인해보는 경우가 되었습니다.\n\n그리고 마지막으로, 현재 경매 중인 Siri 왕관을 가져가기 위해 구글과 OpenAI 간에 매우 소문난 '전투'를 고려할 때, 우리는 애플에 대해 이야기해야 합니다.\n\n## 이러한 목표가 계속되는 이유?\n\n애플과의 협업의 잠재적 이익을 고려할 때, Siri 계약을 따내는 것이 OpenAI의 목표였을 수도 있습니다.\n\n\n\n지연 시간이 뛰어나고 애정 어린 음성 행동, 여러 데이터 유형에서의 뛰어난 기능, 그리고 중요하게도 좋은 화면 시각 기능을 자랑하는 OpenAI가 프리미티브 시리를 개선하기 위해 Apple과 파트너십을 맺고 싶어하는 것은 비밀이 아닙니다.\n\n사실, Apple이 놀라운 기기 내 모델을 내놓지 않는 이상, 사용자들은 무관심하게 될 것이며 즉시 최첨단 기술과 비교할 것입니다.\n\n결국, 이는 분명히 Apple에게는 좋지 않은 PR 전망이지만, 자본이 풍부하여 자본주주 매입으로 자본주가 역사상 최대의 1310억 달러를 보유한 회사인 Apple은 여전히 시리를 해결하지 못했습니다. Apple은 이에 대한 여지가 거의 없습니다.\n\n그러므로 내부 'AI 난국'을 정리하고 좋은 AI 제품을 제공하기 시작할 때까지, GPT-4o(또는 Google이 오늘 자랑하는 것)에 베팅하고자 하는 유혹이 높아질 것입니다.\n\n\n\n그렇다면, 그 제휴가 어떻게 구체화될지 알기 위해 추측으로 나아가야 할 것입니다.\n\n애플은 사용자의 개인정보 보호에 매우 열정적이라고 알려져 있습니다. 이는 명백히 저작권과 보안 규정을 위반하여 모델을 훈련시킨 회사로부터 Siri를 위한 클라우드 기반 LLM 솔루션을 갖는 것과 호환되지 않는 것으로 보입니다.\n\n그러나 윤리가 돈에 방해를 받을 때, 기업들이 어떤 선택을 하는지 우리는 압니다. 의심스러운 윤리적 측면을 떠나서라도 돈이 관련된 상황에서 기업들이 선택하는 것을 말이죠.\n\n## 수십억 달러로부터 수백억 달러까지\n\n\n\n모든 것을 종합하면, OpenAI는 항상 실망시키지 않습니다. 그러나 이번에는 과거와는 다르게 그들의 의도가 명확하지 않을 수도 있습니다.\n\nGenAI 제품은 약속을 지키지 못하는 것으로 알려져 있으며, 이는 ChatGPT와 같은 경우에도 사실입니다. 지연 시간과 부족한 교차 모달 추론 등 여러 가지 이유로 이러한 상황이 발생합니다.\n\n지금 OpenAI는 인터넷 이후로 가장 큰 발견로 여겨지는 기대치에 부응하는 AI를 마침내 제공하는 제품을 갖고 있다고 생각합니다.\n\n이게 바로 그런 경우인지 여전히 이른 것 같지만, 이러한 가능성은 구글을 겁나게 할 민감한 지점을 만들고, AI의 다음 지평으로 자신들의 가장 큰 릴리스에 충분한 시간을 확보할 기회를 제공합니다.\n\n\n\n그러나 GPT-4o는 여전히 제한이 있으며 GPT-4보다 AGI에 더 가까워지는 것이 아니라는 점에서 명백합니다.\n\n하지만 이 제품은 강력한 AI를 널리 이용할 수 있게 함으로써 Generative AI를 사회에 훨씬 가깝게 만들어줍니다(제품은 무료로 제공될 예정임). 실제로 수십억 명에게까지 Siri를 통해 제공된다면 AI가 약속한 것을 실현하는 데 필요한 것이 바로 이런 접근방법이죠.","ogImage":{"url":"/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_0.png"},"coverImage":"/assets/img/2024-05-15-ChatGPT-4oOpenAIsNewFlagshipModelsFullReview_0.png","tag":["Tech"],"readingTime":7},{"title":"GPT4 Omni - 그저 음성 어시스턴트 이상의 무언가","description":"","date":"2024-05-15 11:33","slug":"2024-05-15-GPT4OmniSomuchmorethanjustavoiceassistant","content":"\n\n![image](/assets/img/2024-05-15-GPT4OmniSomuchmorethanjustavoiceassistant_0.png)\n\n오늘은 OpenAI의 봄 발표일이었고, 정말 놀라운 소식이었어요. 여러분도 동의하실 거라고 생각해요. 이번 밤 대부분을 새 음성 어시스턴트와 노는 데에 보냈거든요. (영화 HER에 언급된 것과 매우 정확한 시나리오입니다).\n\n그런데 혁명적이고 놀라운 음성 기능이 있을 뿐만 아니라, GPT4o 모델은 그 이상을 제공해요.\n\n솔직히 말하자면, 음성 기능에 정말 매료되어서 현실감을 느꼈죠. 그래서 한참을 걸려서 모델의 기술적 공지사항을 정말 자세히 읽어 보았는데, 방금 그것을 듣고 다시 놀라버렸어요. \n\n\n\nGPT4o를 생각했을 때는 기본적으로 GPT-4 Turbo의 최적화된 버전일 뿐이라고 생각했어요. 이번에는 더 나은 추론 능력, 더 작은 지연 시간 및 음성 대화용으로 훈련되었어요. 그들은 이미 보유한 기술을 Whisper와 TTS와 함께 최적화된 새 모델과 통화를 결합하여 ChatGPT에 매우 효과적으로 통합했다고 생각했었어요.\n\n그러나 모델의 기술 보고서를 읽은 후에, 이렇게 발견했어요:\n\n텍스트, 오디오 및 비전 멀티모달리티가 모두 포함된 단일 새 모델!!\n\n텍스트/오디오/이미지를 입력으로 받아 텍스트/오디오/이미지로 출력하는 단일 모델이 있어요.\n\n\n\n저는 지난 번에 GenAI의 미래는 모든 모달리티를 고루 보유한 다중 모델링에 있을 것이라고 언급했다는 것을 알고 있어요. 그리고 우리는 그런 의도로 이니셔티브를 볼 수 있어요.\n\n하지만 2024년 5월에 그 미래가 될 줄은 상상도 못했고, 우리는 어떠한 주요 모델도 처리하고 생성할 수 있는 능력을 가진 모델을 보유하고 있는 상황이에요. 그런데 여전히 빠른 응답 시간을 유지하죠.\n\n지금 이것이 혁명적인 것을 넘어섰어요. OpenAI가 또 한 번 선방했죠. 여기에 가까운 것을 갖고 있는 사람은 아무도 없으며, 그러한 모델의 가능성은 너무 커서 우리 마음으로는 처리하기 어렵습니다.\n\n우리는 이전의 개념과 아이디어를 다시 검토해야 해요. 왜냐하면 이전에 현실이 될 수 없었던 제한 사항이 오늘날에는 존재하지 않을 수 있기 때문이죠. 그리고 우리 마음을 새롭게 개조된 아이디어로 준비해야 하며, 그 전에는 상상조차 할 수 없었던 해결책에 대한 새로운 아이디어에 대비해야 해요.\n\n\n\nPS1: 그들의 API에서 모든 모드에 대한 액세스를 아직 공개하지 않았습니다, 현재는 텍스트와 이미지만 사용할 수 있습니다. 그래서 다른 방법을 고민해볼 수 있지만, 지금까지 아직 정해진 날짜가 없는 출시를 기다려야 합니다.\n\nPS2: 예시로 모델은 3D 이미지도 생성합니다.\n\nPS3: 이 모델은 오늘날 GPT-4 Turbo의 절반 가격에 판매되고 있는데, 따라서 Turbo보다 훨씬 더 저렴하면서 효율적입니다.\n\n아래는 최신 최고 모델과 비슷한 성능을 보여주는 일부 벤치마크입니다:\n\n\n\n\n![image](/assets/img/2024-05-15-GPT4OmniSomuchmorethanjustavoiceassistant_1.png)\n\nYou can learn more about this model and see examples of its use on [OpenAI’s website](https://example.com).\nHello GPT-4o | OpenAI\n","ogImage":{"url":"/assets/img/2024-05-15-GPT4OmniSomuchmorethanjustavoiceassistant_0.png"},"coverImage":"/assets/img/2024-05-15-GPT4OmniSomuchmorethanjustavoiceassistant_0.png","tag":["Tech"],"readingTime":2},{"title":"드라유레카, 세계에 대한 Nvidia의 경고","description":"","date":"2024-05-15 11:32","slug":"2024-05-15-DrEurekaNvidiasWarningtotheWorld","content":"\n\n지난 주 Nvidia는 AI 로봇이 얼마나 빠르게 발전하고 있는지를 다시 상기시키며, LLMs의 훌륭한 잠재력을 강조했습니다. 이는 교육과정에서 인간의 개입을 최소화하는 데 큰 기회를 제공합니다.\n\n간단히 말해서, 그들은 AI로 훈련된 더 강력한 로봇의 출현을 보여주었습니다.\n\n로봇 AIs를 훈련하는 AIs입니다.\n\n이를 위해 그들은 몇 달 전에만으로는 불가능했던 다양한 복잡도의 시나리오에서 요가 볼 위에 균형을 유지하는 안드로이드를 훈련시켰습니다.\n\n\n\n이미지: \"/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_0.png\"\n\n확실히, 로봇이 이미 매우 복잡한 현실 과제를 아주 빠르게 수행하는 사실은 AI 로보틱스가 멈출 수 없는 추세임을 모두에게 알립니다.\n\n하지만 그들은 어떻게 이것을 성취했을까요?\n\n# 보상의 중요성\n\n\n\n먼저, AI 로봇이 어떻게 훈련되는지 궁금한 적이 있나요?\n\n우리는 그들이 주어진 보상을 극대화하는 환경에서 행동을 취할 수 있도록 도와주는 정책을 생성합니다.\n\n예를 들어, 100m를 달리는 모델을 훈련하려면 일어서고 걷고 뛰고 결국은 질주하는 방법을 먼저 배워야 합니다. 모델이 그 작업을 이해할 때까지 '도약'마다 보상을 주면서 훈련시킵니다.\n\n하지만 이것을 어떻게 가르칠까요? 보상을 부여함으로써요.\n\n\n\n예를 들어, 모델이 세워지면 점수를 주는 것입니다. 그 반면에 모델이 넘어지면 처벌을 받습니다.\n\n이를 보상 모델링이라고 합니다. 좋은 행동에 보상을, 나쁜 행동에 처벌을 하는 함수를 만드는 것이죠. 이렇게 하면 시간이 지남에 따라 모델은 누적 보상을 최대화하는 몸의 위치와 행동을 결정합니다.\n\n이것은 매우 표준적인 방법이며, Google Deepmind과 같은 최첨단 연구소도 같은 원칙을 사용하여 일어서고, 뛰어들고, 슛을 차고, 심지어 수비를 하는 축구 로봇을 훈련시키고 있습니다.\n\n하지만, 이러한 보상 함수를 정의하는 것은 굉장히 어렵습니다.\n\n\n\n인간의 몸을 생각해보자면, 예를 들어 펜 회전이라는 전반적인 결과에 긍정적이거나 부정적인 영향을 미치는 손의 모든 근육과 관절의 움직임이 있습니다.\n\n액션이 매우 복잡한 경우, 인간은 로봇이 그에 따라 행동하도록 돕는 가장 좋은 보상 함수를 작성하는데 어려움을 겪습니다.\n\n이 복잡성을 감지하기 위해, 볼 걸음걸이 작업을 위해 최종적으로 결정된 보상 함수는 다음과 같습니다:\n\n![이미지](/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_1.png)\n\n\n\n그리고 이 방정식을 찾는 것은 보이는 대로 어렵습니다. 하지만 여기에 중요한 점이 있습니다: 그 함수는 인간이 아닌 AI가 작성했습니다.\n\n# AI의 보상\n\n몇 달 전, DrEureka 뒤의 연구자들이 Eureka를 출시했습니다. 이는 AI 기반 보상 함수 설계 알고리즘입니다.\n\n아래에서 보듯이, 아이디어는 Large Language Model (GPT-4)을 사용하여 반복적 루프를 사용하여 환경(로봇이 행동할 장면을 설명하는 코드) 및 수행해야할 작업을 주면, 이러한 보상 함수를 생성하여 해당 내용을 시뮬레이션된 환경에서 평가하는 것이었습니다.\n\n\n\n시뮬레이션 결과에 따라 피드백이 생성되었고, LLM은 이를 사용하여 품질 기준이 충족될 때까지 새 보상 함수를 생성했습니다.\n\n![image](/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_2.png)\n\n유레카와 팀은 놀라운 결과를 달성했습니다: 로봇을 펜 회전 트릭을 수행하도록 훈련시켰으며, 최고의 CGI 전문가조차 고전할 것입니다.\n\n그 업적은 놀라운 것이었으며, LLM을 반복적으로 사용하여 인간이 설계할 수 없는 복잡한 보상 함수를 만들 수 있음을 증명했습니다. 그리고 거의 인간 개입이 없었습니다.\n\n\n\n이제, 그림을 그리듯이 손가락, 관절, 근육 및 관절 점수를 모델링하는 보상 함수를 쓰고 있다고 상상해보세요.\n\n의심의 여지없이, Eureka는 Nvidia가 AI가 AI를 훈련하는 것이 로봇 공학의 미래임을 세계에 입증했습니다.\n\n그러나 안타깝게도 문제가 하나 있었습니다. 만일 이러한 결과를 현실 세계로 가져가고 싶다면 어떻게 해야 할까요?\n\n그럴 때 DrEureka가 등장합니다.\n\n\n\n# 개방적인 세상 속 인공지능\n\n로봇 공학에서 가장 어려운 문제는 비용과 불확실성을 다루는 것입니다.\n\n실제로 로봇을 훈련하는 것은 매우 비싸고 실제 제약(마찰, 바람, 온도 등)을 다뤄야 하며 물론 고장의 위험도 있습니다.\n\n오늘 논의하는 경우와 같이, 일부 경우에는 영점(Zero-shot) 모드로 작동합니다. 다시 말해서, 모든 훈련은 시뮬레이션에서 수행되며 실제 세계에서는 최선을 바랍니다.\n\n\n\n상상할 수 있는 대로, 이 전환은 매우 중요하며 오류 발생 가능성이 높습니다. 실제 환경에는 높은 불확실성이 포함되어 있습니다. 따라서 로봇을 길에 내려놓기 전에 프로세스에 한 가지 추가 단계를 추가해야 했습니다: 도메인 랜덤화.\n\n## 견고한 로봇 훈련\n\n실제 세계를 생각해보면 불확실성이 가득합니다.\n\n- 온도는 매초 변화합니다.\n- 로봇은 울퉁불퉁한 지형에 부딪힐 수 있습니다.\n- 바람이 다양한 방향으로 불 수 있습니다.\n- 로봇의 관절 운동은 사용으로 인해 덜 부드러워질 수 있습니다.\n- 그리고 물체들이 길에 나타날 수 있습니다.\n\n\n\n수천 개의 다른 예기치 않은 변화 중 하나입니다.\n\n따라서 성공 확률을 극대화하기 위해 연구자들은 시뮬레이션에서 모델을 훈련시키기 위해 제약 조건(품질이나 마찰력과 같은)에 무작위 변화를 도입하여 다양한 환경 시나리오에서 모델을 강화합니다.\n\n그러나 각 제약 조건에 대해 정의할 값이 무엇인지 결정하는 것은 굉장히 어렵습니다. 그래서 연구자들이 생각한 것은 LLMs가 여기에서도 우리를 도울 수 있을까요?\n\n네, 하지만 문제가 있습니다: 도메인 랜덤화 검색 공간 또는 각 환경 제약 조건이 가질 수 있는 가능한 값의 수는 무한하므로, 이 문제는 LLMs에게 매우 어렵습니다.\n\n\n\n이를 완화하기 위해, 그들은 먼저 RAPP (보상 인식 물리 사전)라고 알려진 경량 검색을 수행하여 가능한 제약 값 범위를 물리적으로 가능한 값으로 좁혔습니다 (예: 음의 마찰 값은 불가능합니다).\n\n이는 LLM의 검색 공간을 좁혀 다양한 도메인 조건을 생성하는 데 도움이 됩니다.\n\nRAPP 경계가 설정되면, 그들은 다시 GPT-4를 사용하여 각 제약에 대한 타당한 값을 생성하여 새로운 시나리오 (도메인 무작위화)를 만들었으며, 우리에게 전체 DrEureka 프레임워크를 제공했습니다:\n\n![DrEureka Framework](/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_3.png)\n\n\n\n따라서 전체 프로세스는 다음과 같습니다:\n\n- 작업 및 안전 지침이 정의되며 환경 조건도 정의됩니다.\n- RAPP를 사용하여 각 조건의 값 범위를 제한합니다. 이는 각 환경 제약 조건의 값 범위를 좁힙니다.\n- 그런 다음 LLM은 각 환경에 대한 가능하지만 현실적인 시나리오를 생성합니다(도메인 랜덤화), 로봇을 테스트하기 위한 '새로운' 시뮬레이션 환경 세트를 생성합니다.\n- 병렬로 다른 LLM은 각 특정 환경의 조건을 기반으로 보상 함수를 생성합니다.\n- 그다음 로봇은 해당 보상을 최대화하는 정책을 사용하여 훈련되어 실제 세계에서 예상대로 행동할 수 있도록 학습됩니다. 품질 기준이 충족될 때까지 세 번째 및 네 번째 단계를 반복하는 피드백이 생성됩니다. 목표는 로봇이 실제 세계에서 예상대로 행동한다는 가능성을 극대화하는 정책을 학습하는 보상 함수를 얻는 것입니다.\n\n결국 로봇은 실제로 투입되었고 결과는 스스로 이야기합니다.\n\n# 로봇 시대\n\n\n\nNvidia가 로보틱스에 대해 매우 많은 것을 건너뛰고 있다고 말하는 것은 저평가이에요.\n\n사실, 이 논문의 연구자 가운데 하나인 Jim Fan은 Nvidia의 랜드마크 프로젝트인 프로젝트 Gr00t를 이끄는 중이에요. 이 프로젝트는 현실 세계 에이전트를 위한 기초 모델을 개발하는 프로젝트입니다.\n\n그러나 여전히 명백한 제약이 있으므로 바닥에 발을 디딘 채로 있어야 해요. 예를 들어, 한 번 도메인 매개변수가 설정되면 해당 환경에서의 훈련 실행 동안 고정되는 한계가 있어요.\n\n다시 말해, 마찰력이 일정하게 유지된 채로 남아있는 것처럼, 이는 실제 세계에서는 사실이 아니라는 것이에요. 왜냐하면 새로운 표면마다 변화하기 때문이죠.\n\n\n\n그 말은 그들의 결과가 인공지능 로봇 분야의 발전 속도에 대한 경고 신호로 작용한다는 것을 명확히 보여줍니다. 이 분야는 여전히 빠르게 가속화되고 있는 것으로 보입니다.","ogImage":{"url":"/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_0.png"},"coverImage":"/assets/img/2024-05-15-DrEurekaNvidiasWarningtotheWorld_0.png","tag":["Tech"],"readingTime":5},{"title":"간단한 도구를 사용한 예약된 네트워크 활동 보고서","description":"","date":"2024-05-15 11:30","slug":"2024-05-15-ScheduledNetworkActivityReportsUsingaSimpleSetofTools","content":"\n\n# 소개\n\n평범한 Raspberry Pi 팬 중 수면 패턴이 안 좋은 사람은 아침에 먼저 핸드폰을 꺼내어 사랑하는 기기가 잘 작동하고 있는지 궁금해합니다. 혹은 더 넓은 의미로, 자는 동안 내 네트워크에서 무슨 일이 벌어지고 있는지 궁금해합니다.\n\n![Scheduled Network Activity Reports Using a Simple Set of Tools](/assets/img/2024-05-15-ScheduledNetworkActivityReportsUsingaSimpleSetofTools_0.png)\n\n이 블로그 포스트에서 나는 최근 완료한 작은 프로젝트에 대해 설명할 것입니다. 여기서 나는 몇 가지 도구를 연결하여 Raspberry Pi와 관련된 특정 활동에 대한 아침에 첫 번째 활동 보고서를 보내기 위해 사용했습니다. 아래의 구체적인 사용 사례 자체는 특별히 유용하지는 않지만, 여러분이 유용하다고 생각하는 어떤 방향으로든 확장할 수 있는 장난감 예시입니다.\n\n\n\n# 야간 보고서\n\n많은 기업들이 이제 일반 업무 시간 이외에 대부분의 사이버 공격이 발생한다는 사실을 깨달았습니다. 그에 따라 단순히 백신 소프트웨어와 내부 IT 팀만으로는 부족하다는 것을 깨달았습니다. 대신 내부 팀을 24시간 365일 외부 사고 대응팀과 함께 보강하는 Sophos MDR(관리되는 탐지 및 대응) 서비스와 같은 것이 필요합니다. 네트워크를 모니터링하고 시간별 보고서를 제공하는 다양한 상업용 도구도 있습니다. 하지만 저는 이미 갖고 있는 간단한 도구들을 활용하여 어떻게 하는 지를 보기 위해 자체적인 이른바 DIY 솔루션을 만들기로 했습니다.\n\n# 사용된 도구\n\n본 프로젝트에서는 집 네트워크에 라즈베리 파이 5를 사용했고, 몇 가지 내부 장치(Pinging에 사용되는)와 함께 사용했습니다. 소프트웨어로는 tcpdump(명령줄 기반의 데이터 패킷 분석 도구), tcpdump와 유사한 Wireshark의 명령줄 대체인 tshark, 데이터 분석을 위해 Python(Pandas 및 Matplotlib 포함), 이메일 발송을 위한 sendmail, 그리고 Gmail 계정을 사용했습니다. 이러한 도구들은 몇 가지 bash 스크립트로 연결되었고 cron을 통해 실행 스케줄이 잡혔습니다.\n\n\n\n기본 아이디어는 다음과 같이 3단계로 구성되었어요.\n\n- 나의 라즈베리 파이에 대한 핑을 로깅하기 시작하고 종료할 시간을 선택하여 (예: 밤새) tcpdump 스케줄링\n- 이 데이터를 분석하여 간단한 그래프로 변환하는 파이썬 스크립트를 스케줄링\n- 라즈베리 파이에게 이 그래프를 내가 선택한 시간에 이메일로 보내도록 하는 스케줄링\n\n각 단계를 함께 살펴보겠습니다.\n\n# 파트 1 — tcpdump\n\n\n\ntcpdump을 사용하는 것은 매우 간단하며, 온라인에는 무수히 많은 훌륭한 가이드가 있습니다. 예를 들어 아래 라인은 wlan0 인터페이스에서 ICMP(Internet Control Message Protocol) 패킷을 수신하도록 tcpdump에 지시합니다.\n\n```js\ntcpdump -i wlan0 icmp\n```\n\n이 코드는 화면에 패킷을 표시합니다. 파일에 기록하려면('pings.pcap'에 로그를 기록하는 경우), 다음 명령을 사용해야 합니다.\n\n```js\ntcpdump -i wlan0 icmp -w pings.pcap\n```\n\n\n\n다수의 pcap 파일 가이드가 있습니다 [3].\n\n패킷 캡처의 시작과 종료를 예약하는 것은 적절한 명령어를 두 개의 별도 셸 스크립트에 넣고 cron을 사용하여 예약하는 것만으로도 간단합니다. 이러한 스크립트에 대해 도움이 되는 GitHub 저장소 [4]를 찾아내어 이를 편집했습니다.\n\n실행 중에는 Windows 머신과 WebSSH 앱을 실행중인 휴대전화(모바일 폰)를 사용하여 Raspberry Pi에 핑을 전송했습니다 [5].\n\n이 단계의 마지막 단계는 pcap 파일을 Python이 이해할 수 있는 형식으로 변환하는 것입니다. 이를 위해 tshark를 사용했습니다. 'pings.pcap' 파일을 'pings.csv'로 변환하는 명령어는 유용한 블로그 포스트에서 찾은 내용을 이용하여 아래와 같이 실행했습니다 [6].\n\n\n\n```js\ntshark -N n -r ./pings.pcap -T fields -e frame.number -e _ws.col.Time -e _ws.col.Source -e _ws.col.Destination -e _ws.col.Protocol -e _ws.col.Length -e _ws.col.Info -e tcp.seq -e ip.ttl -E header=y -E separator=, > pings.csv\n```\n\n이 명령어는 그 후 'stop' 스크립트의 끝에 포함되었습니다.\n\n# 파트 2— 파이썬\n\n관심 있는 데이터를 수집하고 CSV 파일로 변환한 후에는 Python으로 할 수 있는 일이 무궁무진합니다. 사용한 스크립트는 매우 간단합니다. 각 IP 주소에서 ping의 수를 계산하고, 데이터의 막대 플롯을 생성한 다음 jpg로 저장합니다.\n\n\n\n```js\n#라이브러리 가져오기\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#데이터 불러오기\npings = pd.read_csv('pings.csv')\n\n#관심 있는 열로 제한하기\npings = pings.iloc[:,0]\n\n#다시 데이터프레임으로 변환하기\npings = pings.to_frame()\n\n#열 이름 변경하기\npings.columns = ['Source_IP']\n\n#Raspberry Pi 자체에서의 핑 제거하기\npings = pings[pings['Source_IP'] != '192.168.68.127']\n\n#IP 주소로 그룹화하고, 수를 세어 정렬하기\npings_gb = pings.groupby(['Source_IP']).size().sort_values()\n\n#그룹화된 데이터를 사용하여 막대 그래프 만들기\nfig = pings_gb.plot(kind = 'bar', rot=0, figsize=(10, 8), fontsize=13)\nfig.set_ylabel(\"Source Count\")\nfig.figure.savefig('pings.jpg')\n```\n\n# 파트 3— Gmail\n\n이 부분은 가장 많은 노력을 필요로 했으며, Gmail 쪽에서의 설정(보안 수준이 낮은 앱 액세스 허용 및 앱 비밀번호 생성)과 Raspberry Pi 쪽에서의 설정(sendmail 설치 및 구성)이 포함되었습니다. 이러한 단계에 대한 두 가지 훌륭한 가이드는 [7, 8]에서 확인할 수 있습니다. 이 가이드를 따르면 var/logs/maillog의 로그 파일에 \"My unqualified host name (raspberrypi) unknown; sleeping for retry\"라는 오류가 계속 표시된다는 것에 유의하십시오. 이 오류를 해결하는 방법은 [9]에서 확인하십시오. 또한, 접근 보안 수준이 낮기 때문에 별도의 Gmail 계정을 만드는 것이 좋을 수 있습니다.\n\n설정을 완료하면 Python에서 생성된 이미지를 첨부한 이메일을 보낼 수 있습니다.\n\n\n\n\n```bash\necho \"Enjoy! from Raspberry Pi\" | mail -s \"Your Overnight Ping Analysis!\" [내 주 이메일 주소] -A pings.jpg\n```\n\n# 모두 함께 사용하기\n\n마지막으로, 위의 단계들을 연결하여 크론을 사용할 수 있습니다. (좋은 크론 도우미를 보려면 여기를 참조하세요 [10]). 이를 위해 크론탭을 수정하여 아래와 유사한 내용으로 편집하세요.\n\n```bash\n0 22 * * * bash /robh/start_tcpdump.sh # 10시에 수집 시작\n0 6 * * * bash /robh/stop_and_convert.sh # 6시에 수집 중지 및 csv로 변환\n5 6 * * * python ping_script.py # 6:05에 파이썬 스크립트 실행\n30 6 * * * bash /robh/email.sh # 6:30에 이미지를 이메일로 보내기\n```\n\n\n\n작업 표정 변경이 완료되었습니다.\n\n\n\n이메일 안에는 Python 스크립트가 생성한 이미지 파일이 포함되어 있었습니다.\n\n![image](/assets/img/2024-05-15-ScheduledNetworkActivityReportsUsingaSimpleSetofTools_2.png)\n\n# 결론\n\n위의 이미지는 매우 간단하지만, 크론을 사용하여 다음 이벤트 체인을 예약하는 원칙을 보여주기를 희망합니다,\n\n\n\n데이터 수집 → CSV로 변환 → Python으로 분석 → 결과 이메일로 전송\n\n수집하는 데이터와 분석 방법은 물론 무한히 맞춤화할 수 있어요. 이 게시물이 여러분의 일간 보고 요구 사항에 유용한 시작점이 되었으면 좋겠어요!","ogImage":{"url":"/assets/img/2024-05-15-ScheduledNetworkActivityReportsUsingaSimpleSetofTools_0.png"},"coverImage":"/assets/img/2024-05-15-ScheduledNetworkActivityReportsUsingaSimpleSetofTools_0.png","tag":["Tech"],"readingTime":5},{"title":"태양 에너지 혁신 시스템 리뷰","description":"","date":"2024-05-15 11:29","slug":"2024-05-15-SolarInnovatorSystemReview","content":"\n\n## 360도 각도에서 지속적으로 패널 각도를 조절할 필요 없이 저렴하고 깨끗한 에너지를 생성하는 DIY 태양 에너지 프로젝트.\n\n![태양 에너지 혁신 기기 사용 설명 도면](/assets/img/2024-05-15-SolarInnovatorSystemReview_0.png)\n\n# 이 태양 에너지 혁신 기기는 무엇입니까?\n\n이제 이 제품이 정확히 무엇인지 설명해 드리겠습니다. 위의 이미지에서 보는 바와 같이, 이 기기는 약 120개의 모노-셀이 부착된 구형 구조물입니다.\n\n\n\n기능:\n\n구 형태:\n\n알루미늄 시트 추가:\n\n구획:\n\n\n\n미세 먼지 축적이 적습니다:\n\n## 동영상 리뷰:\n\n이 동영상은 전체 DIY 프로젝트의 리뷰로, 그 제작 방법과 개인적인 필요에 맞게 만드는 방법을 보여줍니다.\n\n## 제작 간략 개요:\n\n\n\n이제 장치 제작에 들어가 봅시다\n\n![Image 1](/assets/img/2024-05-15-SolarInnovatorSystemReview_1.png)\n\n![Image 2](/assets/img/2024-05-15-SolarInnovatorSystemReview_2.png)\n\n![Image 3](/assets/img/2024-05-15-SolarInnovatorSystemReview_3.png)\n\n\n\n이것은 DIY 프로젝트이므로 필요에 따라 구축해야 합니다. 태양광 출력을 늘리기 위해 크게 만들거나 소형으로 만들어 미니 영역을 위해 사용할 수도 있습니다.\n\n이 장치를 구축하는 데 필요한 모든 도구 및 재료는 이 가이드에 포함되어 있습니다. 여기에서 액세스할 수 있습니다.\n\n# 가이드:\n\n이 안내서는 당신이 기대하는 것 이상을 제공합니다. 따라서 몇 가지 중요한 내용들도 포함되어 있습니다.\n\n\n\n- 재료 목록\n- 치수 및 사양\n- 배선도\n- 비디오 프레젠테이션\n- 추가 혜택과 선물\n\n프로그램의 저자가 세부 사항을 모두 펼쳐놓았고 프로젝트 제작에 매우 도움이 되는 비디오 프레젠테이션을 사용하고 있습니다.\n\n회원 전용 영역에서 모든 비디오를 다운로드하여 태블릿이나 모바일 기기에 저장한 후 저자와 함께 보고 만들어보세요.\n\n# 결과:\n\n\n\n\n![이미지](/assets/img/2024-05-15-SolarInnovatorSystemReview_4.png)\n\n이 장치는 매력적으로 보입니다. 아이디어도 독특하고 혁신적입니다. 우리의 계산에 따르면, 이 시스템은 120 개의 셀로 이루어져 있어서 최대 출력이 3.6에서 3.8 와트, 각 셀당 최대 전압이 0.6에서 0.8 볼트여야 합니다.\n\n그러므로 모든 것을 계산해 보면, 한 대의 태양 혁신기기는 432에서 434 와트를 발전할 수 있어야 합니다.\n\n지금 이 계산은 근사치이며, 장치의 크기와 사용 가능한 자원 및 공간에 따라 달라질 수 있습니다.\n\n\n\n\n# Solar Innovator System에 액세스하세요:\n\n이제 전체 시스템에 액세스하려면 제 웹사이트를 방문하실 수 있습니다. 이 제품에 대한 자세한 리뷰와 장단점 그리고 이 장치를 사용해야 할지 여부에 대한 정보가 제공되어 있습니다.\n\n시스템에 액세스할 수 있는 내 웹사이트 바로 가기: [여기](링크하실 웹사이트 주소를 입력해주세요)","ogImage":{"url":"/assets/img/2024-05-15-SolarInnovatorSystemReview_0.png"},"coverImage":"/assets/img/2024-05-15-SolarInnovatorSystemReview_0.png","tag":["Tech"],"readingTime":2},{"title":"증명 개념 아두이노 UNO에서 비동기 프로그래밍","description":"","date":"2024-05-15 11:27","slug":"2024-05-15-ProofofConceptAsynchronousprogramminginArduinoUNO","content":"\n\n<img src=\"/assets/img/2024-05-15-ProofofConceptAsynchronousprogramminginArduinoUNO_0.png\" />\n\n안녕하세요.\n\n요즘 NodeJS의 작동 방식에 관심을 갖게 되었습니다. 한 스레드에서 여러 작업을 동시에 실행하는 것이 가능하다는 것이 흥미롭게 느껴졌죠. 그래서 스스로에게 물었습니다. 아두이노 UNO에서도 이와 같은 것이 가능할까요?\n\n그래서 이것이 어떻게 작동하는지 알아보기 위해 조금 연구를 해봤는데, Node.js는 한 스레드에서 동시성을 달성하기 위해 2가지 알고리즘을 사용한다는 것을 깨달았습니다.\n\n\n\n- 이벤트 루프: 각기 다른 함수들로 이루어진 작업 목록을 저장하고 실행하는 역할을 담당합니다. 이러한 함수들은 쉽게 코루틴이 될 수 있습니다.\n\n- 코루틴: 실행을 중단하고 나중에 다시 재개할 수 있는 함수 유형입니다. \n\n그래서 제가 Arduino UNO에서 동일한 동시성 시스템을 구현하기로 결정했습니다.\n\n가장 쉬운 부분부터 시작했습니다. 코루틴입니다. C/C++은 내장 코루틴 지원이 없기 때문에, 자체 코루틴 시스템을 만들었습니다. 이를 위해 Duff의 장치라는 매우 유용한 알고리즘을 발견했습니다. 이 알고리즘은 코루틴과 직접적으로 관련되어 있지는 않지만, 코루틴 구현에 도움이 될 것입니다.\n\n\n\n예, 주요 아이디어는 정적 상태 변수를 생성하여 코루틴의 상태를 기억할 수 있도록하는 것입니다. 그런 다음 switch 문을 사용하여 함수를 청크로 분할하고 상태 변수를 사용하여 필요한 부분을 실행할 수 있습니다. 내가 무슨 얘기를 하는지 이해 못 하겠다고요? 그래, 이런 느낌입니다!\n\n```js\nint coroutine(){\nstatic int state = 0;\n\n     switch(state){\n         case 0: do { printf(\"Hello World 0\"); state++; return 1; case 1:; } while(0);\n                 do { printf(\"Hello World 1\"); state++; return 1; case 2:; } while(0);\n                 do { printf(\"Hello World 2\"); state++; return 1; case 3:; } while(0);\n                 do { printf(\"Hello World 3\"); state++; return 1; case 4:; } while(0);\n                 do { printf(\"Hello World 4\"); state++; return 1; case 5:; } while(0);\n                 do { printf(\"Hello World 5\"); state=0; return 1;          } while(0);\n     }\n\n     return -1;\n\n}\n\nvoid setup(){\n     Serial.begin( 9600 );\n}\n\nvoid loop(){\n     coroutine();\n     delay(1000);\n}\n```\n\n![이미지](https://miro.medium.com/v2/resize:fit:852/1*FmR-nOMNpohwvWCPbysUiQ.gif)\n\n좋죠? 하지만 이 방법을 사용하여보다 복잡한 코루틴을 작성한다고 상상해보세요. 그러다가 Duff 장치를 생성하는 것을 단순화하기 위해 매크로를 사용하기로 생각했습니다. 여기에는 몇 가지 매크로가 있습니다:\n\n\n\n```js\n#define coNext         do { _state_ = __LINE__; return 1; case __LINE__:; } while (0)\n#define coGoto(VALUE)  do { _state_ = VALUE   ; return 1;                 } while (0)\n#define coYield(VALUE) do { _state_ = VALUE   ; return 1; case VALUE:;    } while (0)\n```\n\n```js\n#define coStart static int _state_ = 0; { switch(_state_) { case 0:;\n#define coEnd do { _state_ = 0; return -1; } while (0)\n#define coStop } _state_ = 0; return -1; }\n#define coSet(VALUE) _state_ = VALUE\n#define coGet _state_\n```\n\n참고: 매우 유용한 다른 매크로도 추가되었습니다.\n\n```js\n#define coDelay(VALUE)  do { static auto tm = millis()+VALUE; while( millis() < tm ){ coNext; } tm = millis()+VALUE; break; } while (0)\n#define coUDelay(VALUE) do { static auto tm = micros()+VALUE; while( micros() < tm ){ coNext; } tm = micros()+VALUE; break; } while (0)\n#define coWait(VALUE)   do { while( !VALUE ){ coNext; } } while(0)\n```\n\n이 구현의 작은 단점 중 하나는 switch 문을 사용하여 코루틴을 실행하기 때문에 코루틴 내에서 switch 문을 사용할 수 없다는 것입니다. 해당 경우는 코드를 완전히 깨뜨릴 것입니다.\n\n\n\n\n이전 예제를 복제한다면 이렇게 될 것입니다:\n\n```js\nint coroutine(){\ncoStart\n\n     printf(\"Hello World 0\"); coNext;\n     printf(\"Hello World 2\"); coNext;\n     printf(\"Hello World 3\"); coNext;\n     printf(\"Hello World 4\"); coNext;\n     printf(\"Hello World 5\"); coNext;\n     printf(\"Hello World 6\");\n\ncoGoto(0);\ncoStop\n}\n\nvoid setup(){\n     Serial.begin( 9600 );\n}\n\nvoid loop(){\n     coroutine();\n     delay(1000);\n}\n```\n\n이제 실제 Arduino 프로젝트에서 이것을 적용한다면 어떻게 될까요?\n\n아두이노 UNO에서 실행되는 3개의 비동기 프로세스를 실행하는 프로그램을 작성하고 싶다고 가정해보겠습니다. 이 프로그램은 일련의 LED를 켜고 끄게 할 것입니다. 아래에 코드가 있습니다:\n\n\n\n```js\nint coroutine1(){\n     static bool b=0;\ncoStart\n\n     digitalWrite(7,b);\n     coDelay(300); b=!b;\n\ncoGoto(0);\ncoStop\n}\n\nint coroutine2(){\n     static bool b=0;\ncoStart\n\n     digitalWrite(6,b);\n     coDelay(1000); b=!b;\n\ncoGoto(0);\ncoStop\n}\n\nint coroutine3(){\n     static int x=0; static bool b=0;\n     unsigned char pin[] = { 13, 12, 11, 10, 9, 8 };\ncoStart\n\n     while( x-->0 ){\n         digitalWrite( pin[x], b );\n         coDelay(100);\n     } b=!b; x=6;\n\ncoGoto(0);\ncoStop\n}\n\nvoid setup(){\n     unsigned char pin[] = { 13, 12, 11, 10, 9, 8, 7, 6 };\n     for( auto &x: pin ) pinMode( pin, OUTPUT );\n}\n\nvoid loop(){\n     coroutine1();\n     coroutine2();\n     coroutine3();\n}\n```\n\n![Animation](https://miro.medium.com/v2/resize:fit:852/1*LwDRI8sOZe-EVOxAEsgLHw.gif)\n\n알아보기 쉽게 Arduino UNO에서 비동기 프로그래밍이 가능하다는 것을 확인할 수 있습니다. 시간이 부족해 이 데모에 대한 이벤트 루프를 구현하지 못했지만, loop 함수가 좋은 대체재가 됩니다. 이 글에 흥미를 느끼셨다면 제가 두 번째 파트를 작성해 이벤트 루프에 대해 설명해드릴게요. 이 프로젝트가 마음에 들었기를 바라며, 우리 다음에 또 뵙겠습니다.\n\n이 글을 마치기 전에, 제가 만든 Nodepp라는 프레임워크를 소개하고 싶어요. 이 프레임워크는 NodeJS와 매우 유사한 구문으로 Arduino에서 비동기 코드를 작성할 수 있게 해주는 C++ 프레임워크입니다. 즐겁게 이용하시기를 바랍니다.\n\n\n\n\nhttps://www.arduino.cc/reference/en/libraries/nodepp/","ogImage":{"url":"/assets/img/2024-05-15-ProofofConceptAsynchronousprogramminginArduinoUNO_0.png"},"coverImage":"/assets/img/2024-05-15-ProofofConceptAsynchronousprogramminginArduinoUNO_0.png","tag":["Tech"],"readingTime":5}],"page":"95","totalPageCount":156,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}