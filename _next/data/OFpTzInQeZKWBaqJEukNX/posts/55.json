{"pageProps":{"posts":[{"title":"React Fiber에 대한 설명 더 부드러운 UI를 위한 점진적 렌더링","description":"","date":"2024-06-19 23:46","slug":"2024-06-19-ReactFiberExplainedIncrementalRenderingforSmootherUIs","content":"\n\n<img src=\"/assets/img/2024-06-19-ReactFiberExplainedIncrementalRenderingforSmootherUIs_0.png\" />\n\n# React Fiber이 무엇인가요?\n\nReact Fiber는 React 라이브러리의 완전히 새로운 핵심 조정 알고리즘입니다. 조정은 React가 UI를 응용 프로그램 데이터 상태에 맞게 최신 상태로 가져오는 방법입니다. 이는 렌더링 작업을 \"파이버(fibres)\"라고 불리는 작은 부분으로 분할합니다. 이렇게 하면 React가 이러한 부분들을 조각조각 처리하고 필요할 때 작업을 프레임 간에 분배할 수 있습니다. 이 방식은 렌더링 프로세스에서 더 많은 유연성을 제공하며 업데이트를 우선 순위 지정하고 스케줄링할 수 있는 기능을 제공합니다.\n\n## 이게 왜 중요한가요?\n\n<div class=\"content-ad\"></div>\n\n산업에서 많은 개발자들이 React Fiber 아키텍처에 대해 완전히 알지 못하는 경우가 많습니다. React Fiber는 렌더링 프로세스를 작은 단위로 나눔으로써 훨씬 효율적인 애플리케이션을 만들 수 있는 기회를 제공하며, 웹 개발 분야에서 엄청난 발전을 이뤘습니다.\n\n## 이전 알고리즘 (스택 조정자)\n\n이전 조정 알고리즘인 스택 조정자는 React가 업데이트를 동기적으로 한 번 적용했습니다. 이는 컴포넌트의 상태가 변경될 때 React가 전체 컴포넌트 트리를 탐색하고 이전 트리와 비교한 다음 필요한 업데이트를 한 번에 DOM에 적용한 것을 의미합니다. 이 방법은 간단한 애플리케이션에는 좋았지만, 복잡한 애플리케이션에서는 브라우저를 렌더링하는 동안 브라우저를 잠그는 등의 문제가 발생할 수 있어서 쟁점이 되었습니다.\n\n## Fiber 조정자\n\n<div class=\"content-ad\"></div>\n\nReact Fiber는 렌더링 작업을 \"파이버(fibers)\"로 분할하여 점진적으로 처리할 수 있게 함으로써 우리가 논의하는 조정 유형에 새로운 시각을 제공합니다. 이를 통해 React는 렌더링 작업이 서로 다른 프레임에 고르게 분배되어 일시적으로 작동을 중지하고 나중에 다시 작동할 수 있게 됩니다. 이 접근 방식의 주요 이점은 전반적인 성능과 반응 속도 향상입니다.\n\n# React-Fiber 뒤에 숨겨진 주요 개념\n\n## 1. 점진적 렌더링\n\n우선순위 기반 업데이트: React Fiber는 모든 업데이트를 서로 다른 우선순위로 나눕니다. 사용자 상호작용과 같은 중요한 주의를 필요로 하는 업데이트는 즉시 처리되고, 데이터 가져오기와 같은 비교적 중요하지 않은 업데이트는 백그라운드에서 처리되어 시스템 반응성을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n## 2. 동시성\n\nReact Fiber은 멀티스레드이며 병렬 처리 또는 동시성을 지원합니다. 이는 응용 프로그램에서 상호 작용을 도입하는 다양한 하위 인터페이스가 있을 때 특히 유용합니다. 항상 순조로운 상호 작용 UI를 유지하기 위해 큐의 맨 위에 중요하게 처리해야 할 업데이트를 유지하면서 비교적 중요도가 낮은 작업에 대해 일정량의 진전을 보입니다.\n\n## 3. 트리 구조\n\nReact Fiber는 각 항목이 상태, 데이터 및 계층 위치와 함께 UI의 일부인 Fiber 트리를 사용합니다. 이 구조를 통해 React는 DOM의 부분을 업데이트하려는 필요한 변경 사항을 더 잘 이해하고 추적하여 전체 성능 및 렌더링 속도를 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n# 화해 과정\n\n![화해과정](/assets/img/2024-06-19-ReactFiberExplainedIncrementalRenderingforSmootherUIs_1.png)\n\n우리가 상태를 변경할 때, React는 메인 스레드가 비어있을 때까지 기다렸다가 작업 중인 트리(Work In Progress, WIP)를 구축하기 시작합니다. 이 WIP 트리는 fiber를 사용하여 구축되며, 그 구조는 코드 내의 컴포넌트를 반영합니다. 렌더링 또는 화해 단계에서는 WIP 트리를 구성하고 변경 사항을 식별하는 비동기 작업이 진행됩니다. 이 단계는 메인 스레드가 다른 작업을 처리해야 할 경우 중지될 수 있으며, 작업의 중요도에 따라 우선순위가 부여됩니다. 메인 스레드가 다시 비어있게 되면, 중단된 지점부터 WIP 트리를 계속 구축합니다.\n\n![화해과정](/assets/img/2024-06-19-ReactFiberExplainedIncrementalRenderingforSmootherUIs_2.png)\n\n<div class=\"content-ad\"></div>\n\n두 번째 단계인 커밋 단계는 전체 WIP 트리가 완료된 후 시작됩니다. 이 단계는 동기적으로 진행되며 중단될 수 없습니다. React는 이 단계에서 현재 트리와 진행 중인 작업 트리의 포인터를 교환하여 DOM에 변경 사항을 적용한 다음, 이러한 파이버를 DOM에 렌더링합니다. 교환 후, 새로운 진행 중인 작업 트리는 향후 상태 변경에 준비가 된 상태입니다.\n\n# 실제 세계 응용프로그램\n\n![이미지](/assets/img/2024-06-19-ReactFiberExplainedIncrementalRenderingforSmootherUIs_3.png)\n\nFacebook: 뉴스 피드 업데이트, 실시간 알림 및 실시간 채팅과 같은 기능.\n\n<div class=\"content-ad\"></div>\n\n인스타그램: 무한 스크롤링, 라이브 스토리, 실시간 댓글 등의 기능을 제공합니다.\n\nWhatsApp Web: 실시간 업데이트, 여러 채팅 창, 미디어 처리 등의 기능을 제공합니다.\n\n# 참고 자료\n\n이 참고 자료들이 React Fiber와 해당 아키텍처에 대한 이해를 돕는 데 도움이 되었습니다. 유용하게 활용하시기를 바랍니다.\n\n<div class=\"content-ad\"></div>\n\n# Andrew Clark: 리액트의 다음 단계 - ReactNext 2016\n\n- 리액트 파이버 아키텍처 - Github 저장소\n\n- SMOOSHCAST: 다니엘 아브라모프와 함께하는 리액트 파이버 심층 탐구\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-19-ReactFiberExplainedIncrementalRenderingforSmootherUIs_0.png"},"coverImage":"/assets/img/2024-06-19-ReactFiberExplainedIncrementalRenderingforSmootherUIs_0.png","tag":["Tech"],"readingTime":4},{"title":"통계 학습에 대한 소개 - 소개","description":"","date":"2024-06-19 23:44","slug":"2024-06-19-AnIntroductiontoStatisticalLearningIntroduction","content":"\n\n## \"An Introduction to Statistical Learning: with Applications in Python by Hastie et. al.\" 에서의 첫 번째 날 메모를 공유합니다. 이것은 제 데이터 과학 학습 문서의 일부입니다.\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_0.png)\n\n## 기계 (통계) 학습 (ML)은 데이터를 이해하는 데 사용되는 다양한 도구 모음을 가리킵니다.\n\n이것은 주로 다음과 같이 분류됩니다:\n\n<div class=\"content-ad\"></div>\n\n- 지도 학습: 하나 이상의 입력을 기반으로 출력을 예측하거나 추정하는 (통계적) 모델을 구축하는 것을 말합니다.\n- 비지도 학습: 데이터로부터 관계와 구조를 학습하는 시스템(모델 또는 알고리즘)을 구축하는 것으로, 입력은 있지만 지도 학습에서와 같이 지도 출력이 없습니다.\n\n지도 학습과 비지도 학습의 차이점은 출력 데이터(동의어: 종속 변수, 목표 변수 또는 결과로 \"y\"로 표시됨)의 가용성에 있습니다.\n\n먼저 필수 라이브러리를 가져와서 Python 코드를 준비해보겠습니다.\n\n```js\n# 라이브러리 가져오기\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ISLP import load_data # 책의 저자가 제공하는 ISLP\n```\n\n<div class=\"content-ad\"></div>\n\n기계 학습의 경우 몇 가지 예시가 있습니다:\n\n## 1. 임금 데이터: 지도 학습 — 회귀\n\n데이터를 가져와 봅시다.\n\n```js\n## ISLP에서 임금 데이터 가져오기\ndf_wage = load_data('Wage')\ndf_wage\n```\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_1.png)\n\n이 데이터는 미국 대서양 지역의 남성 집단의 임금과 관련된 요인을 조사하는 데 사용할 수 있습니다. 결과 변수는 임금이며 입력 변수는 데이터의 나머지 변수입니다.\n\n데이터에서 임금은 양적 값이므로 회귀 문제에 직면하고 있음을 의미합니다. 따라서 연속적 또는 양적 결과를 예측하는 작업에 참여하게 됩니다.\n\n만약 직원의 나이, 교육, 그리고 달력 연도와 임금 사이의 관련을 이해하고 싶다면, 각 입력 변수와 임금 간의 관계를 스캐터 플롯과 상자 그림을 사용하여 시각화하여 수행할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n# matplotlib 서브플롯 설정하기\nfig, ax = plt.subplots(1, 3, figsize=(15,4))\n\n# 그림 1\nsns.regplot(data=df_wage, x='age', y='wage', lowess=True, ax=ax[0], scatter_kws={'edgecolor': 'grey', 'facecolor': 'none', 'alpha': 0.5}, line_kws={'color': 'red'})\n\n# 그림 2\nsns.regplot(data=df_wage, x='year', y='wage', lowess=True, ax=ax[1], scatter_kws={'edgecolor': 'grey', 'facecolor': 'none', 'alpha': 0.5}, line_kws={'color': 'red'})\n\n# 그림 3\ntemp = {}\nfor i in df_wage['education'].unique():\n    temp[i] = df_wage[df_wage['education'] == i]['wage'].reset_index(drop=True)\ntemp = pd.DataFrame(temp)\ntemp.columns = temp.columns.str.extract(r'(\\d)')[0].astype(int)\ntemp = temp[[i for i in range(1, 6)]]\ntemp.plot(kind='box', ax=ax[2], color='black', xlabel='교육 수준', ylabel='급여')\n```\n\n<img src=\"/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_2.png\" />\n\n위 그래프를 통해 다음을 알 수 있습니다:\n\n- 연령이 증가함에 따라 급여가 증가하며, 약 40세까지 상승한 후 이후에는 약간 감소하는 경향이 있습니다.\n- 2003년부터 2009년 사이에 급여는 대략적인 선형으로 증가했지만, 데이터의 변동성에 비해 상승폭은 매우 작습니다.\n- 교육 수준이 높을수록 급여가 일반적으로 높습니다. 가장 낮은 교육 수준(1)을 가진 남성은 가장 높은 교육 수준(5)을 가진 사람보다 상당히 낮은 급여를 받는 경향이 있습니다. \n\n<div class=\"content-ad\"></div>\n\n물론 나이, 교육 및 연도를 개별적으로 사용하는 대신 이러한 요소를 결합하여 머신러닝 모델을 적합시키면 더 정확한 예측을 할 수도 있습니다. 이것은 나이, 교육 및 연도에 기반한 임금을 예측하는 머신러닝 모델을 적합하는 것으로 이루어질 수 있습니다.\n\n# 2. 주식 시장 데이터: 지도 학습 — 분류\n\n데이터를 가져와 봅시다.\n\n```js\n# ISLP에서 주식 시장 데이터 가져오기\ndf_smarket = load_data('Smarket')\ndf_smarket\n```\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_3.png)\n\n이 데이터는 2001년부터 2005년까지 5년 동안의 S&P 500 지수의 일일 움직임을 포함하고 있습니다. 이 경우, 우리는 양적 또는 범주적 결과, 즉 오늘의 주식 시장 방향을 예측하는 데 관련되어 있습니다. 이 유형의 문제는 분류 문제라고합니다. 시장이 어느 방향으로 움직일지 정확하게 예측할 수있는 모델은 매우 유용할 것입니다!\n\n마켓 방향이 Lag1, Lag2 및 Lag3의 S&P 백분율 변화에 따라 상승 또는 하락하는 패턴을 이해함으로써 데이터를 조금 이해해 봅시다. 다음과 같이 박스플롯을 생성하여 수행할 수 있습니다:\n\n```javascript\n# matplotlib 서브플롯 설정\nfig, ax = plt.subplots(1, 3, figsize=(14, 4))\n\n# 그림 1\ntemp = {}\nfor i in df_smarket['Direction'].unique():\n    temp[i] = df_smarket[df_smarket['Direction'] == i]['Lag1'].reset_index(drop=True)\ntemp = pd.DataFrame(temp)\ntemp.plot(kind='box', color='black', ax=ax[0], xlabel='Today\\'s Direction', ylabel='Percentage Change in S&P', title='Yesterday')\n\n# 그림 2\ntemp = {}\nfor i in df_smarket['Direction'].unique():\n    temp[i] = df_smarket[df_smarket['Direction'] == i]['Lag2'].reset_index(drop=True)\ntemp = pd.DataFrame(temp)\ntemp.plot(kind='box', color='black', ax=ax[1], xlabel='Today\\'s Direction', ylabel='Percentage Change in S&P', title='Two Days Previous')\n\n# 그림 3\ntemp = {}\nfor i in df_smarket['Direction'].unique():\n    temp[i] = df_smarket[df_smarket['Direction'] == i]['Lag3'].reset_index(drop=True)\ntemp = pd.DataFrame(temp)\ntemp.plot(kind='box', color='black', ax=ax[2], xlabel='Today\\'s Direction', ylabel='Percentage Change in S&P', title='Three Days Previous')\n\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_4.png)\n\n위의 그래프를 기반으로 하면, 오늘의 방향이 상승인지 하락인지에 따라 S&P의 백분율 변화에 대한 lag1(어제), lag2(이틀 전), lag3(사흘 전)은 어떤 차이도 보이지 않습니다. 이는 이 3가지 변수를 기반으로 시장이 어떻게 움직일지 예측하는 간단한 전략이 없음을 시사합니다. 만약 패턴이 매우 단순하다면, 누구나 시장에서 수익을 얻기 위한 간단한 거래 전략을 채택할 수 있을 것입니다. 대신, 이러한 유형의 문제는 머신 러닝 모델을 사용하여 오늘의 시장을 고정도로 예측하는 데 해결될 수 있습니다.\n\n# 3. 유전자 발현 데이터: 비지도 학습 — 차원 축소 및 클러스터링\n\n머신 러닝의 중요한 문제 유형 중 하나는 출력 변수가 없이 입력 변수만 관찰되는 상황, 즉 비지도 학습이라고 불리는 상황을 다루는 것입니다. 이전 예제와 달리 여기서는 출력 변수를 예측하려는 것이 아니라는 점이 다릅니다.\n\n<div class=\"content-ad\"></div>\n\n여기서 유전자 발현 데이터 예시를 확인해 봅시다:\n\n```js\n# 유전자 발현 데이터 가져오기\ndf_gen = load_data('NCI60')\ndf_gen\n```\n\n<img src=\"/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_5.png\" />\n\n이 데이터에는 64개의 암 세포 주석마다 6,830개의 유전자 발현 측정값이 포함되어 있습니다. 특정 출력 변수를 예측하는 대신, 유전자 발현 측정값을 기반으로 세포 주석들 사이에 그룹 또는 클러스터가 있는지를 결정하는 데 관심이 있습니다. 이것은 어려운 질문이며, 그 이유 중 하나는 각 세포 주석당 수천 개의 유전자 발현 측정값이 있어 데이터를 시각화하기 어렵게 만든다고 합니다.\n\n<div class=\"content-ad\"></div>\n\n여기서는 차원 축소와 클러스터링과 같은 비지도학습 기술을 사용하여 데이터의 패턴을 더 잘 이해할 수 있습니다.\n\n시작해 봅시다!\n\n```js\n# 첫 두 개요소에 대한 PCA\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\nZ = pca.fit_transform(df_gen['data'])\n\n# 예시로 4개 클러스터에 대한 K-Means 클러스터링\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(Z)\n\n# 결과 시각화\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\n# 그림 1\nax[0].scatter(x=Z[:,0], y=Z[:,1], edgecolors='black', facecolor='none')\nax[0].set_xlabel('Z1')\nax[0].set_ylabel('Z2')\n\n# 그림 2\nscatter = ax[1].scatter(x=Z[:,0], y=Z[:,1], c=kmeans.labels_)\nlegendc = ax[1].legend(*scatter.legend_elements(prop='colors'), loc=\"upper left\", title=\"Cluster\")\nax[1].set_xlabel('Z1')\nax[1].set_ylabel('Z2')\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n데이터의 처음 두 주성분을 사용하고 있습니다. 이 주성분은 각 세포주에 대해 측정된 6,830개의 발현 측정치를 두 숫자 또는 차원으로 요약합니다. 이 차원 축소로 인해 일부 정보 손실이 있을 수 있지만, 이제는 데이터를 시각적으로 클러스터링하는 것이 가능해졌습니다. 클러스터의 수를 결정하는 것은 종종 어려운 문제입니다. 위 그래프에서는 설명을 위해 4개의 클러스터를 사용하고 있습니다. 이 그래프를 토대로 유사한 특성을 가진 세포주가 이차원 표현에서 서로 가까이에 위치하는 것을 명확히 알 수 있습니다.\n\n계속됩니다...\n\n이것은 제 100일 데이터 과학 학습 여정 중 일부입니다. 더 많은 학습 업데이트를 위해 저를 팔로우해주세요.\n\n제가 학습한 내용에서 여러분도 배울 수 있습니다!\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 마크다운 형식으로 변경해보세요.\n\n<div class=\"content-ad\"></div>\n\n- 작가를 팔로우하고 박수를 50번 치세요 ️👏️️\n- 우리를 팔로우하세요: 뉴스레터\n- 당신도 구루가 되고 싶나요? 우리의 관객에게 도달하기 위해 가장 좋은 기사나 초고를 제출해 보세요.","ogImage":{"url":"/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_0.png"},"coverImage":"/assets/img/2024-06-19-AnIntroductiontoStatisticalLearningIntroduction_0.png","tag":["Tech"],"readingTime":7},{"title":"GitHub Actions를 통한 간단한 모델 재학습 자동화","description":"","date":"2024-06-19 23:41","slug":"2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions","content":"\n\n\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_0.png)\n\n비즈니스에 엄청난 가치를 창출할 수 있는 것은 기계 학습 모델입니다. 그러나 이를 개발하는 것은 일회성 활동이 아닙니다. 대신 모델이 계속 가치를 제공할 수 있도록 지속적인 프로세스여야 합니다. 이것이 MLOps가 나오게 된 이유입니다.\n\nCI/CD 원칙과 기계 학습 개발을 결합한 것을 MLOps라고 합니다. 이를 통해 모델이 지속적인 가치를 제공할 수 있도록 합니다.\n\n기계 학습 모델이 지속적인 이점을 제공하는 한 가지 방법은 필요할 때 재학습하는 것입니다. 예를 들어, 데이터 드리프트가 감지될 경우 모델을 재학습하는 것입니다. 모델 재학습 자동화를 위해 재학습 트리거의 환경을 설정하여 수행할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\nGitHub Actions는 GitHub에서 제공하는 기능으로, CI/CD 플랫폼에 사용되며 GitHub 저장소에서 소프트웨어 개발 프로세스를 자동화하는 데 사용됩니다.\n\n이 기사에서는 GitHub Actions를 사용하여 모델 재학습을 자동화하는 방법을 가르쳐 드립니다. 그 방법을 알아볼까요?\n\n# 준비\n\n이 프로젝트에서는 모델 개발 및 자동화 데모를 수행할 것입니다. 전체 프로젝트 구조는 아래 차트와 같을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_1.png)\n\n일단 GitHub Actions을 이 리포지토리에서 사용할 수 있도록 GitHub 리포지토리를 준비하는 것부터 시작해봅시다. 선호하는 이름으로 빈 리포지토리를 만들 수 있습니다. 저는 이 리포지토리를 만들었어요.\n\n추가로, Docker를 사용하여 모델을 배포하는 것을 시뮬레이션해볼 거예요. 이를 위해 Docker Desktop을 설치해봅시다. 그리고 Dockerhub에 가입하지 않은 경우에는 가입하세요.\n\n그런 다음, Repo 및 Workflow 범위를 지닌 GitHub 개인 액세스 토큰(PAT)을 생성해봅시다. 토큰을 어딘가에 보관하고, 방금 만든 빈 리포지토리로 돌아가봅시다. 설정으로 이동하여 \"비밀 값 및 변수\"를 선택합니다. 그런 다음, PAT, Docker 사용자 이름 및 Docker 비밀번호를 포함하는 리포지토리 비밀값을 생성하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_2.png)\n\nGitHub 저장소를 로컬 또는 작업하는 플랫폼에 복제하세요. 준비가 되었으면 튜토리얼 전체 구조를 준비해 봅시다. 좋아하는 IDE에서 다음과 같이 폴더를 생성하세요.\n\n```js\ndiabetes-project/\n├── data/\n├── notebooks/\n├── scripts/\n├── models/\n├── .github/\n│   └── workflows/\n``` \n\n폴더가 갖춰지면 가상 환경을 설정합니다. 고립된 환경을 원하기 때문에 이는 좋은 관행입니다. 루트 폴더로 이동하여 다음 CLI 코드를 사용하세요.\n\n\n<div class=\"content-ad\"></div>\n\n```js\npython -m venv your_environment_name\n```\n\n가상 환경을 활성화하려면 아래 코드를 실행하세요.\n\n```js\nyour_environment_name\\Scripts\\activate\n```\n\n가상 환경을 활성화한 후, 튜토리얼을 위해 필요한 모든 패키지를 설치할 것입니다. 루트 폴더에 requirements.txt 파일을 생성하고 아래 패키지를 채워넣어주세요.\n\n<div class=\"content-ad\"></div>\n\n```js\nfastapi\nuvicorn\npandas\nscikit-learn\nmatplotlib\nseaborn\nevidently\n```\n\n요구 사항이 준비되면 가상 환경에 패키지를 설치할 것입니다.\n\n```js\npip install -r requirements.txt\n```\n\n모든 준비가 완료되었으므로, 이제 모델을 개발하고 모델 재학습 자동화를 시작할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 모델 개발\n\n이 튜토리얼에서는 공개 도메인 데이터 세트인 Open-Source Diabetes 데이터 세트를 사용할 것입니다. 데이터 세트를 다운로드하여 Data 폴더에 넣어주세요. 저는 데이터 세트를 data.csv로 이름을 변경했지만, 원하시는 이름으로 변경하셔도 됩니다.\n\n우리는 주피터 노트북에서 초기 모델 개발을 진행할 것입니다. 노트북을 만들어서 notebooks 폴더에 넣으세요. 그런 다음, 데이터 세트를 읽어오는 것부터 시작해보겠습니다.\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata_path = '..//data//data.csv'\n\ndf = pd.read_csv(data_path)\n```\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 GitHub Actions 능력을 자동으로 재교육하는 데 초점을 맞추기로 했어요. 데이터 탐색 이외의 것에 초점을 맞출 거예요. 노트북에서 데이터 탐색 부분을 포함했으니, 확인하고 싶다면 방문해주세요.\n\n이제 데이터 전처리와 파이프라인 시작으로 넘어갈게요. 데이터 파이프라인을 사용하여 표준 개발 프로세스를 모방할 거예요.\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nnumeric_features = X.columns\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features)\n    ])\n```\n\n파이프라인이 준비되면 머신 러닝 모델로 랜덤 포레스트 알고리즘을 사용할 거예요. 다른 목적에 맞는 다른 모델을 선택할 수도 있어요.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\npipeline.fit(X_train, y_train)\r\n```\n\n모델을 평가하고 성능을 확인해봐요.\n\n```js\r\nfrom sklearn.metrics import classification_report\n\ny_pred = pipeline.predict(X_test)\n\n# 모델 평가\nreport = classification_report(y_test, y_pred)\nprint(report)\r\n```\n\n<img src=\"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n전반적으로 성능은 만족스러운 수준입니다. 더 나아질 여지는 있지만, 현재 모델을 유지하고 이를 모델 폴더에 저장하겠습니다.\n\n```js\nimport pickle\n\nwith open('..//models//pipeline.pkl', 'wb') as f:\n    pickle.dump(pipeline, f)\n```\n\n모델이 완성되면 우리는 프로덕션 환경에 배포할 것입니다. 이를 API로 배포하고 모델을 컨테이너화하기 위해 Docker를 사용할 것입니다.\n\n모델을 API로 배포하기 위해 app.py라는 파일을 생성하여 스크립트 폴더에 저장해 봅시다. 파일 내부에 다음 코드를 사용하여 모델을 API로 만들 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport pickle\nimport pandas as pd\n\napp = FastAPI()\n\ncolumns = ['임신횟수', '글루코스', '혈압', \n'피하지방', '인슐린', 'BMI', '당뇨위계보이지DNA', '나이']\n\ndict_res = {0: '당뇨가 아님', 1: '당뇨'}\n\npipeline_path = 'models/pipeline.pkl'\nwith open(pipeline_path, 'rb') as pipeline_file:\n    pipeline = pickle.load(pipeline_file)\n\nclass DataInput(BaseModel):\n    data: list\n\n@app.post(\"/predict\")\nasync def predict(input_data: DataInput):\n    try:\n        df = pd.DataFrame(input_data.data, columns=columns)\n        predictions = pipeline.predict(df)\n        results = [dict_res[pred] for pred in predictions]\n    \n        return {\"예측결과\": results}\n    \n    except Exception as e:\n        print(\"에러:\", str(e))\n        raise HTTPException(status_code=400, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n모델 API에 액세스할 수 있는지 테스트해 봅시다. 먼저, 응용 프로그램을 시작하려면 CLI에서 다음 코드를 실행합니다.\n\n```js\nuvicorn scripts.app:app --host 0.0.0.0 --port 8000\n```\n\n그런 다음, Jupyter Notebook에서 다음 코드를 실행하여 API를 테스트합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport requests\n\nurl = \"http://localhost:8000/predict\"\n\ndata = {\n    \"data\": [\n        [1, 85, 66, 29, 0, 26.6, 0.351, 31]\n    ]\n}\n\nresponse = requests.post(url, json=data)\nprint(response.json())\n```\n\nAPI에 전달하는 데이터의 위치가 훈련 데이터와 동일한지 확인해주세요. API가 잘 작동하면 Docker 이미지를 빌드하고 허브에 푸시할 것입니다.\n\n먼저 루트 폴더에 dockerfile을 생성해주세요. 해당 파일에 다음 코드를 채워넣어주세요.\n\n```js\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY scripts/app.py app.py\nCOPY models models\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n<div class=\"content-ad\"></div>\n\n위 코드에서는 Python 환경을 설정하고 API를 실행하는 데 필요한 파일을 컨테이너로 복사하여 포트 8000에서 수신 대기하는 방법을 안내합니다.\n\nDockerfile로 이미지를 빌드하는 방법은 준비가 되면 다음 코드를 사용하면 됩니다.\n\n```js\ndocker build -t username/image_name -f Dockerfile .\ndocker login -u username\ndocker push username/image_name:latest\n```\n\n위 코드에서 username을 귀하의 Dockerhub 사용자명으로, image_name을 선호하는 응용 프로그램 이름으로 변경해주세요. 성공한다면, 내 것과 같이 Dockerhub에 귀하의 이미지가 표시될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그래서 우리가 모델 API를 Docker에 넣고 Dockerhub에 푸시한 이유는 무엇인가요? 이것은 응용 프로그램을 실행할 모든 환경에서 일관성을 보장하기 때문입니다.\n\n또한 GitHub Actions가 다음 섹션에서 모델을 다시 학습하고 다시 이 컨테이너로 푸시하는 데 얼마나 강력한지를 보여줍니다. 따라서 우리는 모델을 배포하기 위해 이미지를 가져오기만 하면 됩니다.\n\n아래 코드를 실행하여 Dockerhub에서 이미지를 가져와 로컬 환경에서 실행해보세요.\n\n```js\ndocker login -u username\ndocker pull username/image_name:latest\ndocker run -d -p 8000:8000 username/image_name:latest\n```\n\n<div class=\"content-ad\"></div>\n\n지금까지 제품 모델이 운영 중입니다. 다음 부분에서는 GitHub Actions를 사용하여 모델을 특정 트리거로 다시 학습하는 방법을 살펴보겠습니다.\n\n# GitHub Actions를 활용한 모델 재학습\n\n제가 언급했듯이, 머신 러닝 모델은 지속적인 프로젝트입니다. 이를 통해 어떤 가치를 제공하려면 중요한데, 왜냐하면 모델이 항상 동일한 품질을 유지할 것으로 기대하기 어렵기 때문입니다. 특히, 드리프트가 발생하면 더욱 그렇습니다.\n\n이 튜토리얼에서는 운영 데이터셋에서 데이터 드리프트가 감지될 때 자동으로 모델 재학습을 수행하는 방법을 배우겠습니다. 먼저, 데이터셋에서 드리프트를 감지하는 방법을 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 코드를 사용하여 데이터셋에서 drift를 시뮬레이션해 보겠습니다.\n\n```js\nimport numpy as np\n\ndef introduce_drift(data, drift_features, drift_amount=0.1, random_seed=42):\n    np.random.seed(random_seed)\n    drifted_data = data.copy()\n    \n    for feature in drift_features:\n        if feature in data.columns:\n            drifted_data[feature] += np.random.normal(loc=0, scale=drift_amount, size=data.shape[0])\n    \n    return drifted_data\n    \nfeatures_to_drift = ['Glucose', 'BloodPressure', 'SkinThickness', 'Pregnancies']\n\ndrifted_data = introduce_drift(X_test, features_to_drift, drift_amount=50)\ndrifted_data = drifted_data.reset_index(drop=True)\n```\n\n위 코드에서는 Test 데이터의 일부 열을 drift했습니다. drift_amount를 조절하여 데이터가 얼마나 변하는지 제어할 수 있습니다.\n\n튜토리얼에는 학습 데이터(참조)와 drift 데이터(신규)가 필요합니다. 나중에 다시 학습할 때 사용할 타겟 열도 저장해두는 것이 좋습니다.\n\n<div class=\"content-ad\"></div>\n\n\nreference_data['Outcome'] = y_train.reset_index(drop=True)\r\ndrifted_data['Outcome'] = y_test.reset_index(drop=True)\r\n\r\ndrifted_data.to_csv('..//data//new_data.csv', index=False)\r\nreference_data.to_csv('..//data//reference_data.csv', index=False)\r\n\n\r\nEvidently(제가 Evidently와 어떤 제휴도 없습니다)를 사용하여 제품 데이터가 참조 데이터에 비해 드리프트했는지 확인할 수 있습니다. 다음 코드로 확인할 수 있습니다.\r\n\r\n```python\r\nfrom evidently.metric_preset import DataDriftPreset\r\nfrom evidently.report import Report\r\n\r\ndata_drift_report = Report(metrics=[DataDriftPreset()])\r\n\r\ndata_drift_report.run(current_data=drifted_data.drop('Outcome', axis=1), \r\nreference_data=reference_data.drop('Outcome', axis=1), column_mapping=None)\r\nreport_json = data_drift_report.as_dict()\r\ndrift_detected = report_json['metrics'][0]['result']['dataset_drift']\r\n```\r\n\r\n![이미지](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_4.png)\r\n\n\n<div class=\"content-ad\"></div>\n\n결과에는 나중에 재훈련 자동화를 위해 사용할 드리프트 데이터 세트가 나타납니다.\n\n드리프트 감지를 시뮬레이션하기 위해 drift_detection.py라는 파일을 만들어서 스크립트 폴더에 저장해보겠습니다. 아래 코드를 파일에 채워 넣어주세요.\n\n```js\nimport pandas as pd\nfrom evidently.metric_preset import DataDriftPreset\nfrom evidently.report import Report\n\nreference_data = pd.read_csv('data/reference_data.csv')\nnew_data = pd.read_csv('data/new_data.csv')\n\ndata_drift_report = Report(metrics=[\n    DataDriftPreset()\n])\n\ndata_drift_report.run(reference_data=reference_data.drop('Outcome', axis=1), \n                      current_data=new_data.drop('Outcome', axis=1), column_mapping=None)\n\nreport_json = data_drift_report.as_dict()\ndrift_detected = report_json['metrics'][0]['result']['dataset_drift']\n\nif drift_detected:\n    print(\"데이터 드리프트가 감지되었습니다. 모델을 재훈련합니다.\")\n    with open('drift_detected.txt', 'w') as f:\n        f.write('drift_detected')\nelse:\n    print(\"데이터 드리프트를 감지하지 못했습니다.\")\n    with open('drift_detected.txt', 'w') as f:\n        f.write('no_drift')\n```\n\n위 코드에서 우리는 드리프트 감지 여부를 drift_detected.txt 파일에 저장하고, 드리프트가 감지되었는지 여부에 따라 정보를 출력합니다. 드리프트가 감지된 경우, 모델을 재훈련하고 싶습니다. 이에 대비하여 훈련 스크립트를 준비해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n스크립트 폴더에 train_model.py라는 파일을 만들고 다음 코드로 채워주세요.\n\n```python\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport pickle\n\nreference_data = pd.read_csv('data/reference_data.csv')\nnew_data = pd.read_csv('data/new_data.csv')\n\ndf= pd.concat([reference_data, new_data], ignore_index=True)\n\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nnumeric_features = X.columns\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features)\n    ])\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\npipeline.fit(X, y)\n\nwith open('models/pipeline.pkl', 'wb') as f:\n    pickle.dump(pipeline, f)\n```\n\n위의 코드는 학습 및 드리프트 데이터를 새로운 학습 모델로 결합하고, 이를 사용하여 새 모델을 학습하는 것입니다. 이는 간단한 접근 방식일 뿐이며, 실제 세계의 학습 데이터는 더 많은 준비가 필요하고, 새 모델은 적절한 평가가 필요합니다.\n\n그러나 모든 스크립트가 준비되면, GitHub Actions를 통해 드리프트가 감지될 때 모델을 재학습할 수 있도록 준비할 것입니다. 재학습에 필요한 모든 구성을 포함하는 YAML 파일을 준비해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n그래서, .github\\workflows 폴더에 mlops_pipeline.yml 파일을 생성해봅시다. 폴더 이름이 제대로 되었는지 확인하세요; GitHub Actions은 적절한 이름이 필요합니다. 아래의 코드로 mlops_pipeline.yml을 채워넣어주세요.\n\n```js\nname: Diabetes Retraining Pipeline with Data Drift Detection\n\non:\n  push:\n    paths:\n      - 'data/new_data.csv'\npermissions:\n  contents: write\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v2 \n      with:\n        python-version: 3.9\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run data drift detection\n      run: |\n        python scripts/drift_detection.py\n      continue-on-error: true \n\n    - name: Check for data drift\n      id: check_drift\n      run: |\n        if grep -q 'drift_detected' drift_detected.txt; then\n          echo \"Data drift detected.\"\n          echo \"drift=true\" >> $GITHUB_ENV\n        else\n          echo \"No data drift detected.\"\n          echo \"drift=false\" >> $GITHUB_ENV\n        fi\n      shell: bash\n\n    - name: Model Retraining if Data Drift detected\n      if: env.drift == 'true'\n      run: |\n        python scripts/train_model.py\n\n    - name: Commit and push updated model\n      if: env.drift == 'true'\n      env:\n        GIT_COMMITTER_NAME: github-actions\n        GIT_COMMITTER_EMAIL: github-actions@github.com\n      run: |\n        git config --global user.name \"github-actions\"\n        git config --global user.email \"github-actions@github.com\"\n        git remote set-url origin https://x-access-token:${ secrets.ACTIONS_PAT }@github.com/username/image_name.git\n        git add models/pipeline.pkl\n        git commit -m \"Update model after retraining on $(date -u +'%Y-%m-%d %H:%M:%S UTC')\"\n        git push\n\n    - name: Build Docker image\n      if: env.drift == 'true'\n      run: |\n        docker build -t username/image_name -f dockerfile .\n\n    - name: Log in to Docker Hub\n      if: env.drift == 'true'\n      run: echo \"${ secrets.DOCKER_PASSWORD }\" | docker login -u \"${ secrets.DOCKER_USERNAME }\" --password-stdin\n\n    - name: Push Docker image to Docker Hub\n      if: env.drift == 'true'\n      run: |\n        docker push username/image_name:latest\n\n    - name: Notify about the process\n      run: |\n        if [[ \"$GITHUB_ENV\" == *\"drift=false\"* ]]; then\n          echo \"No data drift detected. No retraining necessary.\"\n        else\n          echo \"Data drift detected. Model retrained and deployed.\"\n        fi\n      shell: bash\n```\n\n위의 YAML에서 수행한 전체 설정 구조는 아래 이미지에 나와 있습니다.\n\n![Simple Model Retraining Automation via GitHub Actions](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_5.png)\n\n\n<div class=\"content-ad\"></div>\n\nGitHub Actions에서 사용하는 트리거는 data 폴더 안에 new_data.csv 파일이 푸시될 때입니다. 그러나 모델 재학습은 drift가 감지될 때만 실행됩니다. 모델을 다시 훈련한 후, 해당 모델을 GitHub 저장소와 Docker Hub에 다시 푸시할 것입니다.\n\n사용자명/image_name Docker 식별자를 꼭 자신의 것으로 변경해주세요. 동일한 식별자를 사용하는 경우 Repository Secrets를 생성할 수도 있습니다.\n\n모든 파일이 준비되면 GitHub 저장소에 푸시해야 합니다. 그런 다음 새로운 drift 데이터를 생성하여 new_data.csv로 저장하고, 해당 데이터를 저장소에 다시 푸시해보세요.\n\nGitHub 저장소의 Actions 탭으로 이동해주세요. 성공적으로 실행되었다면, 'Success' 상태를 가진 'build'라는 작업이 하나 표시될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_6.png)\n\n작업을 클릭하여 프로세스의 모든 세부 정보를 확인할 수 있습니다. 각 단계의 정보를 확인하여 프로세스를 이해하거나 실행에 실패했는지 확인할 수 있습니다.\n\n![image](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_7.png)\n\n저장소의 모델로 이동하면 모델이 업데이트되었는지 확인할 수 있습니다. 모델을 다시 학습했을 때 커밋 메시지를 사용하여 알림을 받습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_8.png)\n\n도커 허브 저장소를 확인해서 이미지가 업데이트되었는지도 확인할 수 있어요.\n\n여기까지면 GitHub Actions를 사용해서 모델 재교육 프로세스를 간단히 할 수 있어요. 여러분이 원하는대로 스크립트를 조정할 수 있어요. 예를 들어 트리거, 재교육 조건, 데이터셋 등을 말이에요.\n\n이 글에서 사용한 코드들이 필요하면, 해당 레포지토리에 푸시해 놓았어요.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 글에서는 GitHub Actions를 사용하여 모델 재학습 프로세스를 자동화하는 방법을 배웠습니다. YAML 파일을 통해 구성을 설정하고 트리거를 결정함으로써 GitHub Actions를 사용하여 필요한 모든 프로세스를 간편하게 처리할 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_0.png"},"coverImage":"/assets/img/2024-06-19-SimpleModelRetrainingAutomationviaGitHubActions_0.png","tag":["Tech"],"readingTime":16},{"title":"파이썬으로 하는 예상 향상 및 가우시안 프로세스 회귀 최적화에 대한 실습","description":"","date":"2024-06-19 23:38","slug":"2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython","content":"\n\n<img src=\"/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_0.png\" />\n\n저희 아내는 비즈니스 전공자인데요, \"인공지능이 뭐에요?\"라고 물어보면 이렇게 답합니다:\n\n그리고 이게 사람들이 인공지능을 설명하는 방식과 매우 일치한다고 생각해요. 조금 수정하면 진실과 많이 다르지 않을 거예요.\n\n<div class=\"content-ad\"></div>\n\n이제, 만약 수학자나 물리학자와 이야기를 나누면, 답변은 훨씬 더 기술적이고 아마 지루할 것입니다 (나는 물리학자이기 때문에 그렇게 말할 수 있어요). 만약 AI를 정의한다면 이렇게 말할 거예요:\n\n...내가 말했던 것처럼 좀 지루하죠. 무슨 말일까요?\n\n# 0. 기계 학습은 최소화 문제로\n\n우리가 가상 지능을 사용해 집값을 예측한다고 가정해봅시다. 가상 지능을 사용하기 전에는 실제로 여러 요소들(위치, 크기, 건축 연도 등)을 기반으로 집 값 평가를 하는 사람이 있습니다. 우리 모두가 알다시피, 가상 지능에는 데이터가 필요하죠. 그래서 트레이닝 세트를 만들기 위해 이 일을 수많은 번 반복해야 합니다.\n다음과 같은 내용이겠지요:\n\n# 0. 기계 학습은 최소화 문제로\n\n우리가 가상 지능을 사용해 집값을 예측한다고 가정해봅시다. 가상 지능을 사용하기 전에는 실제로 여러 요소들(위치, 크기, 건축 연도 등)을 기반으로 집 값 평가를 하는 사람이 있습니다. 우리 모두가 알다시피, 가상 지능에는 데이터가 필요하죠. 그래서 트레이닝 세트를 만들기 위해 이 일을 수많은 번 반복해야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n이미지 파일: /assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_1.png\n\n이 집 모음과 해당하는 가격은 머신러닝 모델을 최적화하는 데 사용됩니다. 예를 들어, 우리가 사는 매우 단순한 상상 속 세계에서 집 값은 오직 크기에만 의존한다고 가정해 봅시다. 예를 들어, 만약 크기가 x이고 해당하는 가격이 y라면 다음과 같습니다:\n\n이미지 파일: /assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_2.png\n\n따라서 크기가 x = 2인 집은 가격이 50x2 = 100(임의의 단위)인 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n이제 머신러닝 모델을 사용할 때 다음과 같이 가정해 봅시다:\n\n![이미지](/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_3.png)\n\n예를 들어, 크기가 x = 1인 새 집이 있다고 가정하면, \nw 대비 오류(Error) 플롯은 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_4.png)\n\n<div class=\"content-ad\"></div>\n\n당연히 이제 우리는 오차를 0으로 만들고 싶습니다. 집의 정확한 가격을 예측하는 데 우리는 머신러닝 모델을 가지고 싶습니다.\n\n실제로는 이 그림에서 보는 것과 같이 플롯이 없습니다. 실제로는 랜덤한 w(또는 랜덤한 벡터 w)를 선택하고 최소 오차를 찾기 위해 최적화합니다.\n\n머신러닝 애호가라면 이 이야기가 낯설지 않을 것입니다. 예를 들어, 그레디언트 디센트라는 것을 들어본 적이 있을 수 있습니다.\n\n# 1. 그레디언트 디센트와 그 한계\n\n<div class=\"content-ad\"></div>\n\nGradient descent은 함수의 오류를 반복적으로 최소화하는 기술로, 기울기의 반대 방향을 따라가면서 이루어집니다. 이런식으로 말이죠:\n\n![이미지는 생략합니다]\n\n수식으로 지루하게 하고 싶진 않지만, 알고리즘의 논리는 다음과 같습니다:\n\n- 랜덤 매개 변수 벡터로 시작합니다. 위의 경우에는 매개 변수가 하나뿐인데, 모델의 차원은 보통 훨씬 더 큽니다 (백만 또는 십억 차원 벡터)\n- 랜덤 매개 변수에 대한 손실 함수의 기울기를 추정합니다.\n- 기울기 방향으로 한 걸음 움직입니다 (위의 그림에서 보는 것처럼)\n- \"수렴에 도달\"하면 중단하고, 그렇지 않으면 단계 2와 3을 반복합니다.\n\n<div class=\"content-ad\"></div>\n\n그림에서 볼 수 있듯이, 기울기가 함수가 증가하는 곳을 알려주는 것이라면, \"기울기의 반대\"는 함수가 감소하는 곳을 알려줍니다: 손실 함수(최소화해야 하는 함수)의 감소 경로를 따르면 게임에 이기는 것이죠.\n\n하지만 모두가 그렇지는 않을까요? 그렇지 않거나, 적어도 항상은 아닙니다. 핵심은 4단계의 \"수렴에 도달할 경우\"라는 문장에 있습니다. \"수렴\"이란 무엇을 의미할까요? \"수렴\"이란 \"지역 수렴\"이나 원한다면 \"지역 최소값으로의 수렴\"을 의미합니다. 매우 혼란스럽죠. 알겠어요. 잠깐 인간처럼 이야기해 볼게요.\n\n위에서 보여드린 함수에 여러 가지 단순화가 있습니다. 단순화 중 하나는 1차원만 있다는 것입니다(이미 이야기했죠). 다른 큰(매우 큰) 단순화는 해당 함수에 최소값이 하나뿐이라는 것입니다. 많은(모든 것 같다고 말할게요) 기계 학습 사례에서는 여러 최솟값이 있는 손실 함수가 있고, 경사 하강 알고리즘을 실행하면 \"지역 최소값에 갇힌다\"는 것을 여기 비볼록 함수에서 볼 수 있습니다:\n\n<img src=\"/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n빨간 케이스에서는 알고리즘의 끝에 있는 파란 십자가가 실제로는 최소값이지만 지역 최소값입니다. 다시 말하면, 오차를 최소화하기 위해 노력하고 있지만 이를 부분적으로만 달성하고 있습니다: 문제의 전역 최소값을 찾지 못하고 있습니다.\n\n이것은 머신 러닝 커뮤니티에서 잘 알려진 문제이지만 사실 더 일반적인 문제입니다:\n\n이 문서에서는 예상 개선 방법을 사용합니다. 환영합니다! 🚂\n\n## 2. 가우시안 프로세스 회귀\n\n<div class=\"content-ad\"></div>\n\n실제 Global Minimum Search를 시작하기 전에 친구인 Gaussian Process Regression(GPR) 또는 Kriging을 알아야 합니다. 이름에서 알 수 있듯이 GPR은 회귀 알고리즘으로, 이 모델의 입력은 벡터이고 출력은 실수(필수적으로 정수일 필요는 없음)입니다.\n\n나는 Gaussian Process Regression을 너무 좋아해서 말을 멈추지 않으면 내 아내가 나를 떠나겠죠. 또한 Gaussian Process Regression을 이상치 탐지 알고리즘으로 사용하는 방법과 생성 모델로 사용하는 방법에 대해 기사를 쓰기도 했습니다.\n\n가우시안 프로세스는 매력적이어서 이에 관해 전문적인 연구를 할만한 충분한 자료가 있으나, Medium 기사를 통해 GPR의 아름다운 세계를 소개하려면 GPR을 기계학습 커뮤니티에서 유명하게 한 Christopher K. I. Williams와 Carl Edward Rasmussen의 말을 사용할 것입니다:\n\n정말 엄청 복잡하잖아요? 간단하게 만들어 볼게요. 내가 이전에 말한 것을 기억하나요? 입력이 벡터이고 출력이 실수인 것에 대해요? 출력이 실수뿐만 아니라 평균 값(평균)과 분산이라고 상상해봐요.\n\n<div class=\"content-ad\"></div>\n\n입력 데이터(학습 데이터) 세트는 확률의 조건부 조건으로 사용됩니다. 이것은 𝜇(𝑥), 𝐶(𝑥,𝑥′) 및 학습 데이터로부터 시작하여 다음 점에 대한 y의 예측을 제공합니다.\n\n예를 통해 이해해 보겠습니다.\n\n## 2.1 가우시안 프로세스 회귀 구현\n\n다음에 대한 매우 간단한 이 방정식부터 시작합시다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_7.png)\n\n우리가 Python과 일부 라이브러리 친구들을 사용하여 그래프를 그리면 이렇게 됩니다:\n\n말씀드리는 대로, 확률 과정이 데이터를 평균 함수와 위에서 설명한 공분산 함수로 모델링한다고 가정할 수 있습니다. 아름다운 이야기입니다. 이제 새 데이터로 사전 확률을 업데이트할 것입니다. 지금 제가 철학을 하고 있는 것처럼 들릴 수 있지만, 실제로 말하고 싶은 것은 사전 확률(위에서 정의한 평균과 공분산)과 훈련 세트 사이의 조건부 확률을 작성하는 것입니다. 저와 같은 수학 좋아하는 사람들에게는 이것이 아이디어입니다:\n\n![image](/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_8.png)\n\n\n<div class=\"content-ad\"></div>\n\nf_'x*'가 모든 가능한 결과이며, k∗는 테스트 포인트와 트레이닝 포인트 간의 공분산 벡터이며, 요소는 𝑘∗(𝑖)=𝑘(𝑥𝑖,𝑥∗)이며, K는 공분산 행렬이며, \\sigma_'n'²는 잡음 값입니다. 요약하자면, 언제나 해야 할 것처럼, 훈련 세트와 테스트 세트가 필요합니다. 이를 수행하는 데는 몇 줄만 있으면 됩니다:\n\n그리고 이것이 플롯입니다:\n\n이제 이 함수를 최소화하려고 한다고 가정해 봅시다. 해당 평균과 분산을 얻기 위해 방정식 [2]를 적용합니다. 이 작업은 아주 몇 줄로 이루어집니다:\n\n이 그림에서:\n\n<div class=\"content-ad\"></div>\n\n- 오렌지색 함수는 대상입니다: 우리가 모델링하려는 알 수 없는 함수입니다 (실제로는 알 수 없음)\n- 짙은 파란색 선과 음영은 평균과 2배의 표준 편차이며, 약 95%의 불확실성을 나타냅니다. 이는 오렌지색 선이 파란 음영 사이에 위치할 것이라는 것을 의미합니다.\n- 훈련 세트는 빨간 십자가 세트입니다. 불확실성은 항상 빨간 십자가에서 0입니다.\n- 테스트 세트는 파란 십자가 세트입니다.\n\n## 3. 전역 최적화\n\n좋아요, 하지만 최적화 문제를 약속했어요. 그게 어디에 있나요?\n\n이전 그림의 예측을 살펴보면, 예측된 최솟값이 있지만 지역 최솟값이며 올바른 전역 최솟값이 아닙니다. 이는 파란색 선의 최솟값이 2에서 4 사이에 위치하기 때문입니다. 오렌지색 선의 최솟값은 0에서 2 사이에 있습니다. 어떻게 찾을까요?\n\n<div class=\"content-ad\"></div>\n\n여기 요령이 있어요: 가운데 큰 불확실성(큰 음영 영역)을 보세요. 이것은 해당 영역에서 무슨 일이 일어날지 전혀 모른다는 것을 의미합니다. 불확실성이 너무 커서 무엇이 발생하는지 알아보아야 해요! 우리는 추정을 개선할 수 있는 영역이 어디인지 알려주는 양을 발견해야 해요. 이를 '예상 향상'이라고 부릅니다.\n\n수학적 내용을 지루하게 설명하고 싶지 않아요(어쨌든 매우 간단해요)만 예상 향상은 다음과 같은 양으로 계산돼요:\n\n그림을 그려볼게요:\n\n초록 선을 보고 이를 위에 있는 GPR 그래프와 비교한다면, 매우 큰 불확실 영역이 있는데 경계가 매우 크고 따라서 예상 향상이 크다는 것을 알 수 있어요. 와! 그것이 우리가 필요한 것이에요.\n\n<div class=\"content-ad\"></div>\n\n가장 큰 향상이 예상되는 영역에 1점을 추가할 예정이에요. 최소값의 변화가 더 이상 크게 없을 때까지 계속 이를 반복합니다. 9회 반복 예제를 보여드릴게요:\n\n사용할 이 블랙 박스 함수를 정의해볼게요:\n\n지금부터 블로그 게시물 아이디어는 아래 코드 라인에 있어요:\n\n![image](/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_9.png)\n\n<div class=\"content-ad\"></div>\n\n결과를 분석해 보겠습니다. 빨간 점은 GPR 예측을 학습하기 위해 사용된 이전 점으로, 파란색 선은 예측을 나타내며 파란색 음영은 불확실성 경계를 보여줍니다. 실제로는 모르는 검은 상자 함수는 주황색입니다. 다음 점은 초록색 점입니다.\n\n첫 번째 그림은 새로운 점을 지역 최소점에 추가하는 것을 보여줍니다.\n두 번째 그림은 도메인의 오른쪽 부분을 탐험하는 것을 보여줍니다 (거의 탐험되지 않은 부분).\n세 번째 그림은 알고리즘이 공간을 계속 탐색하여 지역 최소점을 찾는 것을 보여줍니다.\n네 번째 그림은 도메인의 왼쪽 부분을 탐색하는 것을 보여줍니다.\n다섯 번째 그림부터 아홉 번째 그림까지 알고리즘이 0과 2 사이의 최소값을 찾는 과정을 볼 수 있습니다. 이것이 우리의 전역 최소값입니다. 성공했습니다!\n\n# 4. 결론\n\n이 블로그 포스트에서는 글로벌 최적화 문제에 대해 논의했습니다. 다음 단계를 따라 수행했습니다:\n\n<div class=\"content-ad\"></div>\n\n- 저희는 머신 러닝을 최적화 문제로 정의했어요. 특히, 주택 가격에 대한 선형 회귀의 매우 단순한 예제로, ML 알고리즘을 오차 함수의 최소화로 형식화할 수 있는 방법을 보여드렸어요.\n- 저희는 경사 하강 알고리즘을 설명했어요. 경사 하강은 오차 함수를 수치적으로 최소화하는 기술이에요. 안타깝게도, 이 방법은 알고리즘의 성능이 저하될 수 있어요, 지역 (전역이 아닌) 최솟값에 갇힐 수 있기 때문이죠.\n- 저희는 GPR을 소개했어요. 이는 결과를 예측하는 것뿐만 아니라 불확실성을 측정하는 멋진 방법이에요. 아주 멋져요. 만점 받았네요. :)\n- 저희는 Expected Improvement (EI)을 정의했어요. 이는 우리가 가장 불확실한 지역을 탐색하도록 안내하는데 도움이 돼요. 결국, 이는 가능한 지역 최솟값을 갖는 영역을 탐색하도록 도와 전역 최솟값을 찾을 확률을 높여줘요.\n- 저희는 전역 최솟값 탐색 알고리즘을 수행했어요. 파이썬을 사용해 함수의 최솟값을 시각화와 함께 단계별로 찾는 방법을 보여드렸어요.\n\n# 7. 나에 대해!\n\n다시 한 번 시간 내어 주셔서 감사합니다. 정말로 소중해요 ❤\n\n제 이름은 Piero Paialunga이고, 이 사진 속에 제가 있어요:\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_10.png)\n\n안녕하세요! 저는 신시내티 대학교 항공우주공학부의 박사 후보이자 Gen Nine의 기계 학습 엔지니어입니다. 제 블로그 게시물과 LinkedIn에서 AI 및 기계 학습에 대해 이야기합니다. 만약 이 글을 좋아하시고 기계 학습에 대해 더 알고 싶으시다면:\n\nA. 제 LinkedIn을 팔로우해주세요. 거기서 제 이야기를 모두 공유합니다.\nB. 제 뉴스레터를 구독하세요. 새로운 이야기에 대한 최신 소식을 전해드리고 문의사항이나 의문을 보내주시면 모두 답변해드립니다.\nC. 추천 회원이 되어주세요. 그러면 매월 \"최대 이야기 수\"라는 제한이 없어지며, 전 세계의 기계 학습 및 데이터 과학 최고 작가들이 새로운 기술에 대해 쓴 글을 자유롭게 읽으실 수 있습니다.\nD. 저와 함께 일하고 싶으신가요? Upwork에서 내 요율과 프로젝트를 확인해보세요!\n\n질문이나 협업을 원하시면 메시지를 남겨 주시거나 LinkedIn에서 연락해주세요. 부담없이 문의 주시길 바랍니다!\n\n<div class=\"content-ad\"></div>\n\n피에로 파이아룽가님, 전자 메일 주소를 제공해 주셔서 감사합니다. 호환성을 위해 표 태그를 마크다운 형식으로 변경해 드리겠습니다. 만약 추가로 도움이 필요하시면 언제든지 문의해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_0.png"},"coverImage":"/assets/img/2024-06-19-HandsOnOptimizationwithExpectedImprovementandGaussianProcessRegressioninPython_0.png","tag":["Tech"],"readingTime":9},{"title":"Django Rest FrameworkDFR를 사용하여 Mpesa STK 푸시 및 CallbackView 구현하기","description":"","date":"2024-06-19 23:36","slug":"2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR","content":"\n\n![이미지](/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_0.png)\n\nMpesa와 Safaricom은 네트워킹 분야에서 경쟁사들을 능가하는 뛰어난 위치에 있습니다. Mpesa의 주요 목표는 사용자들이 편리하게 거래할 방법을 마련하는 것이었습니다. 기술의 발전으로, 사용자들이 금전 거래를 수행해야 하는 많은 웹 및 모바일 애플리케이션 내에서 사용될 API를 만들 필요가 있었습니다.\n\nMPESA STK PUSH\n\n대부분의 사람들은 대부분 시간에 핸드폰을 가지고 있기 때문에 핸드폰으로 결제하는 것이 합리적일 것입니다. Mpesa 팀은 라이브 및 샌드박스 API를 소개했습니다. mpesa stk push를 사용하려면 액세스 토큰을 생성해야 합니다. 액세스 토큰은 응용 프로그램이 mpesa api를 사용할 수 있는 시간대를 제공하는 코드입니다. 액세스 토큰 생성을 구현하려면 Daraja 계정이 필요합니다. 그런 다음 홈 뷰의 내 애플리케이션 섹션으로 이동하여 앱을 만듭니다. 앱이 생성되면 다음이 생성됩니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_1.png\" />\n\n여기에 필요한 모든 세부 정보를 제공합니다. 각 앱은 특정 조직을 위해 구축되었기 때문에 거기에 나와 있는 일부 자격 증명은 실제로 필요하지 않을 수 있습니다.\n\n이제 여러분의 views 파일에 액세스 생성 기능을 구현합니다. 참고로, 이 가이드는 여러분이 django와 그 구조를 충분히 숙지하고 있다고 가정합니다. 그러나 보안을 위해, 'pip install python-decouple'을 사용하여 python decouple을 설치하는 것을 강력히 권장합니다. 이는 매개변수를 재배포하지 않고 변경할 수 있도록 설정을 구성하는 데 도움이 됩니다. 다음은 django에서 해당 기능을 구현하는 방법입니다:\n\n<img src=\"/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n실제로 사용자 키와 시크릿은 보안상의 이유로 소스에 노출되어서는 안 되지만 별도의 .env 파일에 넣어야 합니다. 이는 구성에서 사용자 키와 시크릿을 검색하여 콜론 구분자로 연결한 후 결과를 Base64로 인코딩하고 인코딩된 값으로 HTTP 인증 헤더를 구성합니다. 그런 다음 지정된 API URL로 Authorization 헤더 및 JSON Content-Type 헤더와 함께 GET 요청을 보냅니다. JSON 형식으로 예상되는 API 응답은 \"access_token\" 값을 추출하여 함수에서 반환됩니다. 액세스 토큰은 mpesa stk push 함수에서 사용될 것입니다.\n\n![image](/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_3.png)\n\n알겠어요, 앞에서 언급한 대로 Safaricom에서 생성한 키는 기밀이며 소스 코드에 노출되어서는 안 됩니다. 함수에서 모든 거래에 타임 스탬프를 첨부하기 위해 날짜 형식이 필요하며 위에서 보여준 형식을 따라야 합니다. 그런 다음 위 함수로부터 받은 액세스 토큰을 사용합니다. 그런 다음 mpesa 비밀번호가 필요하며 이 비밀번호는 mpesa 숏코드, mpesa 패스키, 그리고 형식 지정된 날짜를 문자열로 변환하여 비밀번호를 인코딩하는 공식이 필요합니다. 그런 다음 수신한 데이터를 사용하는 페이로드를 구성하고 이 경우에는 전화번호와 금액입니다. 그런 다음 거래가 성공적인지 여부에 대한 JSON 응답을 받습니다.\n\nMPESA CALLBACK VIEW\n\n<div class=\"content-ad\"></div>\n\nmpesa stk 페이로드에서는 모든 stk 푸시 트랜잭션 결과가 전송되는 안전한 콜백URL을 포함해야 합니다. 이를 통해 트랜잭션을 추적할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_4.png)\n\n동그라미가 그려진 선은 생성된 모델을 나타냅니다. 모든 거래, 실패한 것도 포함하여 mpesa에서 콜백 URL로 수신된 모든 거래가 거기에 저장됩니다. mpesa에서 수신한 본문은 daraja 웹사이트에서 찾을 수 있는 구조가 있는 딕셔너리입니다. 상태 코드가 200인 응답은 성공적으로 간주되어 SuccessfulResponses 모델에 저장됩니다. POST 메서드를 사용하여 콜백 URL이 데이터를 저장하는 기능에 액세스할 수 있으며, GET 메서드는 모델의 모든 트랜잭션을 볼 수 있는 용도로 사용됩니다. 이 경우 직렬화기를 사용하여 파이썬 객체를 JSON으로 변환합니다. 모든 변환 복잡성은 정의해야 하는 직렬화기에 의해 처리됩니다. 다음은 예시입니다:\n\n![이미지](/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_5.png)\n\n<div class=\"content-ad\"></div>\n\n이 시리얼라이저는 데이터베이스의 AllResponses 모델의 모든 레코드를 변환합니다.\n\n요약하자면, 이것은 stk push를 통한 mpesa 거래의 전체 흐름입니다. 데이터의 전체 흐름을 설명할 때 비 기술용어를 사용하지 않으려 노력했지만, 이는 기본적인 장고 패러다임을 전제로 한 것입니다.\n\n즐겁게 코딩하세요.","ogImage":{"url":"/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_0.png"},"coverImage":"/assets/img/2024-06-19-ImplementingMpesaSTKpushandCallbackViewusingDjangoRestFrameworkDFR_0.png","tag":["Tech"],"readingTime":3},{"title":"파이썬을 활용한 의료 시설 위치 최적화를 위한 오픈 데이터 주도 방법","description":"","date":"2024-06-19 23:33","slug":"2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython","content":"\n\n![이미지](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_0.png)\n\n이 작업은 조아킴 그로미초 교수님과 카이 카이저와 공동 저술되었습니다. 저자(들)는 모든 오류와 누락에 대해 책임이 있습니다.\n\n건강 시설까지의 이동 시간을 정확하게 산출하는 것은 건강 서비스 접근성을 평가하는 데 기본적입니다, 특히 접근 장애가 공공 건강 결과에 중대한 영향을 미칠 수 있는 지역에서. 이러한 계산은 자원 배분, 건강 서비스 이용, 공평한 의료 접근, 미래 시설을 위한 전략적 계획에 필수적입니다. 그러나 이를 계산하기 위해서는 병원 위치, 인구 분포, 그리고 OpenStreetMaps나 구글 또는 맵박스 같은 API로부터 도로 네트워크 데이터를 기반으로 한 이동 시간 계산을 포함한 많은 데이터 처리가 필요합니다.\n\n지리적 변동성인 지형, 도로 상태 및 기후 등도 이동 시간 계산에 기여합니다. 또한 사용 가능한 교통 수단 및 종류도 건강 시설로의 접근을 제한하며, 많은 시골 지역에서 신뢰할 수 있는 대중교통이나 개인 교통 수단 옵션이 부족합니다. 게다가 모든 병원의 지오코딩 된 데이터의 정확성과 가용성은 개발도상국을 포함한 많은 국가에서 종종 부족하여, 접근에 대한 정확한 추정이 줄어듭니다.\n\n<div class=\"content-ad\"></div>\n\n# 방법론 및 사용된 데이터\n\n먼저 우리는 우리가 관심을 갖는 지역의 모양 파일(동락레스테의 한 지방인 바우카우 중 하나)을 인도적 데이터 교환소(HDX)에서 다운로드합니다. HDX는 국가별 경계를 포괄하는 표준화된 자료인 Global Database of Political Administrative Boundaries Database 에 액세스할 수 있습니다. 이 데이터는 Open Database License (ODC-ODbL)에 따라 이용할 수 있습니다.\n\n우리는 Meta에서 고해상도 인구 데이터와 이를 결합합니다. 이 데이터는 Creative Commons Attribution International에 따라 라이선스가 부여됩니다.\n\n접근성을 향상시키는 방법을 결정하기 위해 기존 의료 시설 위치(병원과 진료소)로 시작해야 합니다. 이 데이터의 오픈소스 저장소는 OpenStreetMap입니다. 이것은 출발하기에 좋은 곳이지만, 정부나 세계 보건 기구(World Health Organization)와 같은 국제 발전 기구들이 유지하는 공식 의료 시설 등록 정보만큼 포괄적이지 않을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n다음으로 Openrouteservice API와 MapBox Isochrones API를 사용하여 이동 시간을 계산하고 의료 접근성을 평가합니다. Openrouteservice에서 얻은 API 결과는 어떤 맥락에서든 CC-BY 4.0으로 라이선스가 부여됩니다. Isochrone API의 이용 약관에 대한 자세한 내용은 Mapbox 제품 약관을 참조하십시오.\n\n위 정보를 활용하여 기존 시설의 캐치먼트 영역을 분석하여, 의료 보장 범위에 대한 상세한 시각화를 상호작용적인 지도로 만들어 인구의 접근 가능 여부를 식별할 수 있습니다.\n\n마지막으로 최적화 모델을 실행하여 새로운 의료 시설의 잠재적 장소를 식별합니다.\n\n# 인도네시아의 티모르-레슈테의 행정 구역을 인도네시아 지도 데이터 교환에서 추출하고 Python의 Folium을 사용하여 시각화하기\n\n<div class=\"content-ad\"></div>\n\n다음 코드 스니펫은 GADM (Global Administrative Areas) 데이터에 대한 다운로더를 초기화하며 버전 4.0을 지정합니다. 그런 다음 이 다운로더를 사용하여 티모르-레스테의 관리 경계 데이터를 검색하며 지구상의 첫 번째 관리 수준인 지구나 주를 중점적으로 다룹니다. 지리 공간 데이터를 획득한 후에는 이 스크립트가 Folium 맵 상에 이 경계를 시각화하고, 배경 지도로 OpenStreetMap을 사용합니다.\n\n![이미지 1](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_1.png)\n\n![이미지 2](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_2.png)\n\n![이미지 3](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_3.png)\n\n<div class=\"content-ad\"></div>\n\n# 메타로부터 고해상도 인구 밀도 지도 다운로드 및 시각화하기\n\n이 섹션에서는 메타로부터 고해상도 인구 밀도 지도를 다운로드하고 시각화하는 방법에 대해 설명합니다. 이러한 지도는 30m 해상도에서 인구 추정치를 제공하며, 다양한 그룹을 포함하는 인구 통계를 다룹니다. 또한 160개 이상의 국가에 대해 공개적으로 제공됩니다. 이러한 지도는 인구 성장을 모델링하고 인구 조사 데이터를 사용하여 건물을 감지하고, 건물 밀도를 계산하며, 해당 밀도를 기반으로 타일에 걸쳐 인구 데이터를 분배하여 작성됩니다.\n\n다음의 Python 코드 스니펫에서는 `TLS` ISO 코드로 식별되는 티모르-레스테(Timor-Leste)의 2020년 Facebook 인구 데이터를 검색하는 함수 `fb_pop_data`가 정의되어 있습니다. 데이터가 다운로드되어 GeoDataFrame으로 처리되고, 선택된 행정 경계 범위의 좌표 참조 시스템과 일치시킵니다. 관심 영역(선택된 행정 지역) 내의 인구를 계산하고 표시합니다.\n\n![이미지](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_4.png)\n\n<div class=\"content-ad\"></div>\n\n# 오픈스트릿맵 데이터를 활용하여 티모르-레스테의 의료 시설 매핑\n\n이 코드 세그먼트는 특히 병원과 의원을 위한 티모르-레스테의 의료 시설 데이터를 검색하고 분석하는 데 전념되어 있습니다. 다음과 같은 기능을 수행합니다:\n\n- Overpass API를 통해 티모르-레스테 전역의 병원 및 의원 위치에 대한 OpenStreetMap 쿼리를 수행합니다.\n- 병원 (df_hospitals) 및 의원 (df_clinics)용으로 이름, 좌표 및 편의시설과 같은 필수 데이터를 추출하여 별도의 DataFrame에 저장합니다.\n\n이러한 데이터 세트를 공간 분석을 위해 하나의 GeoDataFrame(df_health_osm)으로 병합합니다.\n\n<div class=\"content-ad\"></div>\n\n- 티모르-레스테에서 의료 시설 수를 결정하기 위해 지역 조인을 실행하여 지정된 관심 지역(AOI) 내의 의료 시설 수를 파악합니다. 이는 티모르-레스테에서 의료 접근성을 평가하는 데 유용한 통찰력을 제공합니다.\n\n![이미지](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_5.png)\n\n# 이소크론 분석을 통한 의료 접근성 평가\n\nget_isochrone_osm 함수는 티모르-레스테의 의료 시설들에 대한 이소크론을 계산합니다. 이소크론은 특정 위치(일반적으로 병원)로부터의 지정된 시간 내에 도달 가능한 영역을 나타내는 다각형입니다. 이 함수는 OpenRouteService API를 사용하여 이러한 다각형을 생성하며, 이동 모드로 걷기를 선택하여 60분 여행 시간을 기준으로 합니다. 그 결과 다각형은 각 시설에 접근 가능한 인구를 결정하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n의료 접근성을 평가하는 중요한 측면 중 하나는 기본 데이터 소스를 기반으로 한 분석의 신뢰성과 민감도를 이해하는 것입니다. 이전 단계에서는 오픈스트리트맵(OpenStreetMap, OSM)의 도로 네트워크 데이터를 사용하여 의료 시설까지의 이동 시간을 계산합니다. 그러나 이 데이터는 품질과 완성도에서 상당한 변동성을 보일 수 있으며, 이는 접근성에 대한 이속론 지도의 정확도 및 결론에 영향을 줄 수 있습니다. 따라서 이러한 불확실성을 해결하기 위해 대안 데이터 소스를 활용한 민감도 분석을 수행하는 것이 좋습니다.\n\nOSM 데이터가 신뢰성이 떨어지거나 오래되어 있는 지역에서는 Mapbox나 Google과 같은 API를 활용하여 보다 최신이거나 완벽한 도로 네트워크 정보를 기반으로 한 접근성의 정확한 계산이 가능합니다. 자원이 허용된다면 이러한 API를 활용하여 OSM 데이터에서의 결과를 보완하거나 검증할 수 있습니다.\n\n예를 들어, Mapbox를 통해 이속론 분석을 수행해보겠습니다. 이 예시는 Mapbox의 이속론 API를 사용하여 기존 병원에서 걸어서 60분 이내에 접근 가능한 지역을 계산하는 방법을 보여줍니다.\n\n아래 코드를 통해 상기 결과를 시각화할 수 있습니다. 이는 주황색 GeoJson 객체로 행정 경계를 개요로 나타내고, 각각의 팝업에 병원 이름을 표시하는 파란핀으로 병원 위치를 표시합니다. 또한, 의료 서비스에 접근할 수 있는 인구를 기준으로 구분하여, 접근하지 못하는 사람들은 빨간색 원 마커로, 접근할 수 있는 사람들은 초록색으로 나타냅니다. 이러한 마커들의 투명도는 인구수에 의해 결정되어, 밀집된 지역과 잘 제공되는 지역과 듬성 듬성한 인프라와 서비스가 부족한 지역을 명확하게 시각적으로 구분하여 의료 접근성의 격차를 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_6.png)\n\n## 행정 경계의 대표적인 잠재 위치 그리드\n\n새로운 병원과 클리닉을 배치하는 것은 포괄적인 건강 관리 지원을 보장하기 위해 중요합니다. 이는 종종 공식 소스로부터 특정 사이트 권장이 없는 경우에는 대상 지역 내에서 대표적인 그리드를 통해 근사화할 수 있는 잠재적 위치를 분석하는 것을 포함합니다.\n\n이러한 그리드는 새로운 건강 시설을 설립할 위치를 고려하는 시작 프레임워크를 제공하여 언더서빈 인구에 대한 접근성을 극대화하기 위한 목적으로 사용됩니다. 이 코드 조각은 특정 지리적 영역 내에서 이 대표적인 그리드를 생성하는 데 사용되는 generate_grid_in_polygon Python 함수를 설명합니다.\n\n\n<div class=\"content-ad\"></div>\n\n해당 기능은 제공된 지오메트리 범위 전체에 대해 간격 매개변수에 의해 결정된 일렬로 배치된 점들의 시리즈를 생성하여 Baucau에 318개의 잠재적 위치를 얻을 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_7.png)\n\n잠재적인 의료 시설의 배치를 최적화하기 위해 우리의 분석은 각 제안된 위치에 대한 이소크로너(isochrone)를 계산하여 기존 시설에 적용된 60분 도보 매개변수를 동일하게 활용합니다. 이 데이터를 활용하여 접근이 제한된 인구 집단을 결정하고 이러한 잠재적 시설이 제공할 수 있는 인구 집단을 확인합니다.\n\n그런 다음 기존 및 잠재적 위치에서의 접근 데이터를 집계하여 모든 잠재적 위치가 개설되고 기존 시설과 함께 개설된다면 가능한 최대 접근을 계산합니다.\n\n<div class=\"content-ad\"></div>\n\n# 수학적 최적화\n\n이제 수학적 최적화를 사용하여 오픈할 최적 병원 하위 집합을 결정할 것입니다. 수학적 최적화에 익숙하지 않은 경우, 이 주피터 책에 있는 실습 소개로 시작하는 것을 권장합니다.\n\n수학적 최적화의 핵심은 우리가 최적화하려는 현실 세계의 디지털 쌍둥이 역할을 하는 수학 모델을 만드는 것입니다. 이러한 모델을 개발한 후 해당 모델에 관련 데이터를 입력하여 최적화 문제의 구체적 예제를 만듭니다. 이러한 예제는 적절한 솔버를 사용하여 해결되어 최상의 실현 가능한 해를 발견하게 됩니다. 이를 최적해라고 합니다.\n\n모델링은 개념적인 과정이며, 모델을 코딩하는 것은 기술적인 작업입니다. 저희는 주피터 책에서와 같이 pyomo 패키지를 사용합니다. 이후 논의되는 모델은 캠브리지 대학 출판사로부터 예정된 교재의 3.1 연습문제로 소개되며, 위에 언급된 주피터 책은 이 책의 온라인 동반자로 제공됩니다.\n\n<div class=\"content-ad\"></div>\n\n강력한 솔버를 활용하여 문제를 해결할 수 있습니다. 우리가 설명할 문제와 같은 경우, Gurobi는 뛰어난 상용 솔버이며, MIT 라이선스 하에 제공되는 우수한 오픈 소스 대안인 HiGHS가 있습니다.\n\n모델링 프로세스는 결정해야 할 사항을 식별하여 결정 변수를 정의하는 것으로 시작합니다. 이러한 결정과 변수에 이름을 붙인 후, 해당 변수의 함수를 사용하여 목표와 제약 조건을 형식화합니다.\n\n- 목표 함수는 솔루션의 품질을 측정합니다.\n- 제약 조건은 솔루션이 모든 필요한 규칙을 준수하여 실행 가능한 것으로 간주될 수 있도록 보장합니다.\n\n우리의 경우와 많은 다른 경우, 함수는 선형일 것이며, 변수는 예/아니오 결정을 나타내기 위해 이진 형태를 가질 것입니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 접근 가능한 개방형 병원이 가구 당 하나의 변수를 필요로하고, 이를 서비스하는지 여부를 결정하는 또 다른 변수가 있습니다. 또한 각 병원당 변수가 필요하며, 해당 병원이 여는지 여부를 나타냅니다. 모델을 표현하는 전형적인 수학적 표기법은 변수의 지원 집합 이름, 지수 및 데이터에서 유도된 모델 매개변수를 명명하여 시작됩니다.\n\n저희 최적화 과제에서 이러한 것들이 포함됩니다:\n\n집합\n\n- 𝐼 — 가구 집합\n- 𝐽 — 잠재적인 병원 위치 집합\n- 𝐽𝑖 — 집합 내 가구 𝑖∈𝐼의 닿을 수 있는 잠재적인 병원 위치. 참고: 𝐽𝑖⊆𝐽.\n\n<div class=\"content-ad\"></div>\n\n매개변수\n\n- 𝑣𝑖 — 가구 𝑖∈𝐼의 가구원 수.\n\n## 체 Church와 ReVelle의 논문에 기재된 최대 커버링 모델\n\n이 모델은 각 가구 𝑖∈𝐼에 대해 해당 가구가 𝑗∈𝐽에서 개설된 병원에 의해 서비스를 받을 수 있는지를 나타내는 변수 𝑧𝑖를 정의하며, 완전한 모델은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_8.png)\n\n첫 번째 줄은 가구 수를 최대화하는 목표를 설명하고, 두 번째 줄 (_subject to:_ 이후)은 첫 번째 제약 조건을 나열합니다: 각 가구는 도달 가능한 최소한 하나의 병원이 열려 있을 때에만 서비스를 받습니다. 그런 다음, 열 병원 수는 선택을 제한하며, 마지막으로 사용되는 변수의 이진성이 지정됩니다.\n\n위 모델은 최대 𝑝개의 병원을 선택합니다. 원래 논문에서는 Church와 ReVelle이 정확히 𝑝개의 병원을 선택했지만, 우리 모델에는 나중에 논의할 장점이 있습니다.\n\n## 수학적 모델 구현하기\n\n\n<div class=\"content-ad\"></div>\n\n모델을 설계한 후에는 구현이 수학적 표현에서 개념을 코드로 번역합니다. 번역은 대부분 일대일로 이루어지며, 주된 차이점은 변수를 사용하기 전에 항상 프로그래밍에서 처럼 변수를 선언한다는 것입니다. 반대로, 수학적 공식은 전통적으로 변수를 끝에 선언합니다. 저희는 모델을 코딩하기 위해 패키지 Pyomo를 사용합니다.\n\n- 최적화 모델 정의\n\nmodel_max_covering 함수는 Pyomo를 사용하여 수학적 최적화 모델을 정의합니다. 다음과 같은 여러 매개변수를 사용합니다:\n\n- w: 각 가구 ID의 인구 수가 포함된 사전\n- I: 가구 ID 세트\n- J: 잠재적 병원 위치 ID 세트\n- JI: 각 가구 ID를 그들에게 서비스를 제공할 수 있는 잠재적 병원 ID 세트로 매핑하는 사전\n- p: 오픈할 최대 병원 수\n\n<div class=\"content-ad\"></div>\n\n모델은 의사결정 변수 x(이진, j 위치에 병원을 열지 여부)와 z(이진, i 가 서비스를 받는지 여부)를 설정합니다. 목적은 총 인구를 최대한 커버하는 것이며, 가구가 서비스를 받는 경우에는 도달 가능한 병원이 열리기 때문에 제약 조건을 따릅니다. 예산인 -p를 초과하지 않도록 전체 병원 수를 최대화합니다.\n\n### 2. 입력 데이터 준비\n\n그런 다음 코드는 데이터프레임에서 관련 데이터를 추출합니다:\n\n- J_existing: 기존 병원 ID 집합\n- J_potential: 잠재적인 새 병원 위치 ID 집합\n- I1: 모든 가구 ID 집합\n- IJ_existing: 기존 병원 ID를 해당 병원이 서비스를 제공할 수 있는 가구 집합으로 매핑한 사전\n- IJ_potential: 잠재적인 병원 ID를 해당 병원이 서비스를 제공할 수 있는 가구 집합으로 매핑한 사전\n- JI1: 이전 두 가지로부터 역으로 매핑한 것으로, 각 가구에 대해 잠재적인 병원 집합을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n3. 최적화 모델 호출\n\nget_ids 함수는 주어진 예산(개설할 병원 수)에 대해 최적화 모델을 초기화하고 해결하기 위해 정의됩니다. 이 함수는 모델 인스턴스 m1을 생성하고, 제안된 제약 조건을 사용하여 개선할 기존 병원을 설정하며, 기존 병원의 수를 추가하여 예산 p를 조정합니다.\n\n그런 다음 HiGHS 솔버를 사용하여 모델을 해결하고 선택된(개설된) 병원들의 ID가 반환됩니다.\n\n4. 최적 솔루션 찾기\n\n<div class=\"content-ad\"></div>\n\n다음 코드 블록에서는 결과를 저장하기 위한 result_list라는 리스트가 생성됩니다. 다양한 예산(개설할 병원 수)에 대한 결과가 저장됩니다.\n\n루프는 다양한 예산 값에서 실행되며, 잠재적 위치의 총 수까지 반복됩니다. 각 예산에 대해:\n\n- get_ids 함수를 호출하여 개설할 병원 ID의 최적 집합을 가져옵니다.\n- 선택된 병원을 기반으로 액세스가 있는 인구와 액세스가 없는 인구가 계산됩니다.\n- 액세스가 있는 인구의 백분율이 계산됩니다.\n- 이 백분율과 해당 예산이 result_list에 추가됩니다.\n\n이 프로세스를 통해 개설하는 병원 수와 인구를 커버하는 백분율 간의 균형을 탐색하고, 결과적으로 파레토 곡선 시각화를 만들 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n파레토 곡선 시각화는 최적화 모델 실행 결과를 다른 예산(개설할 병원 수)으로 2차원 그래프에 도식화하는 것을 의미합니다. 이는 개설된 시설 수와 그에 따른 인구 보호율 사이의 트레이드 오프를 시각화할 수 있게 합니다. 구체적으로, x축은 개설된 병원 수(예산)를 나타내고, y축은 의료 시설에 접근할 수 있는 인구 백분율을 보여줍니다. 예산(따라서 개설된 병원 수)이 증가함에 따라 인구 보호율도 향상되는 경향이 있습니다. 그러나 일반적으로 개설된 병원이 추가됨에 따라 개선 속도는 감소하며, 일정 시점을 넘어 병원이 추가될수록 둔화되는 곡선이 나타납니다. 이러한 곡선을 파레토 곡선 또는 파레토 프론티어라고 합니다. 이것은 우위에 있지 않은 솔루션 집합을 나타내며, 한 목표(예: 인구 보호율)를 증가시키려면 다른 목표(예: 병원 수/예산 증가)를 희생해야 하는 솔루션입니다. 파레토 곡선 시각화는 이 맥락에서 중요합니다. 결정자들이 관련된 트레이드 오프를 이해하고 정보에 기반한 선택을 할 수 있도록 돕기 때문입니다. 예를 들어:\n\n- 곡선이 가파른 초기 기울기를 가지고 있다면, 몇 개의 병원을 추가함으로써 인구 보호율을 크게 향상시킬 수 있어 투자가 매우 효과적일 수 있습니다.\n- 곡선이 빠르게 평평해진다면, 일정 시점을 넘어 병원을 추가로 개설해도 보호율이 크게 증가하지 않을 수 있어, 감소하는 수익을 시사합니다.\n- 곡선은 트레이드 오프가 덜 유리해지는 \"무릎\"이나 포인트를 나타낼 수 있어, 두 목표를 균형있게 맞출 수 있는 잠재적인 좋은 지점을 식별하는 데 도움을 줍니다.\n\n아래 코드는 Plotly.express 모듈을 사용하여 최적화 결과의 파레토 프론티어를 시각화하는 데 사용됩니다.\n\n# 이제, 가장 중요한 질문: 이 최적화된 위치들은 어디에 있을까요?\n\n<div class=\"content-ad\"></div>\n\n저희 최적화 모델은 여러 전략적 위치를 식별하여 새로운 의료 시설을 개선해 전반적인 접근성을 향상시켰습니다. 추가 5개 시설을 오픈하는 예산 제약 조건 하에, 분석 결과 이러한 위치를 선택함으로써 47.4% (기존 시설만 사용하는 경우)에서 61.93%로 인구 접근률을 증가시키는데 가장 효과적인 선택지로 제시합니다.\n\n![이미지](/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_9.png)\n\n요약하자면, 이 블로그 글은 개방 데이터, 지리 정보 분석 및 수학적 최적화 기술을 활용하여 공평한 의료 접근 계획의 중요 도전에 대응하는 능력을 보여줍니다. Python의 능력을 활용하고 다양한 데이터 소스를 통합함으로써, 우리는 의료 시설 위치를 평가하고 새로운 시설을 위한 최적 위치를 식별하기 위한 견고한 방법론을 개발했습니다.\n\n이 사례 연구는 티모르-레스테에 초점을 맞췄지만, 이 블로그에서 제시된 방법론은 보편적으로 적용 가능하며, 의료 시설에 대한 물리적 접근 문제에 직면한 다른 지역이나 국가에 적응하여 투자 우선순위를 결정할 수 있도록 돕습니다.\n\n<div class=\"content-ad\"></div>\n\n이 튜토리얼의 전체 코드는 Colab 노트북에서 찾을 수 있어요. 당신이 파이썬 프로그래머가 아니더라도, 우리는 이 기여가 최적화 모델을 사용해 새로운 세대의 의사 결정 지원을 위한 오픈으로 이용 가능한 지리적 데이터를 활용하는 가능성과 프로세스에 관한 직관적인 감각을 제공해 줄 것을 바라고 있어요.","ogImage":{"url":"/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_0.png"},"coverImage":"/assets/img/2024-06-19-AnOpenData-DrivenApproachtoOptimisingHealthcareFacilityLocationsUsingPython_0.png","tag":["Tech"],"readingTime":12},{"title":"포켓몬은 몇 마리나 들어갈까요","description":"","date":"2024-06-19 23:30","slug":"2024-06-19-HowManyPokmonFit","content":"\n\n<img src=\"/assets/img/2024-06-19-HowManyPokmonFit_0.png\" />\n\n이 게시물에서는 PuLP 라이브러리를 사용하여 Python에서 최적화 문제를 모델링하고 해결하는 방법을 보여드리고 포켓몬 데이터를 활용하여 프로세스를 설명하겠습니다. Python에서 최적화 문제를 해결하는 방법에 대해 더 배우고 싶으시거나 포켓몬을 좋아하시는 분이면, 이 튜토리얼을 통해 쉽게 이해할 수 있는 단계를 안내받을 수 있습니다.\n\n그럼 출발해봅시다! 🚀\n\n# 포켓몬 역사를 조금 알아보기\n\n<div class=\"content-ad\"></div>\n\n오리지널 포켓몬의 제1세대는 1996년에 일본에서 발매된 포켓몬 레드와 그린 게임에 소개된 151마리의 포켓몬으로 구성되어 있어요. 이후 여러 세대와 버전의 포켓몬이 추가로 출시되어 1,000여 마리가 넘는 포켓몬이 생성되었는데, 전자 게임, 애니메이션, 트레이딩 카드, 상품 등에서 포켓몬은 문화적 상징으로 자리매김했습니다. 그럼에도 불구하고, 이 튜토리얼에서는 제1세대의 원래 151마리 포켓몬 데이터만 사용할 거에요.\n\n원래의 포켓몬 게임에서는 두 가지 주요 목표가 있습니다 — 포켓몬 챔피언이 되는 것과 전국도감을 완성하는 것입니다. 포켓몬 챔피언이 되기 위해서는 여덟 개의 체육관 리더를 이기고 해당하는 여덟 개의 체육관 뱃지를 획득해야 합니다. 그런 다음 엘리트 포와 현재의 포켓몬 챔피언을 물리칠 때 될 수 있는 포켓몬 챔피언이 될 수 있어요! 전국도감을 완성하기 위해서는 야생에서 잡거나 전투에서 얻거나 교환하여 151마리의 포켓몬을 모두 포함해야 해요.\n\n게임의 주요 제한 사항 중 하나는 결투 중에 활성 팀에 포켓몬을 최대 6마리 밖에 가지고 다닐 수 없다는 것이에요. 그렇다면, 만약 151마리 포켓몬을 모두 잡았다면, 어떤 6마리의 포켓몬을 팀에 포함해야 할지 고를 때 관련 질문이 생기죠. 어떤 6마리의 포켓몬을 선택할지 결정하려면, 먼저 각 포켓몬의 전투에서의 효과를 평가하기 위해 측정 지표(즉, 목표 함수)를 선택해야 합니다. 그런 다음 목표는 정의된 목표 함수를 최대화하는 포켓몬 팀을 선택하는 것이 되겠죠.\n\n각 포켓몬은 게임 플레이 메카닉을 형성하는 여러 독특한 특성을 가지고 있어요. 게임을 즐기고 다양한 환경에서 포켓몬 결투의 승자를 결정하도록 도와주죠. 각 포켓몬에는 HP(체력), 공격력, 방어력, 특수공격력, 특수방어력, 스피드와 같은 6가지 기본 통계가 있어요. 기본 통계 외에도 화재, 물, 전기 등 포켓몬 타입, 포켓몬 기술, 현재 포켓몬 레벨 등과 같은 다양한 특성이 포켓몬의 효과성에 기여합니다. 그래도, 포켓몬 커뮤니티에서 효과성을 평가하는 인기 지표는 포켓몬의 총 기본 통계(BST)입니다. BST는 포켓몬의 기본 통계의 총합이라고 생각하시면 됩니다!\n\n<div class=\"content-ad\"></div>\n\n만약 우리가 선택해야 할 포켓몬이 6마리뿐이라는 단 하나의 제약 조건만 있다면, 문제는 비교적 쉽고 명백한 해결책을 갖게 됩니다. 즉, 각 후보 포켓몬의 BST를 계산하고, 그 중에서 가장 효과적인 상위 6마리의 포켓몬을 우리 팀에 포함시키는 것입니다.\n\n본 게시물에서 최적화 문제의 좀 더 세밀한 버전을 탐색하기 위해 추가적인 제약 조건을 가정할 것입니다. 구체적으로, 선택한 포켓몬들의 총 무게에 제한이 있다고 가정할 것입니다. 즉, 선택한 포켓몬들의 총 무게는 1,000kg 이하이어야 합니다. 이 추가적인 제약 조건은 우리에게 포켓몬 데이터를 사용한 가상의 난쟁이 문제(knapsack problem)를 구성할 수 있게 해줍니다.\n\n# 난쟁이 문제에 대해 어떻게 생각하시나요?\n\n난쟁이 문제는 다음과 같이 표현될 수 있는 고전적인 결합 최적화 문제입니다:\n\n<div class=\"content-ad\"></div>\n\n다시 말해, 낙석 문제에서는 특정 가치와 크기를 가진 항목 집합을 일정 크기의 낙석가방에 담아야 합니다. 문제는 총 항목 중에서 낙석가방에 맞게 최대 총 가치를 얻을 수 있는 부분집합을 찾는 것입니다.\n\n가장 흔한 낙석 문제의 경우는 0/1 낙석 문제인데, 각 항목에 대해 하나의 인스턴스만 있는 문제입니다. 다시 말해, 0/1 낙석 문제에서 각 항목은 고유하며 낙석가방에 포함시킬지 여부를 한 번 선택하거나 아예 선택하지 않습니다. 유사하게, 포켓몬 문제에서는 151가지의 고유한 포켓몬이 있으며, 각각의 포켓몬은 우리의 활약팀에 한 번 포함되거나 전혀 포함되지 않을 것입니다.\n\n![포켓몬이 맞게 들어가는지 확인할 수 없습니다.](/assets/img/2024-06-19-HowManyPokmonFit_1.png)\n\n0/1 낙석 문제는 바이너리 정수 프로그래밍(BIP) 문제로서 이렇게 정식으로 정의할 수 있습니다, 좀 멋있게 말하면:\n\n<div class=\"content-ad\"></div>\n\n특히:\n\n- υi는 항목 i의 가치입니다.\n- wi는 항목 i의 무게입니다.\n- W는 배낭의 총 무게 수용량이며,\n- i는 항목 i가 배낭에 포함되는지 여부를 나타내는 결정 변수로, 각각 1 또는 0입니다.\n\n배낭 문제의 다른 변형들도 있습니다. 예를 들어, 제한된 배낭 문제(각 항목을 주어진 숫자까지 여러 번 선택할 수 있는 문제)와 무제한 배낭 문제(각 항목을 원하는 만큼 여러 번 선택할 수 있는 문제)가 있습니다.\n\n배낭 문제는 NP-어려운 문제로 알려져 있지만, 작은 경우에는 정확한 알고리즘을 사용하여 효율적으로 해결할 수 있습니다. 따라서, 이 게시물에서는 PuLP Python 라이브러리를 사용하여 PokeAPI 데이터를 활용하여 0/1 배낭 문제를 모델링하고 해결합니다.\n\n<div class=\"content-ad\"></div>\n\n# 모두 잡기!\n\n저는 Jupyter Lab 노트북을 사용하여 PokeAPI 데이터를 가져오고, PuLP로 최적화 문제를 모델링하고 해결하며, 데이터의 간단한 Plotly 시각화를 만들 것입니다.\n\n## PokéAPI에서 데이터 가져오기\n\n분석에 필요한 다양한 포켓몬 특성을 획득하기 위해 PokeAPI에서 제공하는 데이터를 사용할 것입니다. 더 구체적으로, requests 라이브러리를 이용하여 Python에서 PokeAPI에 접근할 수 있습니다. 'limit = 151'을 설정하여 PokeAPI에서 1세대의 처음 151마리 포켓몬과 그 URL을 쉽게 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\nimport requests\n\nurl = \"https://pokeapi.co/api/v2/pokemon?limit=151\"\nresponse = requests.get(url)\ndata = response.json()\npokemon_urls = [pokemon['url'] for pokemon in data['results']]\r\n```\n\n그런 다음 URL을 반복하면서 각 항목에 대해 API를 호출하여 151마리의 포켓몬의 속성도 가져올 수 있습니다. 각 포켓몬에 대해 BST 계산에 필요한 기본 통계 및 포켓몬 유형을 가져오겠습니다. pokemon_data라는 딕셔너리에 각각의 151마리 포켓몬 이름과 해당 속성을 저장합니다.\n\n```js\r\ncounter = 0\npokemon_data = []\nfor url in pokemon_urls:\n    response = requests.get(url)\n    pokemon = response.json()\n    base_stats = {stat['stat']['name']: stat['base_stat'] for stat in pokemon['stats']}\n    pokemon_data.append({\n        'name': pokemon['name'],\n        'weight': pokemon['weight'],\n        'hp': base_stats.get('hp', 0),\n        'attack': base_stats.get('attack', 0),\n        'defense': base_stats.get('defense', 0),\n        'special_attack': base_stats.get('special-attack', 0),\n        'special_defense': base_stats.get('special-defense', 0),\n        'speed': base_stats.get('speed', 0),\n        'type': [t['type']['name'] for t in pokemon['types']],\n    })\n    counter += 1\r\n```\n\n데이터프레임 형식으로 데이터셋을 살펴볼 수도 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nimport pandas as pd \ndf = pd.DataFrame(pokemon_data)\ndf\n```\n\n![Pokemon Image](/assets/img/2024-06-19-HowManyPokmonFit_2.png)\n\n그리고 이제 PuLP로 배낭 문제를 모델링할 준비가 되었습니다!\n\n## PuLP을 사용하여 문제 모델링하기\n\n<div class=\"content-ad\"></div>\n\n최적화 문제를 구조화하는 매우 첫 번째 단계는 결정 변수를 정의하는 것입니다. 우리의 경우, 우리가 결정하려고 하는 것은 어떤 포켓몬을 팀에 포함할 것인지입니다. 다시 말해, 151마리의 포켓몬 각각에 대해 그 포켓몬을 팀에 포함할지(따라서 이 포켓몬에 대한 결정 변수는 1이 될 것) 아닐지를 결정하려고 합니다(따라서 결정 변수는 0이 될 것). 결과적으로, 이 문제의 결정 변수는 이 문제에서 포켓몬이 우리 팀에 포함되어 있는지 여부를 나타내는 이진 결정 변수 x입니다.\n\n결정 변수 x를 정의한 후, 우리는 문제의 목적 함수도 정할 수 있습니다. 즉, 우리가 최적화하려는 대상은 무엇인가요. 여기서 우리는 포켓몬 팀의 총 BST를 최적화하려고 하며, 더 구체적으로 그 값을 최대화하려고 합니다. 따라서, 문제의 목적 함수는 다음과 같이 표현할 수 있습니다:\n\n![식](/assets/img/2024-06-19-HowManyPokmonFit_3.png)\n\n마지막으로, 우리는 문제의 제약 조건도 정해야 합니다. 이는:\n\n<div class=\"content-ad\"></div>\n\n- 결정 변수 x는 바이너리(0 또는 1)여야 합니다.\n- 우리의 포켓몬 팀의 총 무게는 1,000kg 이하이어야 합니다.\n- 선택된 포켓몬의 총 수는 정확히 6마리여야 합니다.\n\n![이미지](/assets/img/2024-06-19-HowManyPokmonFit_4.png)\n\n파이썬으로 이를 작성하려면, 우선 PuLP 라이브러리를 가져와 최적화 문제의 인스턴스를 초기화합니다.\n\n```python\nimport pulp\n\nprob = pulp.LpProblem(\"포켓몬 팀 최적화\", pulp.LpMaximize)\n```\n\n<div class=\"content-ad\"></div>\n\n그런 다음, 결정 변수를 정의합니다. 여기에는 바이너리 변수 제약 조건이 직접 통합됩니다.\n\n```js\nx = pulp.LpVariable.dicts(\"x\", range(len(pokemon_data)), cat='Binary')\n```\n\n그리고 목적 함수는 다음과 같습니다.\n\n```js\nprob += pulp.lpSum(\n    (pokemon['hp'] + pokemon['attack'] + pokemon['defense'] + pokemon['special_attack'] + pokemon['special_defense'] + \n     pokemon['speed']) * x[i] for i, pokemon in enumerate(pokemon_data)\n), \"Total Combat Effectiveness\"\n```\n\n<div class=\"content-ad\"></div>\n\n…그리고 문제 제약 사항입니다.\n\n```js\n# 총 무게 제약 사항\nmax_weight_capacity = 1000\nprob += pulp.lpSum(pokemon['weight'] * x[i] for i, pokemon in enumerate(pokemon_data)) <= max_weight_capacity, \"무게 한도\"\n\n# 포켓몬 총 수 제약 사항\nprob += pulp.lpSum(x[i] for i in range(len(pokemon_data))) == 6, \"팀 크기\"\n```\n\n이렇게 하면 파이썬에서 최적화 문제의 모델을 완전히 정의하였으며, 이제 해결할 준비가 되었습니다.\n\n## 포켓몬 드림 팀\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 Markdown 형식으로 변경할 수 있습니다:\n\n\nThe defined problem can be solved simply by:\n\n```js\nprob.solve()\n```\n\n… which returns ‘1’, meaning that the status of our problem is ‘LpStatusOptimal’, meaning the problem has an optimal solution. prob.solve() may also return other outputs, as for instance ‘-1’ (LpStatusInfeasible), meaning that there is no feasible solution for the problem given the constraints, or ‘-2’ (LpStatusUnbounded), indicating that the solution is unbounded.\n\nIn any case, given that our problem has an optimal solution, we can display it by:\n\n\n<div class=\"content-ad\"></div>\n\n```js\n선택된_포켓몬 = [포켓몬_데이터[i]['이름'] for i in range(len(포켓몬_데이터)) if pulp.value(x[i]) == 1]\nprint(\"선택된 포켓몬:\", 선택된_포켓몬)\n```\n\n![이미지](/assets/img/2024-06-19-HowManyPokmonFit_5.png)\n\n그리고 ✨보세요✨\n\n우리의 포켓몬 드림 팀을 얻었어요!\n\n<div class=\"content-ad\"></div>\n\n문제의 제약 조건(허용 중량 및/또는 포켓몬 수 변경, 포켓몬 유형 추가 등), 다른 목적 함수 정의 또는 기본 통계 중 하나에 대한 다른 계수 할당으로 인해 서로 다른 팀을 얻을 수 있다고 하는 것 같아요.\n\n이 문제의 맥락에서는 포켓몬의 BST가 포켓몬의 무게와 어떻게 관련되는지 확인하는 것도 흥미롭고, 전체 151마리 포켓몬 중에서 선택 가능한 포켓몬 팀의 시각적 표현도 가질 수 있어요. Plotly 산점도 차트로 이를 할 수 있어요:\n\n```js\n# 모든 포켓몬의 BST 계산\nfor pokemon in pokemon_data:\n    pokemon['BST'] = (pokemon['hp'] + pokemon['attack'] + pokemon['defense'] +\n                                             pokemon['special_attack'] + pokemon['special_defense'] + \n                                             pokemon['speed'])\n    \nall_pokemon_df = pd.DataFrame(pokemon_data)\nselected_df = pd.DataFrame(selected_pokemon_data)\n\n# 산점도 생성\nfig = go.Figure()\n\n# 모든 포켓몬 추가\nfig.add_trace(go.Scatter(\n    x=all_pokemon_df['weight'],\n    y=all_pokemon_df['BST'],\n    mode='markers',\n    name='모든 포켓몬',\n    marker=dict(size=10, color='blue', opacity=0.6),\n    text=all_pokemon_df['name']\n))\n\n# 선택된 포켓몬 추가\nfig.add_trace(go.Scatter(\n    x=selected_df['weight'],\n    y=selected_df['total_combat_effectiveness'],\n    mode='markers',\n    name='선택된 포켓몬',\n    marker=dict(size=12, color='red', opacity=0.9),\n    text=selected_df['name']\n))\n\n# 제목 및 레이블, 크기 조정\nfig.update_layout(\n    title=\"포켓몬 선택 최적화\",\n    xaxis_title=\"무게 (kg)\",\n    yaxis_title=\"BST\",\n    legend_title=\"범례\",\n    width=1000, \n    height=600,\n    showlegend=True\n)\n\n# 플롯 표시\nfig.show()\n```\n\n# 제 생각\n\n<div class=\"content-ad\"></div>\n\n포켓몬 챔피언십을 위해 팀을 구성하는 것 외에도 배낭 문제는 여러 다양한 분야에서 실제 결정 과정에서 발생합니다. 이러한 실제 상황에는 예산 할당, 화물/트럭 적재, 투자 포트폴리오 최적화, 의약품 제조 또는 식단 계획 등이 포함됩니다. 이러한 문제를 프로그래밍적으로 인식, 모델링 및 해결하는 것은 어떤 조직에게도 중요한 통찰력을 제공할 수 있습니다. PuLP는 파이썬 환경에서 효율적으로 최적화 문제를 모델링하고 해결할 수 있는 강력한 라이브러리이며, 이는 파이썬의 다재다능성을 완전하게 보여줍니다.\n\n# 출처\n\n데이터는 포켓몬 API에서 수집되었습니다. © 2013–2023 Paul Hallett 및 포켓몬 API 기여자에 의해 제공되며, 3-Clause BSD 라이선스 하에 공개되었습니다. 전체 라이선스 세부 정보는 포켓몬 API GitHub 페이지를 방문해 주세요.\n\n✨읽어 주셔서 감사합니다!✨\n\n<div class=\"content-ad\"></div>\n\n이 게시물을 즐겼나요? 친구가 되어요!\n\n💌 제 Medium 또는 LinkedIn에 함께해요!\n\n💼 Upwork에서 저와 함께 일해요!","ogImage":{"url":"/assets/img/2024-06-19-HowManyPokmonFit_0.png"},"coverImage":"/assets/img/2024-06-19-HowManyPokmonFit_0.png","tag":["Tech"],"readingTime":10},{"title":"데이터 과학자가 알아야 할 선형 대수학 개념","description":"","date":"2024-06-19 23:27","slug":"2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow","content":"\n\n## 데이터 과학\n\n![이미지](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_0.png)\n\n선형 대수는 모든 데이터 과학 및 머신 러닝 작업의 기반입니다.\n\n이것은 이론적 모델을 실용적인 솔루션으로 변환하는 언어입니다.\n\n<div class=\"content-ad\"></div>\n\n데이터에서 학습할 수 있도록 하는 원칙을 내포하고 있어요.\n\n![이미지](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_1.png)\n\n이것들은 다음과 같이 사용됩니다.\n\n- 데이터의 표현: 데이터를 구조화하고 조작하는 구조화된 방법으로, 복잡한 데이터셋을 행렬로 표현할 수 있도록 합니다.\n- 차원 축소: PCA와 같은 기법은 중요한 정보를 잃지 않으면서 모델 효율성을 높이기 위해 변수의 수를 줄이는 데 선형대수를 활용합니다.\n- 최적화: 그래디언트 디센트는 ML의 핵심 엔진으로, 함수의 최솟값을 찾기 위해 선형대수를 사용합니다.\n- 피쳐 엔지니어링: 선형 변환과 행렬 연산을 통해 기존 데이터에서 새로운 피처를 생성합니다.\n- 유사성 측정: 임베딩은 벡터로 저장되며, 오늘날 추천 시스템과 AI 챗봇에서 사용됩니다.\n- 그 밖에도 많아요!\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 선형 대수학 개념, 시각적 설명 및 코드 예제를 살펴볼 거에요.\n\n시작해 봅시다!\n\n코드 → Deepnote 노트북\n\n# 목차\n\n<div class=\"content-ad\"></div>\n\n### 벡터\n\n- **단위 벡터**: 단위 벡터\n- **벡터 연산**\n  - **벡터 덧셈**\n  - **스칼라 곱**\n  - **닷 프로덕트**\n- **벡터 공간**\n  - **영 공간 (커널)**\n  - **Span**\n  - **기저**\n  - **선형 독립성**\n\n# 행렬\n\n- **함수로서의 행렬**\n- **선형 변환**\n- **역행렬**\n- **특이 행렬**\n- **항등 행렬**\n- **대각 행렬**\n- **직교 행렬**\n- **행렬 곱셈**\n- **트레이스**\n- **행렬식**\n- **랭크**\n- **고유 벡터와 고유값**\n\n![선형 대수학 개념](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_2.png)\n\n이것이 선형 대수학의 기본적인 구성 요소입니다.\n\n<div class=\"content-ad\"></div>\n\n벡터를 생각하는 방법은 3가지가 있어요.\n\n첫 번째는 물리학적인 시각입니다. 벡터는 공간에 향하는 화살표로 정의되며, 길이와 방향에 의해 결정됩니다. 평면상의 벡터는 2차원이고, 우리가 사는 공간에 있는 벡터는 3차원입니다.\n\n두 번째는 컴퓨터 과학적 시각입니다. 벡터는 숫자의 순서대로 나열된 목록입니다. 이 목록의 길이가 차원을 결정합니다.\n\n세 번째는 수학자의 시각입니다. 벡터는 서로 더해지거나 숫자로 곱해지는 모든 것이 될 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n## 단위 벡터\n\n단위 벡터는 크기가 1인 벡터입니다. 종종 크기에 관계없이 벡터의 방향을 나타내는 데 사용됩니다.\n\n# 벡터 연산\n\n## 벡터 덧셈\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_3.png\" />\n\n두 벡터를 요소별로 더하여 새로운 벡터를 형성하는 것을 의미합니다.\n\n<img src=\"/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_4.png\" />\n\n## 스칼라 곱\n\n\n<div class=\"content-ad\"></div>\n\n표 태그를 마크다운 형식으로 변경하세요. \n\n## 내적\n\n형식적으로는 두 벡터의 유클리드 크기와 사이의 각도의 코사인의 곱으로, 벡터의 길이와 방향 관계를 모두 반영한다.\n\n![image](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_5.png)\n\n<div class=\"content-ad\"></div>\n\n직관적으로 생각해보면 한 벡터의 방향성 성장을 다른 벡터에 적용하는 것이라고 생각할 수 있습니다. 또는 \"한 벡터가 다른 벡터에게 얼마나 많은 밀어내기/에너지를 주는가?\"라고 생각할 수도 있습니다. 결과는 우리가 원래의 벡터를 얼마나 더 강하게 만들었는지를 보여줍니다 (양수, 음수 또는 0).\n\n![Image 6](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_6.png)\n\n만약 내적이 0이라면, 그것은 벡터들이 직교한다는 것을 말해줍니다.\n\n![Image 7](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_7.png)\n\n<div class=\"content-ad\"></div>\n\n반가운 비유를 하나 소개하겠습니다.\n\n빨간 화살표 벡터는 당신의 속도를 나타내고, 파란 화살표 벡터는 부스터 패드의 방향을 나타냅니다. 숫자가 클수록 더 강력한 파워를 의미합니다. 점곱은 당신이 받을 부스터 양을 나타냅니다.\n\n이 공식을 사용하면, |a|는 당신의 진입 속도, |b|는 최대 부스트이며, 받게 되는 부스트의 백분율은 cos(𝛉)이며, 전체 부스트는 |a| |b| cos(𝛉)가 됩니다.\n\n![이미지](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_8.png)\n\n<div class=\"content-ad\"></div>\n\n# 벡터 공간\n\n벡터(또는 선형) 공간은 더하고 숫자로 곱할 수 있는 벡터의 모음입니다. 이 숫자는 이 문맥에서 스칼라라고 불립니다.\n\nV가 벡터 공간이라고 불리기 위해서는 공리 목록을 만족해야 합니다.\n\n이미지를 표시하는 대신 Markdown 형식으로 표를 변경했습니다.\n\n![Vector Space Table](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_9.png)\n\n<div class=\"content-ad\"></div>\n\n## 널 공간 (커널)\n\n널 공간은 행렬과 곱해졌을 때 영벡터가 되는 벡터들의 집합입니다.\n\n이는 방정식 Ax = 0의 해를 나타냅니다. 여기서 A는 주어진 행렬입니다.\n\n주어진 행렬에 곱해졌을 때 두 벡터를 원점(영벡터)으로 수렴시키는 부분공간으로서 행렬의 널 공간을 시각화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## Span\n\n주어진 두 벡터 v와 w의 선형 결합인 av + bw를 통해 도달할 수 있는 모든 가능한 벡터의 집합이며, 여기서 a와 b는 모든 실수입니다.\n\n대부분의 벡터 쌍에 대해, 2차원 벡터 평면의 모든 점에 도달할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_10.png)\n\n두 벡터가 일치하는 경우, 원점을 지나는 단일 선에 제한됩니다.\n\nspan의 개념은 basis의 개념에 기초합니다.\n\n## Basis\n\n\n<div class=\"content-ad\"></div>\n\n기저는 전체 벡터 공간을 구성하는 선형 독립적인 벡터들의 모임입니다. 이는 벡터 공간 내의 모든 벡터를 기저 벡터의 선형 조합으로 표현할 수 있다는 것을 의미합니다.\n\n이들을 공간 내 모든 다른 벡터들을 위한 기본 요소로 생각해보세요.\n\n하나의 벡터를 화살표로 생각하는 것이 도움이 되지만, 벡터들의 집합에 대해서는 점으로 생각해보세요. 대부분의 기저 벡터 쌍은 공간의 전체 2차원 시트를 채울 수 있습니다.\n\n## 선형 독립성\n\n<div class=\"content-ad\"></div>\n\n일련의 벡터가 선형 독립적인 경우 집합 내의 벡터들이 결과적으로 ax + by 형태인 어떤 식으로도 나타낼 수 없는 경우입니다.\n\n## 행렬\n\n행렬은 입력과 연산을 행과 열로 구성하는 방법입니다.\n\n![이미지](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_11.png)\n\n<div class=\"content-ad\"></div>\n\n여기 2행 2열의 행렬이 있어요.\n\n구조화된 방식으로 문제를 해결할 수 있는 수학적 도구입니다.\n\n## 함수로서의 행렬\n\n행렬을 함수로 생각할 수 있어요. 파이썬 함수가 입력 매개변수를 받아 처리하고 출력을 반환하는 것처럼, 행렬 변환은 선형 변환을 통해 입력 벡터를 출력 벡터로 변환합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Linear Transformation](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_12.png)\n\n## 선형 변환\n\n![선형 변환](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_13.png)\n\n선형 변환은 두 벡터 공간 간의 매핑 V → W로, 벡터 덧셈과 스칼라 곱셈의 연산을 보존하는 것을 말합니다.\n\n\n<div class=\"content-ad\"></div>\n\n실제로 행렬 A를 벡터 x에 적용하여 다른 벡터 y를 얻는 것(Ax = y 작업을 통해)은 선형 변환입니다.\n\n이것은 데이터 과학에서 많이 사용됩니다:\n\n- 차원 축소: PCA는 선형 변환을 사용하여 고차원 데이터를 낮은 차원 공간으로 매핑합니다.\n- 데이터 변환: 데이터 집합을 정규화하거나 표준화하는 것은 선형 변환이다.\n- 피처 엔지니어링: 기존 피처의 조합을 통해 새로운 피처를 생성하는 것.\n\n다음은 몇 가지 형태의 행렬입니다:\n\n<div class=\"content-ad\"></div>\n\n## 역행렬\n\n행렬은 그 역행렬과 곱해지면 항등 행렬이 됩니다.\n\n## 특이 행렬\n\n특이 행렬은 역행렬을 가지지 않는 정방 행렬입니다. 이는 행렬의 행렬식이 0이거나 랭크가 크기보다 작은 것과 동일합니다.\n\n<div class=\"content-ad\"></div>\n\n## 항등 행렬.\n\n항등 행렬은 주 대각선에는 1의 값을, 그 외의 곳에는 0의 값을 갖는 정사각 행렬입니다. 행렬 곱셈에서 곱셈 항등원으로 작용하여 어떤 행렬에 적용해도 그 행렬을 변경시키지 않습니다. 그냥 숫자 1과 마찬가지로 작용합니다.\n\n## 대각 행렬\n\n대각 행렬은 모든 주 대각선을 제외한 항목이 0인 정사각 행렬입니다. 고유값을 찾거나 행렬식을 계산하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 직교 행렬\n\n![직교 행렬](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_14.png)\n\n실수 요소를 갖는 정방 행렬은 전치가 역행렬과 같으면 '직교'로 간주됩니다.\n\n형식적으로, 행렬 A가 AᵀA=AAᵀ = I를 만족하면 A는 직교 행렬입니다. 여기서 I는 항등 행렬입니다.\n\n<div class=\"content-ad\"></div>\n\n기하학적으로, 행렬은 그 열과 행이 직교하는 단위 벡터인 경우 직교합니다. 다시 말해, 서로 수직이며 크기가 1인 벡터입니다.\n\n두 벡터가 서로 직교하고(90도) 그들 사이의 내적이 0이면 두 벡터는 직교한다는 것을 기억하세요.\n\n## 행렬 곱셈\n\n행렬 곱셈을 수행하는 데에 행렬을 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n안녕하세요! 아래는 선형대수에 관한 직관적인 가이드에서 가져온 멋진 시각화입니다.\n\n![Visualization](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_15.png)\n\n각 입력 데이터를 각 연산을 통해 흘리는 것을 상상해 보세요.\n\n![Operations](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_16.png)\n\n<div class=\"content-ad\"></div>\n\n여기 작업의 예시가 있어요.\n\n![이미지](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_17.png)\n\n작업을 수행한 결과는 다음과 같아요.\n\n![이미지](/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_18.png)\n\n<div class=\"content-ad\"></div>\n\n입력은 [3 x 2] 행렬이며, 우리의 작업 행렬은 [2 x 3]입니다. 그 결과는 [2 x 3] [3 x 2] = [2 x 2]입니다.\n\n입력의 크기는 작업의 크기와 일치해야 합니다.\n\n## Trace\n\n행렬의 Trace는 모든 대각 요소의 합입니다. 기저 변경에 불변이며, 행렬에 대한 고유값의 합인 행렬에 대한 정보적인 값을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n## 행렬식\n\n행렬식은 출력 변환의 크기를 의미해요.\n\n입력이 단위 벡터인 경우(면적이나 부피가 1일 때), 행렬식은 변환된 면적이나 부피의 크기를 나타냅니다.\n\n예를 들어 이 행렬을 살펴보죠. A의 면적이 6배로 스케일링된 경우, 변환의 행렬식은 6이 되는 거죠.\n\n<div class=\"content-ad\"></div>\n\n음수 determinant는 전체 공간이 뒤집혔음을 알려줍니다. 이 변환은 종이 더미를 뒤집는 것과 비슷합니다.\n\n빨간색과 녹색 축의 방향이 뒤바뀐 것을 주목하세요.\n\nDeterminant가 0이면 행렬이 \"파괴적\"이며 뒤집을 수 없습니다. 0으로 곱하는 것과 비슷하게 정보가 손실됩니다.\n\nDeterminant는 행렬이 역행렬인지를 알려줄 수 있습니다. det(A)가 0이면 역행렬이 존재하지 않으며 행렬은 특이합니다.\n\n<div class=\"content-ad\"></div>\n\n## Rank\n\n행렬에서 선형 독립 열/행 벡터의 최대 개수를 나타내는 것입니다. 그것은 행 또는 열에 의해 만들어진 벡터 공간의 차원을 나타냅니다.\n\n또한 선형 변환 후 출력 차원의 개수를 알려줍니다.\n\n변환의 출력이 단일 선 (일차원이라고 함)인 경우, 해당 변환이 1의 순위를 가진다고 말합니다.\n\n<div class=\"content-ad\"></div>\n\n만약 모든 벡터가 일부 2차원 평면에 있을 경우, 해당 변환은 랭크 2를 가졌다고 말합니다.\n\n2x2 행렬의 경우 랭크 2가 가장 좋습니다. 이것이 full rank로 알려져 있죠. 이것은 기저 벡터가 전체 2차원 공간과 0이 아닌 determinant를 표현할 수 있다는 것을 의미합니다.\n\n그러나 3x3 행렬의 경우, 랭크 2는 더 안 좋은데, 완전히 무너진 것은 아닙니다. 하지만, 랭크 1보다는 낫다고 볼 수 있죠.\n\n## 고유벡터와 고유값\n\n<div class=\"content-ad\"></div>\n\n고유 벡터와 고유 값은 변환의 \"축\"을 나타냅니다.\n\n고유 벡터는 선형 변환 후에도 방향이 변하지 않는 입력값입니다. 방향은 변하지 않지만 크기는 변할 수 있습니다. 이 크기, 즉 고유 벡터가 확대되거나 축소되는 정도가 고유 값입니다.\n\n지구본을 회전시킬 때 생각해보세요; 극을 제외한 모든 위치가 새로운 방향을 향합니다. 그들의 방향은 변하지 않습니다.\n\n여기 고유 벡터의 시각적 예시가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](https://miro.medium.com/v2/resize:fit:600/1*d34D2o-Gx1IOgFnuuJ2kog.gif)\n\n행렬 A와 벡터 v에 대해, Av = λv이면 λ가 고유값이고, v가 행렬 A의 고유벡터입니다.\n\n다른 말로, 정방 행렬 A의 고유벡터는 행렬 곱셈 = 스칼라 곱셈인 벡터입니다.\n\n# 읽어 주셔서 감사합니다!\n\n\n<div class=\"content-ad\"></div>\n\n# 자원\n\n해커들 방식\n\n- 코더를 위한 계산 선형 대수학\n- 파이썬을 활용한 응용 기계 학습을 위한 선형 대수학 소개\n\n시각화\n\n<div class=\"content-ad\"></div>\n\n- 그래픽 선형 대수학 — LA를 수행하는 새로운 방법\n- 3Blue1Brown의 선형대수학 본질 — 놀라운 애니메이션, 개념 시각화\n- 인벡터라이즈\n- 직관적인 수학\n\n논문/강의/교재\n\n- 딥 러닝에 필요한 행렬 미적분\n- 데이터 분석, 신호 처리 및 머신 러닝을 위한 행렬 방법 | 수학 | MIT 오픈코스웨어\n- 올바르게 수행하는 선형 대수학\n- 선형대수학 4페이지로 알아보기.pdf\n\n# 연락을 유지하세요!\n\n<div class=\"content-ad\"></div>\n\n비트그릿 데이터 사이언스 퍼블리케이션을 팔로우하면 최신 소식을 받아보실 수 있어요!\n\n데이터 사이언스 및 인공지능 최신 동향을 다른 데이터 과학자들과 함께 논의하고 싶나요? 저희 디스코드 서버에 가입해보세요!\n\n워크숍 및 다가오는 대회 정보를 받아보려면 비트그릿을 팔로우하세요!\n\n디스코드 | 웹사이트 | 트위터 | 링크드인 | 인스타그램 | 페이스북 | 유튜브","ogImage":{"url":"/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_0.png"},"coverImage":"/assets/img/2024-06-19-LinearAlgebraConceptsEveryDataScientistShouldKnow_0.png","tag":["Tech"],"readingTime":9},{"title":"파이썬에서 Uncle Bob의 Clean Code 원칙 적용하는 방법","description":"","date":"2024-06-19 23:23","slug":"2024-06-19-HowtoApplyUncleBobsCleanCodePrinciplesinPython","content":"\n\n![이미지](/assets/img/2024-06-19-HowtoApplyUncleBobsCleanCodePrinciplesinPython_0.png)\n\n온클 밥, 모든 코더의 아버지,는 2008년에 '클린 코드'라는 책을 출판했습니다. 소프트웨어 엔지니어링에 진지하다면, 꼭 이 책을 읽어야 합니다. 이 글에서는 이 책을 요약하고 그 원칙들을 우리의 파이썬 코드에 어떻게 적용할 수 있는지 살펴보겠습니다. 이는 책에서 논의된 SOLID 원칙이 아니라, 코드베이스에서 그것을 따라야 하는 실제 상황에 대한 이야기입니다.\n\n로버트 마틴이 논의한 이 원칙들은 어떤 프로그래밍 언어에도 적용 가능하므로, 내 생각에는 모든 소프트웨어 개발자에게 필수적입니다. 나쁜 코드는 아무도 도와준 적이 없습니다.\n\n이 원칙들이 어떻게 작용하는지 확인하고 싶으신가요? 여기서 제 YouTube 비디오를 확인해보세요: [YouTube 비디오 링크](링크 주소)\n\n<div class=\"content-ad\"></div>\n\n## 깔끔한 코드란 무엇인가요?\n\n로버트 C. 마틴이 쓴 'Clean Code: Agile Software Craftsmanship'에서 소개한 깔끔한 코드란 쉽게 읽고 이해하며 유지보수할 수 있는 코드를 말합니다. 가독성, 간결함, 유지보수성을 강조합니다. 깔끔한 코드는 잘 구조화되어 있고 표준 규칙을 따르며 불필요한 복잡성이 없습니다. 또한 모듈화되어 있고 테스트 가능하며, 단일 책임을 갖는 함수와 메소드를 가지고 있어 코드 수정 및 확장이 쉽게 가능하며 버그를 도입하지 않도록 설계되어 있습니다. 깔끔한 코드의 궁극적인 목표는 견고하고 유연하며 작업하기 즐거운 코드베이스를 만드는 것입니다.\n\n## 깔끔한 코드의 주요 원칙\n\n이러한 일반 규칙은 간단하지만 강력합니다. 자세한 내용을 살펴보고 즐겨 사용하는 프로그래밍 언어인 Python에서 명확한 예제와 함께 이를 어떻게 구현하는지 알아봅시다.\n\n<div class=\"content-ad\"></div>\n\n- 표준 규칙 준수하기\n\n표준 규칙을 따르는 것은 깔끔하고 유지보수가 쉬운 코드를 작성하는 데 중요합니다. Python에서는 PEP 8에 따르는 것이 중요합니다. PEP 8는 일관된 Python 코드를 작성하는 데 대한 가이드라인과 모범 사례를 제공합니다.\n\n- KISS 원칙 준수하기\n\n디자인 시에 간결함을 중요시해야 합니다. 간단한 코드는 보다 쉽게 읽고 이해할 수 있으며 유지보수하기도 쉽습니다.\n\n<div class=\"content-ad\"></div>\n\n이 책의 SOLID 원칙과는 논란이 될 수 있습니다. 적용하는 것이 그렇지 않을 때보다 쉽다고 말할 순 없지만 결국 그들이 당신의 삶을 더 나아지게 만들 수 있다는 것은 사실입니다.\n\n- 보이 스카우트 규칙\n\n보이 스카우트 규칙은 항상 코드를 발견한 것보다 깨끗하게 남겨야 한다고 제안합니다. 이것은 지저분한 코드를 발견하면 변경 사항을 가입하는 동안 정리해야 한다는 것을 의미합니다.\n\n- 항상 근본 원인을 찾으세요\n\n<div class=\"content-ad\"></div>\n\n문제의 원인을 항상 찾아보고, 단기적인 해결책을 찾는 대신에 해결하시는 것이 좋습니다. 이렇게 하면 반복되는 문제가 발생하지 않고 시스템의 무결성을 유지할 수 있습니다.\n\n그렇다면, 이제 우리의 Python 코드에 깔끔한 코드 원칙을 적용하는 방법을 살펴보겠습니다.\n\n## if/else 대신 다형성 선호\n\n간단히 말해 다형성은 서로 다른 객체가 동일한 클래스의 인스턴스로 보일 수 있는 능력입니다. 이는 동일한 작업을 서로 다른 클래스에서 다르게 동작하도록 하는 것을 허용합니다. 예를 들어, 다른 클래스가 동일한 이름의 메소드를 가지고 있는 경우, 다형성을 통해 해당 메소드를 각 클래스의 객체에 대해 호출하고 그 클래스의 구현에 따라 특정 결과를 얻을 수 있습니다. 이는 코드에서 유연성과 재사용을 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n다형성을 활용하면 상속과 메서드 재정의를 통해 복잡한 조건 로직을 피할 수 있습니다. 이 접근 방식은 개방/폐쇄 원칙과 일치합니다.\n\n- if/else를 사용한 나쁜 예시\n\n```js\ndef get_discount(customer_type): \n    if customer_type == \"regular\": \n        return 0.1 \n    elif customer_type == \"premium\": \n        return 0.2 else: \n    return 0.0\n```\n\n- 다형성을 활용한 좋은 예시\n\n<div class=\"content-ad\"></div>\n\n```python\nclass Customer:\n    def get_discount(self):\n        return 0.0\n\nclass RegularCustomer(Customer):\n    def get_discount(self):\n        return 0.1\n\nclass PremiumCustomer(Customer):\n    def get_discount(self):\n        return 0.2\n\ndef get_customer_discount(customer):\n    return customer.get_discount()\n```\n\n## Use Dependency Injection\n\n의존성 주입(Dependency Injection, DI)은 클래스에 의존성을 주입할 수 있도록 하는 디자인 패턴으로, 클래스 간의 결합도를 줄이고 테스트 용이성과 유지보수성을 향상시킵니다.\n\n클래스가 자체적으로 의존성을 내부적으로 생성하는 경우, 해당 클래스는 해당 의존성에 강하게 결합됩니다. 이는 클래스가 의존성의 특정 구현에 대해 직접적으로 알고 있고 의존하고 있다는 것을 의미하며, 이를 수정하지 않고는 의존성을 변경하거나 대체하기가 어려워집니다. 이는 의존성 역전 원칙(Dependency Inversion Principle)을 위반하는 것으로, 고수준 모듈이 저수준 모듈에 의존하지 않고, 둘 다 추상화에 의존해야 한다는 원칙에 어긋납니다.\n\n\n<div class=\"content-ad\"></div>\n\n한편, 의존성 주입은 외부에서 클래스로 의존성을 주입할 수 있게 합니다. 이는 클래스가 의존성을 어떻게 생성해야 하는지 알 필요가 없다는 것을 의미합니다. 그 대신 외부에서 제공되는 의존성에 의존하는 방식입니다. 이는 클래스 간의 느슨한 결합을 촉진하는데, 클래스는 구체적인 구현이 아닌 추상화(인터페이스 또는 추상 클래스)에만 의존합니다. 또한 클래스를 테스트하기가 더 쉬우며, 의존성을 테스트 중에는 목업 또는 테스트용 대체품으로 교체할 수 있습니다.\n\n총론적으로, 의존성 주입은 더 유연하고 유지보수하기 쉬운 코드를 이끌며, 객체지향 설계 원칙을 더 잘 준수하게 만듭니다.\n\n- 의존성 주입이 없는 경우\n\n```js\nclass Service: \n    def init(self): \n        self.repository = Repository()\n```\n\n<div class=\"content-ad\"></div>\n\n```python\n    def perform_action(self):\n        data = self.repository.get_data()\r\n```\n\n- DI\n\n```python\r\nclass Service: \n    def init(self, repository): \n        self.repository = repository\n    def perform_action(self):\n        data = self.repository.get_data()\n        # perform action with data\n\nrepository = Repository() \nservice = Service(repository)\r\n```\n\n## Prevent Over-Configurability and Don’t Use Flag Arguments\n\n<div class=\"content-ad\"></div>\n\n소프트웨어를 간단하게 유지하는 것은 불필요한 설정이나 옵션을 추가하지 않는 것을 의미합니다. 플래그 인수는 함수를 복잡하고 이해하기 어렵게 만들 수 있습니다.\n코드에서 과도한 구성 가능성은 복잡성, 유지 관리 부담 증가, 코드 냄새, 가독성 저하와 같은 문제를 일으킬 수 있습니다. 함수가 너무 많은 구성 옵션을 가지고 있는 경우, 단일 책임 원칙(SRP)을 위반하고 있는 것으로, 책임이 불명확하고 코드 구성이 떨어지게 됩니다. 또한, 긴 복잡한 구성이 코드를 읽고 이해하기 어렵게 만들 수 있습니다.\n\n- 플래그 인수를 사용한 나쁜 예시\n\n```js\ndef create_user(name, email, is_admin=False): \n    user = User(name, email) \n    if is_admin: \n        user.set_admin_permissions() \n    return user\n```\n\n- 플래그 인수를 사용하지 않은 좋은 예시\n\n<div class=\"content-ad\"></div>\n\n```js\ndef create_user(name, email): \n    return User(name, email)\n\ndef create_admin_user(name, email): \n    user = User(name, email) \n    user.set_admin_permissions() \n    return user\n```\n\n## Law of Demeter를 따르세요\n\n클래스는 직접적인 의존성만을 알아야 합니다. 이는 느슨한 결합과 캡슐화를 장려하여 코드를 모듈화하고 유지보수하기 쉽게 만듭니다.\n\n- Law of Demeter를 위반하는 나쁜 예\n\n<div class=\"content-ad\"></div>\n\n```python\ndef get_user_info(user): \n    address = user.get_address() \n    city = address.get_city() \n    return city\n```\n\n- 지데르 법칙을 잘 따른 좋은 예시\n\n```python\ndef get_user_info(user): \n    return user.get_city()\n```\n\n## 논리적 의존성 회피하기\n\n<div class=\"content-ad\"></div>\n\n클래스 내의 메서드는 동일한 클래스 내의 다른 메서드의 내부 상태나 동작에 의존해서는 안 됩니다. 각 메서드는 독립적이고 독립적이어야 합니다.\n\n- 논리적 의존성이 있는 나쁜 예시\n\n```js\nclass Calculator: \n    def init(self): \n        self.result = 0\n    def add(self, number):\n        self.result += number\n    \n    def subtract(self, number):\n        self.result -= number\n    \n    def get_result(self):\n        return self.result\n```\n\n- 논리적 의존성이 없는 좋은 예시\n\n<div class=\"content-ad\"></div>\n\n\nclass Calculator: \n    def add(self, a, b): \n        return a + b\n    def subtract(self, a, b):\n        return a - b\n\n\n## 부작용 방지\n\n부작용이 없는 함수는 예측 가능하고, 테스트하기 쉽고, 모듈화되어 있으며, 병렬 실행에 안전하며, 일반적으로 유지보수가 용이하고 가독성이 좋은 코드로 이어집니다. 이러한 함수들은 입력에만 의존하고 외부 상태를 수정하지 않고 출력을 생성해야 합니다.\n\n이것은 또한 SOLID의 \"단일 책임 원칙\"의 예입니다. 함수는 한 가지 일만 수행해야 합니다. 즉, 부작용을 발생시키면 안 됩니다. 변수 2개를 더한 함수라면 콘솔에 뭔가를 로깅해서는 안 됩니다. 데이터베이스에서 사용자를 생성하는 함수라면 검증을 수행해서도 안 됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n- 부작용이 있는 안좋은 예시\n\n```python\ndef add_to_list(item, item_list=[]): \n    item_list.append(item) \n    return item_list\n```\n\n- 부작용이 없는 좋은 예시\n\n```python\ndef add_to_list(item, item_list=None): \n    if item_list is None: \n        item_list = [] \n    \n    new_list = item_list + [item] \n    return new_list\n```\n\n<div class=\"content-ad\"></div>\n\n## 깨끗한 코드를 읽는 가치가 있을까요?\n\n절대로요. 로버트 C. 마틴의 Clean Code는 소프트웨어 개발에 진지한 사람에게 꼭 필요한 책입니다. 이 책은 읽기 쉽고 유지보수가 쉽며 효율적인 코드 작성에 대한 실용적인 조언을 제공하여 더 나은 소프트웨어 품질과 더 쉬운 유지보수를 이루어냅니다. 이 책에 투자하는 시간은 귀하의 코딩 스킬과 전문 실무 방법을 크게 향상시킬 것입니다.\n\n## 결론\n\n요약하면, 우리는 Uncle Bob의 Clean Code에서 여러 원칙을 다뤄보았고 파이썬에서의 적용 방법을 보여주었습니다. 이 책을 꼭 읽어보시기를 강력히 추천합니다. 서로의 코드 품질을 높이는 데 노력합시다.\n\n<div class=\"content-ad\"></div>\n\n클린 코드 책 요약 페이지를 보려면 GitHub 페이지를 확인해보세요.\n\n코딩 즐기세요!","ogImage":{"url":"/assets/img/2024-06-19-HowtoApplyUncleBobsCleanCodePrinciplesinPython_0.png"},"coverImage":"/assets/img/2024-06-19-HowtoApplyUncleBobsCleanCodePrinciplesinPython_0.png","tag":["Tech"],"readingTime":7},{"title":"파이썬 프로젝트를 위한 완벽한 패키지 프로토 타입 제안","description":"","date":"2024-06-19 23:20","slug":"2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects","content":"\n<img src=\"/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_0.png\" />\n\n# 소개\n\nPython 패키지의 구조를 세울 때 고려해야 할 많은 옵션이 있고 많은 결정을 내려야 합니다. 그러나 한 번 디자인 선택사항을 결정하면 기본적인 구조적 변경을 만들기가 매우 어려워집니다.\n\n또한 사용자가 자신의 프로젝트에 네임스페이스를 포함하기 시작했다면, 어떠한 변경을 가하더라도 그 사용자가 소스 코드를 변경해야 할 필요가 생깁니다.\n\n<div class=\"content-ad\"></div>\n\n그러므로 처음에 꾸미고 배치 결정을 잘 내리는 것이 매우 중요합니다.\n\n## 문제\n\n파이썬 패키지를 구성하는데 여러 옵션이 많기 때문에 적절한 디자인을 찾거나 적어도 나중에 문제를 일으키지 않을 디자인을 찾는 것이 어려울 수 있습니다.\n\n## 기회\n\n<div class=\"content-ad\"></div>\n\n일반 패키지 레이아웃을 구성하고 모든 도전 과제를 해결하는 데 필요한 스켈레톤 프로젝트에서 프로토타입을 구축할 수 있다면, 앞으로 패키지를 만드는 것은 빠르고 간단한 복사 및 붙여넣기만으로 완료될 것입니다.\n\n## 앞으로의 계획\n\n제안된 완벽한 패키지 프로토타입에 대한 요구 사항을 설정한 다음, 모든 이러한 요구 사항을 해결하는 표준 프로젝트 구조를 생성할 수 있습니다.\n\n# 배경\n\n<div class=\"content-ad\"></div>\n\n여러 해 동안 많은 파이썬 패키지를 작성해 왔고, 프로젝트 구조를 각각 실험하고 수정하여 완벽한 구조를 찾으려 했지만, 항상 어느 한 측면에서는 불완전한 상태였습니다.\n\n그로 인해 완벽한 패키지 구조에 대한 요구 사항을 작성하고, 모든 원하는 것을 이루기 위한 방법을 찾기 위해 연구에 착수했습니다.\n\n# 요구 사항\n\n- 인기 있는 전문 패키지의 네임 스페이스 관례를 모방해야 합니다 (예: statsmodels.regression.linear_model에서 RegressionResultsWrapper를 가져옵니다).\n- 코드와 클래스를 잘 구조화된 폴더 및 파일 세트로 구성해야 합니다.\n- 패키지 사용자에게 노출되는 네임스페이스가 statsmodels.regression.linear_model과 같이 잘 지어진 체계를 반영해야 하며, 폴더와 파일 이름을 철저하게 따라서는 안 됩니다.\n- 하위 폴더의 클래스가 다른 하위 폴더 및 폴더 계층구조 상위 폴더의 클래스를 참조하고 액세스할 수 있어야 합니다.\n- 패키지 코드를 개발하고 테스트할 수 있는 Jupyter 노트북이 포함된 최상위 폴더를 포함해야 합니다.\n- 패키지의 모든 코드 및 클래스를 발견할 수 있는하위 폴더로 구성된 단위 테스트가 포함된 최상위 폴더를 포함해야 합니다.\n- 모든 패키지 폴더와 하위 폴더의 클래스 docstring에서 자동으로 생성된 문서가 포함된 최상위 폴더를 포함해야 합니다.\n- Jupyter 노트북, 단위 테스트 및 패키지에 접근하는 다른 프로젝트에서 패키지 위치를 하드 코딩하는 것을 피해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 솔루션이 작동하는 것을 증명하기\n\n솔루션을 증명하기 위해 2개의 테스트 클래스를 포함하는 빈 프로젝트 템플릿 또는 스켈레톤을 만들었습니다 -\n\n- class BaseLearner()는 알고리즘 클래스 집합을 위한 베이스 클래스로 설계된 가상의 클래스입니다.\n- class Fisher(BaseLearner)는 \"Fisher\" 알고리즘 코드를 포함할 가상의 클래스로, 이 클래스는 BaseLearner(폴더 구조의 완전히 다른 부분에 위치한)를 상속받는 것이 목적입니다.\n\n# 뒤의 시작하기\n\n<div class=\"content-ad\"></div>\n\n모든 요구 사항을 해결하고 두 개의 테스트 클래스에 대한 개요를 제공하는 완료된 프로젝트/패키지 구조 미리보기입니다 (Fisher 클래스가 fisher_file.py에 포함되어 있고 BaseLearner 클래스가 base_file.py에 포함되어 있는 것을 알립니다)\n\n![이미지](/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_1.png)\n\n# 그렇다면 큰 문제는 무엇인가요?\n\n이 구조는 모든 요구 사항을 해결했을 것으로 보이지만, 이 구조에는 주의할 필요가 있는 몇 가지 근본적인 문제가 있습니다...\n\n<div class=\"content-ad\"></div>\n\n# 네이밍 규칙\n\n주어진 패키지 레이아웃에 따라 BaseLearner를 가져오기 위한 코드는 다음과 같습니다.\n\n```python\nfrom common.base_file import BaseLearner\n```\n\n원하는 가져오기는 다음과 같습니다.\n\n<div class=\"content-ad\"></div>\n\nghpackage.common 모듈에서 BaseLearner를 가져왔습니다.\n\n따라서 이 패키지를 사용하는 사람들이 사용하는 네이밍 규칙이 직관적이지 않고 statsmodels.regression.linear_model에서 사용하는 표준과 일치하지 않을 수 있습니다.\n\n# 상대 및 절대 참조\n\nfisher_file.py에서는 ghtestpackage.common 모듈의 BaseLearner를 불러오는 코드가 작동하지 않을 것입니다. 왜냐하면 현재 위치에서 상대적인 참조를 사용했기 때문입니다. 즉, ghtestpackage.common 모듈에서 BaseLearner를 해결하려는 시도는 algorithms.ghtestpackage.common에서 BaseLearner를 찾지 못할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 큰 문제가 있어요\n\n그 문제는 더 심각해지고 있어요. 다양한 요구 사항을 통합하려고 하면서 이 패키지는 한꺼번에 3가지 역할을 수행하려고 노력하고 있어요:\n\n- 외부에서 참조할 수 있는 패키지.\n- Jupyter 노트북을 실행하는 환경.\n- pytest 단위 테스트를 실행하는 환경.\n\n문제는, 이 3가지 다른 사용 사례가 서로 다른 방식으로 실행되며 다른 상대적인 위치에서 실행을 시작한다는 것이 거의 불가능하다는 것이에요.\n\n<div class=\"content-ad\"></div>\n\n패키지가 Python 프로그램으로 가져올 때, 해당 패키지의 시작 지점은 부모 폴더입니다. 다음 시스템 경로를 고려하면 확인할 수 있습니다...\n\n```js\n[\n  \"c:\\\\Users\\\\GHarr\\\\OneDrive\\\\Python Projects\\\\Public-Github\\\\Package Structure\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\python310.zip\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\DLLs\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\lib\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\",\n  \"\",\n  \"C:\\\\Users\\\\GHarr\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\lib\\\\site-packages\",\n  \"C:\\\\Users\\\\GHarr\\\\OneDrive\\\\Python Projects\\\\Packages\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\lib\\\\site-packages\\\\win32\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\lib\\\\site-packages\\\\win32\\\\lib\",\n  \"c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\lib\\\\site-packages\\\\Pythonwin\",\n];\n```\n\n저는 현재 Anaconda를 사용 중이며, project-env 채널을 선택한 상태입니다. 경로에서 알 수 있듯이, **site-packages**가 **c:\\\\Users\\\\GHarr\\\\anaconda3\\\\envs\\\\project-env\\\\lib\\\\site-packages**에 위치하고 있습니다.\n\nAnaconda에서 CMD.exe 프롬프트를 실행하고 **site-packages** 폴더의 디렉토리를 나열하여 쉽게 확인할 수 있습니다...\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_2.png)\n\n프로그램이 pandas에서 DataFrame을 가져오는 것과 같이 import를 실행할 때, 참조는 C:\\Users\\GHarr\\anaconda3\\envs\\project-env\\Lib\\site-packages에서 시작되며 DataFrame이 포함 된 pandas라는 이름의 디렉토리를 찾을 것으로 예상합니다.\n\n주피터 노트북은 다르게 동작합니다. 그들의 \"홈\" 위치(또는 시작 실행 경로)는 .ipynb 소스 파일을 포함하는 폴더이며, 참조는 주피터 노트북에서 상대적으로(및 아래로) 진행됩니다.\n\n마지막으로 Visual Studio Code에서의 pytest 단위 테스트는 다시 다르게 동작합니다. VS Code 안에서 pytest 단위 테스트의 홈 / 실행 경로는 프로젝트의 루트 폴더입니다.\n\n<div class=\"content-ad\"></div>\n\n요약하면 다음과 같습니다...\n\n- 참조된 패키지는 프로젝트 폴더의 상위를 기준으로 상대적인 경로를 참조합니다.\n- Jupyter Notebook은 노트북을 포함하는 폴더를 기준으로 상대적인 경로를 참조합니다.\n- pytest 유닛 테스트는 프로젝트의 루트 폴더를 기준으로 상대적인 경로를 참조합니다.\n\n# 큰 문제 해결하기\n\n오랫동안 이 난제에 대한 유일한 해결책은 내 패키지에서 가져와야 하는 프로젝트의 import 코드에 직접 경로를 삽입하는 것이었습니다...\n\n<div class=\"content-ad\"></div>\n\n이 방법은 작동하지만 일부 심각한 단점이 있습니다.\n\n먼저 이 코드를 실행하는 데 매우 느립니다. 프로젝트가 처음으로 로드되고 VS Code에서 실행될 때, 외부 참조를 해결하고 가져오기를 처리하는 데, 꽤 좋은 i7 프로세서를 사용해도 52초가 걸립니다.\n\n다음 단점은 ghlibrary 프로젝트가 구조가 잘못되었다는 것입니다.\n\n예를 들어, dag_tools.py 및 causal_tools.py가 ghlibrary의 루트에 직접 존재하며 위치를 변경하는 옵션이 제한되어 있습니다. 따라서 시간이 지남에 따라 이러한 소스 파일이 커졌고, VS Code는 린팅, 도구 팁 및 기타 기능을 처리하는 데 느려졌습니다.\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 Markdown 형식으로 변경해주세요.\n\n또한, 패키지에 액세스하는 모든 프로젝트에 경로를 추가하는 코드도 포함되어야 하며, 실제로 패키지 위치를 하드코딩해야 합니다. 이 코드는 구성 파일로 옮길 수 있지만, 여러 프로젝트가 패키지에 액세스하고 패키지가 이동한다면 모두 업데이트해야 합니다.\n\n마지막으로, 이 접근 방식을 취하는 것은 패키지를 더 넓은 Python 커뮤니티에 배포하는 옵션을 방지한다는 가장 큰 단점입니다.\n\n홈/실행 폴더는 부모 폴더가 아닌 패키지 폴더이므로 pandas나 pgmpy와는 다르게 작동합니다. 그 이상으로, Python 커뮤니티가 수동 참조 코드를 포함하고 처리 시간이 느려진다는 것을 기대하는 것은 합리적이지 않습니다.\n\n더 나은 대안이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n사이트 패키지의 경로는 sys.path를 실행하여 쉽게 식별할 수 있습니다. 이미 제 컴퓨터에서는 C:\\Users\\GHarr\\anaconda3\\envs\\project-env\\lib\\site-packages임을 확인했습니다.\n\n파일 이름과 .pth 확장자가 붙은 파일을 생성한 다음, 해당 파일을 site-packages 폴더에 저장하면 Anaconda가 자동으로 읽어 시작 시 삽입할 패키지 경로를 추가할 수 있습니다.\n\n여기에 제 .pth 파일이 어떻게 생겼는지 알려드리겠습니다...\n\n![링크명](/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_3.png)\n\n<div class=\"content-ad\"></div>\n\n이 의미는 만약 ghtestpackage가 C:\\Users\\GHarr\\OneDrive\\Python Projects\\Packages의 하위 폴더로 생성된다면, 모든 Python 프로젝트의 검색 경로에 자동으로 포함되며 그 안에 포함된 모든 파일 및 클래스를 참조하고 가져올 수 있게 됩니다…\n\n- 한 번에 특정 경로를 모든 프로젝트에 하드 코딩할 필요가 사라집니다.\n- 또한, 컴퓨터 전체에서 Packages 경로에 대한 단일 참조만 있기 때문에 pandas 및 기타 인기 있는 패키지와 완전히 동일한 방식으로 작동합니다.\n- 만약 이 패키지가 GitHub을 통해 분산되고 pip 사용자가 이를 사이트 패키지 폴더에 다운로드한다면 계속해서 작동할 것입니다.\n\n모든 이러한 것들이 한 줄의 코드를 한 파일에 추가함으로써 해결됩니다!\n\n큰 문제가 해결되면 나머지 요구사항을 달성하는 것이 훨씬 쉬워질 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 남은 문제 해결하기\n\n## 패키지를 서브 폴더와 파일로 구성하기\n\n제가 소스 파일을 base_file.py 및 fisher_file.py로 명명하고 서브 폴더를 base_folder 및 fisher_folder로 명명한 것을 알아채셨을지도 모릅니다.\n\n소스 파일을 서브 폴더와 파일로 구성해야 하지만, 반대로 가져오기(import)할 때 이러한 이름을 사용하고 싶지는 않을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n예를 들어, 다음과 같이 import하는 것은 너무 길고 직관적이지 않습니다.\n\nfrom ghlibrary.algorithms.fisher_folder.fisher_file import Fisher\n\n더 나은 import 방법은 다음과 같습니다.\n\nfrom ghlibrary.algorithms import Fisher\n\n<div class=\"content-ad\"></div>\n\n하지만 코드를 폴더 및 파일로 계층적으로 분할하는 유연성과 패키지의 소비자에게 표시되는 가져오기를 여전히 제어할 수도 있습니다. 이것은 패키지 각 수준에 있는 **init**.py 파일을 통해 이루어집니다.\n\nghpackage\\algorithms\\fisher_folder부터 시작합니다. 여기에 있는 **init**.py 파일은 다음과 같습니다.\n\n```python\nfrom .fisher_file import Fisher\n```\n\n이는 전처리기에게 현재 폴더의 fisher_file.py를 찾도록 지시합니다. ( .은 현재 폴더에 대한 표기법입니다.) 그리고 Fisher 클래스는 이제 fisher_file이 아닌 fisher_folder에 직접 존재하며 해당 폴더에서 참조할 수 있습니다.\n\n이 시점에서 가져오기를 from ghlibrary.algorithms.fisher_folder import Fisher로 단축시킬 수 있으나 더 나은 방법이지만 완벽하지는 않습니다.\n\n<div class=\"content-ad\"></div>\n\n다음 단계는 알고리즘 폴더에 다음과 같이 **init**.py를 제공하는 것입니다...\n\n이전 단계에서 fisher_folder에 추가한 참조를 취하고, 이를 알고리즘 폴더에서 사용할 수 있도록 만듭니다.\n\nfrom .fisher_folder import Fisher은 전처리기에게 현재 폴더에서 시작 (.은 여기에서 시작하라는 뜻이며, algorithms에서 시작) 하고 fisher_folder로 한 수준 아래로 이동하여 Fisher 클래스를 가져오도록 지시합니다.\n\n패키지를 사용하는 소비자는 이제 다음을 사용할 수 있습니다 -\n\n<div class=\"content-ad\"></div>\n\nghlibrary.algorithms에서 Fisher를 가져와주세요.\n\n베이스 클래스를 마무리하기 위해서 동일한 처리가 필요합니다.\n\nghpackage\\common 하위 폴더의 **init**.py 파일은 다음과 같습니다...\n\n그리고 common 폴더에 추가 하위 폴더가 없기 때문에 그게 전부입니다.\n\n<div class=\"content-ad\"></div>\n\n소비자 분들은 다음과 같이 참조할 수 있어요...\n\n```python\nfrom ghpackage.common import BaseLearner\n```\n\n... 심지어 BaseLearner 클래스가 base_file.py에 저장되어 있는 경우에도요.\n\n하위 폴더와 파일로 계층적으로 구성된 프로젝트 클래스들은 이제 클라이언트가 다음과 같이 참조할 수 있어요...\n\n<div class=\"content-ad\"></div>\n\n아직 풀지 않은 수수께끼가 하나 있습니다. Fisher 클래스의 소스 코드를 살펴보십시오...\n\n문제는 Fisher()가 ghtestpackage/algorithms/fisher_folder/fisher_file.py에 있고 ghtestpackage.common에서 from ghtestpackage.common import BaseLearner를 사용하여 BaseLearner를 가져오는 방법입니다.\n\nBaseLearner는 ghtestpackage/common/base_file.py에 위치해 있고 따라서 ghtestpackage.common에서 from ghtestpackage.common import BaseLearner를 사용하는 것은 동작하지 않아야 합니다.\n\n동작하는 이유는 .pth 파일을 사용하여 Anaconda에 C:\\Users\\GHarr\\anaconda3\\envs\\project-env\\lib\\site-packages를 포함하도록 알려주었기 때문입니다. 이 경로는 ghtestpackage의 상위 폴더입니다. 따라서 ghtestpackage.common에서 BaseLearner를 참조하는 것은 패키지의 상위 폴더에서 해결됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 단계에서는 \"완벽한 패키지 프로토타입\"이 생성되었고, 남은 일은 그것이 예상대로 작동하는지 증명하기 위해 철저히 테스트하는 것입니다.\n\n# 완벽한 패키지 프로토타입 테스트\n\n증명 및 테스트를 시작하려면 ghtestpackage/common/base_file.py의 BaseLearner() 및 ghtestpackage/algorithms/fisher_folder/fisher_file.py의 Fisher()의 소스 코드를 확인해보세요...\n\nBaseLearner()이 서로 다른 폴더 위치의 여러 알고리즘을 위한 기본 클래스가 될 것이라는 아이디어 때문에, 이 코드는 common 폴더로 분리되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이번 데모에서 기본 클래스에는 서브 클래스에서 호출할 수 있어야 할 단일 메서드가 있습니다. 각 클래스는 호출되었음을 증명하기 위해 icecream을 사용하여 디버그 메시지를 출력합니다.\n\n## 주피터 노트북 클라이언트 테스트\n\nghtestpackage\\notebooks 폴더에는 ghtest_notebook.ipynb라는 주피터 노트북이 있습니다. 이는 \"완벽한 패키지\" 구조가 패키지 코드를 개발하고 테스트하는 데 도움이 되는 노트북을 포함할 수 있어야 한다는 요구 사항을 충족하기 위한 것입니다.\n\n노트북에 있는 코드는 다음과 같습니다...\n\n<div class=\"content-ad\"></div>\n\n그리고 이것이 출력 결과입니다...\n\n```js\nic | \"BaseLearner.init\";\nic | \"BaseLearner.init\";\nic | \"fisher.init\";\nic | \"BaseLearner.test\";\n```\n\n...결과적으로 베이스 클래스와 알고리즘 클래스를 참조하고 인스턴스화하고 호출할 수 있다는 것을 증명했습니다.\n\n또한 주의할 점은 완전히 별도의 프로젝트에 노트북이 생성되고 위의 코드가 셀에 입력된 경우에도 모든 작업이 올바르게 수행된다는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## pytest 클라이언트 테스트 중\n\n여기에는 ghtestbackage/unit_tests/test_base/test_base.py에 저장된 기본 클래스를 테스트하기 위한 코드가 있습니다...\n\n... 그리고 여기에는 ghtestbackage/unit_tests/test_fisher/test_fisher.py의 코드가 있습니다.\n\n다시 말하지만, .pth 파일의 지시에 따라 패키지의 부모 폴더를 시스템 경로에 추가했기 때문에 참조가 작동합니다.\n\n<div class=\"content-ad\"></div>\n\n최종 증명은 VS Code의 단위 테스트 패널로 이동하여 단위 테스트가 식별되고 발견되었는지 확인하고, 모두 실행하여 오류 없이 완료되는지 확인하는 것입니다...\n\n![image](/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_4.png)\n\n이것은 제안된 완벽한 패키지 프로토타입의 주요 이점 중 하나입니다 —\n\n# 문서 작성\n\n<div class=\"content-ad\"></div>\n\n마지막 요구 사항은 문서 생성과 관련이 있습니다. 한 가지 방법은 모듈, 클래스 및 함수에 포괄적인 독스트링이 있는지 확인하는 것입니다(이 스텁은 VS Code에서 자동으로 생성될 수 있습니다).\n\n다음은 잘 구조화되고 포괄적인 독스트링의 예시입니다…\n\n이러한 독스트링은 작성하는 데 시간이 걸리지만 메서드의 기능, 매개변수, 반환 값, 호출 방법 예시 및 맥락적인 참고 사항을 완벽하게 기록합니다.\n\n이 접근 방식을 모든 소스 코드에 적용하면 pydoc를 사용하여 HTML 형식의 도움말 파일을 자동으로 생성하는 것이 쉬워집니다.\n\n<div class=\"content-ad\"></div>\n\n제안된 프로젝트 프로토타입에는 makedocs.bat 파일이 포함된 docs라는 폴더가 있습니다...\n\n```js\npython -m pydoc -w \"..\\algorithms\\fisher_folder\\fisher_file.py\"\npython -m pydoc -w \"..\\common\\base.py\"\n```\n\n해야 할 일은 단순히 docs 폴더에서 makedocs를 실행하면 docstrings에서 자동으로 문서가 생성됩니다...\n\n![이미지](/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_5.png)\n\n<div class=\"content-ad\"></div>\n\n많은 다양한 방법으로 도움 파일을 작성하고 생성할 수 있지만 여기서는 pydoc을 선택했습니다. pydoc은 코드와 클래스의 목적과 사용을 설명하는 문서를 빠르고 쉽게 생성할 수 있는 방법을 제공하기 때문입니다.\n\n# 마지막으로 — 디버깅 테스트\n\n현재 제안된 것은 거의 작동하지만 한 가지 더 있습니다...\n\n유닛 테스트가 실패하거나 예상치 못한 결과를 내는 경우 디버깅이 필요할 수 있으며 이 구성에서는 VS Code에서 유닛 테스트 내에서 디버깅을 시작하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n디버깅을 시도하면 \"Invalid message: Found duplicate in \"env\": PATH\"와 함께 프로세스가 크래시됩니다.\n\n오류 대화상자에서는 launch.json을 열 수 있는 옵션이 제공되며, 이를 통해 .vscode 폴더에 다음과 같이 보이는 launch.json 파일이 생성됩니다.\n\n그런데 여전히 제대로 작동하지 않습니다. 목적 속성을 설정하기 위해 추가로 라인을 추가해야 합니다.\n\n여기에 파이널 프루프가 있습니다 — pytest 단위 테스트인 test_fisher_3()가 디버깅되고 있으며, 곧 \"test_fisher\"를 커맨드 라인에 출력할 예정입니다...\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_6.png\" />\n\n# 결론\n\n어떤 프로그래밍 프로젝트도 잘 구조화되고 잘 작성된 코드에 의존하여 코드가 효율적으로 구축되고 예기치 않은 결과를 출력할 때 효과적으로 디버깅 및 수정할 수 있습니다.\n\n또한, 원래 프로그래머거나 다른 사람에 의해 언젠가는 코드를 변경해야 할 것이고 미래 관리 가능성의 효과성을 위해서는 잘 작성된 코드가 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n그럼, 코드를 포함하는 프로젝트가 잘 구조화되고 조직화되어 훌륭한 코드를 제공할 수 있는 기반이 제공되기 때문에 이것이 가능합니다.\n\n프로젝트가 1000줄 이상을 포함하는 경우, 여전히 상대적으로 적은 규모이지만, .py 파일을 여러 개에 걸쳐 지능적으로 분산하지 않으면 VS Code가 느려지고 린팅, 코드 개요 및 타입-어헤드와 같은 기능이 제대로 동작하지 않을 수 있습니다.\n\n이 모든 것의 핵심은 첫 번째 코드 줄을 작성하기 전에 프로젝트 구조를 올바르게 가져야 한다는 것입니다.\n\n이 기사는 요구 사항의 집합을 설명하여 테스트를 통해 이러한 요구 사항을 충족하는 레이아웃 및 구조를 제시함으로써 \"완벽한\" 패키지 구조에 대한 한 제안을 제시했습니다.\n\n<div class=\"content-ad\"></div>\n\n주요 도전 과제는 전문적인 네임스페이스의 최상의 폴더 및 파일 레이아웃을 통합하고 클래스가 .pth 파일을 구성하고 **init**.py 파일의 항목을 통해 일반 프로젝트, Jupyter 노트북 및 pytest 단위 테스트로 가져올 수 있도록 하는 것이었습니다.\n\n파이썬 프로젝트용 완벽한 패키지 프로토타입에 대한 다른 제안이 많을 수 있겠지만, 이 프로젝트는 매우 잘 작동합니다.\n\n# 추가 섹션: 프로토타입 빠르고 쉽게 사용하기...\n\n패키지 프로토타입 프로젝트는 다음 링크를 통해 GitHub에서 다운로드할 수 있습니다...\n\n<div class=\"content-ad\"></div>\n\n... 그리고 다음 단계를 따릅니다...\n\n- 프로젝트를 다운로드하세요.\n- 이를 개발 패키지를 저장할 부모 폴더 바로 아래에 위치시킵니다.\n- docs 하위 폴더에서 mypackages.pth 파일을 Anaconda 설치의 site-packages 폴더로 복사합니다.\n- mypackages.pth 파일을 편집하여 로컬 컴퓨터에서 만든 경로로 대체합니다.\n- Jupyter Notebook 및 pytest 유닛 테스트를 실행하여 참조 및 네임스페이스를 확인합니다.\n- 뼈대 프로젝트를 복사하여 Fisher() 및 BaseLearner()를 실제 클래스로 대체하고 제품 코드로 배치를 시작합니다.\n\n만약 로컬 컴퓨터에서 site-packages 폴더가 어디에 있는지 모호하다면, 간단히 sys를 import하고 sys.path 명령을 실행하여 다음과 유사한 경로를 찾아보세요...\n\n```js\nC:\\Users\\GHarr\\anaconda3\\envs\\project-env\\lib\\site-packages\n```\n\n<div class=\"content-ad\"></div>\n\n# 소통하고 연락하기...\n\n이 기사가 마음에 드셨다면 제 소식을 받아보려면 팔로우해주세요.\n\n인과 추론(저의 다른 최근 기사 참조)에 대한 생각 또는 견해가 있다면 언제든지 연락주세요. 이 흥미로운 새로운 데이터 과학 분야가 어디로 향하고 있는지 알려주세요. 메시지를 남겨주시면 저에게 연락드리겠습니다.\n\n제 이전 기사는 제 연구와 인과 추론과 관련된 모든 내용이 담겨 있는 제 블로그(The Data Blog)에서 확인하실 수 있습니다. 감사합니다.\n","ogImage":{"url":"/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_0.png"},"coverImage":"/assets/img/2024-06-19-AProposedPerfectPackagePrototypeforPythonProjects_0.png","tag":["Tech"],"readingTime":14}],"page":"55","totalPageCount":156,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}