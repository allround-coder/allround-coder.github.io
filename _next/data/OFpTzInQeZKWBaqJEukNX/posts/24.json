{"pageProps":{"posts":[{"title":"의견 SSR 프론트엔드 유닛 테스트의 가치가 없는 이유","description":"","date":"2024-06-22 04:49","slug":"2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt","content":"\n\n## 프로덕션에서 개발자들이 직면하는 문제와 해결 방법 강조\n\n안녕하세요! 제 경력 동안 프론트엔드 개발자로 일하면서 테스트가 우선순위인 프로젝트에 참여한 적이 없다는 사실을 알게 되었습니다. 사실, 테스트가 필수적이지는 않았죠.\n\n일반적인 작업 흐름은 기획, 개발, QA, 소유주 확인, 그리고 릴리스로 진행되곤 했습니다. 결과가 항상 100% 버그 없는 것은 아니었지만, 문제는 개발과 크게 상관이 없는 경우가 많았습니다. 즉, 계획 단계에서 사용자의 요구사항을 사전에 고려하지 않았거나 릴리스 이후에 최종 사용자가 특정 기능에 부정적인 피드백을 준 경우였습니다. 어쨌든, 가장 필요한 것은 최고 속도로 지속적인 배포를 보장하는 것이었습니다.\n\n이런 환경에서 우리의 상황은 아마 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![OpinionSSRFront-EndUnitTestingNotWorthIt](/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_0.png)\n\n# 요약\n\n먼저, 문서에서 나온 프론트엔드 단위 테스트는 쓸모없고, 많은 비용이 들면서 아무것도 알려주지 않는다는 문제를 강조합니다. 그런 다음, 실제로 우리가 생산 중에 직면한 문제들은 우리가 원하는대로 렌더링되지 않거나, 우리가 원하는 방식으로 렌더링되지 않는 것이었습니다. 따라서, 실제 문제에 어떻게 대처할 수 있는 지를 제안하였는데, 그것은 간단한 if문일 것입니다. 마지막에, '나쁜 것'을 개선하는 방법과 제안된 해결책의 '추악한 점'을 개선하는 방법을 논의합니다. 거의 비전적인 개선 사항으로 글을 마무리합니다.\n\n# 면책조항\n\n\n<div class=\"content-ad\"></div>\n\n아래에 있는 내용이 모두 해당되지 않을 수 있지만, FAANG-tier 레거시 프로젝트와 같은 경우 상품 카드가 여러 가지 상태를 가진 프로토타입 디자인 시스템 또는 백엔드가 아닌 프런트엔드에 매우 복잡한 비즈니스 로직을 가진 경우를 제외하고, 저는 꼭 확인하거나 여러분이 아는 프런트엔드 개발자와 공유하길 강력히 권장합니다.\n\n# 문제\n\n솔직히 말해서, useRef()에 대해서는 문서가 꽤 확장되었지만, 이 개념에 완전히 동의합니다. 모든 문서 조각을 보면(특히 React에서), 많은 '하면 안 되는 일'과 충분한 '해야 하는 일'이 없다는 생각이 듭니다. 똑같이 프런트엔드 테스팅 예제에 대해서도 마찬가지입니다. 그들이 보여주는 것은 컴포넌트 렌더링 여부를 확인하는 것뿐인 \"안녕, 세계\" 수준의 테스트일 뿐입니다.\n\n이로 인해 우리는 토론의 핵심으로 도달했습니다. 프런트엔드 앱에서 무엇을 테스트해야 할까요? 더 구체적으로 말하면, SSR Gatsby 웹사이트에서는 무엇을 테스트해야 할까요? 백엔드가 없지만 Storyblok CMS와 Bitbucket의 CI/CD를 가진 경우 빌드가 실패하면 배포가 방지되는 상황에서요.\n\n<div class=\"content-ad\"></div>\n\n기술 슈퍼바이저가 나에게 일반적으로 요구하는 것은 유닛 테스트였어요. 왜냐하면 그것들이 가장 “인기가 많고” “관리하기 쉽다”고 생각하기 때문이에요. 이 아이디어의 핵심은 당신이 개발하고 컴포넌트를 독립적으로 테스트하고, 그것이 정상적으로 작동하고 올바르게 동작하는지 확인하는 것이에요. 하지만 프런트엔드 유닛 테스트는 우리에게 무엇을 보여줄까요? 공식 문서와 몇 가지 관련 기사를 살펴보도록 하죠.\n\n# React에서 테스트에 대해 전문가들은 무엇을 말하나요?\n\n## React 문서 (테스팅 개요 — 테스팅 섹션은 오직 레거시 문서만 포함됨):\n\n- 컴포넌트 트리 렌더링\n- 완전한 앱 렌더링\n\n<div class=\"content-ad\"></div>\n\n## Jest 문서\n\n- 스냅샷 테스트\n- DOM 테스트\n\n## React 테스트 레시피 (이전 문서만)\n\n- 렌더링\n- 데이터 가져오기\n- 모듈 모의\n- 이벤트\n- 타이머\n- 스냅샷 테스트\n\n<div class=\"content-ad\"></div>\n\n요약하면, FreeCodeCamp의 \"리액트에서 유닛 테스트 작성 방법\"이라는 기사는 테스트를 다음과 같은 카테고리로 나누어 내는 뛰어난 일을 해냅니다:\n\n- 컴포넌트가 props와 함께 렌더링되는지 여부\n- 컴포넌트가 상태 변경과 함께 어떻게 렌더링되는지\n- 컴포넌트가 사용자 상호작용에 어떻게 반응하는지\n\n이 시점에서 저는 이 기사를 읽기 전에도 거의 동일한 분류를 구성하고 있었기 때문에 진짜 궁금증이 생겼습니다. 우리는 다음을 확인하려고 노력하고 있기 때문입니다:\n\n- 컴포넌트가 렌더링되는지 여부\n- 컴포넌트가 영향을 받는지 여부\n- 컴포넌트가 상호작용 가능한지 여부\n\n<div class=\"content-ad\"></div>\n\n\n![OpinionSSRFront-EndUnitTestingNotWorthIt_1.png](/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_1.png)\n\n나는 각 시나리오에서 완전히 헷갈려 하고 있다. 어쩌면, 너무 복잡하고 오버로드된 앱들에서는 개발 중에 적어도 하나의 질문에 대답할 수 없을지도 모르지만 나는 localhost에서 이러한 질문 중 어느 하나에도 갇히지 않아서 운이 좋은 편이다. 그래서, 중요한 것에 대답하지 못하는 단위 테스트가 왜 필요한지 묻고 싶다. 내 현재 프로젝트를 살펴보고 무엇을 확인하는 데 유용할지 찾아보자.\n\n# 실제로 생산에서 어떤 문제가 발생하나요?\n\n위에서 언급한 대로, 문서에 실제 시나리오가 있었으면 좋겠지만, 그럴 만한 것이 없다면 웹사이트에서 만난 문제들을 분류해보려 한다.\n\n\n<div class=\"content-ad\"></div>\n\n## 컴포넌트가 필요한 일부 props을 받지 못하고 렌더링됩니다.\n\n- 세부 정보: 예를 들어, 우리가 타이틀 필드를 가진 HeroSection.tsx를 가지고 있으며, 이 컴포넌트에는 수직 여백이 몇 개 있습니다. CMS에서 아무 것도 오지 않아도 프로젝트가 크래시되지 않습니다.\n- 결과: 텍스트 없이 여백이 렌더링됩니다.\n- 영향: 사용자 경험이 나빠지며, 보기 좋지 않습니다.\n\n## 컴포넌트가 필요한 props을 받지 못하고 빌드가 충돌합니다.\n\n- 세부 정보: 상품명과 가격이 있는 항목을 기대하면서, 어떤 확인도 없이 (조건부 체이닝조차 없이!) 단순히 items.map()을 사용합니다.\n- 결과: 빌드가 파이프라인에서 충돌하고, 프로덕션에서 아무 변화도 없습니다.\n- 영향: 나쁜 개발 경험, 기능 배송이 느려집니다.\n\n<div class=\"content-ad\"></div>\n\n에러가 발생하면 \"RENDERED vs. NOT RENDERED\" 범주 내에 해당하는 문제에 직면하게 될 수 있어요. 상태 변경이나 사용자 조작에 응답하지 않는 것을 배포한 적이 없다는 건 저에게는 생각조차 해본 적이 없었어요. 만약 이와 같은 것을 개발하고 QA가 이를 식별하지 못한다면, 아마도 테스트도 도움이 되지 않을 것이고 이 글도 솔직히 도움이 되지 않을 겁니다. 하지만, 아주 복잡한 마이크로 프론트엔드 구성요소가 설계 키트 없이 연결돼 있는 경우...\n\n![2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_2.png](/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_2.png)\n\n# 어떻게 실제 문제에 대처할 수 있을까요?\n\n사람들이 처음으로 제안하는 것은 \"단위 테스트를 작성하세요!\"라고 말하는 것인데, 음... 아닙니다.\n\n<div class=\"content-ad\"></div>\n\n여기에는 \"아니요\"에 대한 몇 가지 층이 있어요:\n\n## 개발 속도가 두 배 이상 느려집니다. 문서의 공식 예제를 한 번 더 살펴보세요.\n\n- 구성요소 8줄 vs 테스트 30줄 Testing Recipes — React\n- 구성요소 24줄 vs 테스트 18줄 Testing React Apps · Jest\n\n## Storyblok 데이터 구조를 모의하는 것이 정말 어려워요.\n\n<div class=\"content-ad\"></div>\n\n- CMS에서 많이 사용되는 richtext입니다. 게다가 Storyblok의 rich text를 역공학하여 성공하더라도 즉시 긴밀하게 결합된 레거시가됩니다. 이 span의 피라미드는 외부에서 업데이트 할 때마다 깨질 수도 있습니다. 또는 다른 방식을 채택한 다른 CMS로 이동할 수도 있습니다.\n- 내부 및 \"중첩\" 루트가 있는 외부의 링크, 일반 URL이 있는 외부의 링크, 2가지 매우 다른 구조를 테스트하기 위해 가장하는 것입니다. 우리가 받는 것을 알 수 없기 때문에.\n- 파일 및 이미지와 같은 에셋. 예를 들어 .svg. 이상적인 항목이지만 .png의 사용량 감소와 테스트의 구식화 문제를 줄이려고 일주일에 한 번 디자인 리뷰를 할 수 있습니다.\n\n## 그리고 가장 중요한 것\n\n하나의 컴포넌트에 \"필요한\" props로 테스트를 통과시키고 props 없이 실패한다는 것은 본질적으로 아무 것도 해결하지 않습니다. 우리는 페이지를위한 템플릿을 개발하기 때문에 가능한 한 재사용 가능해야하며 모든 props을 필수로 만드는 것은 유연성을 깨뜨리게 될 것입니다.\n\n다행히도 \"RENDERED vs. NOT RENDERED\" 이분법에 접근할 \"저렴한\" 방법이 있습니다. 모든 컴포넌트에 \"필수\" props을 검사하는 if 문만 있으면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_3.png)\n\n만약 CMS에서 무언가를 기대하고 있지만 컴포넌트가 전혀 렌더링되지 않길 원한다면, null을 반환하면 됩니다. 이것은 말 그대로 빙산의 일각에 불과합니다.\n\n## if 문장만으로 충분할까요?\n\n이미 반복된 느낌일 수 있지만, 사실 \"아니요\". 이번에는 \"아니지만 충분합니다.\" \n\n\n<div class=\"content-ad\"></div>\n\n그러면 결과에 대해 알아봅시다:\n\n## 장점\n\n- 실제 유닛 테스트보다 \"가장 체\"하고 훨씬 간결합니다.\n- 일부 이상하게 보이는 컴포넌트를 렌더링하지 않습니다. (그리고 CSS 접근 방식에 따라 공백도 표시되지 않을 수도 있습니다.)\n\n## 단점\n\n<div class=\"content-ad\"></div>\n\n- 서투르고 반복적인 부분이 있습니다.\n- 모든 구성 요소가 아닌지 확인할 수 없습니다. (이것에 대해 생각해 보세요!)\n\n## 문제점\n\n- 컴포넌트가 렌더링되지 않을 때, 그것이 괜찮은지 아닌지 말할 기회가 없습니다. 예를 들어, 콘텐츠 작성자가 버튼 레이블을 빠뜨린 경우 컴포넌트가 사라지고 전체적인 모습은 괜찮아 보일 수 있지만 페이지의 일부가 없으며 5개 언어로 번역된 수십 개의 페이지에 대해 사이트 전반적으로 디버깅할 수 없습니다. 하지만, 유닛 테스트에서도 정확히 같은 상황이 발생합니다!\n\n# 문제를 개선하는 방법\n\n<div class=\"content-ad\"></div>\n\n## DRY 원칙 적용\n\nDRY(Don't Repeat Yourself) 원칙부터 적용해보겠습니다. 재사용이 가능하도록 함수를 작성해야 합니다. 함수는 다음과 같은 작업을 수행해야 합니다:\n\n- 우리가 전달하는 속성들이 무엇이고 그 수가 몇 개인지 예측할 수 없으므로 객체 기반으로 일반화되어야 합니다.\n- 각 필드 값에 대해 반복합니다.\n- 이러한 값들이 모두 허용 가능한지 확인합니다. 가능한 데이터 구조를 예측하지 못할 경우 업데이트해야 할 수도 있습니다.\n- 값 중 일부가 “필수” 기준을 충족하지 못하는 경우 false를 반환합니다.\n- 모든 필드/속성이 \"채워져\" 있다면 true를 반환합니다.\n\n제 구현은 아마도 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_4.png\" />\n\n배열 확인을 더 자세히 살펴보세요. 이미지와 Storyblok 유형에 대해서는 if 문을 확장해야 할 필요가 있겠지만, 방향은 명확해야 합니다. 개발 환경에서 컴포넌트 이름을 두 번째 매개변수로 전달하고 부족한 필드 이름을 로깅하여 컴포넌트 이름별로 그룹화하는 방법으로 더 개선할 수 있지만, 지금은 간단하게 유지하는 것을 선호합니다.\n\n이제 우리는 모든 컴포넌트에 쉽고 일관된 if 문을 갖게 됩니다:\n\n<img src=\"/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_5.png\" />\n\n\n<div class=\"content-ad\"></div>\n\n# 만약 팀이 이를 무시한다면?\n\n이를 실제로 사용할 수 있도록 하기 위해, 우리는 마침내 의미있는 테스트가 필요합니다. 나는 jest.spyOn(object, methodName)을 사용하는 것을 제안합니다. 테스트 구현은 필요하지 않지만, 모든 컴포넌트에서 해당 메소드가 호출되었는지 확인해야 합니다.\n\n# UGLY를 개선하는 방법들\n\n모든 것이 올바르게 처리되면, CMS로부터 \"필수\" 필드가 부족한 컴포넌트를 렌더링하지 않는 설정이 있습니다. 하지만 유연성이 필요한 몇몇 컴포넌트와 일반 CMS의 유연성 때문에 이러한 \"필수\" 필드를 누락할 수 있으며 결과적으로 페이지에 컴포넌트가 없을 수 있습니다. 모든 것이 작동하나, 요금제가 없는 요금 페이지는 많은 의미가 없습니다. 특히 수십 개의 페이지와 5개 국가 언어에 대한 로컬라이제이션을 진행하는 다른 부서들이 작업 중인 상황에서는 상황이 더욱 복잡해집니다.\n\n<div class=\"content-ad\"></div>\n\n내가 제안하는 해결책은 뛌륭하진 않지만, 각 구성요소에 대해 if 문과 똑같이 작동합니다. 메커니즘은 다음과 같이 동작합니다:\n\n- onCreatePage() 사이클 동안 Storyblok에서 오는 모든 필드를 확인합니다.\n- 각 필드가 비어 있는지 확인하고 \"filled\" 및 \"empty\" 값으로 결과를 객체에 저장합니다.\n- 페이지 별로 필터링할 수 있는 간단한 디버그 페이지를 만들고 \"all\" 및 \"empty\" 값으로 전환할 수 있는 기능을 추가합니다.\n\n이 메커니즘을 구현하는 것이 너무 복잡하기 때문에 이 디버거의 CLI 버전만 쇼케이스하겠습니다:\n\n![디버거 CLI 버전](/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_6.png)\n\n<div class=\"content-ad\"></div>\n\n이제 내용 작성자들은 무엇이 빠졌는지 명확히 알아보고 그것이 우연히 빠진 것인지 확인할 수 있습니다. 그런 다음 필요하다면 Storyblok에 가서 해당 필드를 수정하면 됩니다.\n\n![이미지](/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_7.png)\n\n결론  \n여기까지 읽어 주셔서 감사합니다! 프론트엔드에 대한 무분별한 유닛 테스트가 좋지 않다는 것을 설득하지는 못했더라도 적어도 SSR 생성 웹사이트의 테스트 접근 방식을 고민해 볼 수 있기를 바랍니다.\n\n다음 단계를 생각해보면 디버거를 자동화해 보고 싶습니다. 최신 빌드 이후 \"비어 있는\" 필드가 되어버린 콘텐츠 변경을 기업의 슬랙 채널에 보고하는 차이 시스템을 설정해야 합니다. 이상적으로는 CMS의 마지막으로 게시된 페이지에 대한 보고서를 생성하여 해당 페이지의 비어 있는 필드를 강조해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n위 내용을 공유하고 다시 게시해주세요. 전면 테스트에 대한 나의 비전통적인 접근에 대한 비평과 토론을 환영합니다. 그리고 절대 잊지 말아 주세요. 당신이 하는 모든 것은 어떤 의미가 있어야 합니다. \"최상의 사례\"를 따르는 것이 허용되지 않습니다. 그것이 전면 응용 프로그램의 단위 테스트인 경우에 특히 그렇습니다.\n\n# 솔직하게 설명하자면 🚀\n\nPlain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n- 저자를 박수하고 팔로우하는 것을 잊지 마세요! ️👏️️\n- 저희를 팔로우하세요: X | LinkedIn | YouTube | Discord | 뉴스레터\n- 다른 플랫폼 방문: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io에서 더 많은 콘텐츠를 확인하세요","ogImage":{"url":"/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_0.png"},"coverImage":"/assets/img/2024-06-22-OpinionSSRFront-EndUnitTestingNotWorthIt_0.png","tag":["Tech"],"readingTime":9},{"title":"동시성 및 병렬 처리 입문 방법 배우기","description":"","date":"2024-06-22 04:48","slug":"2024-06-22-IntroductiontoConcurrencyandParallelism","content":"\n\n\n![Introduction to Concurrency and Parallelism](/assets/img/2024-06-22-IntroductiontoConcurrencyandParallelism_0.png)\n\n# 소개\n\n소프트웨어 개발자는 동시성과 병렬성을 사용하여 고성능 시스템을 구축할 수 있습니다. 이는 모든 프로그래머가 활용할 수 있는 중요한 도구입니다.\n\n이는 다중 스레딩 및 다중 처리가 설명된 동시성과 병렬성에 관한 이론적인 노트입니다. 이것은...\n","ogImage":{"url":"/assets/img/2024-06-22-IntroductiontoConcurrencyandParallelism_0.png"},"coverImage":"/assets/img/2024-06-22-IntroductiontoConcurrencyandParallelism_0.png","tag":["Tech"],"readingTime":1},{"title":"파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지","description":"","date":"2024-06-22 04:46","slug":"2024-06-22-LinearRegressionfromScratch","content":"\n\n\n![Linear Regression](/assets/img/2024-06-22-LinearRegressionfromScratch_0.png)\n\n안녕하세요! 가장 간단한 머신 러닝 기술 중 하나인 선형 회귀로 시작합니다. 이 게시물의 수학적 부분은 선형 대수와 미적분의 좋은 이해력이 필요할 것입니다. 이 부분은 다음 시리즈에도 해당될 것이죠. 이는 머신 러닝의 많은 부분을 뒷받침하고 있고, 깊은 이해를 위한 선행 요건입니다. 그렇다면 함께 알아보도록 하죠!\n\n# 단순 선형 회귀\n\n선형 회귀는 여러 점들을 고려하여 최적의 선을 찾는 과제입니다. 최적의 선을 찾는 방법을 알아내기 전에, 이것이 실제로 무엇을 의미하는지를 이해해야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n그림 1의 선이 이 선보다 더 데이터에 잘 맞는 것을 직관적으로 알 수 있어요:\n\n![Figure 1](/assets/img/2024-06-22-LinearRegressionfromScratch_1.png)\n\n왜냐하면 그림 1의 점들이 그림 2의 점들보다 선으로부터 더 멀리 떨어져 있어요. 이 직관을 수학적으로 어떻게 형식화할지 알아보도록 할게요. 이렇게 하면 “가장 잘 맞는”이 무엇을 의미하는지 명확하게 정의할 수 있을 거예요.\n\n간단하고 시각화하기 쉽게 하기 위해, 2차원에서 시작할게요. 이 경우 데이터 포인트는 (x, y) 쌍이고 위의 그림처럼 그래프에 표시할 수 있어요. 우리는 데이터를 가장 잘 나타내는 f(x) = kx와 같은 선형 함수를 찾고 싶어해요. 이 모델은 원점을 통과하는 선을 가정해요. 우리는 아직 원점 이외의 교차점의 가능성에 대해 고려하지 않을 거예요.\n\n<div class=\"content-ad\"></div>\n\nn개의 데이터 포인트가 있는 컬렉션이 있다고 가정해 보겠습니다.\n\n![Linear Regression from Scratch 2](/assets/img/2024-06-22-LinearRegressionfromScratch_2.png)\n\n각 x값에 대해 모델을 사용하여 예측된 y값을 얻을 수 있습니다. 이러한 상황은 하나의 독립 변수(x)와 하나의 종속 변수(y)만 있는 단순 선형 회귀로 알려져 있습니다. 예측된 y값과 실제 y값을 구분하기 위해 프라임 기호를 사용할 것입니다. 따라서 특정 x값에 대한 모델의 예측된 y값은 다음과 같은 공식으로 표시됩니다.\n\n![Linear Regression from Scratch 3](/assets/img/2024-06-22-LinearRegressionfromScratch_3.png)\n\n<div class=\"content-ad\"></div>\n\n이제 x 값들을 하나의 벡터에, y 값들을 다른 벡터에 넣어봅시다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_4.png)\n\n이를 벡터화(Vectorization)라고 합니다. 데이터 과학 문제에 대한 많은 이점이 있습니다. 여러 개별 값을 벡터로 결합하면 수학 공식이 훨씬 더 간결하고 이해하기 쉬워집니다. 코드에서의 벡터화도 성능을 향상시킵니다. 큰 값 배열에 대해 벡터 산술을 수행하는 것이 각각 하나씩 처리하는 루프를 거쳐 동작하는 것보다 훨씬 빠릅니다. Numpy와 같은 많은 숫자 계산 라이브러리가 빠른 벡터 산술을 위해 설계되었습니다. 벡터화는 또한 GPU와 같은 하드웨어를 사용한 병렬화도 가능하게 합니다. 여러 개의 배열 요소에 동시에 연산을 수행합니다. 한 번 더 언급하자면, 각 연산이 하나씩 차례로 이루어지는 루프를 사용하지 않고는 이것이 불가능할 것입니다.\n\n또한 우리가 예측한 y 값들을 모두 담은 벡터를 생성할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_5.png\" />\n\n<img src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_6.png\" />\n\n최적 적합 선을 찾기 위해서는 y'와 실제 값인 y의 벡터 사이의 거리를 알아야 합니다. 이 두 벡터의 차이를 살펴볼 수 있습니다: y' - y. 그러나 이것은 벡터 자체이며, 모델의 오류를 나타내는 단일 숫자를 원합니다. 우리는 제곱합 오류(SSE)를 사용할 것입니다. SSE는 ||y' - y||²로, 차이 벡터의 제곱 크기와 같습니다. 이것은 \"제곱합\"으로 불립니다. 왜냐하면 y' - y의 제곱된 항목들의 합과 같기 때문입니다:\n\n<img src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_7.png\" />\n\n<div class=\"content-ad\"></div>\n\n왜 ||y’ — y||²을 사용하는지 궁금하다면 단순히 ||y’ — y||만 사용하는 것보다 제곱 크기를 사용하는 게 훨씬 간단하다는 점이 하나의 답일 수 있습니다. ||y’ — y||는 합계 외부에 하나의 추가 제곱근 기호가 있습니다:\n\n\n<img src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_8.png\" />\n\n\n이로 인해 미분을 할 때 공식을 처리하는 것이 훨씬 더 까다로워집니다.\n\n이제 선형 회귀 모델의 오차를 정의했으니, 이를 최소화하는 방법을 찾아야 합니다. SSE에 대한 표현을 확장해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-LinearRegressionfromScratch_9.png)\n\n방정식 1에 따라 y'에 kx를 대입하면,\n\n![이미지](/assets/img/2024-06-22-LinearRegressionfromScratch_10.png)\n\nx와 y를 일정한 값으로 유지할 때 오차를 최소화하는 k의 값을 찾아야 합니다. 이를 위해서는 식 1의 k에 대한 미분값을 0으로 설정하고 해를 구할 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n\n![Linear Regression 1](/assets/img/2024-06-22-LinearRegressionfromScratch_11.png)\n\n![Linear Regression 2](/assets/img/2024-06-22-LinearRegressionfromScratch_12.png)\n\n이를 통해 제곱 오차의 합을 최소화하는 k 값을 알 수 있습니다. 이 지식을 바탕으로 SimpleLinearRegressor를 코딩할 수 있습니다. 이것은 단 하나의 인스턴스 변수를 갖게 될 것입니다 — 기울기, k.\n\n```js\nclass SimpleLinearRegressor:\n    \"\"\"단순 선형 회귀를 수행합니다.\"\"\"\n\n    def __init__(self):\n        self.k = None\n```\n\n<div class=\"content-ad\"></div>\n\n```python\ndef predict(self, x):\n    \"\"\"\n    입력값 x 또는 x 값 벡터를 사용하여 예측된 y값을 제공합니다.\n    :param x: 입력 값(들).\n    :return: 예측된 y값(들).\n    \"\"\"\n\n    if self.k is None:\n        raise RegressionModelNotFitError('앗! 모델이 아직 피팅되지 않았어요!')\n\n    return self.k * x\n```\n\n또한 x와 y 벡터를 사용하여 Equation 2를 기반으로 k의 적절한 값을 찾는 fit 메서드가 필요합니다. 이것이 클래스의 본질입니다.\n\n```python\ndef fit(self, x, y):\n    \"\"\"\n    주어진 x 값과 y 값 벡터를 기반으로 모델을 맞춥니다.\n    :param x: x값 벡터.\n    :param y: y값 벡터.\n    :return: 적합된 모델의 제곱 오차 합.\n    \"\"\"\n\n    self.k = x @ y / (x @ x)\n    diff = self.predict(x) - y\n    return diff @ diff\n```\n\n<div class=\"content-ad\"></div>\n\n모델을 테스트하기 위해 데이터를 생성해야 합니다. 범위 내에서 임의의 x 값을 생성하고 선형 모델을 사용하여 해당하는 y 값을 계산한 다음 이 y 값에 가우시안 노이즈를 추가하는 함수를 만들겠습니다.\n\n```js\ndef generate_noisy_data(n_points, slope, x_range, noise_stddev):\n    \"\"\"\n    추가된 가우시안 노이즈를 이용해 선형 관계에 기반한 데이터 점을 생성합니다.\n    :param n_points: 생성할 데이터 점의 수.\n    :param slope: 직선의 기울기.\n    :param x_range: x 값을 추출할 범위.\n    :param noise_stddev: 각 y 값에 추가할 가우시안 노이즈의 표준 편차.\n    :return: x 값과 y 값의 벡터.\n    \"\"\"\n\n    x = np.random.uniform(*x_range, n_points)\n    y = slope * x + np.random.normal(scale=noise_stddev, size=n_points)\n    return x, y\n```\n\nSimpleLinearRegressor가 무작위로 생성된 데이터에서 원래의 기울기를 얼마나 잘 복원하는지 살펴봅시다. 시각화를 위해 matplotlib를 사용하겠습니다.\n\n```js\nx_range = np.array([0, 5])\nx, y = generate_noisy_data(n_points=20, slope=0.42, x_range=x_range, noise_stddev=0.5)\nplt.scatter(x, y)\n\nregressor = SimpleLinearRegressor()\nfit = regressor.fit(x, y)\nslope = regressor.k\nplt.plot(x_range, [0, 2 * x_range[1]], color='red')\nplt.text(3, 0, f'오차: {\"{:.2f}\".format(fit)}\\n예측된 기울기: {\"{:.2f}\".format(slope)}')\nplt.show()\n```\n\n<div class=\"content-ad\"></div>\n\n아래는 한 번의 실행 결과입니다:\n\n![Linear Regression](/assets/img/2024-06-22-LinearRegressionfromScratch_13.png)\n\n보이시다시피, 이 모델은 훌륭한 작업을 합니다! 회귀 모델에 의해 예측된 기울기는 `generate_noisy_data`에 입력된 기울기와 소수점 셋째 자리까지 일치합니다.\n\n# 다중 선형 회귀\n\n<div class=\"content-ad\"></div>\n\n하나의 독립 변수 x와 하나의 종속 변수 y로 선형 회귀를 수행하는 방법을 배웠습니다. 이제 y가 m개의 독립 변수에 의존한다고 가정해 보겠습니다. 따라서 우리는 (m + 1)차원 데이터를 다루게 됩니다. 우리가 가진 데이터가 다음과 같은 n개의 데이터 포인트일 수 있습니다:\n\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_14.png)\n\n\n여기서 x_ij는 i번째 데이터 포인트에서 j번째 독립 변수의 값을 나타냅니다.\n\n데이터를 벡터화하여 정리하는 것은 항상 좋은 첫 번째 단계입니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 이전과 마찬가지로 모든 y 값들을 벡터로 모아낼 수 있어요:\n\n이제는 x 데이터가 두 개의 인덱스를 가지고 있기 때문에 xs에 대해 벡터를 사용하는 것이 더 이상이 아니에요. 대신, 각 행이 하나의 데이터 포인트인 행렬로 모아낼 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n이제부터 이 행렬의 항목을 나타내는 변수로 대문자 X_ij와 소문자 x_ij를 서로 바꿔 사용할 거에요.\n\n지금 데이터에 맞추려고 하는 선형 모델은 조금 더 복잡해 보여요:\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_17.png)\n\n독립 변수 각각에 대한 계수 또는 \"기울기\"인 βs가 있는 m개의 계수가 있어요.\n\n<div class=\"content-ad\"></div>\n\n각 데이터 포인트의 벡터를 만들 수 있어요.\n\n![Vector](/assets/img/2024-06-22-LinearRegressionfromScratch_18.png)\n\n행렬 X는 이러한 벡터를 각각의 행으로 갖고 있다고 생각할 수 있어요.\n\n![Matrix X](/assets/img/2024-06-22-LinearRegressionfromScratch_19.png)\n\n<div class=\"content-ad\"></div>\n\nβ 계수들의 벡터를 만들어보세요.\n\n\nEquation 3은 매우 간결하게 다음과 같이 표현될 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n하지만 각 예측 값 y'_i에 대한 방정식을 모두 예측 값의 벡터로 결합하여보다 간결하게 할 수 있습니다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_22.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_23.png)\n\n단순 선형 회귀와 마찬가지로, 우리는 제곱 오차의 합 ||y' - y||²를 최소화하려고 합니다.\n\n<div class=\"content-ad\"></div>\n\n등식 4를 사용하여 SSE 공식을 X, y, 그리고 β를 사용하여 확장할 수 있습니다.\n\n![Equation-24](/assets/img/2024-06-22-LinearRegressionfromScratch_24.png)\n\n익숙하게 느껴지나요? 이는 단순 선형 회귀에서의 오차 공식과 매우 비슷합니다. 우리는 그것을 최소화하는 β의 값을 찾아야 합니다. 먼저 ||y||² 항은 β에 영향을 미치지 않으므로 무시됩니다. 따라서 실제로 최소화해야 할 값은 다음과 같습니다.\n\n![Equation-25](/assets/img/2024-06-22-LinearRegressionfromScratch_25.png)\n\n<div class=\"content-ad\"></div>\n\n우리는 여기서 멈출 수 있습니다. Numpy에는 X와 y만 입력으로 사용하여 β의 적절한 값을 찾을 수 있는 numpy.linalg.lstsq 메서드가 있습니다. 기술적으로는 Python과 Numpy만 사용해야 한다는 내 규칙을 위반하는 것은 아니지만, 이것은 \"처음부터 선형 회귀\"에 대한 포스트에서 속임수 같아 보입니다. 대신, 수학적인 부분으로 들어가겠습니다.\n\nExpression 2를 최소화하기 위해 그래디언트를 β에 대해 제로로 설정하고 해결해야 합니다. 이를 위해 Expression 2를 구성별 형식으로 변환한 다음, β의 각 구성 요소에 대해 개별적으로 미분을 수행할 것입니다.\n\n점곱의 구성별 공식을 사용하여,\n\n<div class=\"content-ad\"></div>\n\n행렬-벡터 곱셈의 경우,\n\n![matrix-vector multiplication](/assets/img/2024-06-22-LinearRegressionfromScratch_27.png)\n\n식 2를 요소별 형태로 변환할 수 있습니다:\n\n![componentwise form](/assets/img/2024-06-22-LinearRegressionfromScratch_28.png)\n\n<div class=\"content-ad\"></div>\n\n이제 특정  β_l  컴포넌트에 대한 식 3의 미분을 취해 봅시다:\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_30.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_31.png)\n\n<div class=\"content-ad\"></div>\n\n식 4를 단순화하려면 두 합의 미분을 취해야 합니다:\n\n![식1](/assets/img/2024-06-22-LinearRegressionfromScratch_32.png)\n\n그리고\n\n![식2](/assets/img/2024-06-22-LinearRegressionfromScratch_33.png)\n\n<div class=\"content-ad\"></div>\n\n각각을 개별적으로 다루어 봅시다.\n\n식 5 미분\n\n식 5는 다음과 같이 확장할 수 있습니다:\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_34.png)\n\n<div class=\"content-ad\"></div>\n\n위의 내용을 번역해 드리겠습니다.\n\n영어로 된 내용은 \"j나 k 둘 중 하나가 l과 같지 않은 부분, 그리고 k는 l과 같지만 j는 아닌 부분, 그리고 j가 l과 같지만 k는 아닌 부분, 그리고 j와 k가 모두 l과 같은 부분\"을 뜻합니다. j와 k 둘 다 l과 같거나 같지 않아야 하기 때문에, 이 네 항목은 모든 가능성을 포함합니다. 이 모든 부분이 결합하여 Expression 5의 원래 합계와 동일합니다.\n\nExpression 7의 두 가운데 항목은 인덱스 변수의 이름이 다를 뿐 동일합니다 (j vs k). 이름이 임의적이므로 우리는 세 번째 합에 있는 인덱스 변수의 이름을 j로 변경할 수 있으며, 따라서 두 항목은 같은 값을 갖습니다. 따라서 식은 다음과 같이 다시 쓸 수 있습니다.\n\n<img src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_35.png\" />\n\n이제 미분을 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_36.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_37.png)\n\n첫 번째 항이 베타_l을 포함하지 않기 때문에 0이 됩니다.\n\n식 6 미분\n\n\n<div class=\"content-ad\"></div>\n\n식 6의 도함수를 찾는 것은 훨씬 간단합니다:\n\n![](/assets/img/2024-06-22-LinearRegressionfromScratch_38.png)\n\n![](/assets/img/2024-06-22-LinearRegressionfromScratch_39)\n\n여기에서 두 번째 합계를 다시 β_l을 포함하는 부분과 β_l을 포함하지 않는 부분으로 분할하였습니다. 후자는 미분 중에 0으로 만들어집니다.\n\n<div class=\"content-ad\"></div>\n\n모든 것을 합해 봅시다\n\n이제 방금 발견한 미분식, Expression 8과 9를 식 4에 대입하고 간단히 정리해 보겠습니다. 그런 다음, 구성 요소 형식에서 벡터 형식으로 다시 변환할 수 있습니다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_40.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_41.png)\n\n<div class=\"content-ad\"></div>\n\n이 시점에서 우리는 다음 항등식을 사용할 수 있습니다.\n\n\n![식1](/assets/img/2024-06-22-LinearRegressionfromScratch_42.png)\n\n\n이를 통해 방정식 5를 더 변형할 수 있습니다.\n\n\n![식2](/assets/img/2024-06-22-LinearRegressionfromScratch_43.png)\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_44.png\" />\n\n그러면 끝났어요! 에러의 기울기가 영인 경우 β는 반드시 방정식 6을 따라야 합니다.\n\n기울기를 영으로 설정하면 실제로 최적의 해결책을 보장하는지 궁금할 수 있습니다. 결국, 이는 단지 전역 최소값이 아닌 지역 최소값을 찾을 수도 있습니다. 다행히 선형 회귀는 볼록 최적화 문제입니다. 이 수학 스택 익스체인지 답변에서 증명이 제공됩니다. 볼록 최적화 문제의 중요한 특성 중 하나는 어떤 지역 최소값도 전역 최소값이 될 수 있다는 것이기 때문에 걱정할 것이 없습니다.\n\n솔루션이 올바름을 확인했으므로, 이제 방정식 6을 β에 대해 해결해야 합니다. Numpy에는 numpy.linalg.solve 함수가 제공되지만, 이 방정식이 하나의 솔루션만 있는 경우에만 작동합니다. 다른 옵션으로는 행렬을 축소된 행 사다리꼴 형태로 변환하는 것이 있지만, 놀랍게도 Numpy에는 이를 위한 유틸리티가 없습니다. 일부 조사를 한 결과, numpy.linalg.qr이라는 것을 발견했는데, 이 함수는 입력 행렬의 QR 분해를 수행합니다. 수학 스택 익스체인지의 답변 및 그 댓글이 방정식 풀이에 QR 분해를 사용하는 방법을 배우는 데 도움이 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n만약 A가 정사각 행렬이고(X^TX도 정사각이어야 함), 선형 방정식 Ax = b를 해결하려면 먼저 직교하는 정사각 행렬 Q와 상삼각 행렬 R을 찾아야 합니다. 여기서 QR = A가 성립합니다. Ax = b 방정식은 QRx = b로 변환됩니다. Q가 역행렬이어야 하므로 방정식은 Rx = Q^-1b로 더욱 단순화될 수 있습니다. R은 상삼각 행렬이고, 오른쪽 부분은 단순히 벡터이므로 Uv = w를 해결할 수 있는 능력이 있으면 충분합니다.\n\n저는 작업 수행을 위해 solve_upper_triangular 함수를 만들었습니다. 선형 방정식을 해결하는 방법에 대해서는 자세히 설명하지 않겠습니다. 단순히 행의 마지막에서 시작하여 역방향으로 작업하며, 각 행에서 이전에 설정된 변수 값들을 대체하고, 남은 변수 중 계수가 0이 아닌 변수에 대해 하나를 제외한 모든 변수에 값을 1로 할당하고, 다른 변수들에 대한 마지막 변수를 나머지 변수들을 이용하여 구합니다.\n\n```js\ndef solve_upper_triangular(a, b):\n    \"\"\"\n    선형 방정식 ax = b를 x에 대해 해결합니다.\n    :param a: 크기가 n x n인 상삼각 행렬.\n    :param b: n 차원 벡터.\n    :return: ax = b를 만족하는 x 벡터.\n    \"\"\"\n\n    tracker = np.zeros(a.shape[1])\n    result = np.zeros(a.shape[1])\n\n    for row, val in zip(a[::-1], b[::-1]):\n        unset_var_indices = np.where((tracker == 0) & (row != 0))[0]\n\n        if len(unset_var_indices) == 0:\n            if np.isclose(result @ row, val):\n                continue\n            else:\n                raise UnsolvableError('주어진 a와 b 값으로 인해 해결할 수 없는 방정식입니다.')\n\n        tracker[unset_var_indices] = 1\n        result[unset_var_indices[1:]] = 1\n        i = unset_var_indices[0]\n        result[i] = (val - result @ row) / row[i]\n\n    return result\n```\n\n이제 MultipleLinearRegressor를 생성할 준비가 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nclass MultipleLinearRegressor:\n    \"\"\"다중 선형 회귀를 수행합니다.\"\"\"\n\n    def __init__(self):\n        self.beta = None\n```\n\n단순 선형 회귀와 마찬가지로 predict 메서드와 fit 메서드를 갖게 될 것입니다.\n\npredict 메서드는 간단히 행렬 X 또는 벡터 x와 β 사이의 행렬 곱을 계산합니다.\n\n```js\ndef predict(self, x):\n    \"\"\"\n    주어진 x값 배열로부터 예측된 y값을 제공합니다.\n    :param x: x값의 벡터 또는 행렬.\n    :return: 예측된 y값의 벡터.\n    \"\"\"\n\n    if self.beta is None:\n        raise RegressionModelNotFitError('앗! 모델이 적합되지 않았습니다!')\n\n    return x @ self.beta\n```\n\n<div class=\"content-ad\"></div>\n\nfit 메소드는 방정식 6을 해결하기 위해 X^TX를 QR 분해한 다음 solve_upper_triangular을 사용하여 Rβ = Q^-1X^Ty의 해를 찾습니다. 또한 적합한 모델의 제곱 오차의 합을 반환합니다.\n\n```js\ndef fit(self, x, y):\n    \"\"\"\n    x-값 행렬과 해당 y-값 벡터를 기반으로 모델을 적합합니다.\n    :param x: x-값 행렬.\n    :param y: y-값 벡터.\n    :return: 적합된 모델의 제곱 오차의 합.\n    \"\"\"\n\n    x_t = x.transpose()\n    q, r = np.linalg.qr(x_t @ x)\n    vec = np.linalg.inv(q) @ x_t @ y\n    self.beta = solve_upper_triangular(r, vec)\n    diff = self.predict(x) - y\n    return diff @ diff\n```\n\n다중 선형 회귀기의 성능을 살펴봅시다. 이전과 매우 유사한 generate_noisy_data 함수를 만들겠습니다. 이 함수는 매개변수 벡터 β를 받아들이고 X 행렬과 데이터 포인트의 y-값 벡터를 생성한 다음 이전과 같이 각 y-값에 가우시안 노이즈를 추가합니다.\n\n```js\ndef generate_noisy_data(n_data_points, n_independent_variables, beta, x_range, noise_stddev):\n    \"\"\"\n    가우시안 노이즈가 추가된 선형 관계를 기반으로 데이터 포인트를 생성합니다.\n    :param n_data_points: 생성할 데이터 포인트 수.\n    :param n_independent_variables: 각 데이터 포인트에서의 독립 변수 수.\n    :param beta: 독립 변수의 계수 벡터.\n    :param x_range: x-값을 추출할 범위.\n    :param noise_stddev: 각 y-값에 추가할 가우시안 노이즈의 표준 편차.\n    :return: x-값 행렬과 y-값 벡터.\n    \"\"\"\n\n    x = np.random.uniform(*x_range, (n_data_points, n_independent_variables))\n    y = x @ beta + np.random.normal(scale=noise_stddev, size=n_data_points)\n    return x, y\n```  \n\n<div class=\"content-ad\"></div>\n\n이제 데이터를 생성하고 회귀자가 원래 β를 얼마나 잘 복원하는지 살펴보는 시간입니다.\n\n```js\nregressor = MultipleLinearRegressor()\nx, y = generate_noisy_data(n_data_points=500,\n                           n_independent_variables=10,\n                           beta=np.array([-10, 5, -8, -2, 1, -3, 4, -5, -1, 3]),\n                           x_range=np.array([-100, 100]),\n                           noise_stddev=50)\nsse = regressor.fit(x, y)\nprint(f'Sum Squared Error: {sse}')\nprint(f'Beta: {regressor.beta}')\n```\n\n한 번 실행한 결과는 다음과 같습니다.\n\n```js\nSum Squared Error: 1259196.6403705715\nBeta: [-9.95436533  5.02469925 -7.95431319 -1.97266714  1.03726794 -2.95935233\n  4.03854255 -4.98106051 -1.01840914  3.0410695]\n```\n\n<div class=\"content-ad\"></div>\n\n위에서 확인할 수 있듯이, 원래 매개변수 값에 꽤 가까운 결과를 얻는 데 잘 작동하는 것 같습니다.\n\n## 편향(bias)에 대하여\n\n지금까지 y절편이 0인 회귀 모델에 대해만 논의했습니다. 그러나 이는 모든 데이터에 적합한 것은 아닙니다. 만약 x⋅β + b와 같은 모델인 f(x) = x⋅β + b를 원한다면 어떻게 해야 할까요? 여기서 b는 0이 아닌 상수입니다. 기계 학습의 맥락에서 이 값 b를 편향(bias)이라고 부르며, 모든 모델 입력이 0일 때에도 데이터를 특정 값으로 '편향'시킨다는 의미입니다.\n\n이러한 고려 사항은 회귀 모델에 편향을 추가하는 데 많은 노력이 필요하지 않다는 것으로 추가 사항으로 남겨두었습니다: 회귀 모델에 편향을 추가하는 것은 데이터에 항상 1로 설정된 추가 독립 변수를 추가하는 것과 동일합니다. 예를 들어, 우리가 2차원 데이터를 가지고 있고 회귀자에 편향을 추가하려는 경우, f(x) = kx 형태의 모델을 적합시키는 대신에\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_45.png)\n\n변수 x_1은 원본 데이터를 나타내고, x_2는 모든 데이터 포인트에서 1로 설정됩니다. 데이터 포인트 (x, y)는 이렇게 (x_1, x_2, y) = (x, 1, y)가 됩니다. x_1 = x이고 x_2 = 1을 대입하면, 방정식 7은 다음과 같이 단순화됩니다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_46.png)\n\n여기서 β_1은 기울기이고, β_2는 바이어스입니다. 우리는 다중 선형 회귀를 사용하여 이 모델을 적합시킬 수 있습니다. 고차원 데이터의 경우, 이 과정은 비슷하게 작동합니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n여기서 한 가지! 이것이 바로 처음부터 선형 회귀입니다. 즐겁게 즐겼고 무언가를 배웠으면 좋겠어요. 어떤 피드백과 건설적인 비평도 환영합니다. 다음 포스트에서는 선형 분류에 대해 다룰 예정이니 기대해주세요.\n\n모든 코드는 github에서 확인하실 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-LinearRegressionfromScratch_0.png"},"coverImage":"/assets/img/2024-06-22-LinearRegressionfromScratch_0.png","tag":["Tech"],"readingTime":16},{"title":"2024년에 AI 배우는 로드맵","description":"","date":"2024-06-22 04:41","slug":"2024-06-22-RoadmaptoLearnAIin2024","content":"\n\n## AI를 배우고 싶나요?\n\n![AI 학습 로드맵](/assets/img/2024-06-22-RoadmaptoLearnAIin2024_0.png)\n\nAI를 배우고 싶지만 어떻게 시작해야 할지 모르시나요?\n\n저는 2020년에 무료 데이터 과학, 기계 학습 및 AI MOOC의 베스트 20을 작성했어요. 하지만 많은 강의를 듣는 것이 답은 아니라는 것을 깨달았죠.\n\n<div class=\"content-ad\"></div>\n\n튜토리얼 지옥을 벗어나서 진짜로 배우려면 손을 더럽히고, 알고리즘을 제로부터 짜고, 논문들을 구현하며, AI를 활용하여 재미있는 부수 프로젝트를 해결하는 게 중요해요.\n\n본 문서는 이 철학을 따르는 무료 커리큘럼을 작성하려고 노력했어요. 저는 이 중 일부 강좌를 진행하고 있는데요, 함께 공부하고 싶다면 트위터나 링크드인을 통해 연락해주세요!\n\n그리고 이 내용에 빠진 부분이 있다고 생각되면 댓글을 남겨주세요!\n\n하지만 먼저, 커리큘럼에 대한 몇 가지 주의사항과 학습 팁을 전해드릴게요.\n\n<div class=\"content-ad\"></div>\n\n# Top-down 방식\n\n이 교육과정은 top-down 방식을 따릅니다 — 코드를 먼저 작성하고 이론을 나중에 다룹니다.\n\n저는 필요에 의해 배우는 것을 좋아합니다. 그래서, 무언가를 해결해야 하거나 프로토 타입을 만들어야 할 때, 필요한 정보를 넓고 넒게 찾아서 해당 정보를 공부하고 이해한 후에 행동으로 옮깁니다.\n\n예를 들어, 저는 기본 수준에서 LLMs를 이해하는 AI 엔지니어가 되고 싶습니다. 이를 위해서는 transformer를 처음부터 코딩하고 GPU에서 LLMs를 세밀하게 튜닝하는 기술이 필요합니다. 지금은 그것을 할 수 없습니다. 왜냐하면 제 지식에는 여전히 빈틈이 있기 때문에 그 빈틈을 채우고자 하는 목표가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이것은 NLP에 중점을 둔 것입니다. 컴퓨터 비전이나 강화 학습과 같은 다른 AI 전문화를 찾고 계신 경우, 아래에 댓글을 남기거나 Twitter 또는 Linkedin에서 DM으로 연락해주세요. 추천 몇 가지를 전해 드릴게요.\n\n링크를 많이 던지기 전에 제가 무언가를 배우기 시작하기 전에 중요한 두 가지를 알았더라면 좋았을 텐데요.\n\n# 공개로 배우세요\n\n배워야 할 것이 많고, 특히 AI에서는 매주 새로운 혁명적인 논문과 아이디어들이 나오기 때문에 학습이 끝나지 않을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n가장 큰 실수는 혼자서 배우는 것입니다. 그렇게 하면 자신에게 어떤 기회도 만들지 못합니다. 완료했다고 말할 수 있는 것을 제외하고는 보여줄 것이 없습니다. 더 중요한 것은 정보를 어떻게 활용하여 공유할지, 그 정보에서 어떤 혁신적인 아이디어와 해결책이 나왔는지입니다.\n\n그러니까, 공개적으로 배워야 합니다.\n\n종합하자면, 만드는 습관을 가져야 합니다.\n\n이것은 다음을 의미할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 블로그와 튜토리얼 작성\n- 해커톤 참여 및 다른 사람들과 협업\n- 디스코드 커뮤니티에서 질문하고 대답하기\n- 열정적으로 하는 사이드 프로젝트에 참여\n- 새롭게 발견한 흥미로운 것에 대해 트윗하기\n\n그리고 트위터에 대해 이야기할 때,\n\n# 트위터 사용하기\n\n적절한 사람을 팔로우하고 올바르게 활용한다면, 오늘날 누구나 함께 있어야 할 가치가 가장 높은 소셜 플랫폼입니다.\n\n<div class=\"content-ad\"></div>\n\n누구를 팔로우해야 하는지 궁금하다면 Suhail이 제작한 이 AI 목록을 확인해보세요.\n\nTwitter를 어떻게 사용해야 할지 궁금하다면 Near의 \"Twitter 성공 가이드\"를 읽어보세요.\n\nTwitter에서 사람들에게 다이렉트 메시지를 보내려면 진실하고 간결하며 구체적인 요청을 하는 게 중요해요. Sriram Krishnan의 \"차가운 이메일 작성 방법\" 가이드도 DM에 적용할 수 있어요.\n\n트윗하는 법이 알고 싶다면, Instructor의 창시자 Jason이 작성한 \"트윗의 구성 요소\"를 읽어보세요. 그는 0명에서 단 몇 달 사이에 14,000명의 팔로워를 얻었답니다.\n\n<div class=\"content-ad\"></div>\n\n만약 이 문구를 읽고 계시다면, 트위터로 저를 팔로우해 주세요!\n\n무엇을 하고 있는지 메시지를 보내주세요! 멋진 프로젝트에 함께 참여하는 것을 항상 기대하고 있습니다.\n\n이제 시작해 봅시다.\n\n## 목차\n\n<div class=\"content-ad\"></div>\n\n- 수학\n- 도구\n   - Python\n   - PyTorch\n- 머신 러닝\n   - 처음부터 작성\n   - 경쟁\n   - 사이드 프로젝트 수행\n   - 배포\n   - 부가적인 활동\n- 딥러닝\n   - Fast.ai\n   - 더 많은 경쟁 참여\n   - 논문 구현\n   - 컴퓨터 비전\n   - NLP\n- 대규모 언어 모델\n   - Neural Networks: Zero to Hero 시청\n   - 무료 LLM 부트캠프\n   - LLM으로 빌드\n   - 해커톤 참여\n   - 논문 읽기\n   - 처음부터 Transformers 작성\n   - 일부 좋은 블로그\n   - Umar Jamil 시청\n   - 오픈소스 모델 실행 방법 익히기\n   - 프롬프트 엔지니어링\n   - LLM 세밀 조정\n   - RAG\n- 최신 정보 유지 방법\n- 유용한 다른 커리큘럼/리스트들 \n\n# 수학\n\n<div>\n   <img src=\"/assets/img/2024-06-22-RoadmaptoLearnAIin2024_1.png\" />\n</div>\n\n머신 러닝은 선형 대수, 미적분, 확률, 통계의 세 가지 수학 기초를 중시합니다. 각각이 알고리즘이 효과적으로 동작하도록 하는데 중요한 역할을 합니다.\n\n<div class=\"content-ad\"></div>\n\n- 선형 대수학: 데이터의 표현과 조작을 위한 수학적 도구로, 행렬과 벡터가 알고리즘이 정보를 해석하고 처리하는 언어를 형성합니다.\n- 미적분학: 기계 학습에서 최적화를 위한 엔진으로, 알고리즘이 그래디언트와 변화율을 이해하여 학습하고 개선할 수 있도록 합니다.\n- 확률과 통계: 불확실성 하에서의 의사 결정을 위한 기초로, 알고리즘이 결과를 예측하고 무작위성과 변동성 모델을 통해 데이터에서 학습할 수 있도록 합니다.\n\n이는 프로그래머 관점에서 수학에 대한 멋진 시리즈입니다: Weights & Biases의 수학을 통한 기계 학습(코드)\n\n선형 대수학에 대한 코드 중심 접근 방식을 원한다면, fast.ai 창시자들에 의한 Computational Linear Algebra(비디오, 코드)를 참고하세요.\n\n파이썬을 활용한 응용 기계 학습을 위한 선형 대수학 소개서를 함께 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n만약 전통적인 것을 더 선호한다면, 잉글랜드의 임페리얼 칼리지의 강의인 선형대수학 및 다변수 미적분을 참고해보세요.\n\n3Blue1Brown의 Essence of Linear Algebra 및 Essence of Calculus를 시청해보세요.\n\n통계학에 대한 기본 개념을 알고 싶다면 StatQuest의 Statistics Fundamentals를 시청해보세요.\n\n보충 자료\n\n<div class=\"content-ad\"></div>\n\n- 책: 수학 머신러닝을 위한\r\n- 논문: 딥러닝에 필요한 행렬 미적분\n\n# 도구\n\n![도구 이미지](/assets/img/2024-06-22-RoadmaptoLearnAIin2024_2.png)\n\n## Python\n\n<div class=\"content-ad\"></div>\n\n초보자분들은 여기서 시작하세요: Practical Python Programming.\n\n이미 Python에 익숙하신 분은 Advanced Python Mastery를 선택해보세요.\n\n두 강의 모두 David Beazley가 집필한 Python Cookbook의 저자로 유명한 강의입니다.\n\n이후에는 James Powell의 강연 몇 개를 보세요.\n\n<div class=\"content-ad\"></div>\n\n파이썬 디자인 패턴을 읽어보세요.\n\n추가 정보\n\n- 책: 유창한 파이썬, 2판 (코드)\n- 팔로우할 팟캐스트: Real Python 및 Talk Python\n\n## PyTorch\n\n<div class=\"content-ad\"></div>\n\n알라딘 페르손의 파이토치 튜토리얼을 시청해보세요\n\n파이토치 웹사이트는 정말 멋진 곳이에요.\n\n- 파이토치 예제\n- 공식 파이토치 튜토리얼\n- FAQ 페이지\n\n퍼즐로 지식을 테스트해보세요\n\n<div class=\"content-ad\"></div>\n\n- srush/Tensor-Puzzles: 퍼즐을 해결하세요. PyTorch 실력을 향상시키세요\n\n부가 정보\n\n- 책: 딥러닝을 위한 PyTorch 프로그래밍\n\n# 기계 학습\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-RoadmaptoLearnAIin2024_3.png)\n\n100페이지짜리 머신 러닝 책을 읽어보세요.\n\n## 처음부터 써보기\n\n읽으면서 알고리즘을 처음부터 짜보세요.\n\n\n<div class=\"content-ad\"></div>\n\n아래의 저장소들을 살펴보세요\n\n- eriklindernoren/ML-From-Scratch\n- JeremyNixon/oracle\n- trekhleb/homemade-machine-learning\n\n도전하고 싶다면, 이 코스를 따라가며 PyTorch를 제작해보세요.\n\n- MiniTorch: 머신러닝 엔지니어링 DIY 코스 (동영상, 코드)\n\n<div class=\"content-ad\"></div>\n\n## 경쟁\n\n학습한 내용을 경쟁에서 실습해 보세요.\n\n- Bitgrit 및 Kaggle과 같은 플랫폼에서 ML 경쟁에 참여하세요. 이 기사에서 더 많은 정보를 확인할 수 있습니다.\n- 지난 우승 솔루션을 살펴보고 연구하세요\n\n## 사이드 프로젝트 만들기\n\n<div class=\"content-ad\"></div>\n\n위키 보이키가 쓴 글 'Getting machine learning to production' 을 읽어보세요.\n\n그녀는 또한 책을 위한 의미론적 검색인 Viberary를 만들면서 배운 것에 대해 썼습니다.\n\n데이터셋을 구하고 모델을 만드세요 (즉, NASA 지구 데이터를 얻기 위해 earthaccess를 사용하세요).\n\nStreamlit으로 UI를 만들어 Twitter에 공유하세요.\n\n<div class=\"content-ad\"></div>\n\n## 배포하세요\n\n모델을 제품으로 출시하세요. 실험을 추적하세요. 모델을 어떻게 모니터링하는지 배우세요. 데이터 및 모델 드리프트를 직접 경험해보세요.\n\n다음은 일부 훌륭한 자원입니다\n\n- Made With ML\n- DataTalksClub/mlops-zoomcamp: 무료 MLOps 강의\n- chiphuyen/machine-learning-systems-design\n- Evidently AI — ML 시스템 디자인: 300개의 사례 연구\n- stas00/ml-engineering: 머신러닝 엔지니어링 온라인 서적\n\n<div class=\"content-ad\"></div>\n\n## 부가정보\n\n- PyTorch와 Scikit-Learn을 사용한 기계학습 (코드)\n- [1811.12808] 기계학습에서의 모델 평가, 모델 선택 및 알고리즘 선택\n- 머신러닝 면접서 · MLIB\n\n# 딥러닝\n\n![AI 학습을 위한 지도](/assets/img/2024-06-22-RoadmaptoLearnAIin2024_4.png)\n\n<div class=\"content-ad\"></div>\n\n위에서 아래로 내려오는 방식이 좋다면, fast.ai부터 시작해보세요.\n\n## Fast.ai\n\n- fast.ai (파트1, 파트2) + W&B 스터디 그룹\n\nfast.ai를 좋아하셨나요? Full Stack Deep Learning도 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n더 포괄적이고 전통적인 강의를 찾고 계시다면, François Fleuret의 UNIGE 14x050 - 딥 러닝을 확인해보세요.\n\n이론을 필요로 할 때는 이런 책들이 좋습니다.\n\n- Dive into Deep Learning (PyTorch, NumPy/MXNet, JAX, TensorFlow 코드 예시 포함)\n- Ian Goodfellow, Yoshua Bengio, Aaron Courville의 Deep Learning\n- Neural networks and deep learning\n- Understanding Deep Learning (실습 노트북과 함께)\n\n트위터를 스크롤하는 대신 핸드폰에서 The Little Book of Deep Learning을 읽어보세요.\n\n<div class=\"content-ad\"></div>\n\n신경망이 수렴되는 동안 이를 읽어보세요.\n\n- 신경망 훈련을 위한 레시피\n- 심층 신경망: 33년 전과 33년 후\n\n### 더 많은 경연에 참여하세요\n\n- PlantTraits2024 — FGVC11 | Kaggle (컴퓨터 비전)\n\n<div class=\"content-ad\"></div>\n\n## 논문 구현\n\nlabml.ai Annotated PyTorch Paper Implementations을 확인해보세요.\n\nPapers with Code는 훌륭한 자료입니다. 그들의 웹사이트에서 BERT가 설명되어 있습니다.\n\n아래는 딥러닝 내 특화된 자원들입니다.\n\n<div class=\"content-ad\"></div>\n\n## 컴퓨터 비전\n\n많은 사람들이 CS231n: 딥 러닝을 위한 컴퓨터 비전을 추천합니다. 도전적이지만 극복한다면 가치가 있어요.\n\n## 강화학습\n\n강화학습에 대해 이 두 가지가 좋아요:\n\n<div class=\"content-ad\"></div>\n\n- OpenAI에서 제공하는 Deep RL 시작하기\n- 🤗 Hugging Face의 Deep Reinforcement Learning Course\n\n## 자연어 처리\n\n다른 훌륭한 Stanford 강의, CS 224N | 딥러닝을 활용한 자연어 처리\n\nHugging Face NLP Course를 배워보세요\n\n<div class=\"content-ad\"></div>\n\n이 멋진 NLP 항목들을 확인해보세요.\n\n좋은 기사와 설명들\n\n- BERT 연구 - Ep. 1 - 주요 개념 및 소스 · Chris McCormick\n- The Illustrated Word2vec - Jay Alammar\n- The Illustrated BERT, ELMo, 등 (NLP가 전이 학습을 해결한 방법)\n- LSTM 네트워크 이해 - colah의 블로그\n- PyTorch RNN Scratch 구현 - Jake Tae\n\n부가적인 정보\n\n<div class=\"content-ad\"></div>\n\n- Natural Language Processing with Transformers Book\n\n# Large Language Models\n\n![Image](/assets/img/2024-06-22-RoadmaptoLearnAIin2024_5.png)\n\nFirst, watch [1hr Talk] Intro to Large Language Models by Andrej.\n\n<div class=\"content-ad\"></div>\n\n그 다음 Alexander Rush의 '5가지 공식으로 큰 언어 모델'을 알아보세요 — Cornell Tech\n\n## 신경망 시청: 제로부터 히어로로\n\n0부터 역전파를 설명하고 코딩한 다음, 제로에서 GPT를 직접 작성하는 방법까지 알려줍니다.\n\nAndrzej Karpathy의 '신경망: 제로에서 히어로로'\n\n<div class=\"content-ad\"></div>\n\n그는 방금 새 비디오를 공개했어요 → GPT 토크나이저 만들기에 도전해보세요\n\n그리고 Jay Mody의 'NumPy로 GPT를 60줄로 살펴보기'도 함께 확인해보세요.\n\n## 무료 LLM 부트캠프\n\nFull Stack Deep Learning에서 무료로 제공된 유료 LLM 부트캠프를 소개합니다.\n\n<div class=\"content-ad\"></div>\n\n이 프로그램은 즉시 엔지니어링, LLMOps, LLM을 위한 UX, 그리고 1시간 안에 LLM 앱을 출시하는 방법을 가르칩니다.\n\n부트 캠프를 마치고 나서 무언가를 만들고 싶어질 것입니다.\n\n## LLM과 함께 빌드하기\n\nLLM과 함께 앱을 만들고 싶나요?\n\n<div class=\"content-ad\"></div>\n\n거대한 언어 모델을 활용한 애플리케이션 개발\nAndrew Ng 저\n\nHuyen Chip의 제작용 LLM 애플리케이션 빌딩 읽기\n\n또한 Eugene Yan의 LLM 기반 시스템 및 제품을 구축하기 위한 패턴\n\n레시피를 위해 OpenAI Cookbook을 참조하세요.\n\n<div class=\"content-ad\"></div>\n\nVercel AI 템플릿을 사용하여 시작해보세요.\n\n## 해커톤 참여\n\nlablab.ai는 매주 새로운 AI 해커톤을 개최합니다. 팀을 꾸리고 싶다면 알려주세요!\n\n더 심층적으로 이론에 대해 공부하고 모든 것이 어떻게 작동하는지 이해하고 싶다면:\n\n<div class=\"content-ad\"></div>\n\n## 논문 읽기\n\n세바스찬 라슈카의 대규모 언어 모델 이해에 대한 훌륭한 기사가 있습니다. 그는 읽어야 할 몇 가지 논문을 나열하였습니다.\n\n그는 또한 최근 2024년 1월에 읽어야 할 논문에 대한 다른 기사를 발행했으며, 이 기사는 미스트랄 모델을 다루고 있습니다.\n\n그의 서브스택 Ahead of AI를 팔로우하세요.\n\n<div class=\"content-ad\"></div>\n\n## 제로부터 Transformer 구현하기.\n\n개요를 읽으려면 The Transformer Family Version 2.0 | Lil’Log를 참조해주세요.\n\n가장 편한 형식을 선택하고 제로부터 구현해보세요.\n\n논문\n\n<div class=\"content-ad\"></div>\n\n- 어텐션만 해라\n- 이해할 수 있는 트랜스포머\n- 하버드의 주석 달린 트랜스포머\n- 트랜스포머처럼 생각하기\n\n블로그\n\n- 제로 부터 트랜스포머 만들기 — 첫 번째 파트: 어텐션 메커니즘 (파트 2) (코드)\n- 세바스찬 라쉬카 박사의 대형 언어 모델의 셀프 어텐션 메커니즘을 이해하고 코딩하기\n- 제로 부터 트랜스포머를 만들기\n\n비디오\n\n<div class=\"content-ad\"></div>\n\n- 파이토치로부터 Transformer를 완전히 설명하여 트레이닝 및 추론을 구현하기\n- NLP: BERT와 Transformer를 스크래치에서 구현하기\n\n이제는 전적으로 Transformer를 코딩할 수 있어요. 하지만 더 많은 것이 기다리고 있어요.\n\n이 스탠포드 CS25 — Transformers United 비디오들을 확인해보세요.\n\n## 몇 가지 좋은 블로그들\n\n<div class=\"content-ad\"></div>\n\n- 점근적 하강법으로 미친듯이 — 처음부터 LLM 구축하기\n- 일러스트로 보는 Transformer — Jay Alammar\n- 어텐션과 Transformer에 대한 직관적인 이해 by Eugene Yan\n- GPT 가속화하기 — KV 캐시 | 불패를 향한 발전\n- 자가 어텐션을 넘어서: 작은 언어 모델이 다음 토큰을 예측하는 방법\n- 처음부터 Llama 만들기 (또는 울지 않고 논문 구현하기) | Brian Kitano\n- LoRA 개선하기: 처음부터 Weight-Decomposed Low-Rank Adaptation (DoRA) 구현하기\n\n## Watch Umar Jamil\n\n그의 깊이 있는 훌륭한 비디오를 통해 논문을 설명합니다. 또한 코드도 보여줍니다.\n\n- LoRA: 대규모 언어 모델에 대한 저랭크 적응 — 시각적 설명 + PyTorch 코드로 처음부터\n- Mistral / Mixtral 설명: 슬라이딩 윈도우 어텐션, 희소한 전문가 혼합, 롤링 버퍼\n- 어텐션만으로 충분하다 (Transformer) — 모델 설명(수학 포함), 추론 및 훈련\n- LLaMA 설명: KV-Cache, 로터리 위치 임베딩, RMS Norm, 그룹화된 쿼리 어텐션, SwiGLU\n- 검색 증강 생성 (RAG) 설명: 임베딩, 문장 BERT, 벡터 데이터베이스(HNSW)\n\n<div class=\"content-ad\"></div>\n\nLLM과 관련된 몇 가지 더 링크를 추가했어요. 이 링크들이 모두를 다 소개한 건 아니에요. LLM에 대한 더 포괄적인 실러버스는 LLM 실러버스를 참고해보세요.\n\n## 오픈소스 모델을 실행하는 방법을 배워보세요.\n\nollama를 사용하세요: Llama 2, Mistral, 그리고 다른 대형 언어 모델을 로컬에서 사용해보세요.\n\n최근에 파이썬 및 자바스크립트 라이브러리가 출시되었어요.\n\n<div class=\"content-ad\"></div>\n\n## 프롬프트 엔지니어링\n\n가벼운 톤으로 요청하신 내용을 한국어로 번역해드리겠습니다.\n\n## 프롬프트 엔지니어링\n\n프롬프트 엔지니어링 | Lil’Log 읽기\n\nIse Fulford(OpenAI)와 Andrew Ng에 의한 개발자를 위한 ChatGPT 프롬프트 엔지니어링\n\nDeepLearning.ai에서는 무료로 수강할 수 있는 다른 짧은 강의도 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n## LLM 세밀 조정\n\n허깅페이스 세밀 조정 가이드를 읽어보세요.\n\n좋은 안내서: 세밀 조정 — GenAI 안내서\n\n악소로틀을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n이 글은 좋아요: Fine-tune a Mistral-7b model with Direct Preference Optimization | Maxime Labonne가 쓴 글\n\n## RAG\n\nAnyscale에서 좋은 글: Production을 위한 RAG 기반 LLM 애플리케이션 구축\n\nAman Chadha가 작성한 검색 증강 생성에 대한 포괄적인 개요\n\n<div class=\"content-ad\"></div>\n\n# 최신 정보를 얻는 방법\n\n뉴스레터 + 팟캐스트 + 트위터의 조합\n\n논문을 위해서는 AK(@_akhaliq)를 팔로우할 수 있습니다.\n\n팟캐스트는 Swyx와 Alessio가 진행하는 Latent Space가 최고로 생각됩니다.\n\n<div class=\"content-ad\"></div>\n\n그들의 디스코드에 가입해보세요.\n\n그들은 Smol Talk이라는 뉴스레터도 운영하고 있어요. 이 뉴스레터는 모든 주요 AI 디스코드를 요약합니다.\n\n내가 좋아하는 다른 뉴스레터들은:\n\n- The Batch | DeepLearning.AI | AI News & Insights\n- Deep Learning Weekly\n- Interconnects | Nathan Lambert\n- AI Tidbits | Sahar Mor\n\n<div class=\"content-ad\"></div>\n\n이 기사에 더 많은 내용이 있습니다.\n\n# 유용할 수 있는 다른 커리큘럼/목록.\n\n내 목록은 철저한 것이 아니었지만, 여전히 더 찾고 싶다면 몇 가지가 있습니다.\n\n- openai/syllabus.md\n- AI Canon | Andreessen Horowitz\n- AI Learning Curation — LLM Utils\n- Threshold to the AI Multiverse | Open DeepLearning\n- louisfb01/start-llms: 2023년 LLM 스킬을 개선하기 위한 완벽한 가이드\n\n<div class=\"content-ad\"></div>\n\n이제 충분한 시간을 들여서 쓰고 정리를 했으니, 이제는 많은 것을 얻을 차례입니다. 배우고 무언가를 만들어 봐요.\n\n이것이 당신의 인공지능 여정에 도움이 되기를 바래요!\n\n만약 여기까지 읽었다면, 꼭 연락하거나 댓글을 남겨주세요 :)\n\n새로운 소식을 받기 위해 bitgrit 데이터 과학 게시물을 팔로우하는 것을 잊지 마세요!\n\n<div class=\"content-ad\"></div>\n\n최신 데이터 과학과 인공 지능 분야의 최신 소식을 다른 데이터 과학자들과 함께 논의하고 싶으신가요? 우리의 디스코드 서버에 참여해보세요!\n\n워크숍 및 다가오는 대회 소식을 받아보려면 Bitgrit를 팔로우해주세요!\n\n디스코드 | 웹사이트 | 트위터 | 링크드인 | 인스타그램 | 페이스북 | 유튜브","ogImage":{"url":"/assets/img/2024-06-22-RoadmaptoLearnAIin2024_0.png"},"coverImage":"/assets/img/2024-06-22-RoadmaptoLearnAIin2024_0.png","tag":["Tech"],"readingTime":12},{"title":"파이썬으로 AI 텍스트-비디오 모델 처음부터 구축하는 방법","description":"","date":"2024-06-22 04:36","slug":"2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython","content":"\n\n![Building an AI Text-to-Video Model from Scratch Using Python](/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_0.png)\n\n애정하는 여러분, 반가워요!\n\n오픈AI의 Sora, 안정성 AI의 Stable Video Diffusion 등 2024년에 가장 인기 있는 인공지능 트렌드 중 하나인 텍스트-비디오 모델들이 대규모 언어 모델 이후 많이 등장하고 있습니다. 이 블로그에서는 제가 직접 작성한 작은 규모의 텍스트-비디오 모델을 만들어보려고 해요. 텍스트 프롬프트를 입력하면 학습된 모델이 해당 프롬프트를 기반으로 비디오를 생성할 거예요. 여기서는 이론적 개념을 이해하는 것부터 전체 아키텍처 코딩하고 최종 결과물을 생성하는 과정까지 모두 다루고 있어요.\n\n저는 고급 GPU를 갖고 있지 않아서 작은 규모 아키텍처를 코딩했어요. 다양한 프로세서에서 모델을 학습하는데 걸리는 시간을 비교해보겠습니다:\n\nCPU에서 실행하면 모델 학습에 훨씬 오랜 시간이 걸릴 거예요. 코드 변경을 빠르게 테스트하고 결과를 확인하려면 CPU는 최적의 선택이 아닙니다. 더 효율적이고 빠른 학습을 위해 Colab이나 Kaggle의 T4 GPU를 사용하는 것을 권장해요.\n\n더 궁금한 점이 있으시면 언제든지 물어주세요!\n\n<div class=\"content-ad\"></div>\n\n이 블로그에서 코드를 복사하여 붙여 넣는 것을 피하려면 GitHub 저장소에 노트북 파일과 모든 코드 및 정보가 포함되어 있습니다.\n\n여기에는 처음부터 Stable Diffusion을 만드는 방법을 안내하는 블로그 링크가 있습니다:\n\n# 목차\n\n- 무엇을 만들고 있는가\n- 필수 조건\n- GAN 아키텍처 이해하기\n∘ GAN이란?\n∘ 실세계 응용\n∘ GAN이 어떻게 작동하는가?\n∘ GAN 훈련 예제\n- 준비 단계 설정\n- 훈련 데이터 코딩\n- 교육 데이터 전 처리\n- 텍스트 임베딩 레이어 구현\n- Generator 레이어 구현\n- Discriminator 레이어 구현\n- 교육 매개변수 코딩\n- 훈련 루프 코딩\n- 훈련된 모델 저장\n- AI 비디오 생성\n- 빠진 것은 무엇일까?\n- 나에 대해\n\n<div class=\"content-ad\"></div>\n\n# 무엇을 구축하고 있는가\n\n우리는 전통적인 기계 학습 또는 딥 러닝 모델과 유사한 방식을 따를 것입니다. 데이터 세트에서 훈련을 받은 후 보이지 않는 데이터에서 테스트하는 것이 기본적인 접근 방식입니다. 텍스트에서 비디오로 변환하는 맥락에서, 우리는 10만 개의 비디오 데이터 세트를 사용하여 개가 공을 가져오고 고양이가 쥐를 쫓는 내용을 가르치겠습니다. 우리 모델을 훈련시켜서 고양이가 공을 가져오거나 개가 쥐를 쫓는 비디오를 생성하도록 할 것입니다.\n\n이러한 훈련 데이터 세트는 인터넷에서 쉽게 이용할 수 있지만, 필요한 컴퓨팅 파워는 극히 높습니다. 따라서 우리는 Python 코드에서 생성된 움직이는 물체들의 비디오 데이터 세트를 사용할 것입니다.\n\n저희는 OpenAI Sora가 사용하는 확산 모델 대신 GAN(생성 적대적 신경망) 아키텍처를 사용하여 모델을 만들 것입니다. 확산 모델을 사용하려고 했지만, 메모리 요구 사항 때문에 그 기능이 중단되었습니다. GAN은 반면에 훈련과 테스트가 더 쉽고 빠릅니다.\n\n<div class=\"content-ad\"></div>\n\n# 사전 요구 사항\n\n우리는 OOP (Object-Oriented Programming)을 사용할 것이기 때문에, 기본적인 이해를 갖고 있어야 하며 뉴럴 네트워크에 대한 이해도 필요합니다. GANs (Generative Adversarial Networks)에 대한 지식은 필수는 아니지만, 여기서 그 아키텍처를 다룰 것이므로 참고하시면 좋습니다.\n\n# GAN 아키텍처 이해\n\nGAN을 이해하는 것은 중요합니다. 우리의 아키텍처의 많은 부분이 이에 의존하기 때문입니다. 무엇인지, 그 구성 요소는 무엇인지 등을 탐구해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## GAN이란 무엇인가요?\n\nGenerative Adversarial Network (GAN)은 주어진 데이터셋으로부터 새로운 데이터(예: 이미지 또는 음악)를 생성하는 하나의 신경망과 그 데이터가 실제인지 가짜인지 구별하려는 다른 신경망이 경쟁하는 딥 러닝 모델입니다. 이 과정은 생성된 데이터가 원본과 구별할 수 없을 때까지 계속됩니다.\n\n## 실제 세계 응용\n\n- 이미지 생성: GAN은 텍스트 프롬프트로 현실적인 이미지를 생성하거나 기존 이미지를 수정하여 해상도를 향상시키거나 흑백 사진에 색상을 추가합니다.\n- 데이터 증강: 다른 기계 학습 모델을 훈련하기 위해 합성 데이터를 생성하며, 예를 들어 사기 탐지 시스템을 위해 사기 거래 데이터를 생성합니다.\n- 누락된 정보 완성: GAN은 누락된 데이터를 채울 수 있으며, 에너지 응용 프로그램에 대한 지형 지도에서 장력 이미지를 생성하는 등의 작업을 수행할 수 있습니다.\n- 3D 모델 생성: 2D 이미지를 3D 모델로 변환하여, 수술 계획을 위해 현실적인 장기 이미지를 생성하는 의료 분야와 같은 분야에서 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n## GAN이 작동하는 방식은?\n\nGAN은 생성자(generator)와 판별자(discriminator) 두 개의 딥 뉴럴 네트워크로 구성되어 있습니다. 이 두 네트워크는 적대적인 설정에서 함께 훈련되며, 하나는 새로운 데이터를 생성하고 다른 하나는 데이터가 진짜인지 가짜인지 판별합니다.\n\n다음은 GAN 작동 방식의 간단한 개요입니다:\n\n- 훈련 데이터 분석: 생성자는 훈련 데이터를 분석하여 데이터 특성을 식별하고, 판별자는 독립적으로 동일한 데이터를 분석하여 해당 특성을 학습합니다.\n- 데이터 수정: 생성자는 데이터의 일부 특성에 잡음(랜덤 변경)을 추가합니다.\n- 데이터 전달: 수정된 데이터는 그런 다음 판별자에게 전달됩니다.\n- 확률 계산: 판별자는 생성된 데이터가 원본 데이터셋에서 온 확률을 계산합니다.\n- 피드백 루프: 판별자는 생성자에게 피드백을 제공하여 다음 주기에서 잡음을 줄이도록 안내합니다.\n- 적대적 훈련: 생성자는 판별자의 오류를 최대화하려고 하고, 판별자는 자신의 에러를 최소화하려고 합니다. 많은 훈련 반복을 통해 두 네트워크는 개선되고 발전합니다.\n- 평형 상태: 판별자가 더 이상 진짜와 생성된 데이터를 구별하지 못할 때까지 훈련이 계속되며, 이는 생성자가 현실적인 데이터를 생성하는 것을 성공적으로 배웠음을 나타냅니다. 이 시점에서 훈련 과정이 완료됩니다.\n\n<div class=\"content-ad\"></div>\n\n## GAN 훈련 예시\n\n이미지 간 변환을 예로 들어 GAN 모델을 설명해 보겠습니다. 이때는 인간의 얼굴을 수정하는 데 초점을 맞춥니다.\n\n- 입력 이미지: 입력은 실제 인간 얼굴 이미지입니다.\n- 속성 수정: 생성자는 눈에 선글라스를 추가하는 등 얼굴의 속성을 수정합니다.\n- 생성된 이미지: 생성자는 선글라스가 추가된 이미지 세트를 생성합니다.\n- 판별자의 역할: 판별자는 실제 이미지(선글라스를 쓴 사람)와 생성된 이미지(선글라스가 추가된 얼굴)의 혼합을 받습니다.\n- 평가: 판별자는 실제 이미지와 생성된 이미지를 구별하려고 합니다.\n- 피드백 루프: 만약 판별자가 가짜 이미지를 올바르게 식별하면 생성자는 더 현실적인 이미지를 만들기 위해 매개변수를 조정합니다. 생성자가 판별자를 성공적으로 속이면 판별자는 감지를 개선하기 위해 매개변수를 업데이트합니다.\n\n이 적대적 프로세스를 통해 두 네트워크가 계속해서 발전합니다. 생성자는 현실적인 이미지를 만드는 데 더 잘해지고 판별자는 가짜를 식별하는 데 더 잘해지며 균형이 이루어질 때까지 계속 발전합니다. 판별자가 더 이상 실제 이미지와 생성된 이미지를 구별하지 못할 정도로 적절한 평형점에 도달하면 GAN이 성공적으로 현실적인 수정을 만들기 위해 배웠다고 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 무대를 준비합니다\n\n파이썬 라이브러리 그룹과 작업할 것입니다. 그러니, 이제 그들을 가져오겠습니다.\n\n```python\n# 운영 체제와 상호 작용하기 위한 운영 체제 모듈\nimport os\n\n# 무작위 숫자 생성을 위한 모듈\nimport random\n\n# 숫자 연산을 위한 모듈\nimport numpy as np\n\n# 이미지 처리를 위한 OpenCV 라이브러리\nimport cv2\n\n# 이미지 처리를 위한 Python Imaging Library\nfrom PIL import Image, ImageDraw, ImageFont\n\n# 딥 러닝을 위한 PyTorch 라이브러리\nimport torch\n\n# PyTorch에서 사용자 정의 데이터셋 만들기 위한 Dataset 클래스\nfrom torch.utils.data import Dataset\n\n# 이미지 변환을 위한 모듈\nimport torchvision.transforms as transforms\n\n# PyTorch의 신경망 모듈\nimport torch.nn as nn\n\n# PyTorch의 최적화 알고리즘\nimport torch.optim as optim\n\n# PyTorch에서 시퀀스를 패딩하는 함수\nfrom torch.nn.utils.rnn import pad_sequence\n\n# PyTorch에서 이미지 저장하는 함수\nfrom torchvision.utils import save_image\n\n# 그래프 및 이미지 플로팅을 위한 모듈\nimport matplotlib.pyplot as plt\n\n# IPython 환경에서 풍부한 콘텐츠 표시를 위한 모듈\nfrom IPython.display import clear_output, display, HTML\n\n# 이진 데이터를 텍스트로 인코딩 및 디코딩하는 모듈\nimport base64\n```\n\n모든 라이브러리를 가져왔으니, 다음 단계는 GAN 아키텍처를 훈련할 때 사용할 훈련 데이터를 정의하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 훈련 데이터 코딩하기\n\n적어도 10,000개의 비디오가 훈련 데이터로 필요해요. 왜냐하면 작은 숫자로 테스트해봤더니 결과가 매우 좋지 않았어요. 거의 아무것도 보이지 않았죠. 다음으로 중요한 질문은 무엇일까요? 이 비디오들은 무엇에 관한 걸까요? 우리의 훈련 비디오 데이터셋은 서로 다른 방향으로 움직이는 원을 포함해요. 그래서 이제 코드를 작성하고 10,000개의 비디오를 생성해보죠.\n\n```js\n# 'training_dataset'이름의 디렉토리 생성\nos.makedirs('training_dataset', exist_ok=True)\n\n# 데이터셋을 생성할 비디오 수 정의\nnum_videos = 10000\n\n# 비디오 당 프레임 수 정의 (1초 비디오)\nframes_per_video = 10\n\n# 데이터셋 내 각 이미지의 크기 정의\nimg_size = (64, 64)\n\n# 모양의 크기 정의 (원)\nshape_size = 10\r\n```\n\n기본 매개변수를 설정한 후, 다음은 훈련 데이터셋의 텍스트 프롬프트를 정의해야해요.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 원에 대한 텍스트 프롬프트와 해당 움직임을 정의합니다.\nprompts_and_movements = [\n    (\"circle moving down\", \"circle\", \"down\"),  # 원을 아래로 움직입니다\n    (\"circle moving left\", \"circle\", \"left\"),  # 원을 왼쪽으로 움직입니다\n    (\"circle moving right\", \"circle\", \"right\"),  # 원을 오른쪽으로 움직입니다\n    (\"circle moving diagonally up-right\", \"circle\", \"diagonal_up_right\"),  # 원을 대각선으로 위쪽 오른쪽으로 움직입니다\n    (\"circle moving diagonally down-left\", \"circle\", \"diagonal_down_left\"),  # 원을 대각선으로 아래쪽 왼쪽으로 움직입니다\n    (\"circle moving diagonally up-left\", \"circle\", \"diagonal_up_left\"),  # 원을 대각선으로 위쪽 왼쪽으로 움직입니다\n    (\"circle moving diagonally down-right\", \"circle\", \"diagonal_down_right\"),  # 원을 대각선으로 아래쪽 오른쪽으로 움직입니다\n    (\"circle rotating clockwise\", \"circle\", \"rotate_clockwise\"),  # 원을 시계방향으로 회전시킵니다\n    (\"circle rotating counter-clockwise\", \"circle\", \"rotate_counter_clockwise\"),  # 원을 반시계방향으로 회전시킵니다\n    (\"circle shrinking\", \"circle\", \"shrink\"),  # 원을 축소시킵니다\n    (\"circle expanding\", \"circle\", \"expand\"),  # 원을 확대시킵니다\n    (\"circle bouncing vertically\", \"circle\", \"bounce_vertical\"),  # 원을 수직으로 튕기게 합니다\n    (\"circle bouncing horizontally\", \"circle\", \"bounce_horizontal\"),  # 원을 수평으로 튕기게 합니다\n    (\"circle zigzagging vertically\", \"circle\", \"zigzag_vertical\"),  # 원을 수직으로 지그재그로 움직입니다\n    (\"circle zigzagging horizontally\", \"circle\", \"zigzag_horizontal\"),  # 원을 수평으로 지그재그로 움직입니다\n    (\"circle moving up-left\", \"circle\", \"up_left\"),  # 원을 왼쪽 위로 움직입니다\n    (\"circle moving down-right\", \"circle\", \"down_right\"),  # 원을 오른쪽 아래로 움직입니다\n    (\"circle moving down-left\", \"circle\", \"down_left\"),  # 원을 왼쪽 아래로 움직입니다\n]\n```\n\n이제 이러한 프롬프트를 사용하여 원의 여러 움직임을 정의했습니다. 다음으로 프롬프트를 기반으로 이 원을 움직이는 수학적 방정식을 코딩해야 합니다.\n\n```js\n# 매개변수가 있는 함수 정의\ndef create_image_with_moving_shape(size, frame_num, shape, direction):\n  \n    # 특정 크기와 흰색 배경으로 새로운 RGB 이미지를 생성합니다\n    img = Image.new('RGB', size, color=(255, 255, 255))  \n\n    # 이미지에 대한 그리기 컨텍스트를 생성합니다\n    draw = ImageDraw.Draw(img)  \n\n    # 이미지의 중앙 좌표를 계산합니다\n    center_x, center_y = size[0] // 2, size[1] // 2  \n\n    # 모든 움직임의 중심으로 위치를 초기화합니다\n    position = (center_x, center_y)  \n\n    # 각 방향을 해당 위치 조정이나 이미지 변환으로 매핑하는 딕셔너리를 정의합니다\n    direction_map = {  \n        # 프레임 번호에 따라 아래로 위치 조정\n        \"down\": (0, frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 왼쪽으로 위치 조정\n        \"left\": (-frame_num * 5 % size[0], 0),  \n        # 프레임 번호에 따라 오른쪽으로 위치 조정\n        \"right\": (frame_num * 5 % size[0], 0),  \n        # 대각선 위 오른쪽으로 위치 조정\n        \"diagonal_up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 대각선 아래 왼쪽으로 위치 조정\n        \"diagonal_down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n        # 대각선 위 왼쪽으로 위치 조정\n        \"diagonal_up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 대각선 아래 오른쪽으로 위치 조정\n        \"diagonal_down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 이미지를 시계방향으로 회전\n        \"rotate_clockwise\": img.rotate(frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),  \n        # 프레임 번호에 따라 이미지를 반시계방향으로 회전\n        \"rotate_counter_clockwise\": img.rotate(-frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),  \n        # 상하로 튕기는 효과를 위해 위치 조정\n        \"bounce_vertical\": (0, center_y - abs(frame_num * 5 % size[1] - center_y)),  \n        # 좌우로 튕기는 효과를 위해 위치 조정\n        \"bounce_horizontal\": (center_x - abs(frame_num * 5 % size[0] - center_x), 0),  \n        # 상하로 지그재그 효과를 위해 위치 조정\n        \"zigzag_vertical\": (0, center_y - frame_num * 5 % size[1]) if frame_num % 2 == 0 else (0, center_y + frame_num * 5 % size[1]),  \n        # 좌우로 지그재그 효과를 위해 위치 조정\n        \"zigzag_horizontal\": (center_x - frame_num * 5 % size[0], center_y) if frame_num % 2 == 0 else (center_x + frame_num * 5 % size[0], center_y),  \n        # 프레임 번호에 따라 오른쪽 위로 위치 조정\n        \"up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 왼쪽 위로 위치 조정\n        \"up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 오른쪽 아래로 위치 조정\n        \"down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n        # 프레임 번호에 따라 왼쪽 아래로 위치 조정\n        \"down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1])  \n    }\n\n    # direction_map에 방향이 있는지 확인합니다\n    if direction in direction_map:  \n        # 방향이 위치 조정에 매핑되는지 확인합니다\n        if isinstance(direction_map[direction], tuple):  \n            # 조정에 따라 위치를 업데이트합니다\n            position = tuple(np.add(position, direction_map[direction]))  \n        else:  # 방향이 이미지 변환에 매핑되는 경우\n            # 변환에 따라 이미지를 업데이트합니다\n            img = direction_map[direction]  \n\n    # 이미지를 numpy 배열로 반환합니다\n    return np.array(img)\n```\n\n위 함수는 선택한 방향에 따라 각 프레임마다 원을 움직이는 데 사용됩니다. 모든 비디오를 생성하기\n\n<div class=\"content-ad\"></div>\n\n```js\n# 비디오 수만큼 반복하여 생성\nfor i in range(num_videos):\n    # 미리 정의된 목록에서 무작위로 프롬프트와 이동 선택\n    prompt, shape, direction = random.choice(prompts_and_movements)\n    \n    # 현재 비디오를 위한 디렉터리 생성\n    video_dir = f'training_dataset/video_{i}'\n    os.makedirs(video_dir, exist_ok=True)\n    \n    # 선택된 프롬프트를 비디오 디렉터리 내의 텍스트 파일에 작성\n    with open(f'{video_dir}/prompt.txt', 'w') as f:\n        f.write(prompt)\n    \n    # 현재 비디오의 프레임 생성\n    for frame_num in range(frames_per_video):\n        # 현재 프레임 번호, 모양, 방향을 기반으로 이동 모양이 있는 이미지 생성\n        img = create_image_with_moving_shape(img_size, frame_num, shape, direction)\n        \n        # 생성된 이미지를 비디오 디렉터리에 PNG 파일로 저장\n        cv2.imwrite(f'{video_dir}/frame_{frame_num}.png', img)\r\n```\n\n위의 코드를 실행하면 전체 훈련 데이터 세트를 생성합니다. 훈련 데이터 세트 파일 구조는 다음과 같습니다.\n\n![Training Dataset Structure](/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_1.png)\n\n각 훈련 비디오 폴더에는 프레임과 텍스트 프롬프트가 포함되어 있습니다. 훈련 데이터세트 샘플을 살펴보겠습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![https://miro.medium.com/v2/resize:fit:1400/1*mzizetR6zOyIheNFtKpo0A.gif](https://miro.medium.com/v2/resize:fit:1400/1*mzizetR6zOyIheNFtKpo0A.gif)\n\n훈련 데이터셋에서 원이 위로 올라가고 오른쪽으로 이동하는 동작을 포함시키지 않았습니다. 이것을 보고 우리가 훈련한 모델을 보지 않은 데이터에서 테스트하는 프롬프트로 사용할 것입니다.\n\n한 가지 더 중요한 점은 우리의 훈련 데이터에는 장면에서 멀어지는 물체나 카메라 앞에 부분적으로 나타나는 많은 샘플이 포함되어 있다는 것입니다. 이는 OpenAI 소라 데모 비디오에서 관찰한 것과 유사합니다.\n\n![https://miro.medium.com/v2/resize:fit:1400/1*RP5M_TEt2H4Mo6OhnlcRLA.gif](https://miro.medium.com/v2/resize:fit:1400/1*RP5M_TEt2H4Mo6OhnlcRLA.gif)\n\n\n<div class=\"content-ad\"></div>\n\n우리가 교육 데이터에 이러한 샘플을 포함시킨 이유는 우리의 모델이 원이 극단적인 구석에서 장면에 들어오더라도 모양을 유지할 수 있는지 테스트하기 위해서입니다.\n\n이제 교육 데이터가 생성되었으므로 교육 비디오를 PyTorch와 같은 딥 러닝 프레임워크에서 주로 사용되는 기본 데이터 유형인 텐서로 변환해야 합니다. 또한, 정규화와 같은 변환 작업을 수행하여 데이터를 더 작은 범위로 스케일링하여 교육 아키텍처의 수렴과 안정성을 개선하는 데 도움이 됩니다.\n\n# 교육 데이터 전처리\n\n텍스트-비디오 작업을 위한 데이터 세트 클래스를 작성해야 합니다. 이 클래스는 교육 데이터 세트 디렉토리에서 비디오 프레임과 해당 텍스트 프롬프트를 읽어 PyTorch에서 사용할 수 있도록 만들어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\n# torch.utils.data.Dataset 클래스를 상속받아 데이터셋 클래스 정의\nclass TextToVideoDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        # 루트 디렉토리와 옵션으로 주어진 변형(transform)으로 데이터셋 초기화\n        self.root_dir = root_dir\n        self.transform = transform\n        # 루트 디렉토리의 모든 하위 디렉토리 나열\n        self.video_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n        # 프레임 경로와 해당 프롬프트를 저장할 리스트 초기화\n        self.frame_paths = []\n        self.prompts = []\n\n        # 각 비디오 디렉토리마다 반복\n        for video_dir in self.video_dirs:\n            # 비디오 디렉토리에 있는 모든 PNG 파일 나열하고 파일 경로 저장\n            frames = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.png')]\n            self.frame_paths.extend(frames)\n            # 비디오 디렉토리에 있는 프롬프트 텍스트 파일 읽어서 내용 저장\n            with open(os.path.join(video_dir, 'prompt.txt'), 'r') as f:\n                prompt = f.read().strip()\n            # 각 프레임에 해당하는 프롬프트를 반복해서 prompts 리스트에 저장\n            self.prompts.extend([prompt] * len(frames))\n\n    # 데이터셋 전체 샘플 수 반환\n    def __len__(self):\n        return len(self.frame_paths)\n\n    # 주어진 인덱스에 해당하는 샘플 반환\n    def __getitem__(self, idx):\n        # 주어진 인덱스에 해당하는 프레임 경로 가져오기\n        frame_path = self.frame_paths[idx]\n        # PIL (Python Imaging Library)을 사용하여 이미지 열기\n        image = Image.open(frame_path)\n        # 주어진 인덱스에 해당하는 프롬프트 가져오기\n        prompt = self.prompts[idx]\n\n        # 지정된 경우 변형 적용\n        if self.transform:\n            image = self.transform(image)\n\n        # 변형된 이미지와 프롬프트 반환\n        return image, prompt\n```\n\n코드 아키텍처를 작성하기 전에 훈련 데이터를 정규화해야 합니다. 배치 크기는 16으로 설정하고 데이터를 섞어 더 많은 무작위성을 도입할 것입니다.\n\n```python\n# 데이터에 적용할 변형 세트 정의\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # PIL 이미지 또는 numpy.ndarray를 텐서로 변환\n    transforms.Normalize((0.5,), (0.5,))  # 평균과 표준편차를 사용하여 이미지 정규화\n])\n\n# 정의된 변형을 사용하여 데이터셋 로드\ndataset = TextToVideoDataset(root_dir='training_dataset', transform=transform)\n# 데이터셋을 반복할 데이터로더 생성\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n```\n\n# 텍스트 임베딩 레이어 구현하기\n\n\n<div class=\"content-ad\"></div>\n\n트랜스포머 아키텍처에서 본 적이 있을 수 있어요. 텍스트 입력을 임베딩으로 변환하고 멀티 헤드 어텐션에서 추가로 처리할 수 있어요. 여기서는 텍스트 임베딩 레이어를 코드로 작성해야 합니다. 이 레이어를 기반으로 GAN 아키텍처 훈련이 임베딩 데이터와 이미지 텐서에서 이루어질 거에요.\n\n```js\n# 텍스트 임베딩을 위한 클래스 정의\nclass TextEmbedding(nn.Module):\n    # vocab_size와 embed_size 매개변수를 사용하는 생성자 메서드\n    def __init__(self, vocab_size, embed_size):\n        # 슈퍼 클래스 생성자 호출\n        super(TextEmbedding, self).__init__()\n        # 임베딩 레이어 초기화\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n\n    # 순전파 메서드 정의\n    def forward(self, x):\n        # 입력의 임베딩 표현 반환\n        return self.embedding(x)\n```\n\n어휘 사전 크기는 훈련 데이터에 기반하여 결정될 거에요. 임베딩 크기는 10일 거에요. 더 큰 데이터셋을 다룬다면 Hugging Face에서 제공하는 임베딩 모델을 선택해서 사용할 수도 있어요.\n\n# 생성자 레이어 구현하기\n\n<div class=\"content-ad\"></div>\n\n이제 GANs에서 생성자가 하는 역할을 이미 알고 있기 때문에, 이 층을 코딩하고 내용을 이해해봅시다. \n\n```python\nclass Generator(nn.Module):\n    def __init__(self, text_embed_size):\n        super(Generator, self).__init__()\n        \n        # 노이즈와 텍스트 임베딩을 입력으로 받는 완전 연결층\n        self.fc1 = nn.Linear(100 + text_embed_size, 256 * 8 * 8)\n        \n        # 입력을 업샘플링하는 전치 합성곱층\n        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n        self.deconv3 = nn.ConvTranspose2d(64, 3, 4, 2, 1)  # RGB 이미지에 대한 출력은 3채널이 됩니다\n        \n        # 활성화 함수\n        self.relu = nn.ReLU(True)  # ReLU 활성화 함수\n        self.tanh = nn.Tanh()       # 최종 출력을 -1과 1 사이의 값으로 만들기 위한 Tanh 활성화 함수\n\n    def forward(self, noise, text_embed):\n        # 채널 차원을 따라 노이즈와 텍스트 임베딩을 연결합니다\n        x = torch.cat((noise, text_embed), dim=1)\n        \n        # 완전 연결층 다음에 4D 텐서로 재구성합니다\n        x = self.fc1(x).view(-1, 256, 8, 8)\n        \n        # ReLU 활성화를 사용하여 전치 합성곱층을 통한 업샘플링\n        x = self.relu(self.deconv1(x))\n        x = self.relu(self.deconv2(x))\n        \n        # 최종 층에서 출력 값을 -1과 1 사이로 만들기 위해 Tanh 활성화를 사용합니다 (이미지용)\n        x = self.tanh(self.deconv3(x))\n        \n        return x\n```\n\n이 생성자 클래스는 무작위 노이즈와 텍스트 임베딩의 결합에서 비디오 프레임을 생성하는 역할을 합니다. 목표는 주어진 텍스트 설명에 조건부로 현실적인 비디오 프레임을 생성하는 것입니다. 네트워크는 노이즈 벡터와 텍스트 임베딩을 하나의 특성 벡터로 결합하는 완전 연결층 (nn.Linear)으로 시작합니다. 이 벡터는 재구성되고, 레이어는 원하는 비디오 프레임 크기로 특성 맵을 점진적으로 업샘플링하는 일련의 전치 합성곱층 (nn.ConvTranspose2d)을 통해 전달됩니다.\n\n이 레이어들은 비선형성을 위해 ReLU 활성화 함수(nn.ReLU)를 사용하고, 최종 레이어는 출력을 [-1, 1] 범위로 스케일링하기 위해 Tanh 활성화(nn.Tanh)를 사용합니다. 따라서 생성자는 추상적이고 고차원의 입력을 시각적으로 입력 텍스트를 잘 나타내는 일관된 비디오 프레임으로 변환합니다.\n\n<div class=\"content-ad\"></div>\n\n# 판별자 레이어 구현\n제너레이터 레이어를 코딩한 후에는 이제 판별자 부분을 구현해야 합니다.\n\n```js\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        # 입력 이미지를 처리하기 위한 합성곱 레이어\n        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)   # 3개의 입력 채널 (RGB), 64개의 출력 채널, 커널 크기 4x4, 스트라이드 2, 패딩 1\n        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1) # 64개의 입력 채널, 128개의 출력 채널, 커널 크기 4x4, 스트라이드 2, 패딩 1\n        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1) # 128개의 입력 채널, 256개의 출력 채널, 커널 크기 4x4, 스트라이드 2, 패딩 1\n        \n        # 분류를 위한 완전 연결 레이어\n        self.fc1 = nn.Linear(256 * 8 * 8, 1)  # 입력 크기 256x8x8 (마지막 컨볼루션 레이어의 출력 크기), 출력 크기 1 (이진 분류)\n        \n        # 활성화 함수\n        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)  # 음의 기울기 0.2를 가지는 Leaky ReLU 활성화 함수\n        self.sigmoid = nn.Sigmoid()  # 최종 출력을 위한 시그모이드 활성화 함수 (확률)\n\n    def forward(self, input):\n        # LeakyReLU 활성화 함수를 사용하여 입력을 합성곱 레이어를 통과시킴\n        x = self.leaky_relu(self.conv1(input))\n        x = self.leaky_relu(self.conv2(x))\n        x = self.leaky_relu(self.conv3(x))\n        \n        # 컨볼루션 레이어의 출력을 펼침\n        x = x.view(-1, 256 * 8 * 8)\n        \n        # 이진 분류를 위해 시그모이드 활성화 함수를 사용하는 완전 연결 레이어를 통과시킴\n        x = self.sigmoid(self.fc1(x))\n        \n        return x\n```\n\n판별자 클래스는 실제 및 생성된 비디오 프레임을 구별하는 이진 분류기로 작동합니다. 비디오 프레임의 신뢰성을 평가하여 생성기가 더 현실적인 출력물을 생성할 수 있게 안내하는 것이 목적입니다. 이 네트워크는 입력 비디오 프레임에서 계층적 특성을 추출하는 합성곱 레이어(nn.Conv2d)로 구성되어 있으며, Leaky ReLU 활성화(nn.LeakyReLU)가 음의 값에 대해 작은 경사도를 허용하면서 비선형성을 추가합니다. 특성 맵은 그 후 펼쳐지고 완전 연결 레이어(nn.Linear)를 통과한 후 시그모이드 활성화(nn.Sigmoid)로 끝나며, 이는 프레임이 실제인지 가짜인지를 나타내는 확률 점수를 출력합니다.\n\n<div class=\"content-ad\"></div>\n\n판별자를 올바르게 분류하도록 훈련함으로써 생성자는 더 설득력있는 비디오 프레임을 만들기 위해 동시에 훈련됩니다. 이것은 생성자가 판별자를 속이려고 할 때 발생합니다.\n\n# 코딩 훈련 매개변수\n\nGAN을 훈련하기 위해 손실 함수, 옵티마이저 등과 같은 기본 구성 요소를 설정해야 합니다. \n\n```js\n# GPU 사용 가능 여부 확인\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 텍스트 프롬프트를 위한 간단한 어휘 생성\nall_prompts = [prompt for prompt, _, _ in prompts_and_movements]  # prompts_and_movements 리스트에서 모든 프롬프트 추출\nvocab = {word: idx for idx, word in enumerate(set(\" \".join(all_prompts).split()))}  # 각 고유 단어에 인덱스가 할당된 어휘 사전 생성\nvocab_size = len(vocab)  # 어휘 크기\nembed_size = 10  # 텍스트 임베딩 벡터 크기\n\ndef encode_text(prompt):\n    # 주어진 프롬프트를 어휘를 사용하여 인덱스의 텐서로 인코딩\n    return torch.tensor([vocab[word] for word in prompt.split()])\n\n# 모델, 손실 함수, 옵티마이저 초기화\ntext_embedding = TextEmbedding(vocab_size, embed_size).to(device)  # vocab_size 및 embed_size로 TextEmbedding 모델 초기화\nnetG = Generator(embed_size).to(device)  # embed_size로 Generator 모델 초기화\nnetD = Discriminator().to(device)  # Discriminator 모델 초기화\ncriterion = nn.BCELoss().to(device)  # 이진 교차 엔트로피 손실 함수\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))  # 판별자에 대한 Adam 옵티마이저\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))  # 생성자에 대한 Adam 옵티마이저\n```\n\n<div class=\"content-ad\"></div>\n\n이 부분은 사용 가능한 GPU에서 코드를 실행할 수 있도록 변환해야 하는 부분입니다. 우리는 vocab_size를 찾기 위해 코드를 작성했고, 생성자와 구분자 모두 ADAM 옵티마이저를 사용하고 있습니다. 원하는 경우 자체 옵티마이저를 선택할 수 있습니다. 여기서 학습률을 0.0002와 같이 작은 값으로 설정하고, 임베딩 크기는 다른 Hugging Face 모델과 비교했을 때 훨씬 작은 10으로 설정하였습니다.\n\n# 훈련 루프 코딩\n\n다른 모든 신경망과 마찬가지로 GAN 아키텍처 훈련을 유사한 방식으로 코딩할 것입니다.\n\n```js\n# 에폭 수\nnum_epochs = 13\n\n# 각 에폭을 반복\nfor epoch in range(num_epochs):\n    # 각 데이터 배치를 반복\n    for i, (data, prompts) in enumerate(dataloader):\n        # 실제 데이터를 장치로 이동\n        real_data = data.to(device)\n        \n        # 프롬프트를 리스트로 변환\n        prompts = [prompt for prompt in prompts]\n\n        # 구분자 업데이트\n        netD.zero_grad()  # 구분자의 기울기를 0으로 초기화\n        batch_size = real_data.size(0)  # 배치 크기 가져오기\n        labels = torch.ones(batch_size, 1).to(device)  # 실제 데이터용 레이블 생성 (1)\n        output = netD(real_data)  # 실제 데이터를 구분자를 통과시키면서 순전파\n        lossD_real = criterion(output, labels)  # 실제 데이터에 대한 손실 계산\n        lossD_real.backward()  # 기울기 계산을 위한 역전파\n       \n        # 가짜 데이터 생성\n        noise = torch.randn(batch_size, 100).to(device)  # 임의의 노이즈 생성\n        text_embeds = torch.stack([text_embedding(encode_text(prompt).to(device)).mean(dim=0) for prompt in prompts])  # 프롬프트를 텍스트 임베딩으로 변환\n        fake_data = netG(noise, text_embeds)  # 노이즈와 텍스트 임베딩으로부터 가짜 데이터 생성\n        labels = torch.zeros(batch_size, 1).to(device)  # 가짜 데이터용 레이블 생성 (0)\n        output = netD(fake_data.detach())  # 가짜 데이터를 구분자를 통과시키면서 순전파 (detached를 사용하여 생성자로 그라디언트가 흐르는 것을 방지)\n        lossD_fake = criterion(output, labels)  # 가짜 데이터에 대한 손실 계산\n        lossD_fake.backward()  # 기울기 계산을 위한 역전파\n        optimizerD.step()  # 구분자 매개변수 업데이트\n\n        # 생성자 업데이트\n        netG.zero_grad()  # 생성자의 기울기를 0으로 초기화\n        labels = torch.ones(batch_size, 1).to(device)  # 구분자를 속이기 위한 가짜 데이터용 레이블 생성 (1)\n        output = netD(fake_data)  # 가짜 데이터(이제 업데이트됨)를 구분자를 통과시키면서 순전파\n        lossG = criterion(output, labels)  # 생성자의 손실 계산 (구분자의 반응에 따른)\n        lossG.backward()  # 기울기 계산을 위한 역전파\n        optimizerG.step()  # 생성자 매개변수 업데이트\n    \n    # 에폭 정보 출력\n    print(f\"에폭 [{epoch + 1}/{num_epochs}] 손실 D: {lossD_real + lossD_fake}, 손실 G: {lossG}\")\r\n```\n\n<div class=\"content-ad\"></div>\n\n백프로패게이션을 통해 생성자와 식별자의 손실이 조정될 것입니다. 우리는 훈련 루프에 13개의 에포크를 사용했습니다. 다양한 값을 테스트해 보았지만, 에포크를 13보다 높게 설정해도 결과에 큰 차이가 나타나지 않았습니다. 게다가, 과적합의 위험이 있습니다. 더 많은 움직임과 모양을 포함하는 더 다양한 데이터셋이 있다면 더 높은 에포크 값을 사용할 수 있겠지만, 현재 상황에서는 그렇지 않습니다.\n\n이 코드를 실행하면 훈련이 시작되고 각 에포크 이후에 생성자와 식별자의 손실이 출력됩니다.\n\n```js\n## 출력 ##\n\n에포크 [1/13] 손실 D: 0.8798642754554749, 손실 G: 1.300612449645996\n에포크 [2/13] 손실 D: 0.8235711455345154, 손실 G: 1.3729925155639648\n에포크 [3/13] 손실 D: 0.6098687052726746, 손실 G: 1.3266581296920776\n\n...\n```\n\n# 훈련된 모델 저장하기\n\n<div class=\"content-ad\"></div>\n\n훈련이 완료되면 훈련된 GAN 아키텍처의 판별자와 생성자를 저장해야 합니다. 이 작업은 단 두 줄의 코드로 간단히 수행할 수 있습니다.\n\n```js\n# 생성자 모델의 상태 사전을 'generator.pth'라는 파일로 저장합니다\ntorch.save(netG.state_dict(), 'generator.pth')\n\n# 판별자 모델의 상태 사전을 'discriminator.pth'라는 파일로 저장합니다\ntorch.save(netD.state_dict(), 'discriminator.pth')\n```\n\n# AI 비디오 생성\n\n토론한 바와 같이, 훈련되지 않은 데이터에 모델을 테스트하는 접근 방식은 개가 공을 던지고, 고양이가 쥐를 쫓는 훈련 데이터의 예와 비교됩니다. 따라서 테스트 프롬프트는 고양이가 공을 던지거나, 개가 쥐를 쫓는 등의 시나리오를 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n특정 경우에서, 원이 위로 움직이고 오른쪽으로 이동하는 움직임은 우리의 훈련 데이터에 포함되어 있지 않아서 모델은 이 특정 움직임을 모르고 있습니다. 그러나, 다른 움직임에는 훈련을 받았습니다. 이 움직임을 사용하여 훈련된 모델을 테스트하고 성능을 관찰할 수 있습니다.\n\n```js\n# 주어진 텍스트 코멘트를 기반으로 비디오를 생성하는 추론 함수\ndef generate_video(text_prompt, num_frames=10):\n    # 텍스트 코멘트를 기반으로 생성된 비디오 프레임을 담을 디렉토리 생성\n    os.makedirs(f'generated_video_{text_prompt.replace(\" \", \"_\")}', exist_ok=True)\n    \n    # 텍스트 코멘트를 텍스트 임베딩 텐서로 인코딩\n    text_embed = text_embedding(encode_text(text_prompt).to(device)).mean(dim=0).unsqueeze(0)\n    \n    # 비디오 프레임 생성\n    for frame_num in range(num_frames):\n        # 임의의 노이즈 생성\n        noise = torch.randn(1, 100).to(device)\n        \n        # Generator 네트워크를 사용하여 가짜 프레임 생성\n        with torch.no_grad():\n            fake_frame = netG(noise, text_embed)\n        \n        # 생성된 가짜 프레임을 이미지 파일로 저장\n        save_image(fake_frame, f'generated_video_{text_prompt.replace(\" \", \"_\")}/frame_{frame_num}.png')\n\n# 특정 텍스트 코멘트와 함께 generate_video 함수 사용 예시\ngenerate_video('circle moving up-right')\n```\n\n위의 코드를 실행하면 생성된 비디오의 모든 프레임을 포함한 디렉토리가 생성됩니다. 이러한 프레임을 모두 하나의 짧은 비디오로 합치기 위해 약간의 코드를 사용해야 합니다.\n\n```js\n# PNG 프레임을 포함한 폴더 경로 정의\nfolder_path = 'generated_video_circle_moving_up-right'\n\n# 폴더 내 모든 PNG 파일 목록 가져오기\nimage_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n\n# 이름을 기준으로 이미지 정렬 (순차적 번호로 가정)\nimage_files.sort()\n\n# 프레임을 저장할 리스트 생성\nframes = []\n\n# 각 이미지를 읽어서 frames 리스트에 추가\nfor image_file in image_files:\n  image_path = os.path.join(folder_path, image_file)\n  frame = cv2.imread(image_path)\n  frames.append(frame)\n\n# 편리한 처리를 위해 frames 리스트를 넘파이 배열로 변환\nframes = np.array(frames)\n\n# 프레임 속도 정의 (초당 프레임 수)\nfps = 10\n\n# 비디오 작성자 객체 생성\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('generated_video.avi', fourcc, fps, (frames[0].shape[1], frames[0].shape[0]))\n\n# 각 프레임을 비디오에 작성\nfor frame in frames:\n  out.write(frame)\n\n# 비디오 작성자 해제\nout.release()\n```\n\n<div class=\"content-ad\"></div>\n\n새로 생성된 비디오가 있는 폴더 경로로 가리키도록 해주세요. 이 코드를 실행한 후에는 AI 비디오가 성공적으로 생성되었습니다. 어떻게 보이는지 함께 확인해봅시다.\n\n![AI 동영상](https://miro.medium.com/v2/resize:fit:1400/1*AUioBh9zHkh2c3f3nGtpsQ.gif)\n\n동일한 에포크 횟수로 여러 번 훈련을 수행했습니다. 양쪽 경우 모두 원이 반 이상 나타나면서 아래에서 시작합니다. 좋은 점은 우리 모델이 양쪽 경우 모두 오른쪽 위로 이동을 시도했다는 것입니다. 예를 들어, 시도 1에서는 원이 대각선으로 위로 이동한 다음 위로 이동했으며, 시도 2에서는 크기가 작아지면서 대각선으로 이동했습니다. 원이 왼쪽으로 이동하거나 완전히 사라지는 경우는 없었습니다. 이것은 좋은 조짐입니다.\n\n# 무엇이 부족할까요?\n\n<div class=\"content-ad\"></div>\n\n저는 이 아키텍처의 다양한 측면을 테스트해본 결과, 훈련 데이터가 중요하다는 것을 발견했습니다. 데이터셋에 더 많은 동작과 모양을 포함시키면 변별성을 높이고 모델의 성능을 향상시킬 수 있습니다. 데이터가 코드를 통해 생성되기 때문에 더 다양한 데이터를 생성하는 데 많은 시간이 걸리지 않습니다. 대신에 논리를 정제하는 데 집중할 수 있습니다.\n\n뿐만 아니라, 이 블로그에서 논의된 GAN 아키텍처는 비교적 직관적입니다. 고급 기술을 통합하거나 기본 신경망 임베딩 대신 언어 모델 임베딩(LLM)을 사용함으로써 보다 복잡하게 만들 수 있습니다. 또한, 임베딩 크기와 같은 매개변수를 조정하는 것이 모델의 효과를 크게 좌우할 수 있습니다.\n\n# 나에 대해\n\n저는 데이터 과학 석사 학위를 가지고 있으며 NLP와 AI 분야에서 두 년 이상 일해왔습니다. 저를 고용하거나 AI 관련 문의 사항이 있으면 언제든지 저에게 물어보세요! 모든 질문에 대해 이메일로 답변드립니다.\n\n<div class=\"content-ad\"></div>\n\n제 LinkedIn 프로필에 연락하세요: [링크](https://www.linkedin.com/in/fareed-khan-dev/)\n\n이메일로 연락하세요: fareedhassankhan12@gmail.com","ogImage":{"url":"/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_0.png"},"coverImage":"/assets/img/2024-06-22-BuildinganAIText-to-VideoModelfromScratchUsingPython_0.png","tag":["Tech"],"readingTime":26},{"title":"Bun 10에 대한 생각과 평가","description":"","date":"2024-06-22 04:36","slug":"2024-06-22-ThoughtsonBun10","content":"\n\n원래는 https://fek.io에서 발행되었습니다.","ogImage":{"url":"/assets/img/2024-06-22-ThoughtsonBun10_0.png"},"coverImage":"/assets/img/2024-06-22-ThoughtsonBun10_0.png","tag":["Tech"],"readingTime":1},{"title":"JavaScript의 와  비교 잘 알려지지 않은 차이점","description":"","date":"2024-06-22 04:35","slug":"2024-06-22-vsinJavaScriptThelittle-knowndifference","content":"\n\n처음에는 원하는 이미지를 맘대로 바꿀 수 있다고 생각할 수 있어요. \n\n틀렸어요. 당신이 생각한 것과는 다르답니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 마침내 고통스러운 버그를 피하기 위해 한번이라도 차이를 꼭 배워야 합니다…","ogImage":{"url":"/assets/img/2024-06-22-vsinJavaScriptThelittle-knowndifference_0.png"},"coverImage":"/assets/img/2024-06-22-vsinJavaScriptThelittle-knowndifference_0.png","tag":["Tech"],"readingTime":1},{"title":"2024년에 주목해야 할 ES15의 5가지 놀라운 새로운 JavaScript 기능","description":"","date":"2024-06-22 04:34","slug":"2024-06-22-5amazingnewJavaScriptfeaturesinES152024","content":"\n\n<img src=\"/assets/img/2024-06-22-5amazingnewJavaScriptfeaturesinES152024_0.png\" />\n\n2024: ES15로 새롭게 업그레이드된 놀라운 JS 기능이 또 한 해 도래했습니다.\n\n다양한 async 기능부터 구문 배열 sugar 및 현대적인 regex까지, JavaScript 코딩은 이제 어느 때보다 쉽고 빠릅니다.\n\n# 1. 네이티브 배열 그룹화가 등장했습니다\n\n<div class=\"content-ad\"></div>\n\nMarkdown 형식으로 table 태그 변경해주세요:","ogImage":{"url":"/assets/img/2024-06-22-5amazingnewJavaScriptfeaturesinES152024_0.png"},"coverImage":"/assets/img/2024-06-22-5amazingnewJavaScriptfeaturesinES152024_0.png","tag":["Tech"],"readingTime":1},{"title":"마이크로소프트, 2024년에 React 대신 무엇을 선택할까","description":"","date":"2024-06-22 04:34","slug":"2024-06-22-MicrosoftisditchingReact","content":"\n\n<img src=\"/assets/img/2024-06-22-MicrosoftisditchingReact_0.png\" />\n\n최근에 Microsoft Edge 팀이 마이크로소프트팀이 Edge를 더 빨라지도록 개선하는 방법에 대해 기사를 썼어요.","ogImage":{"url":"/assets/img/2024-06-22-MicrosoftisditchingReact_0.png"},"coverImage":"/assets/img/2024-06-22-MicrosoftisditchingReact_0.png","tag":["Tech"],"readingTime":1},{"title":"클리커 앱 텔레그램 미니 앱 개발 가이드 Part 2","description":"","date":"2024-06-22 04:33","slug":"2024-06-22-ClickerAppTelegramMiniAppsPart2","content":"\n\n![이미지](/assets/img/2024-06-22-ClickerAppTelegramMiniAppsPart2_0.png)\n\n이 글의 첫 번째 부분에서는 다음을 배웠어요\n\n- BotFather를 사용하여 Telegram Bot을 만드는 방법\n- TMA 프레임워크 사용법\n- 봇에 미니 앱 추가하는 방법\n\n이제 React + TWA + Node를 사용하여 Notcoin과 비슷한 간단한 클리커 앱을 만들어 볼 거에요.\n\n<div class=\"content-ad\"></div>\n\n# 앱 만들기\n\n단계 1: 프로젝트를 초기화하고 React 앱에서 UI 라이브러리를 사용하려면 TWA 및 기타 종속성을 설치하세요.\n\n```js\nnpm i @twa-dev/sdk\n```\n\n단계 2: 간단한 디자인을 만드세요(복잡하지 않아도 됩니다). 저는 3개의 섹션을 가지고 있을 것입니다: 포인트, 에너지 바와 버튼, 그리고 친구 초대 버튼.\n\n<div class=\"content-ad\"></div>\n\n```js\n<button\n  onClick={() => {\n    WebApp.openTelegramLink(\n      `https://t.me/share/url?url=http://t.me/YOUR_BOT_USERNAME?start=fren=${userId}`\n    );\n  }\n>\n  친구 초대하기\n</button>;\n```\n\nuserId는 현재 사용자의 ID로, WebApp.initData 또는 WebApp.initDataUnsafe 메소드를 통해 얻을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nStep 3: 앱을 배포하고 프로젝트에 메뉴 버튼 링크를 추가하세요 (1부 참조)\n\n# 백엔드 처리\n\nStep 1: 노드 파일을 만들고 텔레그램을 위한 종속성 추가하세요\n\n```js\nconst TelegramBot = require('node-telegram-bot-api');\n\nconst token = 'YOUR_TELEGRAM_TOKEN';\nconst bot = new TelegramBot(token, { polling: true });\n```\n\n<div class=\"content-ad\"></div>\n\n단계 2: 사용자로부터 /start 메시지를 받으면 텍스트와 미니 앱을 열 수 있는 인라인 버튼을 전송합니다.\n\n```js\nbot.onText(/\\/start/, async (msg) => {\n\n  const chatId = msg.chat.id;\n  const user = msg.from;\n  try {\n    const user = await getUser(chatId);\n    console.log(msg.text)\n    if (!user.exists) {\n      if (msg.text.includes('fren')) {\n        const referrerId = msg.text.split('=')[1]\n        createReferralUser(chatId, referrerId)\n      }\n    } \n    createUser(chatId);\n  } catch (error) {\n    console.error('오류:', error);\n  }\n  const opts = {\n    reply_markup: {\n      inline_keyboard: [\n        [\n          {\n            text: 'OctaClick 열기',\n            web_app: {\n              url: 'DEPLOYED_APP_URL'\n            }\n          }\n        ]\n      ]\n    }\n  };\n  bot.sendMessage(chatId, '환영합니다! 아래 버튼을 사용하여 미니 앱을 열어보세요:', opts);\n});\n```\n\n이상입니다!\n\n# 축하합니다! 텔레그램 클리커 앱을 만들었습니다.\n\n<div class=\"content-ad\"></div>\n\n이미지를 삽입했습니다!\n\n다른 글들:\n- 파트 1: 텔레그램 미니 앱 초보자 안내\n- Hotjar: 당신의 웹사이트 탐정","ogImage":{"url":"/assets/img/2024-06-22-ClickerAppTelegramMiniAppsPart2_0.png"},"coverImage":"/assets/img/2024-06-22-ClickerAppTelegramMiniAppsPart2_0.png","tag":["Tech"],"readingTime":3}],"page":"24","totalPageCount":156,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}