{"pageProps":{"posts":[{"title":"실생활 비유로 이해하는 JavaScript call, apply, bind 메서드 사용 방법","description":"","date":"2024-06-22 06:09","slug":"2024-06-22-UnderstandingJavaScriptscallapplyandbindMethodswithReal-LifeAnalogies","content":"\n\n\n![image](/assets/img/2024-06-22-UnderstandingJavaScriptscallapplyandbindMethodswithReal-LifeAnalogies_0.png)\n\n자바스크립트는 함수가 작동하는 문맥(this)을 제어하기 위해 call, apply, bind 세 가지 강력한 메서드를 제공합니다. 이러한 메서드를 이해하면 유연하고 재사용 가능한 코드를 작성할 수 있는 능력이 크게 향상됩니다. 각 메서드를 간단한 설명과 현실적인 비유와 함께 살펴보겠습니다.\n\n# call\n\ncall은 한 객체로부터 메서드를 빌려와 다른 객체에 즉시 사용할 수 있도록 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n안녕하세요!\n\n아래 예시를 보시면 함수 호출 시 사용되는 `table` 태그를 Markdown 형식으로 변경하였습니다.\n\n예시:\n\nAlice가 인사하는 메소드를 가지고 있다고 상상해봅시다:\n\n```js\nconst alice = {\n    name: 'Alice',\n    sayHello: function(greeting) {\n        console.log(greeting + ', ' + this.name);\n    }\n};\n\nalice.sayHello('Hi');  // 결과: \"Hi, Alice\"\n```\n\n<div class=\"content-ad\"></div>\n\n밥이 인사를 하고 싶지만 sayHello 메서드가 없어요. call을 사용하면 앨리스의 메서드를 빌려와서 즉시 사용할 수 있어요:\n\n```js\nconst bob = { name: 'Bob' };\n\nalice.sayHello.call(bob, 'Hello');  // 출력: \"Hello, Bob\"\n```\n\n유사성: call은 앨리스의 메서드 책을 빌려와서 바로 사용하여 밥이 누군가에게 인사할 수 있게 도와주는 것처럼 생각해 보세요.\n\n# apply\n\n<div class=\"content-ad\"></div>\n\napply는 call과 비슷하지만, 인수를 배열로 전달할 수 있습니다.\n\n```js\nfunction.apply(thisArg, [argsArray])\n```\n\n예시:\n\n같은 인사 방법을 사용하면, 만약 인사말이 배열에 저장되어 있다면 apply를 사용할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nconst args = ['안녕'];\n\nalice.sayHello.apply(bob, args);  // 출력: \"안녕, Bob\"\n```\n\n비유: apply는 앨리스의 메서드 책을 빌리는 것처럼 즉시 사용하는 것이지만 개별 지시사항을 주는 대신에 지시사항 목록을 전달합니다.\n\n# bind\n\nbind는 제공된 값으로 this 값을 설정하고 주어진 인수 시퀀스로 호출될 때 새 함수를 생성합니다. call 및 apply와 달리 bind는 함수를 즉시 실행하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfunction.bind(thisArg, arg1, arg2, ...)\n```\n\n예시:\n\n만약 나중에 Bob에게 인사를 준비하고 싶다면, bind를 사용하여 그렇게 할 수 있어요.\n\n```js\nconst greetBobLater = alice.sayHello.bind(bob, '좋은 아침');\n\ngreetBobLater();  // 출력: \"좋은 아침, Bob\"\n```\n\n<div class=\"content-ad\"></div>\n\n비유: bind는 밥을 위해 알람 시계를 설정하는 것과 같습니다. 인사말을 미리 설정하고, 시간이 되면 밥이 사용할 수 있습니다.\n\n# 주요 차이점\n\n— 호출 시기:\n\n- call과 apply는 함수를 즉시 호출합니다.\n- bind는 나중에 호출할 수 있는 새로운 함수를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n— 인수 처리:\n\n- call은 개별적인 인수를 사용합니다.\n- apply는 배열로 인수를 사용합니다.\n- bind는 새 함수를 호출할 때 제공할 수 있는 새 함수를 위한 미리 설정된 인수를 사용할 수 있습니다.\n\n## 사용 사례\n\n- call: 함수를 즉시 호출하고 this 컨텍스트를 제어하고 개별적으로 인수를 전달하고 싶을 때 사용합니다. 예: 하나의 객체에서 메서드를 빌려와서 다른 객체에서 즉시 사용할 때.\n- apply: 함수를 즉시 호출하고 배열에 있는 인수를 사용해야 할 때 사용합니다. 예: 배열에 저장된 매개변수 목록을 사용하는 방법.\n- bind: 특정한 this 컨텍스트와 선택적으로 미리 설정된 인수를 사용하여 나중에 호출할 수 있는 함수를 만들어야 할 때 사용합니다. 예: 이벤트 핸들러나 콜백을 위해 메서드를 미리 설정하는 경우.\n\n<div class=\"content-ad\"></div>\n\n# 간단한 비유를 통한 요약:\n\n- call: \"앨리스야, 밥이 지금 `sayHello` 메서드를 빌려와서 사용해도 될까?\" (즉시 사용)\n- apply: call과 동일하지만 목록 형태의 지시사항 제공: \"여기, 밥, 여기 있는 목록 [`안녕`]을 사용해서 인사해봐.\"\n- bind: \"앨리스, 밥이 `좋은 아침`이라는 것으로 나준비를 해놓을 수 있을까?\" (나중 사용을 위한 준비)\n\n# 결론\n\nJavaScript에서 call, apply 및 bind를 이해하면 함수 실행과 this 바인딩에 대한 더 큰 제어력을 가질 수 있습니다. 이러한 방법은 객체 지향 및 함수형 프로그래밍 패턴에서 특히 유용하며 코드의 유연성과 재사용성을 향상시킵니다. 간단한 비유를 사용하여, call과 apply는 즉시 메서드를 빌려와서 사용하는 것과 유사하며, bind는 미래 사용을 위해 메서드를 준비하는 것과 같습니다.\n\n<div class=\"content-ad\"></div>\n\n초보자든 숙련된 개발자든, 이러한 방법을 숙달하는 것은 더 견고하고 유지보수가 쉬운 JavaScript 코드를 작성하는 데 도움이 될 것입니다. 즐거운 코딩하세요!","ogImage":{"url":"/assets/img/2024-06-22-UnderstandingJavaScriptscallapplyandbindMethodswithReal-LifeAnalogies_0.png"},"coverImage":"/assets/img/2024-06-22-UnderstandingJavaScriptscallapplyandbindMethodswithReal-LifeAnalogies_0.png","tag":["Tech"],"readingTime":3},{"title":"JS 정규표현식 성능 문제 해결 방법","description":"","date":"2024-06-22 06:08","slug":"2024-06-22-JSRegexpPerformanceIssue","content":"\n\n## 자주, 우리는 잠재적인 성능 문제를 인식하지 못하고 간단한 문자열 검색을 위해 정규식 표현을 사용합니다.\n\n![이미지](/assets/img/2024-06-22-JSRegexpPerformanceIssue_0.png)\n\n입력 또는 문자열 유효성 검사의 성능 문제의 일반적인 이유 중 하나는 정규식 검사의 복잡성입니다.\n\n크롬 기반 브라우저 (예: Chrome, Edge, Opera 등)에는 정규식 엔진과 관련된 알려진 문제가 있습니다. Firefox도 마찬가지 문제를 가지고 있습니다. 왜냐하면 SpiderMonkey 엔진이 Chrome의 정규식에 동일한 엔진을 사용하기 때문입니다. 문제는 엔진이 정규식을 테스트하는 방법 (백트래킹 알고리즘)에서 나옵니다. 엔진이 적합한 패턴을 찾을 수 없을 때 검색을 완료하는 데 필요한 시간복잡도가 기하급수적으로 높아지기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제를 확인하려면 새 탭을 열고 about:blank로 이동하십시오. 콘솔에서 다음 코드 조각을 실행하십시오. URL에 사용된 ID의 길이에 따라 정규 표현식 검색을 완료하는 데 걸리는 시간이 다를 수 있습니다.\n\n만약 숫자와 단어 \"ID\"가 연달아 나오는 형식의 ID가 있는지 확인하고 싶다면 아래의 코드를 실행해보세요:\n\n```js\nconst url1 = \"www.somesite1.com/18329719832791721285462id/user\"\nconst url2 = \"www.somesite2.com/18329719832791721285462/user\"\n\nlet start = performance.now();\n/(\\d*)*(id)/.exec(url1);\nconsole.log(performance.now() - start)\n\nstart = performance.now();\n/(\\d*)*(id)/.exec(url2);\nconsole.log(performance.now() - start)\n```\n\n위 코드의 성능평가:\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-22-JSRegexpPerformanceIssue_1.png)\n\n만약 우리가 숫자로 구성된 ID 뒤에 \"id\"라는 단어가 오는 존재를 확인하려고 한다고 가정해봅시다. 우리의 정규 표현식에서는 (\\d*) 표현식이 반복되는 숫자 시퀀스를 찾습니다. 하나의 추가와일드카드로 감싸면 엔진은 시퀀스의 시퀀스를 찾습니다. 와일드카드 하나를 제거하면 여전히 동일한 결과를 얻으면서 검색에 필요한 시간을 크게 줄일 수 있습니다.\n\n(백트래킹 알고리즘의 문제에 대해 더 자세히 설명된 것은 여기에서 확인할 수 있습니다)\n\n결론:\n\n\n<div class=\"content-ad\"></div>\n\n- regexp는 필요한 경우에만 사용하고 기본 선택지로 사용하지 마세요. includes() 및 split() 메소드를 사용하여 동일한 결과를 얻을 수 있습니다.\n- 때로는 성능 문제를 일으키지 않는 새로운 regexp 패턴을 수정하거나 만들 수 있습니다.\n- 각 추가된 문자로 성능 문제가 크게 증가하거나 문자열 검색이 있는 경우 해당 함수가 regexp를 사용한 것일 수 있음을 나타낼 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-JSRegexpPerformanceIssue_0.png"},"coverImage":"/assets/img/2024-06-22-JSRegexpPerformanceIssue_0.png","tag":["Tech"],"readingTime":2},{"title":"내 인생을 구한 6가지 Pandas 테크닉","description":"","date":"2024-06-22 06:04","slug":"2024-06-22-6PandasTechniquesthatSavedMyLife","content":"\n\n\n![img](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_0.png)\n\n만약 귀하가 직장이나 사이드 프로젝트에서 데이터 작업을 한다면, 실제로 대부분의 시간을 데이터를 정리, 조작 및 변환하는 데 쓸 것입니다. 사실 데이터 과학자들 사이에서 80%의 시간이 데이터를 다루는 데 소요된다는 것은 인기 있는 트로프입니다.\n\n이 현실을 감안하면, 데이터 조작을 위한 필수적인 Python 라이브러리인 판다스를 이미 사용해 보셨을 것입니다. 저는 판다스를 광범위하게 사용하여 즉석 분석부터 제품 수준의 데이터 파이프라인 구축에 이르기까지 모든 일에 활용했습니다. 제 경험을 통해, 제 워크플로우를 크게 단순화하고 코드 품질을 향상시킨 6가지 주요 기술을 수집했습니다.\n\n이 글에서는 해당 기술들을 탐구하고, 상징적인 Titanic 데이터셋을 사용하여 효과적으로 적용하는 방법을 보여드릴 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 연쇄\n\n아래에 설명된 모든 기술의 중심에는 연쇄(Chaining)가 있습니다. 이는 데이터에 여러 작업을 한 번에 연속적으로 적용할 수 있는 방법으로, 개별적인 단계들이 아니라 하나의 순차적인 파이프라인처럼 보이는 효율적인 절차를 만들어냅니다. 이 스타일은 맷 해리슨(Matt Harrison)로 인해 pandas에서 인기를 얻었으며, 그는 파이썬 및 데이터 과학 교육자이자 Effective Pandas의 저자입니다(강력 추천).\n\n연쇄는 연산을 독립적으로 만들고 함수를 작고 간단하게 유지하는 기존 프로그래밍과는 다른 방식입니다. 대신, 연쇄는 데이터를 점진적으로 정리하거나 보강하는 연산들의 시리즈를 통해 데이터 중심적인 워크플로우를 더 현실적으로 나타냅니다. 이 접근 방식은 초심자에게는 워크플로우의 잠재적인 길이와 복잡성 때문에 위협적일 수 있습니다. 그러나 연쇄는 몇 가지 기본 원칙을 이해하고 워크플로우를 단계별로 쪼개면서 견고한 데이터 파이프라인을 만드는 간단하고 직관적인 방법이 됩니다.\n\n아래에서 데이터셋에 대해 일반적인 작업 세트를 수행하는 샘플 워크플로우를 생성했습니다. 첫 번째 스니펫에서 변수 재할당을 사용하고, 두 번째에서는 연쇄를 사용했습니다. 두 가지 중 어떤 것이 더 깔끔해 보이나요?\n\n<div class=\"content-ad\"></div>\n\n\n![링크1](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_1.png)\n\n![링크2](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_2.png)\n\n체인 방식으로 변수를 생성할 필요가 없다는 것을 주목하세요! 만약 주피터 노트북에서 이 파이프라인을 실행하고 있다면 셀을 직접 실행하여 결과를 확인할 수 있습니다. 프로덕션에서는 결과를 변수에 저장해야 하지만, 다른 네 개의 변수 인스턴스화를 제거하여 코드를 더 깨끗하고 유지보수하기 쉽게 만듭니다.\n\n체이닝 시 시각적 명확성을 유지하려면 각 인덴트가 파이프라인의 더 깊은 수준에 해당하도록 코드를 계층적으로 배치하세요. 이는 워크플로가 더 복잡해질수록 점점 더 중요해집니다. 또한, .assign 메소드에서 \"lambda\" 함수의 사용에 주목하세요. 처음에는 복잡해 보일 수 있지만, 이 람다는 간단히 앞서 언급된 DataFrame을 참조하는 것뿐입니다. 그룹화된 또는 필터된 데이터에 변환을 적용하거나 여러 의존하는 열을 생성할 때 특히 유용합니다.\n\n\n<div class=\"content-ad\"></div>\n\n이제 우리가 체이닝의 중요성과 이점을 이해했으니, 이 접근 방식을 활용하여 복잡한 작업을 쉽게 처리하는 몇 가지 기술을 살펴보겠습니다.\n\n## 중복 행 검사\n\nPandas의 내장 drop_duplicates 함수는 중복 행을 제거하는 데 유용하지만, 중복된 행 자체를 보여주지는 않습니다. 중복을 식별하려면 .duplicated 메서드를 사용하며, 이 메서드는 중복되는 행에 대한 boolean 시리즈를 반환합니다. 여기서 행이 이전 행의 중복인 경우 해당 행은 True가 됩니다. 그러나 이렇게 하면 종종 코드가 복잡해질 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n우리가 목적을 달성했지만, 접근 방식을 개선할 수 있어요. 먼저 바닐라 불리언 인덱싱을 .loc로 대체하여 동적 필터링 표현을 만들 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_4.png)\n\n그리고 완성! 변수 재할당 없이 중복된 행을 필터링했어요. 위에서 언급한 것처럼 .drop_duplicates를 사용하여 중복된 행을 삭제할 수 있지만, 동일한 표현을 사용하여 이전에 행 중복 필터에 ~ 연산자를 추가하여 필터링할 수도 있어요.\n\n![이미지](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_5.png)\n\n<div class=\"content-ad\"></div>\n\nThe ~ operator reverses the boolean conditions of the filter statement, so that only rows that were not duplicated would be returned. This is useful when you want to examine your data and quickly look at the duplicated and non-duplicated rows.\n\n## Value selection using .loc\n\nOne aspect of pandas I found challenging is accessing a specific value in a cell. While we often work with rows or columns in pandas, sometimes we need to retrieve individual values. The typical approach to doing this looks something like the following:\n\n![Value selection using .loc](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_6.png)\n\n<div class=\"content-ad\"></div>\n\n이 방법은 정말 좋지만, .iloc 속성은 조금 강제로 느껴질 수 있어요. 다행히 .loc 메소드를 사용하기 전에 조금의 준비를 해두면 값을 직접적으로 접근할 수 있어요. 중요한 것은 DataFrame의 색인을 필터링할 때 사용하는 열로 설정하는 것이에요. 이렇게 하면 .loc 문에서 이름을 첫 번째 액세서로 사용하고 원하는 열을 두 번째 액세서로 지정할 수 있어요.\n\n![2024-06-22-6PandasTechniquesthatSavedMyLife_7](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_7.png)\n\n다음 기술들은 모두 .pipe을 중심으로 돌아간답니다. 이는 판다스에서 가장 편리한 메소드 중 하나예요. 이를 사용하면 DataFrame이나 series에 미리 정의되거나 lambda를 통해 표현된 사용자 정의 함수를 연속해서 체인할 수 있어요. 하지만 주의해야 할 점은, pipe 함수를 통해 전달된 DataFrame의 결과는 항상 DataFrame이 되는 것은 아니라는 점이에요. 함수가 반환하는 방식에 따라, 추가적인 판다스 변환이 어려울 수도 있어요. 어떻게 작동하는지 살펴봅시다.\n\n## Pipe Ternary\n\n<div class=\"content-ad\"></div>\n\n판다는 데이터의 행, 열 및 테이블 지향 변환을 수행하는 데 탁월하지만, 조건부 작업을 지원하는 데는 한계가 있다는 게 엽기적이에요. 저는 해결책 중 하나로 .pipe 메소드 내부의 람다 함수에서 삼항 연산자를 사용하는 방법을 활용해왔어요. 가령, 타이타닉 데이터셋을 위한 파이프라인을 생성하는 경우를 생각해보죠. \"Cabin\"이라는 열이 있는 것을 보장할 수 없을 때도 있어요. 이럴 때, 아래와 같은 방법을 사용할 수 있어요:\n\n<img src=\"/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_8.png\" />\n\n이 방법은 동작하지만, 체인이 끊어진다는 문제점이 있어요! 일회성 작업으로는 수용할만한 방법이겠지만, 대규모 파이프라인에서 여러 번 이런 상황이 발생하면 가독성이 떨어질 수 있어요. 대신, 파이프 내부에 간단한 람다 함수를 정의하여 삼항 연산자를 수행할 수 있어요:\n\n<img src=\"/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_9.png\" />\n\n<div class=\"content-ad\"></div>\n\n.pipe을 사용하여 DataFrame의 열에서 “Cabin”이 이미 존재하는지를 확인하는 사용자 정의 삼항 함수를 생성합니다. 만약 존재한다면, 함수는 DataFrame을 바로 반환하고, 그렇지 않다면 열을 할당합니다. 이제 조금 더 화려한 것을 살펴보겠습니다...\n\n## 열의 일부에 변환 적용하기\n\n이 기술은 저의 일상 업무에서 가장 유용한 기법으로, 종종 비슷한 열들이 동일한 변환을 필요로 하는 다양한 데이터 유형으로 구성된 데이터 세트를 다룹니다. 예를 들어 타이타닉 데이터 세트를 살펴봅시다. 각 열에는 널 값이 있지만 “Age”와 “Sex” 열만 forward-fill 하려고 합니다. 이를 어떻게 할 수 있을까요? .assign 메서드와 람다 함수를 사용하여 원래 열을 덮어쓰는 방법 중 하나는 다음과 같습니다:\n\n![image](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_10.png)\n\n<div class=\"content-ad\"></div>\n\n하지만 만약 두 개의 열을 변환하는 대신에 열 개를 변환하고 싶다면 어떻게 해야 할까요? 동일한 변환을 반복해서 입력하는 것은 지루해지며, 변환을 변경해야 하는 경우 열 번 수정해야 합니다. 다행히 .pipe, lambda, 그리고 약간의 파이썬 언패킹 마법을 사용하여 더 나은 방법이 있습니다:\n\n![image](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_11.png)\n\n처음에는 혼란스러워 보일 수 있지만, 다른 연결된 흐름과 마찬가지로 한 단계씩 나누어 살펴봅시다. 먼저, .pipe 메서드의 람다 함수는 DataFrame을 새로운 흐름으로 전달합니다 (네, 흐름 내의 또 다른 흐름입니다). 이 흐름에서 우리는 원하는 열만 포함한 작은 DataFrame을 원본 DataFrame에서 언패킹합니다. 이 기술을 사용하면 작은 DataFrame에만 .ffill을 적용하여 원하는 열에만 집중한 다음, 이 열을 직접 원본 DataFrame에 다시 언패킹할 수 있습니다. 이 접근 방식은 구문적으로 더 명확할 뿐만 아니라 .ffill이 한 번만 호출되므로 성능 최적화도 제공합니다!\n\n다음으로, .pipe가 어떻게 사용되어 준비된 데이터를 시각화 라이브러리로 직접 전달하는 데 도움이 되는지 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## Plotly로 변환된 데이터를 바로 시각화하기\n\n나는 직관성과 시각적 매력 때문에 Plotly를 선호하지만, 이 전략은 Matplotlib, Seaborn 및 pandas DataFrame과 NumPy 시리즈와 직접 통합되는 기타 그래프 라이브러리와도 작동합니다. 보통은 데이터 조작을 먼저 수행한 다음 결과를 시각화하는 두 번째 기능을 생성합니다. 그러나 데이터 작업을 진행할 때 반복 속도가 중요합니다. 이상적으로는 시각화를 연쇄적 흐름에 직접 추가하여 분석 결과를 더 잘 검토할 수 있어야 합니다. .pipe 메서드를 사용하면 이를 할 수 있습니다. 예를 들어, 타이타닉의 티켓 가격 분포를 간단히 살펴보고 싶다면:\n\n![image](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_12.png)\n\n이 흐름에서는 람다 함수 대신에 원하는 함수를 .pipe 메서드의 첫 번째 인수로 직접 전달하고 있습니다. 그 후속 인수들은 px.histogram 함수와 관련이 있으며, 판다스가 이들을 추가 키워드 인수로 전달합니다. .pipe 메서드의 결과는 Plotly 차트이며, 이를 통해 연쇄에 직접 Plotly 메서드를 추가하여 파이프라인과 시각화를 효율적으로 통합할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_13.png)\n\n여기 여러분을 위한 것입니다 — 더 나은 그리고 더 효율적인 판다 코드를 작성하는데 도움이 되는 여섯 가지 생명을 구하는 기술입니다. 각 예제에서, 전체 데이터셋으로 흐름을 시작했습니다. 이것은 의도적입니다. 분석할 때마다 데이터를 직접 소스에서 읽는 것이 일반적으로 더 좋기 때문입니다, 특히 노트북 환경에서. 이 방법을 통해 이전 데이터 변환으로 인해 파이프라인 결과가 변경되는 것을 방지하고, 노트북 출력이 새로 고쳐지거나 삭제되어도 파이프라인을 재현 가능하게 유지할 수 있습니다.\n\n위에서 언급한 코드 예제들을 실험해 보고 싶다면, 제 GitHub의 이 기사의 노트북 버전을 확인해보세요. 저에 대해 더 알고 싶거나, 제 프로젝트와 다른 작업에 대해 더 알고 싶다면, 제 웹사이트를 방문해주세요.","ogImage":{"url":"/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_0.png"},"coverImage":"/assets/img/2024-06-22-6PandasTechniquesthatSavedMyLife_0.png","tag":["Tech"],"readingTime":7},{"title":"MERN 스택 완벽 정복 최적의 프로젝트 아키텍처 가이드","description":"","date":"2024-06-22 05:57","slug":"2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture","content":"\n\nMERN 스택은 MongoDB, Express, React 및 Node.js로 구성된 인기있는 웹 개발 프레임워크입니다. MongoDB는 유연한 JSON과 유사한 문서에 데이터를 저장하는 NoSQL 데이터베이스입니다. Express는 Node.js에서 웹 애플리케이션을 구축하기 위한 가벼운 프레임워크입니다. React는 동적 사용자 인터페이스를 구축하기 위한 강력한 프런트엔드 라이브러리이며, Node.js는 서버 측 코드를 실행할 수 있게 해주는 JavaScript 런타임입니다.\n\n![MERN Stack](/assets/img/2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture_0.png)\n\n잘 구성된 프로젝트 구조는 코드 가독성, 확장성 및 협업의 용이성을 유지하는 데 중요합니다. 개발자가 빠르게 파일을 찾고 응용 프로그램 흐름을 이해할 수 있도록 도와줌으로써 디버깅에 소요되는 시간을 줄이고 생산성을 향상시킵니다.\n\n# 개발 환경 설정하기\n\n<div class=\"content-ad\"></div>\n\n먼저, 모든 애플리케이션 코드를 보관할 주요 폴더를 생성합니다. 이 폴더 내에서 Frontend 및 Backend이라는 두 개의 디렉터리를 생성할 것입니다. Frontend은 npm create vite@latest 명령어를 통해 만들고, Backend은 백엔드를 위한 익스프레스 서버를 설정하기 위해 npm init -y 명령어를 통해 초기화할 것입니다. 이 구분은 React가 백엔드 논리를 직접적으로 프론트엔드 코드와 함께 구현하는 것을 지원하지 않기 때문에 필요합니다. 또한, 이 구조는 프로젝트 조직을 깔끔하고 이해하기 쉽게 유지하는 데 도움이 됩니다.\n\nFrontend 폴더에서 다음 명령어를 아래 스크린샷에 표시된 대로 실행해주세요:\n\n![이미지](/assets/img/2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture_1.png)\n\n다음으로, Backend 폴더로 이동하여 npm init -y를 실행하여 프로젝트를 초기화합니다. 이는 package.json 파일을 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Mastering the MERN Stack: A Guide to Perfect Project Architecture](/assets/img/2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture_2.png)\n\nNow we will install dependencies which will be required for setting up the backend server.\n\n![Mastering the MERN Stack: A Guide to Perfect Project Architecture](/assets/img/2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture_3.png)\n\nWhy are we using these libraries?\n\n\n<div class=\"content-ad\"></div>\n\n- express: Node.js 웹 애플리케이션을 빌드하기 위한 최소한이면서 유연한 웹 애플리케이션 프레임워크입니다.\n- mongoose: MongoDB와 Node.js를 위한 ODM(Object Data Modeling) 라이브러리로, 응용 프로그램 데이터 모델링을 위한 스키마 기반 솔루션을 제공합니다.\n- body-parser: 수신 요청 바디를 구문 분석하는 미들웨어로, req.body 속성 하에 사용할 수 있습니다.\n- cors: 다른 도메인에서 리소스를 요청할 수 있도록 하는 Cross-Origin Resource Sharing을 활성화하는 미들웨어입니다.\n- bcrypt: bcrypt 해싱 알고리즘을 사용하여 비밀번호를 안전하게 해싱하고 비교하는 라이브러리입니다.\n\n이제 우리는 요구 사항이 확장 될 경우에도 우리의 프로젝트를 쉽게 찾고 코딩 할 수 있는 확장 가능한 프로젝트 구조를 위해 여러 폴더를 생성할 것입니다.\n\n![Mastering the MERN Stack: A Guide to Perfect Project Architecture](/assets/img/2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture_4.png)\n\n- controllers: 다양한 애플리케이션 엔드포인트의 백엔드 로직과 구현을 처리합니다.\n- middleware: 컨트롤러에 도달하기 전에 요청을 처리하는 함수들입니다.\n- models: 데이터베이스의 구조를 나타내고 데이터 상호 작용을 처리합니다.\n- node_modules: 프로젝트에 설치된 모든 종속성을 포함하는 디렉토리입니다.\n- routes: 애플리케이션 엔드포인트를 정의하고 컨트롤러 함수에 연결합니다.\n- .env: 환경별 변수와 구성을 저장하는 파일입니다.\n- index.js: 어플리케이션의 주진입점으로, 서버를 초기화하고 실행합니다.\n- .gitignore: Git이 무시하고 추적하지 않아야 하는 파일과 디렉토리를 지정합니다.\n- package-lock.json: 종속성의 정확한 버전을 잠그어 일관된 설치를 보장하는 파일입니다.\n- package.json: 프로젝트의 종속성과 스크립트 목록, 프로젝트 이름 및 버전과 같은 메타데이터를 나열하는 파일입니다.\n\n<div class=\"content-ad\"></div>\n\n# 백엔드 폴더 아키텍처를 왜 선택했나요?\n\n소프트웨어 개발에서 프로젝트를 구체적인 파일과 폴더로 구성하는 것은 여러 가지 중요한 목적을 제공합니다. 컨트롤러, 미들웨어, 라우트 및 모델과 같은 폴더로 코드베이스를 분리함으로써 애플리케이션의 유지보수성과 확장성을 향상시킵니다. 각 폴더는 특정 기능을 캡슐화합니다: 컨트롤러는 애플리케이션 로직을 관리하고 응답을 처리하며, 미들웨어는 주요 로직에 도달하기 전에 요청을 가로채고 처리합니다, 라우트는 엔드포인트를 정의하고 해당 컨트롤러에 연결하며, 모델은 데이터베이스와의 구조 및 상호 작용을 나타냅니다.","ogImage":{"url":"/assets/img/2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture_0.png"},"coverImage":"/assets/img/2024-06-22-MasteringtheMERNStackAGuidetoPerfectProjectArchitecture_0.png","tag":["Tech"],"readingTime":4},{"title":"PKG 모듈로 NestJS 빌드하기 리소스 절약의 게임 체인저","description":"","date":"2024-06-22 05:55","slug":"2024-06-22-BuildingNestJSwithPKGModuleisaResource-SavingGameChanger","content":"\n\n<img src=\"/assets/img/2024-06-22-BuildingNestJSwithPKGModuleisaResource-SavingGameChanger_0.png\" />\n\n## 소개\n\nNestJS 개발 세계에서 효율성과 자원 관리는 매우 중요합니다. 기존의 NestJS 애플리케이션을 구축하는 전통적인 방법은 종종 오랜 시간이 소요되고 상당한 자원을 소비하는 경향이 있습니다. 가장 큰 문제 중 하나는 node_modules 디렉토리의 크기인데, 이는 많은 종속성으로 인해 지나치게 커질 수 있습니다. 이로 인해 빌드 시간이 느려지는 것뿐만 아니라 프로젝트를 관리하고 배포하기 어렵게 만들기도 합니다.\n\n여기에 pkg가 등장합니다. pkg를 활용하면 NestJS 애플리케이션 구축을 간소화하고 종속성 처리를 자동화하며 워크플로우를 최적화할 수 있습니다. 이 현대적인 방법은 일반적인 빌드 방법과 대조적으로 node_modules 디렉토리와 전체 프로젝트 크기를 대폭 줄여 자원을 효율적으로 활용하는 더 효율적이고 자원을 아끼는 대안을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n또한, `pkg`는 코드를 바이너리로 컴파일하여 NestJS 앱을 독립적으로 만들어줍니다. 이를 통해 node_modules를 별도로 설치할 필요 없이 어디에서나 직접 실행할 수 있게 됩니다. 이 가변성은 응용 프로그램을 다양한 환경에 쉽게 배포할 수 있도록 하며, 설정 시간을 줄이고 의존성 관리와 관련된 잠재적인 문제를 줄일 수 있습니다.\n\n# 전통적인 빌드 과정\n\n전통적으로 NestJS 애플리케이션을 빌드하는 과정에는 여러 단계가 포함됩니다:\n\n- 의존성 설치: `npm install`을 사용하여 package.json에 나열된 모든 필수 패키지를 다운로드하고 설치합니다. 이로 인해 의존성이 많은 응용 프로그램의 경우 큰 node_modules 디렉토리가 생성될 수 있습니다.\n- 코드 변환: TypeScript를 사용하여 코드를 TypeScript에서 JavaScript로 변환합니다.\n- 배포: 번들 파일과 node_modules를 서버 또는 배포 환경에 업로드합니다.\n\n<div class=\"content-ad\"></div>\n\n이 방법은 효과적이지만 단점도 있습니다:\n\n- 크기: node_modules 디렉토리가 매우 커져서 응용 프로그램을 관리하고 배포하기 어려울 수 있습니다.\n- 시간: 각 단계는 종속성 설치부터 코드 변환 및 번들링까지 시간이 걸립니다.\n- 복잡성: 종속성 관리와 호환성 확보가 프로젝트가 커짐에 따라 어려울 수 있습니다.\n\n# PKG 접근법\n\npkg는 NestJS 애플리케이션을 단일 이진 파일로 컴파일하는 현대적인 대안을 제공합니다. 이것이 프로세스를 어떻게 간소화하는지 살펴보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- 단일 이진 파일: pkg는 응용 프로그램 및 모든 종속성을 독립적인 이진 파일로 컴파일합니다. 이 이진 파일은 node_modules를 설치할 필요 없이 직접 실행할 수 있습니다.\n- 종속성 관리 축소: 종속성을 단일 이진 파일로 패키징함으로써, pkg는 배포 중 별도의 node_modules 디렉토리가 필요하지 않도록 합니다. 이를 통해 다양한 환경에서 종속성을 관리하는 복잡성을 줄일 수 있습니다.\n- 빠른 배포: 응용 프로그램이 이진 파일로 컴파일되기 때문에 배포는 더 빠르고 간단해집니다. 대상 환경에 종속성을 설치할 필요가 없어 배포 프로세스가 간소화됩니다.\n- 향상된 보안: 응용 프로그램을 이진 파일로 패키징하면 공격 표면을 줄이고 종속성을 빌드 시간에 잠금으로 설정하여 보안을 강화할 수 있습니다.\n\n## 안내: pkg 및 Docker를 사용하여 NestJS 응용 프로그램 구축 및 배포하기\n\n본 안내서에서는 NestJS 응용 프로그램을 설정하고, 리소스 효율성을 높이기 위해 pkg를 사용하여 최적화하고, Windows에서 Docker를 사용하여 배포하는 과정을 안내합니다.\n\n## 사전 요구 사항\n\n<div class=\"content-ad\"></div>\n\n시작하기 전에 다음 항목들이 설치되어 있는지 확인하세요:\n\n- Node.js: 시스템에 Node.js가 설치되어 있는지 확인하세요. nodejs.org에서 다운로드할 수 있습니다.\n- NestJS: npm을 사용하여 NestJS를 글로벌로 설치하세요.\n\n```js\nnpm install -g @nestjs/cli\n```\n\n3. npm pkg: NestJS 애플리케이션을 독립적인 이진 파일로 패키징하는 데 전역으로 pkg를 설치하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\nnpm install -g pkg\n```\n\n4. Windows용 Docker Desktop: Docker Hub에서 Docker Desktop을 다운로드하여 설치하세요.\n\n## 단계 1: 새로운 NestJS 프로젝트 생성\n\n먼저, NestJS CLI를 사용하여 새로운 NestJS 프로젝트를 만들어보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nnest new nestjs-pkg-app\ncd nestjs-pkg-app\n```\n\n이렇게 하면 nestjs-pkg-app 디렉토리에 새로운 NestJS 프로젝트가 생성됩니다.\n\n## 단계 2: pkg를 사용하여 응용 프로그램 컴파일하기\n\n다음으로, 프로젝트 디렉토리로 이동하여 pkg를 사용하여 dist 폴더 내의 main.js 파일을 컴파일합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ncd nestjs-pkg-app\nnpm run build\npkg ./dist/main.js --out-path ./compiled\n```\n\n이 명령어는 NestJS 애플리케이션을 컴파일하여 compiled 디렉토리에 독립 실행 바이너리(main)로 만듭니다.\n\n## 단계 3: NestJS 애플리케이션을 도커화\n\n이제 NestJS 애플리케이션을 패키징하고 도커 컨테이너에서 실행할 수 있도록 Dockerfile을 생성해보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n프로젝트 디렉토리(nestjs-pkg-app)에 Dockerfile이라는 파일을 만들어주세요. 아래 내용을 넣어주세요:\n\n```js\nFROM node:14-alpine\n\nWORKDIR /usr/src/app\n\nCOPY ./compiled ./compiled\n\nEXPOSE 3000\n\nCMD [\"/compiled/main\"]\n```\n\n## 단계 4: Docker 컨테이너 빌드 및 실행하기\n\n작성한 Dockerfile을 사용하여 Docker 이미지를 빌드해보세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n도커 파일에 작성된 지시 사항을 기반으로 nestjs-pkg-app라는 이름의 도커 이미지를 빌드하는 명령어입니다.\n\n이제 빌드된 이미지를 사용하여 도커 컨테이너를 실행하십시오.\n\ndocker run -p 3000:3000 nestjs-pkg-app\n\n<div class=\"content-ad\"></div>\n\n이 명령은 Docker 컨테이너를 시작하고 nestjs-pkg-app이라는 이름으로 지정하며, 로컬 머신의 포트 3000을 컨테이너 내부의 포트 3000에 매핑합니다.\n\n## 단계 5: NestJS 애플리케이션에 액세스\n\n웹 브라우저를 열고 http://localhost:3000으로 이동하여 Docker 컨테이너 내에서 실행 중인 NestJS 애플리케이션에 액세스하세요.\n\n# 비교\n\n<div class=\"content-ad\"></div>\n\n이 섹션에서는 전통적인 빌드 방식과 NestJS 애플리케이션을 빌드하고 배포하는 데 npm pkg를 사용하는 방법을 비교해보겠습니다.\n\n컴파일 없이: node_modules로 인해 대용량(약 200MB)\n\n![image](/assets/img/2024-06-22-BuildingNestJSwithPKGModuleisaResource-SavingGameChanger_1.png)\n\n컴파일 포함: 모든 종속성이 하나의 바이너리로 번들링되어 있는 속한 크기(약 51.5MB)입니다.\n\n<div class=\"content-ad\"></div>\n\n```\n![Building NestJS with PKG Module](/assets/img/2024-06-22-BuildingNestJSwithPKGModuleisaResource-SavingGameChanger_2.png)\n\n이 상세한 비교는 사이즈를 중점으로 하여 기존 빌드와 pkg 간의 선택 시 혜택 및 고려 사항을 독자들에게 명확히 전달해줄 것입니다.\n\n# 결론\n\n요약하면, NestJS 애플리케이션을 빌드하는 데 npm pkg를 활용하는 것은 개발 관행의 중요한 진보를 의미합니다. node_modules 디렉토리의 크기를 줄이고 싱글 이진 파일로의 간단한 배포를 통해, npm pkg는 개발자에게 더 효율적이고 확장 가능한 접근 방식을 제공합니다. 자원 사용량을 최적화하고 배포 워크플로우를 간소화하거나 보안을 강화하려는 경우, npm pkg는 현대적인 개발 요구에 부합하는 매력적인 솔루션을 제공합니다.\n\n\n<div class=\"content-ad\"></div>\n\n오늘 npm pkg를 활용하여 더 효율적이고 자원을 절약하는 NestJS 개발의 잠재력을 발휘해보세요. 당신의 프로젝트와 팀이 그것에 감사할 것입니다.","ogImage":{"url":"/assets/img/2024-06-22-BuildingNestJSwithPKGModuleisaResource-SavingGameChanger_0.png"},"coverImage":"/assets/img/2024-06-22-BuildingNestJSwithPKGModuleisaResource-SavingGameChanger_0.png","tag":["Tech"],"readingTime":5},{"title":"100일 코딩 챌린지로 당신의 스킬을 향상시키는 방법","description":"","date":"2024-06-22 05:54","slug":"2024-06-22-TransformYourSkillswiththe100-Days-of-CodingChallenge","content":"\n\n![이미지](/assets/img/2024-06-22-TransformYourSkillswiththe100-Days-of-CodingChallenge_0.png)\n\n# 소개\n\n코딩 실력을 업그레이드할 준비가 되셨나요? 저희가 주최하는 100일 코딩 챌린지는 바로 이를 도와드립니다. GIAIC (신트 주지사 정보기술 이니셔티브)에서 이 챌린지를 진행하고 있는데, 현장에서 5만 명이 넘는 기술 학생들이 학습하고 있는 가운데 6천 명 이상이 참여해 실제로 인증을 받고 있는 것으로 확인됐습니다. 이 챌린지는 코딩에 대한 열정을 불러일으키고 여러분을 새로운 높이로 이끌어 줄 것입니다.\n\n# 배경\n\n<div class=\"content-ad\"></div>\n\n100일 코딩 챌린지는 또 다른 코딩 프로젝트가 아닙니다. GIAIC의 학생 대사 및 교수진으로 활동 중인 제가 주도하는 잘 구성된 프로그램입니다. Sir Daniyal Nagori(CEO)와 Sir Ameen Alam(교수진원장)과 같은 산업 지도자들의 지원을 받아, 100명 이상의 교수진 구성원들과 함께 최신 코딩 기술을 가르치는 것이 목표입니다. 저희는 JavaScript, TypeScript, Web 개발, React 및 Next.js에 초점을 맞추고 있습니다.\n\n# 목표\n\n100일 동안 무엇을 이룰 수 있을까요? 우리가 추구하는 바는 다음과 같습니다:\n\n- 주요 기술 마스터: JavaScript, TypeScript, Web 개발, React 및 Next.js에 대해 심도있게 공부해 보세요.\n- 실제 프로젝트 구축: 산업에 준비되도록 하는 실용적인 프로젝트를 진행해 보세요.\n- 일관성 유지: 매일 코딩하는 습관을 길러주세요. 어떤 기술도 마스터하기에 중요합니다.\n- 커뮤니티 지원: 당신을 격려하고 모든 단계에서 도와 줄 번창하는 커뮤니티와 소통해 보세요.\n\n<div class=\"content-ad\"></div>\n\n# 매일 참여\n\n매일 참여자들을 어떻게 숙련되고 동기부여시킬까요?\n\n저희의 디스코드 커뮤니티에서 매일 아침 새로운 도전이 당신을 기다리는 모습을 상상해보세요. 까다로운 코드 디버깅부터 완전한 웹 애플리케이션을 만드는 것까지, 매일의 과제는 학습 경험을 향상시키고 당신을 기습 상태로 유지하도록 설계되어 있습니다.\n\n- 구조화된 학습 경로: 매일의 도전은 이전에 배운 내용에 기반을 둡니다.\n- 동료 지원: 저희의 디스코드 커뮤니티는 항상 활동적이며, 지원을 제공하고 팁을 공유하며 마일스톤을 축하합니다.\n- 전문 가이드: 경험이 풍부한 멘토들로부터의 정기적인 체크인과 피드백을 통해 당신이 목표를 달성할 수 있도록 도와줍니다.\n\n<div class=\"content-ad\"></div>\n\n# 자원\n\n코딩 여정을 지원하기 위한 다양한 자원을 제공합니다:\n\n- GitHub 저장소: GitHub 저장소에서 모든 챌린지와 프로젝트 템플릿에 액세스할 수 있습니다. 포크하고 스타를 눌러서 여러분만의 코딩 놀이터를 만들어보세요!\n- 공식 웹사이트: 챌린지에 대한 자세한 정보와 과제 및 자원의 날짜별 분할에 대해 알아보기 위해 공식 웹사이트를 방문해주세요.\n- 디스코드 커뮤니티: 활기찬 디스코드 커뮤니티에 참여하여 다른 코더들과 연결하고 토론에 참여하며 즉각적인 지원을 받아보세요.\n\n# 주요 분야\n\n<div class=\"content-ad\"></div>\n\n웹 개발 세계에서 최신 기술을 업데이트하는 것이 중요합니다. 그래서 우리의 도전 과제는 다음과 같이 집중됩니다:\n\n- JavaScript: 웹 개발의 중추로, 동적 웹 애플리케이션을 만드는 데 필수적입니다.\n- TypeScript: TypeScript로 JavaScript를 업그레이드하여 타입 안전성을 추가하고 코드 품질을 향상시킵니다.\n- 웹 개발: 웹 애플리케이션을 구축하고 배포하는 기초를 마스터합니다.\n- 인터넷 기초: 인터넷이 제공하는 모든 훌륭한 기능과 함께 인터넷의 기초를 마스터합니다.\n- React: 가장 인기 있는 JavaScript 라이브러리 중 하나로 사용자 인터페이스를 구축하는 방법을 배웁니다.\n- Next.js: 이 강력한 React 프레임워크로 쉽게 서버 렌더링 애플리케이션을 구축합니다.\n\n# 커뮤니티 영향\n\n100일 코딩 챌린지의 실제 힘은 커뮤니티에 있습니다. 여기 우리 참가자들의 몇 가지 성공 이야기가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- Sarah Khan: \"이 도전에 참여한 것이 내 커리어를 위해 한 최고의 결정 중 하나였어요. 매일의 과제는 도전적이었지만 보람 있었고, 커뮤니티의 지원이 제게 힘을 주었어요. 이제는 자신 있게 React 애플리케이션을 개발할 수 있어요!\"\n- Ali Raza: \"이 도전의 구조와 일관성 덕분에 저는 규율적으로 작업을 계속할 수 있었어요. 실습 프로젝트와 멘토들로부터의 실시간 피드백을 즐겼어요. 웹 개발에 진지한 분이라면 필수적인 도전이에요.\"\n- Fatima Ahmed: \"이 도전에 참여하기 전에 새로운 기술들과 발목을 잡혔었어요. 올바른 지침과 자원으로 100일 동안 얼마나 많은 것을 배울 수 있는지 놀라웠어요.\"\n\n# 결론\n\n코딩 스킬을 다음 수준으로 끌어올리기 준비가 되셨나요? 오늘 100일 코딩 도전에 참여하여 계속해서 배우고 성장하는 개발자들의 세계적인 운동의 일원이 되어보세요. 시작하려는 초보자든 기술을 갈고 닦고자 하는 숙련된 개발자든, 이 도전은 여러분을 위해 설계되었습니다.\n\n공식 웹사이트를 방문하고 GitHub 저장소를 확인하며, 디스코드 커뮤니티에 참여하여 시작해보세요. 함께 코딩하고 배우며 함께 성장해요!\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-TransformYourSkillswiththe100-Days-of-CodingChallenge_1.png)\n\n최신 첨단 기술 소식을 받아보세요! 저를 팔로우해주세요:\n\n- 트위터: @0xAsharib\n- 링크드인: Asharib Ali\n- 깃허브: AsharibAli\n- 웹사이트: asharib.xyz\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-22-TransformYourSkillswiththe100-Days-of-CodingChallenge_0.png"},"coverImage":"/assets/img/2024-06-22-TransformYourSkillswiththe100-Days-of-CodingChallenge_0.png","tag":["Tech"],"readingTime":3},{"title":"나쁜 추상화가 코드를 망치는 이유","description":"","date":"2024-06-22 05:50","slug":"2024-06-22-BadAbstractionsCouldBeRuiningYourCode","content":"\n\n상당히 큰 코드베이스에서 작업 중이라고 상상해 봅시다. 다음 코드에서 문제점을 발견하셨나요?\n\n```js\nconst icons = {\n  delete: getIconPath(\"delete\"),\n  edit: getIconPath(\"edit\"),\n  save: getIconPath(\"save\"),\n};\n```","ogImage":{"url":"/assets/img/2024-06-22-BadAbstractionsCouldBeRuiningYourCode_0.png"},"coverImage":"/assets/img/2024-06-22-BadAbstractionsCouldBeRuiningYourCode_0.png","tag":["Tech"],"readingTime":1},{"title":" Import Map, Micro Frontend, Nx Monorepo에 대해 이야기할 시간","description":"","date":"2024-06-22 05:47","slug":"2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo","content":"\n\n## Native Import Map Overrides를 활용하여 마이크로 프론트엔드 아키텍처에 상당한 이점을 얻는 방법\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_0.png)\n\n이 기사에서는 대규모 기업의 복잡한 인프라 및 팀 조직을 위한 소프트웨어 개발에 대한 내 인식을 근본적으로 바꾼 아키텍처에 대해 공유하고 있습니다.\n\n오랜 시간 동안 이 기사를 쓰고 싶었는데, 이제 그 때입니다! 언제나 표준을 준수하는 것을 중요시해왔고, 최신 도구로 이 아키텍처를 올바르게 다룰 준비가 되었다고 믿습니다.\n\n<div class=\"content-ad\"></div>\n\nesbuild의 등장, 브라우저에서 ES 모듈의 네이티브 지원, import map의 널리 퍼져가는 채택, Native Federation과 같은 도구의 등장, 그리고 Nx 생태계가 모두 결합되어 유연하고 잘 유지되는 Micro Frontend Architecture를 형성하고 있습니다.\n\n제가 다룰 내용은:\n- 실제 이야기!\n- 브라우저에 대한 간단한 상기\n- 간략한 Micro Frontend Architecture 소개\n- Import Map이란 무엇인가?\n- Import Maps와 Overrides의 전체 잠재력 탐색\n- Nx가 확장 가능한 Micro Frontend Architecture를 가능하게 하는 이유\n- Native Federation은 무엇일까?\n- 마지막으로\n\n# 실제 이야기!\n\n컨텍스트를 조금 더 제공해 드리기 위해, 여러 개의 AngularJS 애플리케이션을 더 최신의 Angular 프레임워크로 마이그레이션하도록 주도했습니다. 클라이언트는 AngularJS가 폐기되었다는 공지를 받은 후에 마침내 그 결정을 내렸습니다 (최신 정보 확인을 부탁드려요 🙏).\n\n<div class=\"content-ad\"></div>\n\n일반적인 마이그레이션 프로세스를 사용하는 것이 불가능했어요. 여러 시나리오를 조사한 후에 마이크로 프론트엔드 아키텍처를 선택했어요. 저희가 본 것처럼, 이는 점진적인 마이그레이션을 용이하게 하고, 격리를 제공하며, 여러 팀의 앱을 하나의 통합 플랫폼으로 통합할 수 있도록 도와줄 수 있어요.\n\n당시에는 마이크로 프론트엔드 아키텍처가 아직 인기가 없었고, single-spa 라이브러리만 충분히 성숙했어요. 이는 AngularJS와 Angular을 포함한 여러 프레임워크를 지원하여 우리에게 완벽한 선택이었어요!\n\nSingle-spa는 기능 플래그를 기반으로 AngularJS 또는 Angular 구현체 간에 전환하여 마이크로 프론트엔드를 조정해줘요:\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_1.png)\n\n<div class=\"content-ad\"></div>\n\nsingle-spa를 사용하면서 마이크로 프론트엔드 아키텍처를 구현하는 것에 대한 이해가 크게 향상되었고, 특히 import map 및 마이크로 프론트엔드 오버라이드의 중요한 이점을 강조했습니다. 이러한 도구들은 로컬 개발, 테스트, 배포 경험을 크게 향상시켰습니다.\n\n# 브라우저에 대한 간단한 알림\n\n다음 내용을 이해하기 위해 먼저 브라우저가 웹 애플리케이션을 실행하는 기본 흐름에 대한 기본 사항을 상기하는 것이 중요하다고 생각합니다:\n\n![웹 브라우저 플로우](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_2.png)\n\n<div class=\"content-ad\"></div>\n\n- 첫 번째 단계는 언제나 애플리케이션을 시작하는 데 필요한 모든 것을 갖춘 index.html 파일을 가져오는 것입니다.\n- 그런 다음, 브라우저는 index.html에서 지시한 모든 파일을 로드합니다. 이로는 주로 JavaScript 및 스타일 시트와 같은 애플리케이션의 주 파일들이 포함됩니다.\n- 그 후에는 애플리케이션 또는 사용자 상호작용에 의해 더 많은 요청이 발생하고, 예를 들어 API를 호출하거나 필요한 기능을 로드하는 것이 있습니다.\n\n# 간단히 말하는 마이크로 프론트엔드 아키텍처\n\n간단한 정의부터 시작해봅시다: 마이크로 프론트엔드 아키텍처는 프론트엔드 애플리케이션을 더 작고 관리하기 쉬운 조각으로 나누는 것을 포함합니다. 각 조각은 애플리케이션의 특정 기능이나 도메인을 담당합니다. 이는 종종 마이크로서비스 개념과 비교되지만 프론트엔드 레이어에서 이루어집니다.\n\n애플리케이션이 마이크로 프론트엔드 아키텍처를 따르는지 정확히 판단하는 것은 마이크로서비스의 이상적 크기를 정의하는 것과 마찬가지로 어려울 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n핵심은 여러 기능을 통합해 하나의 애플리케이션을 만들 수 있는 플랫폼을 갖는 것입니다. 이러한 기능들이 Lazy-loaded 구성 요소이든 마이크로 프론트엔드이든, 원칙은 본질적으로 동일합니다.\n\n## 언제 잘 어울리나요?\n\n마이크로 프론트엔드 아키텍처가 유용한 다양한 사용 사례가 있습니다:\n\n![마이크로 프론트엔드 사용 예시](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_3.png)\n\n<div class=\"content-ad\"></div>\n\n- 다양한 프레임워크: 가장 일반적인 사용 사례는 다양한 기술을 하나의 제품으로 통합하는 것인데, 특히 분리된 시스템을 통합하는 데 유용합니다.\n- 팀 분산화: 팀이 독립적으로 작동할 때, 모놀리폴더(monorepo) 내에서 작동하거나 다른 저장소에서 작동하는 경우, 마이크로 프론트엔드는 그들의 작업을 하나의 일관된 제품으로 통합하기를 쉽게 만들어줍니다.\n- 관심사의 분리: 응용 프로그램을 격리된 도메인 및 기능으로 구성하여 더 나은 조직을 위한 이상적입니다.\n- 복잡한 인프라: 기존 환경에 마이크로 프론트엔드를 플러그인하는 능력은 개발 경험을 크게 향상시킬 수 있습니다! 나중에 이 이미유에 대해 자세히 다루겠습니다.\n\n## 주요 개념\n\n마이크로 프론트엔드 아키텍처에서는 각기 다른 개념을 따르는 다양한 종류의 엔터티를 구분합니다:\n\n<div class=\"content-ad\"></div>\n\n- 마이크로 프론트엔드(또는 마이크로 앱)은 호스트가 탐색 또는 라우팅 시에 로드됩니다. 각 마이크로 프론트엔드는 응용 프로그램 내에서 구분된 기능 또는 도메인에 대해 책임을 집니다. 다른 앱과 마찬가지로 자식 라우트와 여러 컴포넌트를 포함할 수 있습니다.\n- 파셀(컴포넌트 또는 노출로도 불림)은 필요 시 독립적으로 로드됩니다. 공유 컴포넌트나 공유 서비스가 될 수 있으며 어디에서나 플러그인할 수 있습니다.\n\n## 도구/프레임워크\n\n마이크로 프론트엔드 아키텍처의 여러 구현이 있으며, 여기서 세 가지 주목할 만한 것에 대해 알아보겠습니다:\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_5.png)\n\n<div class=\"content-ad\"></div>\n\n- Single-spa: 이 프레임워크는 간단하게 유지되며 여러 기술과 함께 작동합니다. 하지만 그 간단함은 한 가지 기술만 사용하는 경우 더 많은 작업을 해야 할 수도 있음을 의미할 수 있습니다.\n- Webpack Module Federation: 거의 모두가 Webpack을 사용하며, 모듈 페더레이션 기능으로 이 사용자들에게 마이크로 프론트엔드를 쉽게 만들어줍니다. 하지만 다른 도구를 사용하는 경우 다른 해결책을 찾아야 할 수도 있습니다.\n- Native Federation: 이 방법은 Webpack의 방법론의 쉬움을 최신 도구인 esbuild나 Vite와 결합하여, 현대적인 개발 관행과 잘 어울리면서 마이크로 프론트엔드 아키텍처를 지원합니다.\n\n# Import Map이란 무엇인가요?\n\n가장 흥미로운 측면부터 시작해보죠. 내 의견으로는, 임포트 맵은 브라우저 기술 중에서 과소평가된 기술입니다. 모든 브라우저와 호환되며, 브라우저에서 직접 JavaScript 모듈을 지원하는 데 역할을 합니다.\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_6.png)\n\n<div class=\"content-ad\"></div>\n\n## 어떻게 작동합니까?\n\n원리는 매우 간단합니다. ES 모듈이 JavaScript 생태계에 도입된 이후로, 우리 모두가 다음과 같은 구문을 사용하기 시작했습니다:\n\n```js\nimport moment from \"moment\";\nimport { partition } from \"lodash\";  \n```\n\n그러나 브라우저에서 ES 모듈을 네이티브로 사용할 때는 JS 파일의 전체 경로를 지정해야 합니다. 다음과 같이:\n\n<div class=\"content-ad\"></div>\n\n```js\nimport moment from \"https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.30.1/moment.min.js\";\nimport { partition } from \"https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.21/lodash.min.js\";\n```\n\n이 방식은 가독성이나 유지보수 측면에서 좋지 않죠? 그래서 라이브러리 이름을 URL에 매핑하는 import map이 만들어졌습니다:\n\n```js\n<script type=\"importmap\">\n{\n  \"imports\": {\n    \"moment\": \"https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.30.1/moment.min.js\",\n    \"lodash\": \"https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.21/lodash.min.js\"\n  }\n}\n</script>\n```\n\n이것은 TypeScript의 경로 매핑과 유사하게 동작하지만 브라우저에서 직접 작동합니다. 이제 동일한 구문을 사용하여 모듈을 로컬로 불러오거나 브라우저에서 불러올 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 임포트 맵은 다음과 같이 인라인으로 지정하거나 외부 파일로 지정할 수 있습니다.\n\n```js\n<script type=\"importmap\" src=\"assets/shared.importmap.json\"></script>\n<script type=\"importmap\" src=\"assets/remotes.importmap.json\"></script>\n```\n\n## 이것이 마이크로 프론트엔드 아키텍처와 어떤 관련이 있나요?\n\n제가 언급한대로, 마이크로 프론트엔드 아키텍처는 브라우저에서 번들을 동적으로로드하고 실제 앱에 통합하는 방법일 뿐입니다.\n\n<div class=\"content-ad\"></div>\n\n이 관리는 호스트의 역할입니다. 그러나 호스트가 ES 모듈을 로드해야 할 때는, 간단히 JS import 시스템을 활용하여, import 맵의 도움을 받아 해당 모듈을 위치에 매핑할 수 있습니다.\n\n비슷하게, 파셀의 경우, 필요할 때 컴포넌트를 로드해야 할 경우, import 맵은 JS import를 현재 위치로 매핑할 것입니다.\n\n## Import Maps은 덮어쓸 수 있습니다!\n\n동일한 HTML에서 여러 import 맵을 선언할 수 있습니다. 이는 두 개의 import 맵이 동일한 키를 선언할 경우, 마지막 것이 이전 것을 덮어쓸 것입니다.\n\n<div class=\"content-ad\"></div>\n\n'img' 태그를 Markdown 형식으로 바꿔보세요.\n\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_7.png)\n\nHTML에 새로운 import map을 주입함으로써, 어떤 번들이든 후킹/매핑할 수 있습니다. 따라서, 마이크로 프론트엔드, 구성 요소, 심지어 공유 라이브러리를 대체할 수 있습니다!\n\n## 보안\n\n웹 응용 프로그램에서 import map을 덮어 쓰는 것은 보안을 감소시키지 않습니다. 왜냐하면 모든 프론트엔드 자산은 공개되어 있고 클라이언트 측에서 수정할 수 있기 때문입니다. 그러나 여러 서버로부터 자산을로드하는 응용 프로그램의 경우, Content-Security-Policy (CSP)를 구성하는 것이 중요합니다.\n\n\n<div class=\"content-ad\"></div>\n\nCSP는 신뢰할 수 있는 도메인 목록을 화이트리스트로 지정하여 크로스사이트 스크립팅(XSS) 및 기타 보안 위협의 위험을 크게 줄입니다. 이 보안 조치는 클라이언트 측 수정이 가능하더라도 응용 프로그램의 무결성과 사용자 안전을 유지합니다.\n\n# Import Maps 및 Overrides의 전체 잠재력 탐색\n\n임포트 맵 및 번들 로딩을 브라우저에서 직접 오버라이드할 수 있다는 원리를 이해했으니, 이 개념을 개발 프로세스 내에서 어떻게 활용할 수 있는지 알아봅시다:\n\n![image](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_8.png)\n\n<div class=\"content-ad\"></div>\n\n## 로컬 개발\n\n대규모 조직에서 복잡한 로컬 환경을 설정하는 것은 종종 다음과 같은 일들을 수반합니다:\n\n- 로컬 머신 설정을 위해 하루 이상 소요될 수 있음.\n- 백엔드 시스템, 로컬 데이터베이스 또는 외부 환경과의 연결, 로컬 대기 시스템 등과 같은 다양한 소프트웨어 설치.\n- 다중 테넌트에 대한 설정 조정.\n- 아침에 로컬 환경이 부팅될 때까지 기다리면서 커피를 마시며 그 날을 유지될 것을 기대하는 것.\n\n특히 UI를 소량 수정해야 할 때 이러한 복잡성은 상당히 괴로울 수 있습니다. 이 정확한 도전에 대처하기 위해 마이크로 프론트엔드 아키텍처와 import 맵 오버라이드를 결합한 방식으로 대응하려고 노력했습니다.\n\n<div class=\"content-ad\"></div>\n\n복잡한 전체 시스템을 실행하는 대신, 로컬 환경을 외부 환경에 연결하여 이미 구축된 복잡성을 사용할 수 있습니다.\n\n이렇게 하려면 로컬에서 마이크로 프론트엔드를 제공하고 원격 환경에서 임포트 맵 오버라이드 원칙을 사용하면 됩니다.\n\n새로 고침 후 브라우저에서 로드되는 마이크로 프론트엔드는 원격 서버에 있는 것이 아니라 로컬 컴퓨터에 있는 것입니다.\n\n중요한 점은 최신 메인 브랜치를 포함하는 실제 환경에 코드를 직접 통합하고 있다는 것입니다. 이는 우리가 \"내 컴퓨터에서는 작동하는데!\"라는 유명한 시나리오를 넘어설 수 있음을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n## 풀 리퀘스트\n\n구현을 완료했고 (그리고 테스트까지 완료했다😋), 주로 공유 코드베이스로 코드를 병합하기 위해 풀 리퀘스트를 생성합니다.\n\n리뷰 용이성\n다시 한 번 중요한 것은, 리뷰 프로세스를 더 쉽게 만들기 위해 import map overriding의 이점을 활용할 수 있습니다. 리뷰어들이 배포하거나 로컬로 코드를 클론할 필요 없이 변경 사항을 확인할 수 있도록 해줍니다:\n\n이 단계에서 CI는 앱을 빌드하고 수정된 마이크로 프론트엔드를 위한 새 번들을 생성합니다. 또한 업데이트된 번들로 영향을 받는 importmap.json을 생성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nUI e2e 테스트를 간소화하세요\nUI 테스트 (모의)에 영향을 받은 import 맵을 사용할 수도 있습니다. 이 시나리오에서 생성된 영향을 받은 importmap.json은 Playwright 또는 Cypress와 같은 도구에 주입되어 영향을 받은 마이크로 프론트엔드를 직접 테스트할 수 있습니다.\n\n## 승인\n\n이 단계는 코드가 프로덕션 배포 준비가 되었는지 확인해야 하는 시점을 의미합니다. 이를 CI에서 자동화하거나 수동으로 할 수 있습니다 (자동화 부탁드려요 🙏).\n\n일반적으로 이는 하루에 여러 번 실행되며, 가장 최신의 코드베이스가 프로덕션을 모방하는 환경에서 실행됩니다. 이 시나리오에서는 모든 번들의 최신 버전을 포함하는 importmap.json을 생성할 것입니다:\n\n<div class=\"content-ad\"></div>\n\n만약 생성된 최근 importmap.json이 성공적이라면, 이는 프로덕션을 위한 릴리스 후보가 될 수 있습니다.\n\n## 프로덕션\n\n릴리스가 검증되고 준비가 되었다면, 프로덕션으로 배포를 고려할 수 있습니다. 여기에서도 importmap.json을 갖는 것은 상당한 장점을 제공합니다.\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_9.png)\n\n<div class=\"content-ad\"></div>\n\n시큐리티에 배포\n프로덕션 환경으로 번들을 언제든지 배포하거나 업로드할 수 있습니다. importmap.json이 그들을 참조할 때까지 로드되지 않습니다. 그러므로 배포는 최신 import map을 수정하고 업로드하는 것만으로 이루어집니다. 이 배포 과정은 단순히 1초만 소요되며 동결이 필요하지 않으며 사용자에게는 완전히 투명합니다.\n\n이전 번들은 캐시에 유지\n또한 importmap.json은 이전 버전의 번들을 아직 참조할 수 있다는 점이 중요합니다. 사실, 일부 마이크로 프론트엔드가 수정되지 않았다면 그들을 위한 새 버전을 생성할 필요가 없습니다.\n\n이는 사용자들이 그들의 브라우저에 이미 캐시되어 있는 기존 버전을 다시로드할 필요가 없다는 것을 의미합니다. 반면, importmap.json은 절대로 캐시되어서는 안됩니다!\n\n카나리아 배포 및 A/B 테스팅\nimportmap.json의 마지막이자 무시할 수 없는 혜택은 동적으로 생성될 수 있다는 것입니다. 이는 마이크로 프론트엔드가 이전 버전 또는 새 버전을 로드해야 할지 결정할 수 있음을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n결과적으로, 특징 플래그나 인증 사용자 기준에 따라 A/B 테스트나 카나리 배포를 쉽게 진행할 수 있습니다!\n\n# Nx가 확장 가능한 마이크로 프론트엔드 아키텍처를 가능하게 합니다\n\n이 글에서는 Nx의 모든 이점에 대해 깊이 파헤치지는 않겠습니다. 이에 관한 내용은 이전 글에서 상세히 다루었으니, 더 자세한 정보는 Nx 웹사이트를 참고하시기 바랍니다.\n\nJavaScript/TypeScript 저장소에만 한정되지 않고 어떤 코드베이스에도 제공되는 가치에 대한 나의 확신은 확고합니다. 공유, 가시성, 성능 향상, 그리고 관행 준수를 강화하는 Nx의 장점은 보편적으로 적용 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n# 표 형식을 Markdown 형식으로 변경해 보세요.\n\n## Monorepo와 Micro Frontend은 정반대인가요?\n\n결코 그렇지 않아요! Monorepo는 코드 유지 보수, 빌드 및 통합 프로세스를 향상시키는 가치를 더합니다. 반면, 마이크로 프론트엔드 아키텍처는 실행 시 혜택을 제공합니다.\n\n두 전략 모두 관심사 분리와 재사용성을 지지하며, 마이크로 프론트엔드를 모노레포에 포함시킴으로써 상당한 이점을 보여줍니다.\n\n## 영향을 받는 마이크로 프론트엔드\n\n<div class=\"content-ad\"></div>\n\n녋스에서 중요한 개념은 영향을 받은 코드에서 작업을 수행하는 능력입니다. 이 기능은 로컬 개발을 간소화하여 원격 환경에서 한 번에 하나의 마이크로 프론트엔드에 작업할 수 있도록 도와줍니다.\n\n빌드, 린트, 테스트와 같은 작업을 영향을 받은 마이크로 프론트엔드에만 제한함으로써 CI/CD 프로세스의 효율성을 크게 향상시킬 수 있습니다. 영향을 받은 마이크로 프론트엔드를 나열하는 영향 파일(importmap.json)을 활용하면 기존 환경에서 PR을 테스트하고 e2e 테스트를 실행하며 점진적인 배포를 용이하게 할 수 있습니다.\n\n## 단일 버전 정책\n\n독립성과 격리는 마이크로 프론트엔드 아키텍처의 핵심 원칙이지만, 일부 서비스와 컴포넌트를 모든 인스턴스 간에 공유하는 것은 불가피합니다.\n\n<div class=\"content-ad\"></div>\n\n단일 버전 정책과 결합된 모노 레포 접근 방식은 마이크로 프론트엔드가 서로 호환되어 융성적인 생태계를 유지하도록 보장합니다.\n\n# Native Federation에 대해 어떻게 생각하세요?\n\n처음에 언급한 것처럼, 이제 생태계가 충분히 성숙해져 Angular이나 esbuild를 사용하는 다른 프레임워크를 사용하여 Nx 모노레포 내에서 Native Federation을 적용할 수 있습니다.\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_10.png)\n\n<div class=\"content-ad\"></div>\n\n죄송하지만 Native Federation과 함께 import map overrides를 구현하지 못했습니다. 그러나 이 문제는 현재 GitHub에서 논의 중입니다:\n\n해당 원칙은 변함없이 유지됩니다. importmap.json을 직접 사용하는 대신 federation.manifest.json을 재정의할 수 있는 옵션이 있습니다. 이는 응용 프로그램 내에서 사용자 정의 코드를 생성하여 번들 재정의를 활성화해야 합니다.\n\n## 해보고 싶으신가요?\n\n- 먼저, 내 GitHub 저장소를 복제하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\ngit clone git@github.com:jogelin/nx-nf.git && cd nx-nf\n```\n\n2. 원하는 패키지를 설치하기 시작하세요:\n\n```js\npnpm install\n```\n\n3. 다음으로, mf-admin과 같이 마이크로 프론트엔드 하나를 시작할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nnpx nx run mf-admin:serve\n```\n\n4. 그런 다음, 이미 배포된 애플리케이션의 URL인 https://nx-nf-a2d7c.web.app/admin 에 접속하세요. 아래의 이미지와 같이 애플리케이션을 확인할 수 있을 거예요:\n\n![애플리케이션 이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_11.png)\n\n5. 이제, 즐겨 사용하는 브라우저 디버깅 도구를 열고 로컬 서버를 원격 애플리케이션에 연결하려면 로컬 스토리지에 이 항목을 추가하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nlocalStorage.setItem('native-federation-override:mfAdmin', 'http://localhost:4203/remoteEntry.json') // mfAdmin을 로컬 서버로 오버라이드합니다\n```\n\n6. 이후에, mf-admin 마이크로 프론트엔드에 수정을 가해주세요. 예를 들어, \"어드민 페이지에 오신 것을 환영합니다\" 메시지를 \"로컬 어드민 페이지에 오신 것을 환영합니다\"로 변경하세요.\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_12.png)\n\n7. 변경 사항을 적용한 후 페이지를 새로고침하면, 원격 서버에 즉시 변경 사항이 반영된 것을 확인할 수 있습니다!\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_13.png\" />\n\n8. 변경 사항을 되돌리려면 로컬 스토리지에서 항목을 제거하고 페이지를 새로 고쳐서 원래 상태를 다시 확인하실 수 있습니다.\n\n```js\nlocalStorage.removeItem('native-federation-override:mfAdmin');\n```\n\n이 방법을 사용하여 모든 마이크로 프론트 엔드를 재정의할 수 있습니다. 그러나 앞서 말씀드린 대로, 네이티브 페데레이션을 사용하는 방법은 import 맵의 기본 동작을 사용하지 않기 때문에 완전히 네이티브한 것은 아닙니다.\n\n<div class=\"content-ad\"></div>\n\n저의 GitHub 저장소에서 Native Federation, Angular 및 Nx를 활용한 모든 코드를 찾아볼 수 있어요.\n\n# 최종 생각\n\n이 탐구를 통해 브라우저의 네이티브 JavaScript 생태계의 강력함을 발견하고 네이티브 ES 모듈에 대한 지원이 더 빠른 빌드 시간 이상의 개발 경험을 향상시킨다는 것을 강조했어요.\n\nimport 맵 원칙의 단순함과 효과적인 접근 방식은 우아한 해결책으로 복잡한 문제를 해결하는 방법을 보여줍니다. 향후 개발이 더 원활하고 직관적으로 되는 곳을 힌트로 알려주며, 네이티브 브라우저 기능을 선호함으로써 맞춤형 프레임워크 구현에 대한 의존성이 줄어드는 미래를 예측합니다.\n\n<div class=\"content-ad\"></div>\n\n게다가, 이 생태계의 일부로서 Nx의 사용은 개발자들이 증진된 민첩성과 정밀성으로 복잡한 프로젝트에 접근할 수 있도록 강력한 도구 모음을 제공합니다.\n\n이와 같은 네이티브 기능들에 대한 더 많은 기대가 높아지고 있으며, 더 간단하면서도 더 강력한 개발 환경이 약속되고 있습니다. Nx와 브라우저 기능의 발전으로, 우리는 고도의 웹 애플리케이션을 구축하는 것이 더욱 접근 가능하고 효율적인 미래로 나아가고 있습니다.\n\n🚀 기대해 주세요!\n\n# 크레딧\n\n<div class=\"content-ad\"></div>\n\n## 조엘 데닝\n\n조엘 데닝은 single-spa의 비전을 이루어낸 인물로, 웹의 진정한 메커니즘에 대한 깊은 통찰력을 자랑하며, 내 의견으로는 마이크로 프론트엔드 아키텍처의 선구자입니다. single-spa 웹사이트와 그의 유튜브 채널을 꼭 확인해보시길 권해드립니다. 비디오가 초창기로 보일지라도, 조엘은 시대를 앞서가고 있으며, 내용은 오늘날에도 여전히 매우 관련성 있습니다.\n\n![이미지](/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_14.png)\n\n## 만프레드 슈타이어\n\n<div class=\"content-ad\"></div>\n\nManfred는 엔지니어, 건축가, 강연가, 트레이너, 컨설턴트 및 작가로, 이야기를 잘 알고 있습니다. Angular뿐만 아니라 모든 사람들을 위해, 마이크로 프런트엔드 및 웹 아키텍처에 관심이 있는 분들에게 Manfred의 책 'Enterprise Angular: Micro Frontends and Moduliths with Angular'과 Angular Architect 팀의 블로그를 살펴보기를 적극 권유합니다.\n\n# 관련 정보\n\n# 쉽게 이해하기 🚀\n\n<div class=\"content-ad\"></div>\n\nIn Plain English 커뮤니티에 참여해 주셔서 감사합니다! 떠나시기 전에:\n\n- 작가를 박수와 팔로우 해주세요 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | 뉴스레터\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io에서 더 많은 콘텐츠 확인하기","ogImage":{"url":"/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_0.png"},"coverImage":"/assets/img/2024-06-22-ItsTimetoTalkAboutImportMapMicroFrontendandNxMonorepo_0.png","tag":["Tech"],"readingTime":14},{"title":"파이썬으로 모멘텀 전략 구현하는 방법","description":"","date":"2024-06-22 05:44","slug":"2024-06-22-MomentumStrategyusingPython","content":"\n\n이번 주에는 점심 시간에 몇몇 동료들과 흥미로운 대화를 나누었습니다. 그들은 투자에 어떤 방법론을 사용하는지 물어보았어요. 저는 '모멘텀 투자'를 사용한다고 언급했는데, 그들은 정확히 무슨 의미인지 이해하기 어려워했어요. 그래서 이 기사를 쓰기로 결심했습니다. 제가 모멘텀 투자를 위해 따르는 단계를 설명하겠습니다.\n\n# 모멘텀 투자란?\n\n모멘텀 투자는 시장에서 이미 존재하는 추세를 기반으로 이익을 얻고자 하는 강력한 전략입니다. 지난 성과가 우수한 주식에 집중함으로써, 투자자들은 모멘텀의 흐름을 타고 인상적인 수익을 얻을 수 있을지도 모릅니다. 이 기사에서는 Nifty 50 주식을 위한 모멘텀 전략에 대해 자세히 살펴보고, 그 방법론을 설명하며 해당 전략을 구현하는 데 도움이 되는 Python 코드 조각을 제공할 것입니다.\n\n모멘텀 투자는 과거에 우수한 성과를 보인 주식이 가까운 미래에도 계속 우수한 성과를 내리라는 전제에 기반합니다. 이 전략은 특정 기간 동안(예: 지난 1년) 우수한 성과를 보인 주식을 매수하고, 일정 기간(예: 1개월) 보유한 후 포트폴리오를 재평가하는 것을 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n# 전략 개요\n\n우리의 모멘텀 전략은 다음과 같은 간단한 단계로 구성되어 있습니다:\n\n- 주식의 우주 선택: 여기서는 Nifty 50 주식에 초점을 맞출 것입니다.\n- 과거 수익률 계산: 각 주식에 대해 12개월 수익률을 계산합니다.\n- 주식 순위 매기기: 주식을 12개월 수익률에 기반하여 순위 매깁니다.\n- 최고 주식 선택: 수익률이 가장 높은 상위 10개 주식을 선택합니다.\n- 매월 리밸런싱: 매달 포트폴리오를 재평가하고 리밸런싱합니다.\n\n# 전략 백테스팅\n\n<div class=\"content-ad\"></div>\n\n백테스팅은 거래 전략의 성과를 평가하는 데 중요합니다. 이전 데이터에 전략을 적용하여 과거 성과를 평가하고 잠재적인 미래 성과에 대한 통찰력을 얻을 수 있습니다.\n\n파이썬을 사용하여 이 전략을 3년 동안 백테스트하고 해당 결과를 지수(여기서는 Nifty50)의 매수 및 보유 전략과 비교해보겠습니다.\n\n## 단계 1: 데이터 수집\n\n먼저, 지난 3년간 Nifty 50 주식의 히스토리컬 가격 데이터를 수집해야 합니다. 여기서는 야후 파이낸스 API를 사용하여 지난 3년간의 히스토리컬 데이터를 가져왔습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Nifty 50 주식 기호 목록\nnifty50_symbols = [\"RELIANCE.NS\", \"HDFCBANK.NS\", \"INFY.NS\", \"ICICIBANK.NS\", \"TCS.NS\", \"KOTAKBANK.NS\", \n                   \"HINDUNILVR.NS\", \"SBIN.NS\", \"BHARTIARTL.NS\", \"HDFC.NS\", \"ITC.NS\", \"BAJFINANCE.NS\", \n                   \"ASIANPAINT.NS\", \"HCLTECH.NS\", \"LT.NS\", \"MARUTI.NS\", \"AXISBANK.NS\", \"ULTRACEMCO.NS\", \n                   \"WIPRO.NS\", \"NESTLEIND.NS\", \"ONGC.NS\", \"TITAN.NS\", \"SUNPHARMA.NS\", \"M&M.NS\", \n                   \"POWERGRID.NS\", \"JSWSTEEL.NS\", \"TATASTEEL.NS\", \"TECHM.NS\", \"HDFCLIFE.NS\", \"COALINDIA.NS\", \n                   \"BPCL.NS\", \"INDUSINDBK.NS\", \"BAJAJ-AUTO.NS\", \"IOC.NS\", \"BRITANNIA.NS\", \"HEROMOTOCO.NS\", \n                   \"ADANIPORTS.NS\", \"DRREDDY.NS\", \"GRASIM.NS\", \"CIPLA.NS\", \"DIVISLAB.NS\", \"EICHERMOT.NS\", \n                   \"BAJAJFINSV.NS\", \"SHREECEM.NS\", \"TATAMOTORS.NS\", \"SBILIFE.NS\", \"ADANIENT.NS\", \n                   \"DABUR.NS\", \"VEDL.NS\", \"APOLLOHOSP.NS\"]\n\n# 시간 범위 정의\nend_date = datetime.today()\nstart_date = end_date - timedelta(days=365*3)  # 최근 3년간\n\n# 데이터 가져오기\ndata = yf.download(nifty50_symbols, start=start_date, end=end_date)['Adj Close']\n\n# 누락된 값 채우기\ndata = data.fillna(method='ffill').dropna()\n\n# 데이터의 처음 몇 행 표시\nprint(data.head())\n```\n\n## 단계 2: 전략 구현 및 수익률 계산\n\n그다음, 모멘텀 전략을 구현하고 지난 3년간 포트폴리오 수익률을 계산합니다.\n\n```js\ndef calculate_portfolio_returns(data, top_n=10):\n    # 월간 수익률 계산\n    monthly_returns = data.resample('M').ffill().pct_change()\n    \n    # 12개월 수익률 계산\n    twelve_month_returns = monthly_returns.rolling(window=12).apply(lambda x: np.prod(1 + x) - 1, raw=True)\n    \n    # 월별 포트폴리오 가치를 저장할 빈 목록 초기화\n    portfolio_values = []\n    \n    # 초기 자본 부여\n    initial_capital = 100000  # 1 lakh\n    capital = initial_capital\n    \n    # 13번째 달부터 시작하여 각 월 반복\n    for i in range(12, len(twelve_month_returns)):\n        # 현재 달의 12개월 수익률 가져오기\n        current_returns = twelve_month_returns.iloc[i]\n        \n        # 주식을 12개월 수익률에 따라 순위 매기기\n        ranked_stocks = current_returns.sort_values(ascending=False)\n        \n        # 상위 N개 주식 선택\n        top_stocks = ranked_stocks.head(top_n).index\n        \n        # 각 주식에 대한 동일 가중치 계산\n        weight = 1 / top_n\n        \n        # 현재 달의 포트폴리오 수익률 계산\n        portfolio_return = (monthly_returns.iloc[i][top_stocks] * weight).sum()\n        \n        # 자본 업데이트\n        capital = capital * (1 + portfolio_return)\n        \n        # 현재 자본을 포트폴리오 가치 목록에 추가\n        portfolio_values.append(capital)\n    \n    # 포트폴리오 가치 목록을 pandas Series로 변환\n    portfolio_values = pd.Series(portfolio_values, index=twelve_month_returns.index[12:])\n    \n    return portfolio_values\n\n# 포트폴리오 수익률 계산\nmomentum_portfolio_returns = calculate_portfolio_returns(data)\n\n# 포트폴리오 수익률 표시\nprint(momentum_portfolio_returns)\n```\n\n<div class=\"content-ad\"></div>\n\n## 단계 3: Nifty 50 지수와 비교\n\n모멘텀 전략의 성능을 평가하기 위해 해당 전략의 수익률을 동일 기간 동안 Nifty 50 지수의 수익률과 비교합니다.\n\n```js\n# Nifty 50 지수 데이터 가져오기\nnifty50_index = yf.download(\"^NSEI\", start=start_date, end=end_date)['Adj Close']\n\n# Nifty 50 월간 수익률 계산하기\nnifty50_monthly_returns = nifty50_index.resample('ME').ffill().pct_change()\n\n# Nifty 50 누적 수익률 계산하기\nnifty50_cumulative_returns = (1 + nifty50_monthly_returns).cumprod()\n\n# 모멘텀 포트폴리오 누적 수익률 계산하기\nmomentum_cumulative_returns = (1 + momentum_portfolio_returns.pct_change()).cumprod()\n\n# 결과 그래프로 플로팅하기\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.plot(momentum_cumulative_returns, label='모멘텀 포트폴리오')\nplt.plot(nifty50_cumulative_returns, label='Nifty 50 지수', linestyle='--')\nplt.title('모멘텀 포트폴리오 vs Nifty 50 지수')\nplt.xlabel('날짜')\nplt.ylabel('누적 수익률')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n<img src=\"/assets/img/2024-06-22-MomentumStrategyusingPython_0.png\" />\n\n<div class=\"content-ad\"></div>\n\n# 결과 및 분석\n\n위의 그림은 지난 3년간 모멘텀 포트폴리오의 누적 수익률을 Nifty 50 지수와 비교한 것입니다. 다음은 주요 관측 사항입니다:\n\n- 이 기간 동안 모멘텀 포트폴리오가 일반적으로 Nifty 50 지수를 능가하여 모멘텀 전략의 효과를 입증했습니다.\n- 모멘텀 포트폴리오가 상당한 변동을 겪는 등의 변동성이 있었는데, 이는 모멘텀 기반 전략에 특징적인 것입니다.\n- 전반적으로, 모멘텀 전략은 Nifty 50 지수를 단순 보유하는 것보다 더 높은 투자 수익을 제공했습니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n니프티 50 주식들에 대한 모멘텀 투자 전략은 지난 3년 동안 기대를 불러일으켰어요. 12개월 수익률에 기반한 상위 10개 주식을 선택하고 매월 포트폴리오를 리밸런싱 함으로써, 이 전략은 니프티 50 지수를 능가했어요. 하지만 과거 성과가 미래 성과를 반영한다는 점을 명심해야 하며, 투자자는 이 전략을 실행하기 전에 위험 허용 수준 및 투자 목표를 신중히 고려해야 해요.\n\n제공된 Python 코드 스니펫을 사용하여 이 모멘텀 전략을 백테스트하고 원하는 대로 사용자 정의할 수 있어요. 모멘텀 투자는 투자자의 무기로 강력할 수 있지만, 일관된 성공을 거두기 위해서는 규율과 체계적인 접근이 필요해요.","ogImage":{"url":"/assets/img/2024-06-22-MomentumStrategyusingPython_0.png"},"coverImage":"/assets/img/2024-06-22-MomentumStrategyusingPython_0.png","tag":["Tech"],"readingTime":6},{"title":"PostgreSQL 데이터베이스 변경 사항을 벡터 스토어로 스트리밍하는 방법","description":"","date":"2024-06-22 05:42","slug":"2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore","content":"\n\n## CDC (Change Data Capture)를 사용하여 벡터 저장소를 최신 상태로 유지하는 방법, Python 및 Redpanda\n\n![Image](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_0.png)\n\n벡터 데이터베이스와 의미론적 검색의 등장으로 많은 애플리케이션의 검색 결과 품질이 향상되었습니다. 그러나 전통적인 인덱스 유지 보수 문제는 여전히 존재합니다. \"검색 가능한 콘텐츠\"(제품 설명, 웹 페이지, 연구 논문 요약)가 지속적으로 업데이트될 때 사용자의 검색 경험을 방해하지 않고 검색 인덱스를 새로 고치는 방법은 무엇일까요? 많은 팀은 이 문제를 점진적 색인화로 해결합니다.\n\n예를 들어, DoorDash 엔지니어링 팀이 2021년에 점진적 색인화에 대해 쓴 글에서는 CDC와 Apache Kafka를 사용하여 색인을 점진적으로 업데이트하는 방법을 설명했습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 일반적인 검색 인덱스보다 계산량이 많을 수 있는 벡터 데이터베이스로 동일한 작업을 수행하는 방법을 보여드리려고 합니다.\n\n# 점진적 인덱싱을 위한 지속적인 이벤트 기반 벡터 입력 사용하기\n\nCDC를 기반으로 한 인덱싱 파이프라인의 간소화된 버전을 살펴봅시다. 데이터베이스에 새 제품 항목이 추가되는 즉시, 변경 내용의 세부 정보와 함께 이벤트가 Redpanda(카프카와 유사한 메시지 브로커)로 전송됩니다. 소비자 프로세스는 이벤트가 도착하는 즉시 이를 임베딩 모델에 전달합니다. 임베딩이 생성되고 세부 정보가 관련 벡터로 보강됩니다. 이 데이터는 다른 Kafka 토픽으로 스트리밍되어 인계 처리 프로세스가 이를 소비하고 벡터 데이터베이스가 처리할 수 있는 속도에 맞춰 벡터를 업서트합니다.\n\n저는 여러분이 직접 복제할 수 있는 프로토타입 애플리케이션을 작성했습니다. 메시지 브로커로 Redpanda의 로컬 설치를 사용합니다. 파이프라인의 각 단계는 각각의 Docker 컨테이너에서 실행되며, 각 컨테이너는 Quix Streams의 인스턴스를 실행합니다. Quix Streams는 Kafka 프로듀서/컨슈머(예: kafka-python)와 강력한 스트림 처리 라이브러리(예: Faust)인 오픈소스 파이썬 라이브러리입니다.\n\n<div class=\"content-ad\"></div>\n\n아래 다이어그램은 파이프라인의 구성 요소를 보여줍니다.\n\n![다이어그램](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_1.png)\n\n이 파이프라인은 Qdrant(벡터 저장소)와 PostgreSQL(소스 데이터베이스)의 로컬 인스턴스를 사용하며, Redpanda의 토픽을 통해 Quix Streams를 사용하여 변경 사항을 스트리밍합니다.\n\n이 프로토타입의 전체 코드는 동봉된 GitHub 저장소에서 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 준비물\n\n- 파이프라인을 시도하려면 도커가 설치되어 있어야 합니다. 이를 설치하는 가장 간단한 방법은 Linux, Windows 또는 Mac에 Docker Desktop Docker를 설치하는 것입니다.\n- 또한 Git도 필요합니다. 이미 Git이 없다면 Git 웹 사이트에서 다운로드할 수 있습니다.\n\n먼저, 파이프라인을 실행하고 데이터를 입력하고 검색에 미치는 영향을 관찰하는 방법을 안내해 드리겠습니다. 그 후에는 파이프라인을 구동하는 코드를 안내해 드리겠습니다.\n\n# 코드 가져오기\n\n<div class=\"content-ad\"></div>\n\n터미널에서 다음 Git 명령을 실행하여 코드를 가져올 수 있어요.\n\n```js\ngit clone https://github.com/quixio/template-vector-cdc-local\n```\n\n그런 다음, Docker Compose를 실행하세요:\n\n```js\ndocker compose up -d\n```\n\n<div class=\"content-ad\"></div>\n\n이 명령은 8080, 8082 및 5050 포트를 필요로 하는 여러 서비스를 시작합니다. 해당 포트에서 실행 중인 애플리케이션이 있으면 먼저 중지해야 합니다.\n\n콘솔에 다음 예제와 유사한 출력이 표시됩니다:\n\n```js\n[+] Running 10/10\n ✔ Network template-vector-cdc-local_default               Created                                                 0.0s\n ✔ Container template-vector-cdc-local-console-1           Started                                                 0.1s\n ✔ Container template-vector-cdc-local-redpanda-1          Started                                                 0.1s\n ✔ Container postgresdb                                    Started                                                 0.1s\n ✔ Container qdrant                                        Started                                                 0.1s\n ✔ Container create_embeddings                             Started                                                 0.0s\n ✔ Container template-vector-cdc-local-postgresql_cdc-1    Started                                                 0.0s\n ✔ Container streamlit_search                              Started                                                 0.0s\n ✔ Container template-vector-cdc-local-postgresql_admin-1  Started                                                 0.0s\n ✔ Container ingest_to_qdrant                              Started                                                 0.0s\n```\n\n# 데이터베이스 설정\n\n<div class=\"content-ad\"></div>\n\n먼저, 기본 테스트 데이터벤스에 테이블을 생성하고 일부 데이터를 추가하겠습니다.\n\n전자 상거래에 중점을 두고 있으므로 온라인 서점을 운영하고 있으며 도서 카탈로그를 지속적으로 업데이트하고 있습니다. 새로운 책이 추가될 때마다 최신 책 설명에 대한 임베딩이 있는 벡터 저장소를 확인해야 합니다.\n\n여기서 책 카탈로그 업데이트를 시뮬레이션하기 위해 pgAdmin을 사용하여 쿼리를 실행할 것입니다. 이는 PimCore와 같은 \"실제\" PIM(Product Information Management) 시스템을 대신하는 역할을 할 것입니다.\n\n먼저, demo PostgreSQL DB에 pgAdmin을 연결해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## pgAdmin에 로그인하기\n\n- 브라우저에서 http://localhost:5050으로 접속하여 pgAdmin UI를 엽니다.\n- 사용자 이름 \"admin@admin.com\"과 비밀번호 \"root\"로 pgAdmin에 로그인합니다.\n\n## PostgreSQL 데이터베이스에 대한 연결 구성\n\n- `Servers`를 클릭하고 `Register` 서버를 선택합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_2.png)\n\n- 나타나는 대화 상자에서 \"일반\" 탭에 이름을 입력한 후, 호스트를 \"postgresdb\"로 설정하고 \"root\"를 사용자 이름과 암호로 설정하고 저장을 클릭하세요.\n\n![이미지](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_3.png)\n\n# 데이터 추가\n\n<div class=\"content-ad\"></div>\n\n저희 데모 Postgresql 데이터베이스에는 “books” 테이블이 미리 구성되어 있어서 데이터를 추가하기만 하면 됩니다.\n\n- 서버 ' postgresdb ' 데이터베이스로 이동하여 test_db를 마우스 오른쪽 버튼으로 클릭하고 '쿼리 도구'를 선택합니다.\n\n![이미지](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_4.png)\n\n- 나타나는 쿼리 도구에 다음 쿼리를 붙여넣고 실행하여 첫 번째 일괄 Sci-fi 책을 추가합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nINSERT INTO books (name, description, author, year) VALUES\n('The Time Machine', 'A man travels through time and witnesses the evolution of humanity.', 'H.G. Wells', 1895),\n('Brave New World', 'A dystopian society where people are genetically engineered and conditioned to conform to a strict social hierarchy.', 'Aldous Huxley', 1932),\n('An Absolutely Remarkable Thing', 'A young woman becomes famous after discovering a mysterious alien artifact in New York City.', 'Hank Green', 2018),\n('Dune', 'A desert planet is the site of political intrigue and power struggles.', 'Frank Herbert', 1965),\n('Foundation', 'A mathematician develops a science to predict the future of humanity and works to save civilization from collapse.', 'Isaac Asimov', 1951),\n('Snow Crash', 'A futuristic world where the internet has evolved into a virtual reality metaverse.', 'Neal Stephenson', 1992),\n('The War of the Worlds', 'A Martian invasion of Earth throws humanity into chaos.', 'H.G. Wells', 1898),\n('The Hunger Games', 'A dystopian society where teenagers are forced to fight to the death in a televised spectacle.', 'Suzanne Collins', 2008);\r\n```\n\n위 내용대로 변경하면 자동으로 채택되고 데이터는 파이프라인을 통해 전송됩니다.\n\n로컬 Redpanda 인스턴스에서 Redpanda 콘솔로 확인할 수 있습니다: http://localhost:8080/overview\n\nTopics로 이동하여 `postgres-cdc-data`를 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n“OFFSET” 드롭다운에서 “Newest”를 선택하면 메시지가 들어오기 시작해야 해요:\n\n<img src=\"/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_5.png\" />\n\n이제 벡터가 제대로 소화되었는지 확인해봐요.\n\n# Streamlit Vector Search UI 사용하기\n\n<div class=\"content-ad\"></div>\n\n데모 애플리케이션에는 벡터 검색을 테스트할 수 있는 간단한 검색 UI(스트림릿으로 구축됨)가 포함되어 있어요.\n\n다음 URL을 통해 액세스할 수 있어요: http:// localhost:8082\n\n검색 UI에서 \"battle in space\"를 검색해보세요 — 최상위 결과는 \"The War of the Worlds\"여야 해요. 지금은 지구에서 벌어지는 전투에 관한 책이지만, 우리 도서 카탈로그에서 현재 최고의 배치로 보여요.\n\n![이미지](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_6.png)\n\n<div class=\"content-ad\"></div>\n\n우리는 \"화성\"이 \"우주\"와 의미적으로 가깝고 \"침공\"이 \"전투\"와 의미적으로 가깝기 때문에 상위 결과와 일치했을 가능성이 있다고 추측할 수 있습니다. 그러나 문장 임베딩 모델 같이 더 정교한 모델을 사용하면 결과가 다를 수 있습니다. 그러나 그 모델은 꽤 무거운 라이브러리이기 때문에 이 프로토타입에서는 제외했습니다.\n\n이제 목록에 몇 권의 책을 추가하고 검색 결과를 개선할 차례입니다.\n\npgAdmin (http://localhost:5050/)로 돌아가서 다음 SQL 쿼리를 실행하세요.\n\n```js\nINSERT INTO books (name, description, author, year) VALUES\n('Childhood''s End', '평화로운 외계 침공으로 인해 인류의 어린 시절이 종말을 맞이한다.', '아서 C. 클라크', 1953),\n('The Day of the Triffids', '유성우로 대부분의 인구가 실명하면서 공격적인 식물이 지배를 시작한다.', '존 윈덤', 1951),\n('The Three-Body Problem', '인류가 위기의 먼 외계 문명으로부터 가능한 침공을 직면한다.', '류씬', 2008),\n('The Puppet Masters', '찐렁이 모양의 외계인이 지구를 침공하여 인간에 붙어 그들의 마음을 통제한다.', '로버트 A. 하인라인', 1951),\n('The Kraken Wakes', '해양 심층에서 나오는 외계 생명체가 인류를 공격하기 시작한다.', '존 윈덤', 1953),\n('The Invasion of the Body Snatchers', '작은 마을이 주민 중 일부가 식물과 같은 씨앗에서 나오는 완벽한 물리적 사본에 교체되는 것을 발견한다.', '잭 핀니', 1955),\n('Out of the Dark', '외계종족이 지구를 침공하여 인류의 생존 의지를 과소평가한다.', '데이비드 웨버', 2010),\n('Old Man''s War', '지구의 노인들이 별간 전쟁에 참전하게 되며 새로운 외계 문화와 위협을 발견한다.', '존 스캘지', 2005);\n```\n\n<div class=\"content-ad\"></div>\n\n만약 이것이 프로덕션 상황이었다면, 새로운 제목은 책 카탈로그에 등록될 수 있지만 벡터 DB에는 등록되지 않을 수 있어요. 이 때문에 재고 팀에게 정기적으로 책 카탈로그의 스냅샷을 내보내서 벡터 스토어로 가져와야 할지도 모를 거예요. 한편으로, \"우주 전투\"에 관한 책을 찾는 사용자들은 그냥 기다려야 할 수도 있어요. 이건 그냥 안돼요! 그럼 그들이 카탈로그에 등록되면 바로 제목을 찾을 수 있게 해주는게 어때요?\n\n첫 번째 단계에서 한 것과 이번에 두 번째 단계에서 다시 한 것입니다. 벡터 DB를 책 카탈로그와 실시간으로 동기화했어요.\n\n우리의 유사성 검색 결과에 이 업데이트가 어떤 영향을 미쳤는지 확인해보세요.\n\nStreamlit 검색 UI에서 다시 \"우주 전투\"를 검색해보세요 - 이제 상위 결과는 \"Old man's war\"가 될 거예요 - 더 적절한 매치겠죠.\n\n<div class=\"content-ad\"></div>\n\n\n\n![StreamChangesfromaPostgreSQLDatabasetoaVectorStore_7](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_7.png)\n\n\"The War of the Words\"이 상위 자리에서 밀려난 이유는 새로 추가된 용어가 의미론적으로 더 관련된 설명을 갖고 있기 때문이다: 설명에서 \"전쟁\"이라는 용어는 의미에서 \"전투\"에 더 가깝고, \"국제 우주\"는 \"화성\"보다 \"우주\"라는 검색어에 의미론적으로 더 가까워진다.\n\n# 하드웨어 하에서 작동하는 방식\n\n이제 이 프로토타입이 무엇을 하는지 이해했으니, 어떻게 작동하는지 살펴보겠습니다.\n\n\n<div class=\"content-ad\"></div>\n\n먼저, 아키텍처 다이어그램의 처리 부분에 초점을 맞춰 봅시다.\n\n![이미지](/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_8.png)\n\n세 개의 서비스를 볼 수 있어요(메시지 브로커는 물론) — 각각은 작고 계속 실행 중인 파이썬 애플리케이션입니다: \"CDC\", \"임베딩 생성\", \"벡터 DB로 업서트\"\n\n각 애플리케이션은 데이터를 수신하고 처리한 후 Kafka 토픽으로 보내거나 어떠한 형태의 싱크에 쓰기 위해 Quix Streams 파이썬 라이브러리를 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n각 응용 프로그램의 소스 코드를 함께 살펴봅시다.\n\n## CDC 구성\n\nCDC 프로세스 뒤의 코드로 들어가기 전에, 자체 PostgreSQL 데이터베이스에서 CDC를 수행하려면 몇 가지 추가 전제 조건을 언급하는 것이 좋습니다:\n\n1) Write Ahead Log를 논리적으로 설정해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n- \"SHOW wal_level;\" SQL 쿼리를 실행하여 현재 설정을 확인할 수 있어요.\n- 이미 \"logical\"로 설정이 되어 있지 않다면, postgresql.cong 파일을 업데이트하고 wal_level을 wal_level=logical로 설정해 주세요.\n\n2) PostgreSQL이 실행 중인 서버 또는 컨테이너에 wal2json 플러그인을 설치해야 해요.\n\n이 튜토리얼의 PostgreSQL 데이터베이스에는 이미 이러한 선행 조건이 준비되어 있어요. Debezium Source PostgreSQL Connector 대신 Quix Python CDC 커넥터를 사용합니다. 전체 CDC 코드 파일은 이 GitHub 폴더에서 찾을 수 있어요.\n\n## PostgreSQL 데이터베이스에 연결\n\n<div class=\"content-ad\"></div>\n\n포스트그레SQL 연결은 환경 변수를 통해 postgres_helper.py 파일에서 정의됩니다. 따라서, 여러분이 자신의 데이터베이스에 연결하고 싶다면, 관련 변수를 변경하기만 하면 됩니다.\n\n```js\nimport psycopg2\nimport os\n\ndef connect_postgres():\n    # PostgreSQL 상수\n    PG_HOST = os.environ[\"PG_HOST\"] # 기본값은 localhost\n    PG_PORT = os.environ[\"PG_PORT\"] # 기본값은 ??\n    PG_USER = os.environ[\"PG_USER\"] # 기본값은 ??\n    PG_PASSWORD = os.environ[\"PG_PASSWORD\"] # 기본값은 ??\n    PG_DATABASE = os.environ[\"PG_DATABASE\"] # 기본값은 ??\n    \n    conn = psycopg2.connect(\n        database=PG_DATABASE, user=PG_USER, password=PG_PASSWORD, host=PG_HOST, port=PG_PORT\n    )\n    return conn\n… \n```\n\n## 카프카로 변경 로그 항목 생성하기\n\n글을 간결하게 유지하기 위해서, 이 기사에서는 변경 데이터가 어떻게 캡처되는지에 대해 다루지 않겠습니다. 그러나 변경 사항을 캡처하는 데 write-ahead log가 어떻게 사용되는지에 대한 자세한 내용은 postgres_helper.py 파일을 확인해주세요.\n\n<div class=\"content-ad\"></div>\n\n여기서는 데이터의 구조와 Kafka로 생성되는 방법에 초점을 맞춰 봅시다.\n\n먼저, Kafka 프로듀서를 초기화합니다. 저희는 프로듀서로 Quix Streams Python 라이브러리를 사용합니다. 이 라이브러리는 Streaming Dataframes 개념을 사용하여 데이터를 처리하고 정적 데이터셋에 대해 작성된 Pandas 코드를 재사용하기 쉽게 만들어줍니다.\n\nQuix Streams를 사용하여 main.py에서 어플리케이션 및 초기 출력 토픽을 정의합니다:\n\n```js\nfrom quixstreams import Application\n...\napp = Application(broker_address=os.environ['BROKER_ADDRESS']) # Redpanda의 기본값은 'localhost:19092'입니다.\n... \noutput_topic = app.topic(output_topic_name) # 토픽 이름은 환경 변수에 정의되어 있으며, 기본값은 \"posgres-cdc-source\"입니다.\n```\n\n<div class=\"content-ad\"></div>\n\n그런 다음 최신 데이터베이스 변경 사항을 가져와 버퍼에 추가하고, 버퍼를 반복하여 결과를 출력 Kafka 주제로 전송하는 함수를 추가합니다.\n\n```js\n...\n# 네트워크 트래픽을 줄이기 위해 메시지를 100밀리초 동안 버퍼링합니다\ndef main():\n    buffer = []\n    last_flush_time = time.time()\n\n    while run:\n        records = get_changes(conn, PG_SLOT_NAME)\n        for record in records:\n            changes = json.loads(record[0])\n            for change in changes[\"change\"]:\n                buffer.append(change)\n                \n        # 100밀리초가 지났는지 확인\n        current_time = time.time()\n        if (current_time - last_flush_time) >= 0.5 and len(buffer) > 0:\n           # 500밀리초가 지났다면, 버퍼에 있는 모든 메시지 전송\n    with app.get_producer() as producer:\n            for message in buffer:\n                producer.produce(topic=output_topic.name,\n                                    key=PG_TABLE_NAME,\n                                    value=json.dumps(message))\n                print(\"Kafka로 메시지 전송 완료\")\n                # 생산자를 플러시하여 메시지 전송\n                \n            # 버퍼 비우기\n            buffer = []\n            # 마지막 플러시 시간 업데이트\n            last_flush_time = current_time\n        time.sleep(WAIT_INTERVAL) # main.py의 전역 변수로 정의된 대기 간격(현재 0.1초)\r\n```\n\n결과 payload는 다음 구조를 가지고 있습니다:\n\n```js\r\n{\n  \"kind\": \"insert\",\n  \"schema\": \"public\",\n  \"table\": \"books\",\n  \"columnnames\": [\n    \"id\",\n    \"name\",\n    \"description\",\n    \"author\",\n    \"year\"\n  ],\n  \"columntypes\": [\n    \"integer\",\n    \"character varying(255)\",\n    \"text\",\n    \"character varying(255)\",\n    \"integer\"\n  ],\n  \"columnvalues\": [\n    60,\n    \"Old Man's War\",\n    \"지구의 어르신들이 화성간 전쟁에 참전하게 되어, 새로운 외계 문화와 위협을 발견하게 되는 이야기입니다.\",\n    \"John Scalzi\",\n    2005\n  ]\n}\r\n```\n\n<div class=\"content-ad\"></div>\n\n나중에는 이 구조를 간단하게 만들어서 처리하기 쉽게 할 거예요.\n\n# 임베딩 생성\n\n이것이 우리가 \"변환\" 프로세스라고 부르는 것인데, 다른 말로 하면 두 개의 Kafka 토픽 사이에 위치하여 한 쪽에서 읽고 다른 쪽으로 쓰는 역할을 합니다. 전체 소스 코드 파일은 이 GitHub 폴더에 있습니다.\n\nQuix Streams 라이브러리는 변환을 구현하는 간단한 프로세스를 제공합니다. 다른 라이브러리와 달리 생산자와 소비자를 정의하는 대신, 관련 설정을 Application 생성자에 넣어주고 app 인스턴스를 통해 입력 및 출력 토픽을 정의합니다.\n\n<div class=\"content-ad\"></div>\n\n예시:\n\n```js\nfrom quixstreams import Application\n...\napp = Application(\n    broker_address=os.environ['BROKER_ADDRESS'],\n    consumer_group=\"vectorsv1\",\n    auto_offset_reset=\"earliest\",\n    auto_create_topics=True,  # Quix 앱은 아직 존재하지 않는 경우 주제를 자동으로 생성하는 옵션이 있습니다\n)\n\n# JSON 변환기를 사용하여 입력 및 출력 주제 정의\ninput_topic = app.topic(os.environ['input'], value_deserializer=\"json\")\noutput_topic = app.topic(os.environ['output'], value_serializer=\"json\")\n```\n\n아래에서는 app.dataframe 메서드를 사용하여 데이터를 생산하고 소비하는 방법을 볼 수 있지만 먼저 데이터에 적용할 함수를 정의합니다.\n\n첫 번째 함수는 변경 데이터 캡처 페이로드의 구조를 압축합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\ndef simplify_data(row):\n\n    # 새로운 딕셔너리를 생성하여 'kind' 및 zips 열 이름과 값으로 구성\n    new_structure = {\"kind\": row[\"kind\"],\"table\": row[\"table\"]}\n    new_structure.update({key: value for key, value in zip(row[\"columnnames\"], row[\"columnvalues\"])})\n\n    # 선택적으로 정수를 문자열로 변환\n    new_structure[\"year\"] = str(new_structure[\"year\"])\n\n    return new_structure\n```\n\n다음과 같은 페이로드 구조를 얻게 됩니다:\n\n```python\n{\n \"kind\": \"insert\",\n \"table\": \"books\",\n \"id\": 60,\n \"name\": \"Old Man's War\",\n \"description\": \"Earth's senior citizens are recruited to fight in an interstellar war, discovering new alien cultures and threats.\",\n \"author\": \"John Scalzi\",\n \"year\": 2005\n}\n```\n\n두 번째 함수는 단순화된 페이로드의 \"description\" 필드에 대한 임베딩을 생성하기 위해 Qdrant의 FastEmbed 라이브러리를 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\n# 기존 모델 다운로드 및 초기화가 트리거됩니다.\nembedding_model = TextEmbedding()\nprint(\"모델 BAAI/bge-small-en-v1.5을 사용할 준비가 되었습니다.\")\n\n...\n\n# 임베딩 함수 정의\ndef create_embeddings(row):\n    text = row['description']\n    embeddings = list(embedding_model.embed([text]))\n    embedding_list = [embedding.tolist() for embedding in embeddings]\n    finalembedding = embedding_list[0]\n    print(f'벡터 생성됨: \"{finalembedding}\"')\n\n    return finalembedding\r\n```\n\n마지막으로 데이터를 소비하고 함수를 적용하여 데이터를 downstream Kafka 주제로 생성합니다.\n\n```js\r\n# 입력 주제의 메시지 스트림을 기반으로 스트리밍 데이터프레임 초기화:\nsdf = app.dataframe(topic=input_topic)\n\nsdf = sdf.filter(lambda data: data[\"table\"] == targettable) # \"books\" 테이블의 변경 사항만 필터링합니다.\n\nsdf = sdf.apply(simplify_data)\n\nsdf = sdf.update(lambda val: logger.info(f\"수신된 업데이트: {val}\"))\n\n# 필터링된 SDF에서 감지된 새 메시지(행)에 대해 임베딩 함수 트리거\nsdf[\"embeddings\"] = sdf.apply(create_embeddings, stateful=False)\n\n# 타임스탬프 열을 현재 시간(나노초 단위)으로 업데이트합니다.\nsdf[\"Timestamp\"] = sdf.apply(lambda row: time.time_ns())\r\n```\n\nsdf.apply()과 sdf.update()의 차이점에 유의하세요.\n\n<div class=\"content-ad\"></div>\n\n`apply()`은 콜백 함수의 결과를 하류로 전달합니다. 원본 데이터를 가져와 처리하여 새 데이터를 생성합니다. 이 메서드는 원본 데이터 자체를 변경하지 않고 대신 원본을 기반으로 새 버전을 생성합니다.\n\n- 예를 들어, apply()를 사용하여 사전에 새 키를 추가하면 실제로 해당 추가가 포함된 새 사전이 생성됩니다.\n- 우리의 경우, sdf.apply(simplify_data)를 사용하여 CDC 페이로드를 간단한 사전으로 변환하고 sdf.apply(create_embeddings)를 사용하여 벡터를 계산하고 해당 사전 내의 새로운 \"embeddings\" 필드에 기록합니다.\n\n`update()`는 실제 콜백 인수를 하류로 전달합니다. 원본 데이터를 직접 수정하거나 사용할 수 있게 합니다. 그러나 주로 콘솔에 데이터를 기록하거나 외부 데이터베이스에 쓰는 데 사용됩니다(Kafka Streams의 peek() 메서드와 유사합니다).\n\n마지막으로, 우리는 sdf.to_topic을 사용하여 변환된 데이터를 하류 토픽으로 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nsdf = sdf.to_topic(output_topic)\napp.run(sdf)\n```\n\n# 벡터 DB에 Upserting\n\n이 프로세스는 sdf.update() 방법을 다시 사용하지만 먼저 sdf.update()에 전달 할 함수를 정의해야합니다. 즉, 들어오는 벡터와 메타데이터를 벡터 DB에 입력하는 함수를 정의해야합니다. 전체 코드는 이 GitHub 폴더에서 찾을 수 있습니다.\n\n여기서는 환경 변수를 사용하여 벡터 DB에 연결을 정의하고, 스트리밍 데이터프레임 행에서 관련 데이터를 추출하며, upload_points() 메서드를 사용하여 벡터 DB(이 경우 로컬 Qdrant DB)에 항목을 추가합니다.\n\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom quixstreams import Application\nfrom qdrant_client import models, QdrantClient\nimport os\n\nhost = os.getenv(\"qd_host\", \"\")\nport = os.getenv(\"qd_port\", \"\")\ncollection = os.getenv(\"qd_collection\", \"\")\n\nqdrant = QdrantClient(host=host, port=port)\ncollection = collection\n\n# Create collection to store items\nif not qdrant.collection_exists(collection):\n    # Define the collection parameters\n    vector_size = 384\n    # Create the collection\n    qdrant.create_collection(\n        collection_name=collection,\n        vectors_config=models.VectorParams(\n            size=vector_size,  # Vector size is defined by used model\n            distance=models.Distance.COSINE\n        )\n    )\n    print(f\"Collection '{collection}' created.\")\nelse:\n    print(f\"Collection '{collection}' already exists.\")\n\n# Define the ingestion function\ndef ingest_vectors(row):\n\n  single_record = models.PointStruct(\n    id=row['id'],\n    vector=row['embeddings'],\n    payload={key: row[key] for key in ['name', 'description', 'author', 'year']}\n    )\n\n  qdrant.upload_points(\n      collection_name=collection,\n      points=[single_record]\n    )\n\n  print(f'Ingested vector entry id: \"{row[\"id\"]}\"...')\n\napp = Application(\n    consumer_group=\"ingesterV1\",\n    auto_offset_reset=\"earliest\",\n    auto_create_topics=True,  # Quix app has an option to auto create topics\n)\r\n```\n\n마지막으로, 입력 토픽에서 읽어와서 `ingest_vectors` 함수를 `sdf.update()`에 전달합니다. 상기한 바와 같이, 우리는 파이프라인의 종점에 도달했기 때문에 `sdf.update()`를 사용합니다. 데이터를 전달할 다운스트림 토픽이 없으므로 데이터를 \"위치에\" 업데이트하고(즉, 벡터 DB로 보내는 것) 있습니다.\n\n```python\n# JSON 디시리얼라이저와 함께 입력 토픽 정의\ninput_topic = app.topic(os.environ['input'], value_deserializer=\"json\")\n\n# 입력 토픽의 메시지 스트림을 기반으로 스트리밍 데이터프레임을 초기화합니다:\nsdf = app.dataframe(topic=input_topic)\n\n# 데이터 삽입이 이곳에서 발생합니다\nsdf = sdf.update(lambda row: ingest_vectors(row))\napp.run(sdf)\r\n```\n\n# 배운 점\n\n<div class=\"content-ad\"></div>\n\n기본 데이터를 신선하게 유지하는 것은 검색 품질의 중요한 구성 요소입니다. 제품 카탈로그에 새로운 항목이 도착할 때마다 벡터 저장소를 업데이트하여 사용자에게 의미론적으로 정확한 검색 결과를 제공할 수 있었음을 보았습니다.\n\n우리는 데이터베이스에서 데이터를 내보내어 일괄적으로 벡터 저장소에 쓰는 것과 같이 벡터 저장소를 수동으로 업데이트할 수 있었을 것입니다. 그러나 이렇게 하면 제품 카탈로그가 계속 변경되는 프로덕션 전자 상거래 시나리오에서 어떻게 작동하는지, 배치를 어떻게 조직화하며 제품이 카탈로그에 도착한 후에 사용자 검색 쿼리에 포함되기까지 허용할 수 있는 지연 시간이 어떻게 되는지와 관련된 여러 질문이 생깁니다.\n\n데이터가 입력됨과 동시에 임베딩이 생성되고 흡수되는 이벤트 기반 시스템을 설정하면(CDC를 통해), 이러한 질문들을 처리할 필요가 없습니다. 이것이 카프카 기반 아키텍처가 인기 있는 이유입니다.\n\n<div class=\"content-ad\"></div>\n\n많은 대규모 기업이 이미 DoorDash 예제에서 본 것처럼 전통적인 검색 색인 작업을 위해 Apache Kafka와 같은 이벤트 기반 솔루션을 사용하고 있습니다. 텍스트 임베딩을 최신 상태로 유지하는 데 관련된 도전 과제는 비슷하기 때문에 텍스트 임베딩에도 동일한 접근을 적용하는 것이 합리적입니다.\n\n프로덕션에서 점진적으로 데이터를 수집하는 경우 한 가지 주의할 점은 벡터 인덱스를 다시 완전히 계산해야 할 수도 있다는 것입니다. \"6 hard problems scaling vector search\" 라는 기사는 이 문제에 대해 자세히 다루고 있으며 규모에 맞게 이를 수행하려면 좋은 참고 자료가 될 것입니다. 서로 다른 벡터 데이터베이스는 이 문제를 다르게 해결합니다. 예를 들어 특정 벡터 저장소 세그먼트만 다시 계산함으로써 이 문제를 해결합니다. 따라서 프로덕션용 벡터 데이터베이스를 선택할 때 그들의 색인 전략을 고려하는 것이 중요합니다.\n\n그래도 이 간단한 프로토타입이 외부 소스에서 주기적으로 업데이트되는 신선한 데이터에 의존하는 검색 애플리케이션에 벡터 기반 검색을 통합하는 데 출발점을 제공했기를 희망합니다.\n\n- 만약 프로토타입을 작동시키는 데 문제가 있었다면, Quix Streams 사용자를 위한 Slack 커뮤니티에서 내게 메시지를 보내 주시기 바랍니다: https://stream-processing.slack.com/.","ogImage":{"url":"/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_0.png"},"coverImage":"/assets/img/2024-06-22-StreamChangesfromaPostgreSQLDatabasetoaVectorStore_0.png","tag":["Tech"],"readingTime":20}],"page":"21","totalPageCount":156,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}