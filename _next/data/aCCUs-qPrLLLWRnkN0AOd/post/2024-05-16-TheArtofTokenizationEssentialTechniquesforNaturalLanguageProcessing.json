{"pageProps":{"post":{"title":"토큰화의 기술 자연어 처리를 위한 필수 기법","description":"","date":"2024-05-16 04:22","slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing","content":"\n\n토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!\n\n![이미지](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png)\n\n자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.\n\n# 토큰화 기술 발전의 여정\n\n\n\n토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!\n\n## 기본 토큰화 기술\n\n이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:\n\n1. 공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.\n   \n:warning: 한계\n- 문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.\n- 어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.\n- 잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.\n\n\n\n\n![word tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png)\n\n2. 단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.\n⚠️ 한계\n- 단어 사이의 상황적 의미 손실\n- 어휘폭발\n\n![sentence tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png)\n\n3. 문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.\n⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.\n\n\n\n\n💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:\n\n```js\n# NLTK 사용\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nnltk.download('punkt')\n\n# 입력 문장\ntext = \"When I left the place, I didn't take the left turn.\"\n\n# 공백 기준 토큰화\nwhitespace_tokens = text.split()\n\n# 단어 토큰화\nword_tokens = word_tokenize(text)\n\n# 문장 토큰화\nsentence_tokens = sent_tokenize(text)\n\nprint(\"Whitespace Tokenization:\", whitespace_tokens)\nprint(\"Word Tokenization:\", word_tokens)\nprint(\"Sentence Tokenization:\", sentence_tokens)\n```\n\n또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.\n\n# 고급 토큰화 기술\n\n\n\n고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:\n\n️1. N-그램-\n▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.\n▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.\n💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.\n⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.\n\n![image](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png)\n\n2. 바이트 쌍 부호화-\n▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.\n▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.\n▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.\n▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.\n💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.\n⚠️ 한계-\n▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.\n▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.\n\n\n\n<img src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\" />\n\n3. SentencePiece-  \n- SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.\n- 입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.\n- 단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.\n- 처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.\n💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.\n⚠️ 제한 사항-  \n- 언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.\n- 문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.\n\n💻 위의 세 가지 토큰화 기술을 보여주는 코드:\n\n```js\n# 필요한 라이브러리 가져오기\nimport sentencepiece as spm\nfrom tokenizers import ByteLevelBPETokenizer\nmodel_path = \"모델을 저장할 경로\"\ntrain_text = \"훈련을 위한 txt 파일 경로\"\n\n###############################\n# BPE 구현\n###############################\n\nBPE_tokenizer = ByteLevelBPETokenizer()\n\n# utf-8 인코딩된 코퍼스로 토크나이저 훈련시키기\nBPE_tokenizer.train(files=['훈련을 위한 txt 파일 경로'], vocab_size=1000, min_frequency=2)\n\n# 훈련된 토크나이저 저장\nmodel_path = '모델을 저장할 경로'\nBPE_tokenizer.save_model(model_path)\n\n# 훈련된 토크나이저 불러오기\nBPE_tokenizer = ByteLevelBPETokenizer.from_file(f\"{model_path}/vocab.json\", f\"{model_path}/merges.txt\")\n\n# 텍스트 토큰화\ntext = \"I would love to see a lion!\"\nBPE_encoded_tokens = BPE_tokenizer.encode(text)\n\nprint(\"원본 텍스트:\", text)\nprint(\"인코딩된 토큰:\", BPE_encoded_tokens.tokens)\n\n\n###############################\n# SentencePiece 구현\n###############################\n\nspm.SentencePieceTrainer.train(input=train_text, model_prefix=model_path, vocab_size=1000, num_threads=4)\n\n# 사전 훈련된 모델 불러오기\nsp_model = model_path + \".model\"\nsp = spm.SentencePieceProcessor(model_file=sp_model)\n\ntext = \"I would love to see a lion when we reach the zoo!\"\n\n# 서브워드 토큰화 및 토큰 반환\ntokens_subword = sp.encode_as_pieces(text)\n# 서브워드 토큰화 및 토큰 ID 반환\ntokens_ids = sp.encode_as_ids(text)\n# 바이트 수준 토큰화 및 바이트 수준 토큰 ID 반환\ntokens_byte = sp.encode(text)\n\n# 토큰을 다시 텍스트로 디코딩\ndecoded_text = sp.decode_pieces(tokens_subword)\n\nprint(\"원본 텍스트:\", text)\nprint(\"토큰화된 텍스트:\", tokens_subword)\nprint(\"디코딩된 텍스트:\", decoded_text)\n```\n\n\n\n이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.","ogImage":{"url":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png"},"coverImage":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png","tag":["Tech"],"readingTime":6},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!</p>\n<p><img src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png\" alt=\"이미지\"></p>\n<p>자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.</p>\n<h1>토큰화 기술 발전의 여정</h1>\n<p>토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!</p>\n<h2>기본 토큰화 기술</h2>\n<p>이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:</p>\n<ol>\n<li>공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.</li>\n</ol>\n<p>:warning: 한계</p>\n<ul>\n<li>문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.</li>\n<li>어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.</li>\n<li>잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png\" alt=\"word tokenization\"></p>\n<ol start=\"2\">\n<li>단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.\n⚠️ 한계</li>\n</ol>\n<ul>\n<li>단어 사이의 상황적 의미 손실</li>\n<li>어휘폭발</li>\n</ul>\n<p><img src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png\" alt=\"sentence tokenization\"></p>\n<ol start=\"3\">\n<li>문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.\n⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.</li>\n</ol>\n<p>💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-variable constant_\">NLTK</span> 사용\n<span class=\"hljs-keyword\">import</span> nltk\n<span class=\"hljs-keyword\">from</span> nltk.<span class=\"hljs-property\">tokenize</span> <span class=\"hljs-keyword\">import</span> word_tokenize, sent_tokenize\n\nnltk.<span class=\"hljs-title function_\">download</span>(<span class=\"hljs-string\">'punkt'</span>)\n\n# 입력 문장\ntext = <span class=\"hljs-string\">\"When I left the place, I didn't take the left turn.\"</span>\n\n# 공백 기준 토큰화\nwhitespace_tokens = text.<span class=\"hljs-title function_\">split</span>()\n\n# 단어 토큰화\nword_tokens = <span class=\"hljs-title function_\">word_tokenize</span>(text)\n\n# 문장 토큰화\nsentence_tokens = <span class=\"hljs-title function_\">sent_tokenize</span>(text)\n\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"Whitespace Tokenization:\"</span>, whitespace_tokens)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"Word Tokenization:\"</span>, word_tokens)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"Sentence Tokenization:\"</span>, sentence_tokens)\n</code></pre>\n<p>또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.</p>\n<h1>고급 토큰화 기술</h1>\n<p>고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:</p>\n<p>️1. N-그램-\n▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.\n▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.\n💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.\n⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.</p>\n<p><img src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png\" alt=\"image\"></p>\n<ol start=\"2\">\n<li>바이트 쌍 부호화-\n▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.\n▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.\n▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.\n▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.\n💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.\n⚠️ 한계-\n▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.\n▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.</li>\n</ol>\n<img src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\">\n<ol start=\"3\">\n<li>SentencePiece-</li>\n</ol>\n<ul>\n<li>SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.</li>\n<li>입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.</li>\n<li>단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.</li>\n<li>처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.\n💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.\n⚠️ 제한 사항-</li>\n<li>언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.</li>\n<li>문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.</li>\n</ul>\n<p>💻 위의 세 가지 토큰화 기술을 보여주는 코드:</p>\n<pre><code class=\"hljs language-js\"># 필요한 라이브러리 가져오기\n<span class=\"hljs-keyword\">import</span> sentencepiece <span class=\"hljs-keyword\">as</span> spm\n<span class=\"hljs-keyword\">from</span> tokenizers <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">ByteLevelBPETokenizer</span>\nmodel_path = <span class=\"hljs-string\">\"모델을 저장할 경로\"</span>\ntrain_text = <span class=\"hljs-string\">\"훈련을 위한 txt 파일 경로\"</span>\n\n###############################\n# <span class=\"hljs-variable constant_\">BPE</span> 구현\n###############################\n\nBPE_tokenizer = <span class=\"hljs-title class_\">ByteLevelBPETokenizer</span>()\n\n# utf-<span class=\"hljs-number\">8</span> 인코딩된 코퍼스로 토크나이저 훈련시키기\nBPE_tokenizer.<span class=\"hljs-title function_\">train</span>(files=[<span class=\"hljs-string\">'훈련을 위한 txt 파일 경로'</span>], vocab_size=<span class=\"hljs-number\">1000</span>, min_frequency=<span class=\"hljs-number\">2</span>)\n\n# 훈련된 토크나이저 저장\nmodel_path = <span class=\"hljs-string\">'모델을 저장할 경로'</span>\nBPE_tokenizer.<span class=\"hljs-title function_\">save_model</span>(model_path)\n\n# 훈련된 토크나이저 불러오기\nBPE_tokenizer = <span class=\"hljs-title class_\">ByteLevelBPETokenizer</span>.<span class=\"hljs-title function_\">from_file</span>(f<span class=\"hljs-string\">\"{model_path}/vocab.json\"</span>, f<span class=\"hljs-string\">\"{model_path}/merges.txt\"</span>)\n\n# 텍스트 토큰화\ntext = <span class=\"hljs-string\">\"I would love to see a lion!\"</span>\nBPE_encoded_tokens = BPE_tokenizer.<span class=\"hljs-title function_\">encode</span>(text)\n\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"원본 텍스트:\"</span>, text)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"인코딩된 토큰:\"</span>, BPE_encoded_tokens.<span class=\"hljs-property\">tokens</span>)\n\n\n###############################\n# <span class=\"hljs-title class_\">SentencePiece</span> 구현\n###############################\n\nspm.<span class=\"hljs-property\">SentencePieceTrainer</span>.<span class=\"hljs-title function_\">train</span>(input=train_text, model_prefix=model_path, vocab_size=<span class=\"hljs-number\">1000</span>, num_threads=<span class=\"hljs-number\">4</span>)\n\n# 사전 훈련된 모델 불러오기\nsp_model = model_path + <span class=\"hljs-string\">\".model\"</span>\nsp = spm.<span class=\"hljs-title class_\">SentencePieceProcessor</span>(model_file=sp_model)\n\ntext = <span class=\"hljs-string\">\"I would love to see a lion when we reach the zoo!\"</span>\n\n# 서브워드 토큰화 및 토큰 반환\ntokens_subword = sp.<span class=\"hljs-title function_\">encode_as_pieces</span>(text)\n# 서브워드 토큰화 및 토큰 <span class=\"hljs-variable constant_\">ID</span> 반환\ntokens_ids = sp.<span class=\"hljs-title function_\">encode_as_ids</span>(text)\n# 바이트 수준 토큰화 및 바이트 수준 토큰 <span class=\"hljs-variable constant_\">ID</span> 반환\ntokens_byte = sp.<span class=\"hljs-title function_\">encode</span>(text)\n\n# 토큰을 다시 텍스트로 디코딩\ndecoded_text = sp.<span class=\"hljs-title function_\">decode_pieces</span>(tokens_subword)\n\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"원본 텍스트:\"</span>, text)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"토큰화된 텍스트:\"</span>, tokens_subword)\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"디코딩된 텍스트:\"</span>, decoded_text)\n</code></pre>\n<p>이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}