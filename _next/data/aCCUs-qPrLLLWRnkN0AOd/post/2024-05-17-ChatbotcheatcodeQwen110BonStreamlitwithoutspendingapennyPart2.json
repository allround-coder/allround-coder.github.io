{"pageProps":{"post":{"title":"챗봇 치트 코드 Qwen110B로 스트림릿에서 돈을 쓰지 않고 활용하는 방법","description":"","date":"2024-05-17 03:23","slug":"2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2","content":"\n\n제1부에서는 수십억 개의 매개변수를 가진 큰 언어 모델에 무료로 액세스하고 활용할 수 있다는 것을 발견했어요. 제처럼 여러분도 하드웨어 한정으로 고민 중이라면, 이 해킹 방법은 하이엔드 GPU나 유료 구독 없이도 Qwen-110B-chat과 같은 대규모 모델과 상호 작용할 수 있는 기쁨을 선사할 거예요.\n\n제2부에서는 지금부터 체험을 더 향상시키기 위해 스트림릿 인터페이스로 동일한 개념을 적용하여 챗봇에 시각적으로 매력적인 스트리밍 효과를 추가할 거예요.\n\n과정을 되짚어보자면, Python, Gradio_client 및 코딩 능력이 필요해요. AI 챗봇을 텍스트 인터페이스를 통해 만드는 데 초점을 맞추었어요:\n\n- 환경 설정: 먼저 가상 환경을 만들고 필요한 패키지(huggingface_hub, gradio-client 및 streamlit)를 설치하세요. PyTorch나 TensorFlow가 필요하지 않으며, 상호 작용은 API를 통해 이루어질 거예요.\n- Hugging Face API 토큰: 사용자는 Hugging Face에 등록하고 모델 추론 API에 액세스하기 위해 API 토큰을 생성해야 해요.\n- 챗봇 코딩: Hugging Face Spaces에서 Gradio의 \"API를 통해 사용\" 기능을 활용하여 이러한 강력한 모델에 Python 코드로 연결하는 방법을 배웠어요. 특히 여러 언어로 상업적 이용을 위한 라이선스가 허용되는 Qwen 시리즈 모델에 초점을 맞췄어요.\n- 스트리밍 효과: 코드 구조를 살펴보면, 모델과 상호 작용할 수 있는 함수를 만드는 방법을 설명했어요. predict() 및 submit() 메서드 중에서 선택하여 스트리밍 효과와 함께 또는 없이 응답을 생성하는 방법을 강조했어요.\n\n<div class=\"content-ad\"></div>\n\n조금 헤매고 있다면 part 1부터 시작하는 것을 제안해요:\n\n## 핵심 코드부터 Streamlit 인터페이스까지\n\n이걸 꼭 말해야 해요: 터미널에서 모든 앱이 정상 작동하지 않으면 그래픽 인터페이스를 시작하지 말아야 해요.\n\n이건 필수 조건이에요! 그래서 Streamlit 인터페이스를 만드는 것이 아주 쉬울 거에요: 이미 이전 파트에서 라이브러리와 상호작용이 어떻게 작동하는지 확인했기 때문이죠.\n\n<div class=\"content-ad\"></div>\n\n모든 것은 이 핵심을 중심으로 움직입니다:\n\n```js\nfrom gradio_client import Client\n\nclient = Client(\"Qwen/Qwen1.5-110B-Chat-demo\")\nresult = client.submit(\n        query='What is Science?',\n        history=[],\n        system=\"You are a helpful assistant.\",\n        api_name=\"/model_chat\"\n)\nprint(result)\n```\n\n그리고 submit() 메소드를 사용하여 스트리밍 객체/반복자를 얻을 수 있다는 것을 알고 있습니다. Streamlit을 사용하면 스트림을 다루기가 훨씬 쉬워집니다. 사실, 애플리케이션은 항상 페이지 위젯을 새로 고치기 때문에 텍스트 애플리케이션에서 사용되는 지루한 알고리즘을 무시할 수 있습니다. 기억하시나요?\n\n```js\n    final = ''\n    for chunk in result:\n        if final == '':\n            final=chunk[1][0][1]\n            print(chunk[1][0][1], end=\"\", flush=True)\n        else:\n            try:\n                print(chunk[1][0][1].replace(final,''), end=\"\", flush=True)\n                final = chunk[1][0][1]\n            except:\n                pass    \n```\n\n<div class=\"content-ad\"></div>\n\nstring.replace()을 사용하여 이미 생성된 것에서 새로운 단어를 빼내는 작업을 했었는데, 더이상 필요하지 않아요.🥳\n\n# Streamlit 앱\n\n습관적인 사람이라... 그래서 내 코드가 다른 프로젝트와 매우 비슷하다는 사실을 발견할 수 있을 거에요. 그런데 괜찮아요! 결국, 템플릿을 적용하고 수정하는 것이 매번 처음부터 시작하는 것보다 쉽고 빠를 수 있거든.\n\n새 파일을 만들어보세요: 제 파일은 st-Qwen1.5–110B-Chat.py라고 해요. 주요 라이브러리를 가져와 세션 상태 전역 변수를 생성하는 것부터 시작해볼까요?\n\n<div class=\"content-ad\"></div>\n\n```python\nimport streamlit as st\nimport time\nimport sys\nfrom gradio_client import Client\n# Internal usage\nimport os\nfrom time import sleep\n\n\nif \"hf_model\" not in st.session_state:\n    st.session_state.hf_model = \"Qwen1.5-110B-Chat\"\n# Initialize chat history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n```\n\n프로그램에서 전역 변수는 공유되어 사용될 수 있습니다. 또한 session_state라고 불리는 이러한 객체들이 streamlit의 매 실행마다 변경되지 않는 것을 필요로합니다.\n\n그리고 2가지 주요 함수를 정의합니다:\n\n```python\n@st.cache_resource\ndef create_client():   \n    yourHFtoken = \"hf_xxxxxxxxxxxxxxxxxxxxxxx\" #여기에 여러분의 HF 토큰을 넣으세요\n    print(f'{st.session_state.hf_model}에 대한 API Gradio 클라이언트를 로딩 중입니다.')\n    client = Client(\"Qwen/Qwen1.5-110B-Chat-demo\", hf_token=yourHFtoken)\n    return client\n\n# 모든 채팅 메시지를 chathistory.txt에 기록하는 함수\ndef writehistory(text):\n    with open('chathistorywen110b.txt', 'a', encoding='utf-8') as f:\n        f.write(text)\n        f.write('\\n')\n    f.close()\n```\n\n<div class=\"content-ad\"></div>\n\n저희는 @st.cache_resource 데코레이터를 사용하고 있습니다. 이는 Qwen1.5-110에 대한 API gradio 클라이언트를 Streamlit이 매 실행마다 로딩하지 않기를 원하기 때문입니다 (이는 분당 1회 이상 발생할 수 있습니다): Gradio 클라이언트 연결이 실행 중에 변경되지 않을 것이기 때문에, 이 리소스를 특별한 메모리에 캐싱하고 있습니다 (@st.cache_resource). 자세한 내용은 여기에서 확인하실 수 있습니다.\n\n## 일부 그래픽 조정\n\n이제 기본 Streamlit 페이지 요소와 챗봇에 사용할 아이콘을 설정할 수 있습니다.\n\n```js\n#아바타\nav_us = '🧑‍💻'  # './man.png'  #\"🦖\"  # \"🧑‍💻\", \"🤖\", \"🦖\"과 같은 단일 이모지입니다. Shortcut은 지원되지 않습니다.\nav_ass = \"🤖\"   #'./robot.png'\n# 기본 모델 설정\n\n### STREAMLIT UI 시작\nst.image('https://github.com/fabiomatricardi/ChatBOTMastery/raw/main/qwen100logo.png', )\nst.markdown(\"### *Streamlit & Gradio_client로 구동됨*\", unsafe_allow_html=True )\nst.markdown('---')\n\nclient = create_client()\n```\n\n<div class=\"content-ad\"></div>\n\n- 채팅 인터페이스에 로컬 이미지를 사용할 수도 있어요 (코드의 주석을 참고하세요!)\n- 마지막으로, create_client()로 클라이언트 연결을 인스턴스화해요.\n\n# 본문 — 채팅 인터페이스\n\nStreamlit은 자신의 위젯에 변경이 발생할 때마다 또는 입력(버튼, 선택기, 라디오 요소 등)으로 사용자 조작이 호출될 때마다 코드를 맨 위부터 다시 실행해요.\n\n그래서 저희는 대화 기록을 맨 위에 먼저 렌더링하기 시작했어요. 여기서는 뭐라도 새롭게 발명한 건 없어요: Streamlit 블로그의 공식 자습서에서 모두 배웠거든요.\n\n<div class=\"content-ad\"></div>\n\n이것은 표준 렌더링입니다. OpenAI API와 호환되는 chat_completion 형식에 모두 적용 가능합니다.\n\n코드로 돌아가서, 우리는 chat_template 메시지들을 표시하고, 메시지 목록을 반복하며 사용자 프롬프트(myprompt)가 제출되기를 기다립니다.\n\n```js\n# 앱 재실행 시 이전 대화 내용을 보여줍니다\nfor message in st.session_state.messages:\n    if message[\"role\"] == \"user\":\n        with st.chat_message(message[\"role\"],avatar=av_us):\n            st.markdown(message[\"content\"])\n    else:\n        with st.chat_message(message[\"role\"],avatar=av_ass):\n            st.markdown(message[\"content\"])\n# 사용자 입력 받기\nif myprompt := st.chat_input(\"인공지능 모델이란 무엇인가요?\"):\n    # 사용자 메시지를 대화 내역에 추가\n    st.session_state.messages.append({\"role\": \"user\", \"content\": myprompt})\n    # 대화 메시지 컨테이너에 사용자 메시지 표시\n    with st.chat_message(\"user\", avatar=av_us):\n        st.markdown(myprompt)\n        usertext = f\"user: {myprompt}\"\n        writehistory(usertext)\n        # 차후 사용을 위해 대화 상대의 응답을 표시\n```\n\n여기에 이상한 writehistory(usertext) 지시문을 추가하고 있는 것을 볼 수 있습니다. 기억하시나요? 처음에 이 함수를 선언했던 거죠? 저는 모든 대화 내용을 로컬 텍스트 파일에 저장하는 버릇이 있어요. 이는 프롬프트를 분석하거나 미래 활용을 위해 자료를 조직화할 때 매우 편리합니다.🙂\n\n<div class=\"content-ad\"></div>\n\n프롬프트에서 제출된 내용을 확인한 후, gradio 클라이언트 인스턴스(client.submit)를 호출하고 스트리밍을 시작합니다 (message_placeholder.markdown(r[1][0][1]+ \"▌\"))\n\n```js\n    # 채팅 메시지 컨테이너에 어시스턴트 응답 표시\n    with st.chat_message(\"assistant\"):\n        message_placeholder = st.empty()\n        full_response = \"\"\n        res  =  client.submit(\n                query=myprompt,\n                history=[],\n                system=\"You are a helpful assistant.\",\n                api_name=\"/model_chat\"\n                )        \n        for r in res:\n            full_response=r[1][0][1]\n            message_placeholder.markdown(r[1][0][1]+ \"▌\")\n\n        message_placeholder.markdown(full_response)\n        asstext = f\"assistant: {full_response}\"\n        writehistory(asstext)       \n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n```\n\n이게 전부에요. full_response는 최종 텍스트가 들어 있는 변수이므로 대화 기록에도 추가하여 표시합니다.\n\n해결했으면 댓글에 알려주세요 👍\n\n<div class=\"content-ad\"></div>\n\n파이썬 파일을 저장한 후 터미널에서 가상 환경을 활성화한 상태에서 다음과 같이 실행하세요.\n\n```js\nstreamlit run .\\st-Qwen1.5-110B-Chat.py\n```\n\n아래처럼 나와야 합니다... 그리고 기본 브라우저가 로컬 URL인 http://localhost:8501로 열리게 됩니다.\n\n![이미지](/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_0.png)\n\n<div class=\"content-ad\"></div>\n\nStreamlit은 로컬 네트워크로의 편리한 라우팅을 제공합니다. 예를 들어, 핸드폰이 동일한 액세스 포인트에 연결되어 있으면 Network URL로 표시된 주소인 http://192.168.2.6:8501을 통해 핸드폰에서도 이 애플리케이션을 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_1.png)\n\n# 다른 모델 실행에 대한 참고 사항\n\nGitHub 리포지토리에서도 Streamlit Python 파일을 실행하는 방법을 찾을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- OpenELM 3B\n- Phi-3-mini-Instruct 128k\n- QwenMoE\n\n고객 구성이 변경될 예정입니다 (물론...) 그리고 스트리밍 지침도 변경될 것입니다. 이는 API 엔드포인트가 다른 데이터 유형을 반환하기 때문에 발생합니다. OpenELM 및 Phi-3의 경우 순수한 문자열이 반환되므로 어떠한 사전/튜플 위치에 있는 LLM 응답을 추출할 필요가 없습니다. 여기를 살펴보세요:\n\n그리고 또한 PLEASE, 기억해주세요...\n\n![이미지](/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_2.png)\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 모든 복잡성을 다루는 이유는 무엇일까요? 우리는 어떻게 일하는지 배우고, 나만의 AI 비서를 만드는 방법을 알고 싶기 때문입니다. 내가 상상할 수 있는 최고의 목적을 위해 콘텐츠 생성, 학습 자료, 프레젠테이션, 교육 지원 등.\n\n어디에 사용할 건가요?\n\n글이 마음에 드셨으면 좋겣습니다. 이 이야기가 가치를 제공했고 조금이라도 지원하고 싶다면 다음을 해볼 수 있습니다 :\n\n<div class=\"content-ad\"></div>\n\n- 이 이야기에 대해 많이 박수를 쳐 주세요\n- 기억할 가치가 있는 부분을 강조하십시오 (나중에 찾기 쉽고, 더 나은 기사를 쓰는 데 도움이 될 것입니다)\n- Build Your Own AI를 시작하는 방법을 배우려면, 무료 eBook을 다운로드하세요\n- 내 링크를 사용하여 Medium 멤버십 가입하기 - (무제한 Medium 이야기를 읽으려면 매달 $5)\n- Medium에서 나를 팔로우하기\n- 내 최신 기사 읽기 https://medium.com/@fabio.matricardi\n\n여기 몇 가지 더 흥미로운 읽을거리:\n\n추가 학습 자료\n\n![이미지](/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_3.png)\n\n<div class=\"content-ad\"></div>\n\n이 이야기는 Generative AI Publication에서 발행되었습니다.\n\n최신 AI 이야기를 놓치지 않으려면 Substack, LinkedIn 및 Zeniteq에서 저희와 연락하여 AI의 미래를 함께 창조해보세요!\n\n![이미지](/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_4.png)","ogImage":{"url":"/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_0.png"},"coverImage":"/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_0.png","tag":["Tech"],"readingTime":9},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>제1부에서는 수십억 개의 매개변수를 가진 큰 언어 모델에 무료로 액세스하고 활용할 수 있다는 것을 발견했어요. 제처럼 여러분도 하드웨어 한정으로 고민 중이라면, 이 해킹 방법은 하이엔드 GPU나 유료 구독 없이도 Qwen-110B-chat과 같은 대규모 모델과 상호 작용할 수 있는 기쁨을 선사할 거예요.</p>\n<p>제2부에서는 지금부터 체험을 더 향상시키기 위해 스트림릿 인터페이스로 동일한 개념을 적용하여 챗봇에 시각적으로 매력적인 스트리밍 효과를 추가할 거예요.</p>\n<p>과정을 되짚어보자면, Python, Gradio_client 및 코딩 능력이 필요해요. AI 챗봇을 텍스트 인터페이스를 통해 만드는 데 초점을 맞추었어요:</p>\n<ul>\n<li>환경 설정: 먼저 가상 환경을 만들고 필요한 패키지(huggingface_hub, gradio-client 및 streamlit)를 설치하세요. PyTorch나 TensorFlow가 필요하지 않으며, 상호 작용은 API를 통해 이루어질 거예요.</li>\n<li>Hugging Face API 토큰: 사용자는 Hugging Face에 등록하고 모델 추론 API에 액세스하기 위해 API 토큰을 생성해야 해요.</li>\n<li>챗봇 코딩: Hugging Face Spaces에서 Gradio의 \"API를 통해 사용\" 기능을 활용하여 이러한 강력한 모델에 Python 코드로 연결하는 방법을 배웠어요. 특히 여러 언어로 상업적 이용을 위한 라이선스가 허용되는 Qwen 시리즈 모델에 초점을 맞췄어요.</li>\n<li>스트리밍 효과: 코드 구조를 살펴보면, 모델과 상호 작용할 수 있는 함수를 만드는 방법을 설명했어요. predict() 및 submit() 메서드 중에서 선택하여 스트리밍 효과와 함께 또는 없이 응답을 생성하는 방법을 강조했어요.</li>\n</ul>\n<div class=\"content-ad\"></div>\n<p>조금 헤매고 있다면 part 1부터 시작하는 것을 제안해요:</p>\n<h2>핵심 코드부터 Streamlit 인터페이스까지</h2>\n<p>이걸 꼭 말해야 해요: 터미널에서 모든 앱이 정상 작동하지 않으면 그래픽 인터페이스를 시작하지 말아야 해요.</p>\n<p>이건 필수 조건이에요! 그래서 Streamlit 인터페이스를 만드는 것이 아주 쉬울 거에요: 이미 이전 파트에서 라이브러리와 상호작용이 어떻게 작동하는지 확인했기 때문이죠.</p>\n<div class=\"content-ad\"></div>\n<p>모든 것은 이 핵심을 중심으로 움직입니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> gradio_client <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Client</span>\n\nclient = <span class=\"hljs-title class_\">Client</span>(<span class=\"hljs-string\">\"Qwen/Qwen1.5-110B-Chat-demo\"</span>)\nresult = client.<span class=\"hljs-title function_\">submit</span>(\n        query=<span class=\"hljs-string\">'What is Science?'</span>,\n        history=[],\n        system=<span class=\"hljs-string\">\"You are a helpful assistant.\"</span>,\n        api_name=<span class=\"hljs-string\">\"/model_chat\"</span>\n)\n<span class=\"hljs-title function_\">print</span>(result)\n</code></pre>\n<p>그리고 submit() 메소드를 사용하여 스트리밍 객체/반복자를 얻을 수 있다는 것을 알고 있습니다. Streamlit을 사용하면 스트림을 다루기가 훨씬 쉬워집니다. 사실, 애플리케이션은 항상 페이지 위젯을 새로 고치기 때문에 텍스트 애플리케이션에서 사용되는 지루한 알고리즘을 무시할 수 있습니다. 기억하시나요?</p>\n<pre><code class=\"hljs language-js\">    final = <span class=\"hljs-string\">''</span>\n    <span class=\"hljs-keyword\">for</span> chunk <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">result</span>:\n        <span class=\"hljs-keyword\">if</span> final == <span class=\"hljs-string\">''</span>:\n            final=chunk[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>]\n            <span class=\"hljs-title function_\">print</span>(chunk[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>], end=<span class=\"hljs-string\">\"\"</span>, flush=<span class=\"hljs-title class_\">True</span>)\n        <span class=\"hljs-attr\">else</span>:\n            <span class=\"hljs-attr\">try</span>:\n                <span class=\"hljs-title function_\">print</span>(chunk[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>].<span class=\"hljs-title function_\">replace</span>(final,<span class=\"hljs-string\">''</span>), end=<span class=\"hljs-string\">\"\"</span>, flush=<span class=\"hljs-title class_\">True</span>)\n                final = chunk[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>]\n            <span class=\"hljs-attr\">except</span>:\n                pass    \n</code></pre>\n<div class=\"content-ad\"></div>\n<p>string.replace()을 사용하여 이미 생성된 것에서 새로운 단어를 빼내는 작업을 했었는데, 더이상 필요하지 않아요.🥳</p>\n<h1>Streamlit 앱</h1>\n<p>습관적인 사람이라... 그래서 내 코드가 다른 프로젝트와 매우 비슷하다는 사실을 발견할 수 있을 거에요. 그런데 괜찮아요! 결국, 템플릿을 적용하고 수정하는 것이 매번 처음부터 시작하는 것보다 쉽고 빠를 수 있거든.</p>\n<p>새 파일을 만들어보세요: 제 파일은 st-Qwen1.5–110B-Chat.py라고 해요. 주요 라이브러리를 가져와 세션 상태 전역 변수를 생성하는 것부터 시작해볼까요?</p>\n<div class=\"content-ad\"></div>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> streamlit <span class=\"hljs-keyword\">as</span> st\n<span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\">import</span> sys\n<span class=\"hljs-keyword\">from</span> gradio_client <span class=\"hljs-keyword\">import</span> Client\n<span class=\"hljs-comment\"># Internal usage</span>\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> time <span class=\"hljs-keyword\">import</span> sleep\n\n\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">\"hf_model\"</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> st.session_state:\n    st.session_state.hf_model = <span class=\"hljs-string\">\"Qwen1.5-110B-Chat\"</span>\n<span class=\"hljs-comment\"># Initialize chat history</span>\n<span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\">\"messages\"</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> st.session_state:\n    st.session_state.messages = []\n</code></pre>\n<p>프로그램에서 전역 변수는 공유되어 사용될 수 있습니다. 또한 session_state라고 불리는 이러한 객체들이 streamlit의 매 실행마다 변경되지 않는 것을 필요로합니다.</p>\n<p>그리고 2가지 주요 함수를 정의합니다:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-meta\">@st.cache_resource</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_client</span>():   \n    yourHFtoken = <span class=\"hljs-string\">\"hf_xxxxxxxxxxxxxxxxxxxxxxx\"</span> <span class=\"hljs-comment\">#여기에 여러분의 HF 토큰을 넣으세요</span>\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'<span class=\"hljs-subst\">{st.session_state.hf_model}</span>에 대한 API Gradio 클라이언트를 로딩 중입니다.'</span>)\n    client = Client(<span class=\"hljs-string\">\"Qwen/Qwen1.5-110B-Chat-demo\"</span>, hf_token=yourHFtoken)\n    <span class=\"hljs-keyword\">return</span> client\n\n<span class=\"hljs-comment\"># 모든 채팅 메시지를 chathistory.txt에 기록하는 함수</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">writehistory</span>(<span class=\"hljs-params\">text</span>):\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'chathistorywen110b.txt'</span>, <span class=\"hljs-string\">'a'</span>, encoding=<span class=\"hljs-string\">'utf-8'</span>) <span class=\"hljs-keyword\">as</span> f:\n        f.write(text)\n        f.write(<span class=\"hljs-string\">'\\n'</span>)\n    f.close()\n</code></pre>\n<div class=\"content-ad\"></div>\n<p>저희는 @st.cache_resource 데코레이터를 사용하고 있습니다. 이는 Qwen1.5-110에 대한 API gradio 클라이언트를 Streamlit이 매 실행마다 로딩하지 않기를 원하기 때문입니다 (이는 분당 1회 이상 발생할 수 있습니다): Gradio 클라이언트 연결이 실행 중에 변경되지 않을 것이기 때문에, 이 리소스를 특별한 메모리에 캐싱하고 있습니다 (@st.cache_resource). 자세한 내용은 여기에서 확인하실 수 있습니다.</p>\n<h2>일부 그래픽 조정</h2>\n<p>이제 기본 Streamlit 페이지 요소와 챗봇에 사용할 아이콘을 설정할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\">#아바타\nav_us = <span class=\"hljs-string\">'🧑‍💻'</span>  # <span class=\"hljs-string\">'./man.png'</span>  #<span class=\"hljs-string\">\"🦖\"</span>  # <span class=\"hljs-string\">\"🧑‍💻\"</span>, <span class=\"hljs-string\">\"🤖\"</span>, <span class=\"hljs-string\">\"🦖\"</span>과 같은 단일 이모지입니다. <span class=\"hljs-title class_\">Shortcut</span>은 지원되지 않습니다.\nav_ass = <span class=\"hljs-string\">\"🤖\"</span>   #<span class=\"hljs-string\">'./robot.png'</span>\n# 기본 모델 설정\n\n### <span class=\"hljs-variable constant_\">STREAMLIT</span> <span class=\"hljs-variable constant_\">UI</span> 시작\nst.<span class=\"hljs-title function_\">image</span>(<span class=\"hljs-string\">'https://github.com/fabiomatricardi/ChatBOTMastery/raw/main/qwen100logo.png'</span>, )\nst.<span class=\"hljs-title function_\">markdown</span>(<span class=\"hljs-string\">\"### *Streamlit &#x26; Gradio_client로 구동됨*\"</span>, unsafe_allow_html=<span class=\"hljs-title class_\">True</span> )\nst.<span class=\"hljs-title function_\">markdown</span>(<span class=\"hljs-string\">'---'</span>)\n\nclient = <span class=\"hljs-title function_\">create_client</span>()\n</code></pre>\n<div class=\"content-ad\"></div>\n<ul>\n<li>채팅 인터페이스에 로컬 이미지를 사용할 수도 있어요 (코드의 주석을 참고하세요!)</li>\n<li>마지막으로, create_client()로 클라이언트 연결을 인스턴스화해요.</li>\n</ul>\n<h1>본문 — 채팅 인터페이스</h1>\n<p>Streamlit은 자신의 위젯에 변경이 발생할 때마다 또는 입력(버튼, 선택기, 라디오 요소 등)으로 사용자 조작이 호출될 때마다 코드를 맨 위부터 다시 실행해요.</p>\n<p>그래서 저희는 대화 기록을 맨 위에 먼저 렌더링하기 시작했어요. 여기서는 뭐라도 새롭게 발명한 건 없어요: Streamlit 블로그의 공식 자습서에서 모두 배웠거든요.</p>\n<div class=\"content-ad\"></div>\n<p>이것은 표준 렌더링입니다. OpenAI API와 호환되는 chat_completion 형식에 모두 적용 가능합니다.</p>\n<p>코드로 돌아가서, 우리는 chat_template 메시지들을 표시하고, 메시지 목록을 반복하며 사용자 프롬프트(myprompt)가 제출되기를 기다립니다.</p>\n<pre><code class=\"hljs language-js\"># 앱 재실행 시 이전 대화 내용을 보여줍니다\n<span class=\"hljs-keyword\">for</span> message <span class=\"hljs-keyword\">in</span> st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span>:\n    <span class=\"hljs-keyword\">if</span> message[<span class=\"hljs-string\">\"role\"</span>] == <span class=\"hljs-string\">\"user\"</span>:\n        <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">chat_message</span>(message[<span class=\"hljs-string\">\"role\"</span>],avatar=av_us):\n            st.<span class=\"hljs-title function_\">markdown</span>(message[<span class=\"hljs-string\">\"content\"</span>])\n    <span class=\"hljs-attr\">else</span>:\n        <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">chat_message</span>(message[<span class=\"hljs-string\">\"role\"</span>],avatar=av_ass):\n            st.<span class=\"hljs-title function_\">markdown</span>(message[<span class=\"hljs-string\">\"content\"</span>])\n# 사용자 입력 받기\n<span class=\"hljs-keyword\">if</span> myprompt := st.<span class=\"hljs-title function_\">chat_input</span>(<span class=\"hljs-string\">\"인공지능 모델이란 무엇인가요?\"</span>):\n    # 사용자 메시지를 대화 내역에 추가\n    st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span>.<span class=\"hljs-title function_\">append</span>({<span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\">\"content\"</span>: myprompt})\n    # 대화 메시지 컨테이너에 사용자 메시지 표시\n    <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">chat_message</span>(<span class=\"hljs-string\">\"user\"</span>, avatar=av_us):\n        st.<span class=\"hljs-title function_\">markdown</span>(myprompt)\n        usertext = f<span class=\"hljs-string\">\"user: {myprompt}\"</span>\n        <span class=\"hljs-title function_\">writehistory</span>(usertext)\n        # 차후 사용을 위해 대화 상대의 응답을 표시\n</code></pre>\n<p>여기에 이상한 writehistory(usertext) 지시문을 추가하고 있는 것을 볼 수 있습니다. 기억하시나요? 처음에 이 함수를 선언했던 거죠? 저는 모든 대화 내용을 로컬 텍스트 파일에 저장하는 버릇이 있어요. 이는 프롬프트를 분석하거나 미래 활용을 위해 자료를 조직화할 때 매우 편리합니다.🙂</p>\n<div class=\"content-ad\"></div>\n<p>프롬프트에서 제출된 내용을 확인한 후, gradio 클라이언트 인스턴스(client.submit)를 호출하고 스트리밍을 시작합니다 (message_placeholder.markdown(r[1][0][1]+ \"▌\"))</p>\n<pre><code class=\"hljs language-js\">    # 채팅 메시지 컨테이너에 어시스턴트 응답 표시\n    <span class=\"hljs-keyword\">with</span> st.<span class=\"hljs-title function_\">chat_message</span>(<span class=\"hljs-string\">\"assistant\"</span>):\n        message_placeholder = st.<span class=\"hljs-title function_\">empty</span>()\n        full_response = <span class=\"hljs-string\">\"\"</span>\n        res  =  client.<span class=\"hljs-title function_\">submit</span>(\n                query=myprompt,\n                history=[],\n                system=<span class=\"hljs-string\">\"You are a helpful assistant.\"</span>,\n                api_name=<span class=\"hljs-string\">\"/model_chat\"</span>\n                )        \n        <span class=\"hljs-keyword\">for</span> r <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">res</span>:\n            full_response=r[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>]\n            message_placeholder.<span class=\"hljs-title function_\">markdown</span>(r[<span class=\"hljs-number\">1</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">1</span>]+ <span class=\"hljs-string\">\"▌\"</span>)\n\n        message_placeholder.<span class=\"hljs-title function_\">markdown</span>(full_response)\n        asstext = f<span class=\"hljs-string\">\"assistant: {full_response}\"</span>\n        <span class=\"hljs-title function_\">writehistory</span>(asstext)       \n        st.<span class=\"hljs-property\">session_state</span>.<span class=\"hljs-property\">messages</span>.<span class=\"hljs-title function_\">append</span>({<span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"assistant\"</span>, <span class=\"hljs-string\">\"content\"</span>: full_response})\n</code></pre>\n<p>이게 전부에요. full_response는 최종 텍스트가 들어 있는 변수이므로 대화 기록에도 추가하여 표시합니다.</p>\n<p>해결했으면 댓글에 알려주세요 👍</p>\n<div class=\"content-ad\"></div>\n<p>파이썬 파일을 저장한 후 터미널에서 가상 환경을 활성화한 상태에서 다음과 같이 실행하세요.</p>\n<pre><code class=\"hljs language-js\">streamlit run .\\st-<span class=\"hljs-title class_\">Qwen1</span><span class=\"hljs-number\">.5</span>-110B-<span class=\"hljs-title class_\">Chat</span>.<span class=\"hljs-property\">py</span>\n</code></pre>\n<p>아래처럼 나와야 합니다... 그리고 기본 브라우저가 로컬 URL인 <a href=\"http://localhost:8501%EB%A1%9C\" rel=\"nofollow\" target=\"_blank\">http://localhost:8501로</a> 열리게 됩니다.</p>\n<p><img src=\"/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_0.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>Streamlit은 로컬 네트워크로의 편리한 라우팅을 제공합니다. 예를 들어, 핸드폰이 동일한 액세스 포인트에 연결되어 있으면 Network URL로 표시된 주소인 <a href=\"http://192.168.2.6:8501%EC%9D%84\" rel=\"nofollow\" target=\"_blank\">http://192.168.2.6:8501을</a> 통해 핸드폰에서도 이 애플리케이션을 사용할 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_1.png\" alt=\"이미지\"></p>\n<h1>다른 모델 실행에 대한 참고 사항</h1>\n<p>GitHub 리포지토리에서도 Streamlit Python 파일을 실행하는 방법을 찾을 수 있습니다.</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>OpenELM 3B</li>\n<li>Phi-3-mini-Instruct 128k</li>\n<li>QwenMoE</li>\n</ul>\n<p>고객 구성이 변경될 예정입니다 (물론...) 그리고 스트리밍 지침도 변경될 것입니다. 이는 API 엔드포인트가 다른 데이터 유형을 반환하기 때문에 발생합니다. OpenELM 및 Phi-3의 경우 순수한 문자열이 반환되므로 어떠한 사전/튜플 위치에 있는 LLM 응답을 추출할 필요가 없습니다. 여기를 살펴보세요:</p>\n<p>그리고 또한 PLEASE, 기억해주세요...</p>\n<p><img src=\"/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_2.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<h1>결론</h1>\n<p>이 모든 복잡성을 다루는 이유는 무엇일까요? 우리는 어떻게 일하는지 배우고, 나만의 AI 비서를 만드는 방법을 알고 싶기 때문입니다. 내가 상상할 수 있는 최고의 목적을 위해 콘텐츠 생성, 학습 자료, 프레젠테이션, 교육 지원 등.</p>\n<p>어디에 사용할 건가요?</p>\n<p>글이 마음에 드셨으면 좋겣습니다. 이 이야기가 가치를 제공했고 조금이라도 지원하고 싶다면 다음을 해볼 수 있습니다 :</p>\n<div class=\"content-ad\"></div>\n<ul>\n<li>이 이야기에 대해 많이 박수를 쳐 주세요</li>\n<li>기억할 가치가 있는 부분을 강조하십시오 (나중에 찾기 쉽고, 더 나은 기사를 쓰는 데 도움이 될 것입니다)</li>\n<li>Build Your Own AI를 시작하는 방법을 배우려면, 무료 eBook을 다운로드하세요</li>\n<li>내 링크를 사용하여 Medium 멤버십 가입하기 - (무제한 Medium 이야기를 읽으려면 매달 $5)</li>\n<li>Medium에서 나를 팔로우하기</li>\n<li>내 최신 기사 읽기 <a href=\"https://medium.com/@fabio.matricardi\" rel=\"nofollow\" target=\"_blank\">https://medium.com/@fabio.matricardi</a></li>\n</ul>\n<p>여기 몇 가지 더 흥미로운 읽을거리:</p>\n<p>추가 학습 자료</p>\n<p><img src=\"/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_3.png\" alt=\"이미지\"></p>\n<div class=\"content-ad\"></div>\n<p>이 이야기는 Generative AI Publication에서 발행되었습니다.</p>\n<p>최신 AI 이야기를 놓치지 않으려면 Substack, LinkedIn 및 Zeniteq에서 저희와 연락하여 AI의 미래를 함께 창조해보세요!</p>\n<p><img src=\"/assets/img/2024-05-17-ChatbotcheatcodeQwen110BonStreamlitwithoutspendingapennyPart2_4.png\" alt=\"이미지\"></p>\n</body>\n</html>\n"},"__N_SSG":true}