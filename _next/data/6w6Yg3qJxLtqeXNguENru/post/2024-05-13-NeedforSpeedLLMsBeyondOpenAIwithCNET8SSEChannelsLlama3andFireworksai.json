{"pageProps":{"post":{"title":"네이드 포 스피드 C, NET 8 SSE  채널을 활용한 LLMs Beyond OpenAI, Llama3 및 Fireworksai","description":"","date":"2024-05-13 00:03","slug":"2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai","content":"\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*YtK6hvB_PrUvd7uwqrk0-w.gif\" />\n\n# 요약\n\n- OpenAI의 GTP-4는 일반적인 목적의 작업에 있어 압도적인 성능을 보여주지만, 전체적인 처리량(또는 오히려 그 부족함)이 많이 부족하다는 점이 매우 아쉽습니다. 이는 \"오프라인\" 작업에는 훌륭하지만, 사용자들이 더 많은 응답성을 기대하는 응용 프로그램에서는 적합하지 않을 수 있으며, 몇몇 사용 사례들은 하위 UX로 인해 배제될 수도 있습니다.\n- TheFastest.ai 팀의 최근 Hackernews 게시물은 모델과 플랫폼 모두에 대해 이런 차이가 얼마나 클 수 있는지를 강조하고 있습니다. 특히, Groq.com(Musk의 Grok와 혼동하지 말아야 합니다)와 Meta의 Llama 3 70B를 사용한 Fireworks.ai는 일부 작업에서 GPT-4와 비교했을 때 출력에 거의 희생 없이 빠른 처리량을 제공합니다.\n- C#/.NET 8 System.Threading.Channels와 서버 전송 이벤트(SSE)를 결합하면, OpenAI의 처리량과 높은 지연 시간으로 잘 동작하지 않은 작업을 구성할 수 있습니다.\n\n# 소개\n\n\n\nGPT-5를 기다리는 동안, 2024년 5월 OpenAI의 GPT-4가 여전히 LLM으로서 전반적인 성능 면에서 우수하다는 것에 대해 논쟁하는 사람은 거의 없을 것입니다. 그러나 해당 모델은 비교적 낮은 처리량과 높은 대기 시간으로 인해 UX가 더 상호 작용적인 경험을 요구하는 경우에는 최적이 아닐 수 있습니다.\n\n가장 빠른 LLM과 현재 사용 가능한 플랫폼과 OpenAI 간의 대기 시간 차이의 규모가 얼마나 큰지는 명백하지 않을 수 있습니다.\n\n최근 Hackernews 스레드를 통해 TheFastest.ai로 이동하게 되었고, Meta의 Llama 3의 높은 처리량과 Groq.com 및 Fireworks.ai라는 두 플랫폼에 흥미로웠습니다.\n\n(전자는 종종 머스크의 Grok AI와 혼동되기 때문에 불행합니다).\n\n\n\n이 기사에서는 Fireworks.ai, Meta Llama 3 8B/70B, .NET 8, System.Threading.Channels 및 Server Sent Events (SSE)를 사용하여 앱을 만드는 방법을 살펴볼 것입니다.\n\n# 차이를 측정하기\n\n스택의 상단은 Llama-3과 Groq가 지배하고 Fireworks.ai가 상위 5위를 차지하고 있습니다(각 팀이 Fireworks를 선택해야 할 이유에 대해 조금 뒤에 설명하겠습니다).\n\n![이미지](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_0.png)\n\n\n\n대조적으로, OpenAI의 GPT-4는 거의 맨 아래쪽에 위치합니다.\n\n![이미지](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_1.png)\n\nOpenAI의 GPT-4를 사용해본 사람이라면 이미 처리량이 얼마나 낮은지를 알고 있을 것입니다. 하지만 이렇게 측정된 값을 보면 그 간격이 얼마나 큰지 더욱 부각됩니다. Groq의 Llama-3 70B는 GPT-4보다 거의 10배 더 높은 처리량을 가지고 있습니다!\n\n이에 따라, GPT-4는 상호 작용이 필요하지 않은 경우에, 작업이 큰 문맥 창을 요구하는 경우에, 또는 복잡한 프롬프트와 문맥을 사용하여 \"벤치마크 품질\"의 결과가 필요한 경우에 실제로 매우 좋다고 생각했습니다.\n\n\n\n하지만 사용 사례에 다른 요구 사항이 있는 경우는 어떨까요? 속도가 필요한 경우는 어떨까요?\n\n# Groq와 Fireworks를 이용해 시동 걸기\n\nOpenAI의 처리량이 떨어져 사용자 경험을 나빠지게 만들 수 있는 문제 중 하나는, 최종적으로 콘텐츠가 가치를 추가한다 해도 주관적으로 사용자 경험을 나빠지게 할 수 있다는 것입니다.\n\nOpenAI의 ChatGPT를 사용할 때, 채팅 응답에 몇 초가 걸릴 수도 있다는 사실을 SSE가 가려버리기 때문에 명확하게 드러나지 않을 수 있습니다. GPT-4의 처리량이 낮다는 것은 다른 대안을 시도해보기 전까지는 쉽게 알아챌 수 없습니다.\n\n\n\n# Groq.com\n\nGroq는 LLM에 특별히 설계된 사용자 정의 하드웨어로 알려진 \"LPU\" 또는 \"언어 처리 유닛\"을 갖춘 흥미로운 플랫폼입니다:\n\n적어도 문서로 보면, 이것은 마케팅 허세 이상으로 보이며 플랫폼은 객관적으로 고 처리량을 자랑합니다.\n\n하지만 주요 문제는 현재의 SaaS 제공으로 이어집니다:\n\n\n\n![이미지](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_2.png)\n\n무료 티어는 실험 용도로만 사용 가능하며, 그것도 겨우 가능할 뿐입니다.\n\n![이미지](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_3.png)\n\n그래서 Groq은 꽤 빠르지만, 샌드박싱 용도 외에는 사용할 수 없으며, 가능하다면 엔터프라이즈 과금을 통해 사용할 수 있습니다.\n\n\n\n# Fireworks.ai\n\n현재 시점에서 Fireworks의 Llama-3 70B는 전체적으로 9위에 랭크되어 있으며 두 번째로 빠른 Llama-3 70B입니다:\n\n![이미지](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_4.png)\n\n마지막 토큰까지 260ms가 소요되며, 여전히 매우 빠르며 GPT-3.5와 GPT-4 사이의 성능을 제공하여 내 사용 사례에 대한 LLM 성능이 매우 좋습니다.\n\n\n\nFireworks.ai에는 중간 유료 티어가 없지만, 600 RPM은 작은 앱에 사용하기 적합하며 하드 토큰 제한이 없습니다.\n\n![이미지](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_5.png)\n\n오늘 빠르게 무언가를 구축하려는 팀들에게는 Fireworks.ai가 아마도 최선의 선택일 것입니다. (아니, 나는 그들로부터 돈을 받고 있지 않아요)\n\n# .NET 8, System.Threading.Channels 및 Server Sent Events (SSE)와 함께 실용적인 예제\n\n\n\n이 놀라운 처리량을 활용하기 위해서는 한 번에 여러 개의 스트림을 통해 생성한 다음 하나의 최종 출력 스트림으로 병합하는 동시 처리 전략이 필요합니다.\n\n이는 .NET의 System.Threading.Channels를 Server Sent Events (SSE)와 결합하여 이 처리량을 완전히 활용하고 높은 반응성을 갖는 생성 AI 경험을 구축하는 완벽한 사용 사례입니다.\n\n이전에 이 두 주제에 대해 별도로 다뤘었습니다:\n\n- .NET Task Parallel Library vs System.Threading.Channels\n- .NET 6의 System.Threading.Channels를 이용한 동시 처리 (보너스: 간격 트리)\n- .NET 7과 함께하는 Server Sent Events\n\n\n\n오늘은 .NET 8 채널, Semantic Kernel 및 gen AI와 함께 어떤 대화형 경험을 만들 수 있는지 함께 살펴보겠습니다!\n\n![image](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_6.png)\n\n저희 샘플 응용 프로그램은 준비된 재료 목록과 목표 조리 시간을 받아들여서 다음을 할 것입니다:\n\n- 해당 재료로 만들 수 있는 레시피 목록 생성\n- 레시피 중 하나를 무작위로 선택\n- 레시피를 위해 필요한 모든 재료 목록 생성\n- 레시피를 위한 소개 단락 생성\n- 준비된 각 재료에 대한 영양 정보에 대한 간략한 설명 생성\n- 제안된 사이드 디시 목록 생성\n- 순서 목록 생성\n\n\n\n단계 3~6은 병렬로 실행될 수 있지만, 레시피를 먼저 선택해야 하기 때문에 단계 1~2가 먼저 실행됩니다. 그리고 단계를 생성하기 전에 재료 전체 목록을 기다려야 합니다.\n\n# .NET 채널을 이용한 병행 실행\n\nAPI 호출의 진입점은 요청을 받을 단일 POST 엔드포인트입니다:\n\n```js\n// 👇 메인 진입점.\napp.MapPost(\"/generate\", async (\n  HttpContext context,          // 의존성 주입에서 가져옴\n  RecipeGenerator generator,    // 의존성 주입에서 가져옴\n  RecipeRequest request,        // 바디에서 가져옴\n  CancellationToken cancellation = default\n) =>\n{\n  context.Response.Headers.ContentType = \"text/event-stream\";\n\n  await generator.GenerateAsync(\n    request,\n    // 각 단편에 대한 스트리밍 응답을 작성하는 핸들러\n    async (Fragment f) => {\n      await context.Response.WriteAsync(\n        $\"data: {f.Part}|{f.Content}{Environment.NewLine}{Environment.NewLine}\",\n        cancellation\n      );\n      await context.Response.Body.FlushAsync(cancellation);\n    }\n  );\n});\n```\n\n\n\nRecipeGenerator.GenerateAsync 메서드에는 메인 플로우가 포함되어 있어요:\n\n```js\n/// <summary>\n/// 주요 시작점\n/// </summary>\npublic async Task GenerateAsync(\n  RecipeRequest request,\n  Func<Fragment, Task> handler, // 👈 이것은 HTTP 응답 스트림에 연결된 후크에요\n  CancellationToken cancellation = default\n) {\n\n  var (ingredientsOnHand, prepTime) = request;\n\n  // 👇 (1) 3개의 레시피 목록을 생성하고 무작위로 하나를 선택\n  var recipes = await GenerateRecipesAsync(ingredientsOnHand, prepTime, cancellation);\n\n  Console.WriteLine($\"생성된 레시피 수: {recipes.Length}.\");\n\n  var recipe = recipes[Random.Shared.Next(0, 2)];\n\n  // 👇 (2) 모든 레시피를 보유하여 HTML 문자열로 집계\n  var alternates = recipes\n    .Where(r => r.Name != recipe.Name)\n    .Aggregate(new StringBuilder(), (html, r) => {\n      html.Append($\"<li><b>{r.Name}</b> &nbsp;\");\n      html.Append($\"<i>{r.Intro}</i></li>\");\n\n      return html;\n    }).ToString();\n\n  // 👇 (3) 읽기 채널의 리더 측에 대한 반복 작업입니다; 먼저 시작해야 해요\n  var fragmentHandler = async () => {\n    while (await _channel.Reader.WaitToReadAsync()) {\n      if (_channel.Reader.TryRead(out var fragment)) {\n        await handler(fragment);\n      }\n    }\n  };\n\n  var completion = fragmentHandler();\n\n  // 👇 (4) 이제 세대 프롬프트를 동시에 실행해요\n  Task.WaitAll([\n    handler(new (\"alt\", alternates)),\n    GenerateIngredientsAsync(recipe, ingredientsOnHand, request.PrepTime, cancellation),\n    GenerateIntroAsync(recipe, cancellation),\n    GenerateIngredientIntroAsync(ingredientsOnHand, cancellation),\n    GenerateSidesAsync(recipe, cancellation)\n  ]);\n\n  // 👇 (5) 그리고 모든 작업이 완료될 때까지 기다려요.\n  _channel.Writer.Complete();\n\n  await completion;\n}\n```\n\n여기서 Task.WaitAll의 중요한 차이점은 JavaScript의 Promise.all과 개념적으로 비슷하지만, .NET에서는 멀티 스레드인 .NET 런타임 때문에 동시성과 병렬로 실행될 수 있어요. 이 경우 스레드 풀 스케줄러가 각 작업이 다른 스레드에서 실행될지 여부를 결정할 거에요. 채널을 사용하면 출력을 하나의 스레드에 바인딩된 리더에 병합하여 동기화된 액세스가 필요 없어졌어요.\n\n각 세대 작업은 비슷한 패턴을 따라가요:\n\n\n\n```js\nprivate async Task GenerateIntroAsync(\n  RecipeSummary recipe,\n  CancellationToken cancellation = default\n) {\n  var prompt = \"...\";\n\n  await ExecutePromptAsync(\n    \"int\", // 👈 이것은 프론트엔드 출력 대상의 ID와 일치합니다\n    prompt,\n    new () {\n      MaxTokens = 250,\n      Temperature = 0.55,\n      TopP = 0\n    },\n    cancellation: cancellation\n  );\n}\n```\n\n그리고 프롬프트를 실행하는 메서드:\n\n```js\n/// <summary>\n/// 프롬프트를 실행하고 결과를 채널에 작성합니다.\n/// </summary>\nprivate async Task ExecutePromptAsync(\n  string part,\n  string prompt,\n  OpenAIPromptExecutionSettings settings,\n  Action<string>? resultHandler = null,\n  string? modelOverride = null,\n  CancellationToken cancellation = default\n) {\n  // 👇 대화를 초기화합니다\n  var chat = _kernel.GetRequiredService<IChatCompletionService>(\n    modelOverride ?? \"70b\" // 명시된 오버라이드가 없으면 70b를 사용합니다.\n  );\n\n  var history = new ChatHistory();\n  var buffer = new StringBuilder();\n\n  history.AddUserMessage(prompt);\n\n  // 👇 응답을 스트리밍하고 각 부분을 채널에 작성합니다\n  await foreach (var message in chat.GetStreamingChatMessageContentsAsync(\n      history, settings, _kernel, cancellation\n    )\n  ) {\n      await _channel.Writer.WriteAsync( // 👈 채널의 라이터 엔드\n        new(part, message.Content ?? \"\"),\n        cancellation\n      );\n\n      buffer.Append(message.Content); // 👈 전체 출력을 보유하는 버퍼\n  }\n\n  var output = buffer.ToString();\n\n  // 👇 호출자가 전체 결과를 원하는 경우 여기에서 사용할 수 있습니다\n  resultHandler?.Invoke(output);\n}\n```\n\n애플리케이션 실행 중 의존성 주입을 통해 커널 인스턴스가 구성됩니다:```\n\n\n\n```js\r\n// Program.cs\nvar builder = WebApplication.CreateBuilder(args);\n\nvar fireworksEndpoint = new Uri(\"https://api.fireworks.ai/inference/v1/chat/completions\");\nvar groqEndpoint = new Uri(\"https://api.groq.com/openai/v1/chat/completions\");\n\nvar config = builder.Configuration\n  .GetSection(nameof(RecipesConfig))\n  .Get<RecipesConfig>();\n\n// Semantic Kernel을 설정하여 필요한만큼의 LLM을 등록합니다.\nvar kernelBuilder = Kernel.CreateBuilder();\nvar kernel = kernelBuilder\n  .AddOpenAIChatCompletion(\n    modelId: \"accounts/fireworks/models/llama-v3-70b-instruct\",\n    apiKey: config!.FireworksKey,\n    endpoint: fireworksEndpoint,\n    serviceId: \"70b\" // 👈 더 나은 결과를 위해 기본적으로 이 serviceId를 사용합니다\n  )\n  .AddOpenAIChatCompletion(\n    modelId: \"accounts/fireworks/models/llama-v3-8b-instruct\",\n    apiKey: config!.FireworksKey,\n    endpoint: fireworksEndpoint,\n    serviceId: \"8b\" // 👈 더 빠른 속도가 필요한 경우 이 serviceId를 사용합니다\n  )\n  .AddOpenAIChatCompletion(\n    modelId: \"llama3-8b-8192\",\n    apiKey: config!.GroqKey,\n    endpoint: groqEndpoint,\n    serviceId: \"groq-8b\" // 👈 최대 처리량을 위해 이 serviceId를 사용합니다\n  )\n  // 다른 LLM을 여기에 등록합니다.\n  .Build();\n\nbuilder.Services\n  .Configure<RecipesConfig>(\n    builder.Configuration.GetSection(nameof(RecipesConfig))\n  )\n  .AddCors()\n  .AddSingleton(kernel)  // 👈 설정된 kernel을 싱글톤으로 추가합니다\n  .AddScoped<RecipeGenerator>();\r\n```\n\nSemantic Kernel을 통해 여러 LLM 엔드포인트를 구성할 수 있습니다. 이를 사용하여 작은 빠른 LLM이 프로세스를 가속화할 수 있는 플로 구현을 단순화할 수 있습니다.\n\n# SSE를 활용한 동시 스트림\n\n컨텐츠가 생성되면 백엔드는 즉시 프론트엔드로 스트리밍하여 매우 반응이 뛰어난 사용자 경험을 제공합니다. 이 과정은 동시에(그리고 스레드 풀 스케줄러에 따라 확장하여 병렬로) 발생하며, 채널에 수집되어 클라이언트에서 소비될 응답 스트림으로 작성됩니다.```\n\n\n\n이 흐름을 시각화하기 위해 아래 다이어그램을 확인해보세요:\n\n![다이어그램](/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_7.png)\n\nTask.WaitAll 코드 블록은 채널의 공유 가능하고 스레드 안전한 writer 엔드를 전달받은 상태이며, reader 엔드는 HTTP 응답 스트림과 콜백을 통해 연결됩니다.\n\n해당 콜백은 간단히 EventSource의 필요한 형식 명세에 따라 Fragment를 서식화합니다.\n\n\n\n이 경우:\n\n```js\ndata: ing|tomatoes\n\ndata: ing|basil\n\ndata: ste|3. Chop the\n```\n\n프론트엔드는 이러한 메시지 스트림을 받아 UI의 서로 다른 섹션에 누적합니다.\n\n- 첫 번째 부분인 ing은 이 내용이 속하는 프론트엔드 부분을 식별합니다 (이 경우에는 \"재료\")\n- | 이후의 텍스트는 LLM에 의해 작성된 출력 토큰 세트를 의미합니다.\n\n\n\n프론트엔드에서 @microsoft/fetch-event-source는 기본 EventSource를 대체하여 POST 사용을 가능하게 하는 폴리필(polyfill)로 사용됩니다.\n\n수신자는 각 메시지를 가져와 디코드합니다:\n\n```js\nonmessage: (msg) => {\n  var payload = msg.data\n\n  var [part, content] = payload.split('|')\n\n  if (!part || !$el(`#${part}`)) {\n    return // 이 메시지는 버립니다\n  }\n\n  // 👇 이 부분은 새 줄을 인코딩하고 여기서 대체하는 해킹입니다.\n  content = content.replace(/⮑/gi, \"\\n\")\n\n  $el(`#${part}`).innerHTML += content\n},\n```\n\ntext/event-stream의 특이점은 이중 줄바꿈이 메시지 블록의 끝을 나타낸다는 것입니다. 그래서 줄바꿈은 어떤 방식으로든 인코딩되어야 합니다 (다양한 방법이 있습니다). 이 경우, 단일 문자 ⮑을 사용하여 해당 문자를 찾아 \\n으로 대체하는 것이 간답습니다.\n\n\n\nCSS는 그냥 이 부분을 고려하면 됩니다:\n\n```js\n#add, #ing, #ste {\n  white-space: pre-line;\n}\n```\n\nHTML 자체는 간단합니다:\n\n```js\n<!-- 이 블록은 추가 재료를 보관합니다 -->\n<div class=\"additional\">\n  <h2>필요한 재료</h2>\n  <!-- 👇 이 ID는 Fragment.Part와 일치합니다 -->\n  <div id=\"add\"></div>\n</div>\n\n<!-- 이 블록은 단계를 보관합니다 -->\n<div class=\"recipe\">\n  <h2>조리 단계</h2>\n  <!-- 👇 이 ID는 Fragment.Part와 일치합니다 -->\n  <div id=\"ste\"></div>\n</div>\n```\n\n\n\n# 모두가 준비되었으니 이제 앱을 실행하면 다음과 같은 경험을 할 수 있습니다:\n\n![recipe app](https://miro.medium.com/v2/resize:fit:1400/0*uCMJGy8UoyaC4rFX.gif)\n\n레시피 목록을 생성하는 호출이 차단되므로 약간의 초기 지연이 있습니다.\n\n\n\n그러나 한 번 목록이 생성되고 무작위로 선택된 후, 추가적인 생성은 전체 재료 목록에 의해 차단되는 단계만 동시에 발생합니다. (전체 재료 목록을 사용하여 정확한 단계를 생성해야 하기 때문입니다).\n\n# 결론\n\n사용자 경험(UX)이 높은 처리량을 필요로 하며 작은 컨텍스트 창을 통해 작동할 수 있는 애플리케이션의 경우, Fireworks.ai와 Llama-3 8B/70B는 절대적으로 게임 체인저입니다. 그것은 팀이 OpenAI의 GPT 모델의 높은 지연 때문에 전반적인 UX를 희생시키지 않고 사용 사례에 대해 빌드할 수 있도록 해줍니다.\n\nSystem.Threading.Channels를 사용한 .NET 8 웹 API에 그것을 플러그인하고 SSE와 결합하면, 여러 콘텐츠 청크를 동시에 생성하고, 상호작용적인 생성 AI 경험을 더 많이 구축하거나 생성적인 워크플로우를 간단히 가속화하는 새로운 가능성을 열 수 있습니다.\n\n\n\n동일한 기술을 사용하면 (SSE를 제외하고) 낮은 지연 시간 + 높은 처리량 모델 및 플랫폼을 사용하여 여러 프롬프트를 병렬로 처리하여 서버 생성 워크로드의 처리량을 늘릴 수 있습니다.\n\n전체 repository:","ogImage":{"url":"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_0.png"},"coverImage":"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_0.png","tag":["Tech"],"readingTime":13},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h1>요약</h1>\n<ul>\n<li>OpenAI의 GTP-4는 일반적인 목적의 작업에 있어 압도적인 성능을 보여주지만, 전체적인 처리량(또는 오히려 그 부족함)이 많이 부족하다는 점이 매우 아쉽습니다. 이는 \"오프라인\" 작업에는 훌륭하지만, 사용자들이 더 많은 응답성을 기대하는 응용 프로그램에서는 적합하지 않을 수 있으며, 몇몇 사용 사례들은 하위 UX로 인해 배제될 수도 있습니다.</li>\n<li>TheFastest.ai 팀의 최근 Hackernews 게시물은 모델과 플랫폼 모두에 대해 이런 차이가 얼마나 클 수 있는지를 강조하고 있습니다. 특히, Groq.com(Musk의 Grok와 혼동하지 말아야 합니다)와 Meta의 Llama 3 70B를 사용한 Fireworks.ai는 일부 작업에서 GPT-4와 비교했을 때 출력에 거의 희생 없이 빠른 처리량을 제공합니다.</li>\n<li>C#/.NET 8 System.Threading.Channels와 서버 전송 이벤트(SSE)를 결합하면, OpenAI의 처리량과 높은 지연 시간으로 잘 동작하지 않은 작업을 구성할 수 있습니다.</li>\n</ul>\n<h1>소개</h1>\n<p>GPT-5를 기다리는 동안, 2024년 5월 OpenAI의 GPT-4가 여전히 LLM으로서 전반적인 성능 면에서 우수하다는 것에 대해 논쟁하는 사람은 거의 없을 것입니다. 그러나 해당 모델은 비교적 낮은 처리량과 높은 대기 시간으로 인해 UX가 더 상호 작용적인 경험을 요구하는 경우에는 최적이 아닐 수 있습니다.</p>\n<p>가장 빠른 LLM과 현재 사용 가능한 플랫폼과 OpenAI 간의 대기 시간 차이의 규모가 얼마나 큰지는 명백하지 않을 수 있습니다.</p>\n<p>최근 Hackernews 스레드를 통해 TheFastest.ai로 이동하게 되었고, Meta의 Llama 3의 높은 처리량과 Groq.com 및 Fireworks.ai라는 두 플랫폼에 흥미로웠습니다.</p>\n<p>(전자는 종종 머스크의 Grok AI와 혼동되기 때문에 불행합니다).</p>\n<p>이 기사에서는 Fireworks.ai, Meta Llama 3 8B/70B, .NET 8, System.Threading.Channels 및 Server Sent Events (SSE)를 사용하여 앱을 만드는 방법을 살펴볼 것입니다.</p>\n<h1>차이를 측정하기</h1>\n<p>스택의 상단은 Llama-3과 Groq가 지배하고 Fireworks.ai가 상위 5위를 차지하고 있습니다(각 팀이 Fireworks를 선택해야 할 이유에 대해 조금 뒤에 설명하겠습니다).</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_0.png\" alt=\"이미지\"></p>\n<p>대조적으로, OpenAI의 GPT-4는 거의 맨 아래쪽에 위치합니다.</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_1.png\" alt=\"이미지\"></p>\n<p>OpenAI의 GPT-4를 사용해본 사람이라면 이미 처리량이 얼마나 낮은지를 알고 있을 것입니다. 하지만 이렇게 측정된 값을 보면 그 간격이 얼마나 큰지 더욱 부각됩니다. Groq의 Llama-3 70B는 GPT-4보다 거의 10배 더 높은 처리량을 가지고 있습니다!</p>\n<p>이에 따라, GPT-4는 상호 작용이 필요하지 않은 경우에, 작업이 큰 문맥 창을 요구하는 경우에, 또는 복잡한 프롬프트와 문맥을 사용하여 \"벤치마크 품질\"의 결과가 필요한 경우에 실제로 매우 좋다고 생각했습니다.</p>\n<p>하지만 사용 사례에 다른 요구 사항이 있는 경우는 어떨까요? 속도가 필요한 경우는 어떨까요?</p>\n<h1>Groq와 Fireworks를 이용해 시동 걸기</h1>\n<p>OpenAI의 처리량이 떨어져 사용자 경험을 나빠지게 만들 수 있는 문제 중 하나는, 최종적으로 콘텐츠가 가치를 추가한다 해도 주관적으로 사용자 경험을 나빠지게 할 수 있다는 것입니다.</p>\n<p>OpenAI의 ChatGPT를 사용할 때, 채팅 응답에 몇 초가 걸릴 수도 있다는 사실을 SSE가 가려버리기 때문에 명확하게 드러나지 않을 수 있습니다. GPT-4의 처리량이 낮다는 것은 다른 대안을 시도해보기 전까지는 쉽게 알아챌 수 없습니다.</p>\n<h1>Groq.com</h1>\n<p>Groq는 LLM에 특별히 설계된 사용자 정의 하드웨어로 알려진 \"LPU\" 또는 \"언어 처리 유닛\"을 갖춘 흥미로운 플랫폼입니다:</p>\n<p>적어도 문서로 보면, 이것은 마케팅 허세 이상으로 보이며 플랫폼은 객관적으로 고 처리량을 자랑합니다.</p>\n<p>하지만 주요 문제는 현재의 SaaS 제공으로 이어집니다:</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_2.png\" alt=\"이미지\"></p>\n<p>무료 티어는 실험 용도로만 사용 가능하며, 그것도 겨우 가능할 뿐입니다.</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_3.png\" alt=\"이미지\"></p>\n<p>그래서 Groq은 꽤 빠르지만, 샌드박싱 용도 외에는 사용할 수 없으며, 가능하다면 엔터프라이즈 과금을 통해 사용할 수 있습니다.</p>\n<h1>Fireworks.ai</h1>\n<p>현재 시점에서 Fireworks의 Llama-3 70B는 전체적으로 9위에 랭크되어 있으며 두 번째로 빠른 Llama-3 70B입니다:</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_4.png\" alt=\"이미지\"></p>\n<p>마지막 토큰까지 260ms가 소요되며, 여전히 매우 빠르며 GPT-3.5와 GPT-4 사이의 성능을 제공하여 내 사용 사례에 대한 LLM 성능이 매우 좋습니다.</p>\n<p>Fireworks.ai에는 중간 유료 티어가 없지만, 600 RPM은 작은 앱에 사용하기 적합하며 하드 토큰 제한이 없습니다.</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_5.png\" alt=\"이미지\"></p>\n<p>오늘 빠르게 무언가를 구축하려는 팀들에게는 Fireworks.ai가 아마도 최선의 선택일 것입니다. (아니, 나는 그들로부터 돈을 받고 있지 않아요)</p>\n<h1>.NET 8, System.Threading.Channels 및 Server Sent Events (SSE)와 함께 실용적인 예제</h1>\n<p>이 놀라운 처리량을 활용하기 위해서는 한 번에 여러 개의 스트림을 통해 생성한 다음 하나의 최종 출력 스트림으로 병합하는 동시 처리 전략이 필요합니다.</p>\n<p>이는 .NET의 System.Threading.Channels를 Server Sent Events (SSE)와 결합하여 이 처리량을 완전히 활용하고 높은 반응성을 갖는 생성 AI 경험을 구축하는 완벽한 사용 사례입니다.</p>\n<p>이전에 이 두 주제에 대해 별도로 다뤘었습니다:</p>\n<ul>\n<li>.NET Task Parallel Library vs System.Threading.Channels</li>\n<li>.NET 6의 System.Threading.Channels를 이용한 동시 처리 (보너스: 간격 트리)</li>\n<li>.NET 7과 함께하는 Server Sent Events</li>\n</ul>\n<p>오늘은 .NET 8 채널, Semantic Kernel 및 gen AI와 함께 어떤 대화형 경험을 만들 수 있는지 함께 살펴보겠습니다!</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_6.png\" alt=\"image\"></p>\n<p>저희 샘플 응용 프로그램은 준비된 재료 목록과 목표 조리 시간을 받아들여서 다음을 할 것입니다:</p>\n<ul>\n<li>해당 재료로 만들 수 있는 레시피 목록 생성</li>\n<li>레시피 중 하나를 무작위로 선택</li>\n<li>레시피를 위해 필요한 모든 재료 목록 생성</li>\n<li>레시피를 위한 소개 단락 생성</li>\n<li>준비된 각 재료에 대한 영양 정보에 대한 간략한 설명 생성</li>\n<li>제안된 사이드 디시 목록 생성</li>\n<li>순서 목록 생성</li>\n</ul>\n<p>단계 3<del>6은 병렬로 실행될 수 있지만, 레시피를 먼저 선택해야 하기 때문에 단계 1</del>2가 먼저 실행됩니다. 그리고 단계를 생성하기 전에 재료 전체 목록을 기다려야 합니다.</p>\n<h1>.NET 채널을 이용한 병행 실행</h1>\n<p>API 호출의 진입점은 요청을 받을 단일 POST 엔드포인트입니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">// 👇 메인 진입점.</span>\napp.<span class=\"hljs-title class_\">MapPost</span>(<span class=\"hljs-string\">\"/generate\"</span>, <span class=\"hljs-keyword\">async</span> (\n  <span class=\"hljs-title class_\">HttpContext</span> context,          <span class=\"hljs-comment\">// 의존성 주입에서 가져옴</span>\n  <span class=\"hljs-title class_\">RecipeGenerator</span> generator,    <span class=\"hljs-comment\">// 의존성 주입에서 가져옴</span>\n  <span class=\"hljs-title class_\">RecipeRequest</span> request,        <span class=\"hljs-comment\">// 바디에서 가져옴</span>\n  <span class=\"hljs-title class_\">CancellationToken</span> cancellation = <span class=\"hljs-keyword\">default</span>\n) =>\n{\n  context.<span class=\"hljs-property\">Response</span>.<span class=\"hljs-property\">Headers</span>.<span class=\"hljs-property\">ContentType</span> = <span class=\"hljs-string\">\"text/event-stream\"</span>;\n\n  <span class=\"hljs-keyword\">await</span> generator.<span class=\"hljs-title class_\">GenerateAsync</span>(\n    request,\n    <span class=\"hljs-comment\">// 각 단편에 대한 스트리밍 응답을 작성하는 핸들러</span>\n    <span class=\"hljs-keyword\">async</span> (<span class=\"hljs-title class_\">Fragment</span> f) => {\n      <span class=\"hljs-keyword\">await</span> context.<span class=\"hljs-property\">Response</span>.<span class=\"hljs-title class_\">WriteAsync</span>(\n        $<span class=\"hljs-string\">\"data: {f.Part}|{f.Content}{Environment.NewLine}{Environment.NewLine}\"</span>,\n        cancellation\n      );\n      <span class=\"hljs-keyword\">await</span> context.<span class=\"hljs-property\">Response</span>.<span class=\"hljs-property\">Body</span>.<span class=\"hljs-title class_\">FlushAsync</span>(cancellation);\n    }\n  );\n});\n</code></pre>\n<p>RecipeGenerator.GenerateAsync 메서드에는 메인 플로우가 포함되어 있어요:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">/// &#x3C;summary></span>\n<span class=\"hljs-comment\">/// 주요 시작점</span>\n<span class=\"hljs-comment\">/// &#x3C;/summary></span>\npublic <span class=\"hljs-keyword\">async</span> <span class=\"hljs-title class_\">Task</span> <span class=\"hljs-title class_\">GenerateAsync</span>(\n  <span class=\"hljs-title class_\">RecipeRequest</span> request,\n  <span class=\"hljs-title class_\">Func</span>&#x3C;<span class=\"hljs-title class_\">Fragment</span>, <span class=\"hljs-title class_\">Task</span>> handler, <span class=\"hljs-comment\">// 👈 이것은 HTTP 응답 스트림에 연결된 후크에요</span>\n  <span class=\"hljs-title class_\">CancellationToken</span> cancellation = <span class=\"hljs-keyword\">default</span>\n) {\n\n  <span class=\"hljs-keyword\">var</span> (ingredientsOnHand, prepTime) = request;\n\n  <span class=\"hljs-comment\">// 👇 (1) 3개의 레시피 목록을 생성하고 무작위로 하나를 선택</span>\n  <span class=\"hljs-keyword\">var</span> recipes = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">GenerateRecipesAsync</span>(ingredientsOnHand, prepTime, cancellation);\n\n  <span class=\"hljs-title class_\">Console</span>.<span class=\"hljs-title class_\">WriteLine</span>($<span class=\"hljs-string\">\"생성된 레시피 수: {recipes.Length}.\"</span>);\n\n  <span class=\"hljs-keyword\">var</span> recipe = recipes[<span class=\"hljs-title class_\">Random</span>.<span class=\"hljs-property\">Shared</span>.<span class=\"hljs-title class_\">Next</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">2</span>)];\n\n  <span class=\"hljs-comment\">// 👇 (2) 모든 레시피를 보유하여 HTML 문자열로 집계</span>\n  <span class=\"hljs-keyword\">var</span> alternates = recipes\n    .<span class=\"hljs-title class_\">Where</span>(<span class=\"hljs-function\"><span class=\"hljs-params\">r</span> =></span> r.<span class=\"hljs-property\">Name</span> != recipe.<span class=\"hljs-property\">Name</span>)\n    .<span class=\"hljs-title class_\">Aggregate</span>(<span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">StringBuilder</span>(), <span class=\"hljs-function\">(<span class=\"hljs-params\">html, r</span>) =></span> {\n      html.<span class=\"hljs-title class_\">Append</span>($<span class=\"hljs-string\">\"&#x3C;li>&#x3C;b>{r.Name}&#x3C;/b> &#x26;nbsp;\"</span>);\n      html.<span class=\"hljs-title class_\">Append</span>($<span class=\"hljs-string\">\"&#x3C;i>{r.Intro}&#x3C;/i>&#x3C;/li>\"</span>);\n\n      <span class=\"hljs-keyword\">return</span> html;\n    }).<span class=\"hljs-title class_\">ToString</span>();\n\n  <span class=\"hljs-comment\">// 👇 (3) 읽기 채널의 리더 측에 대한 반복 작업입니다; 먼저 시작해야 해요</span>\n  <span class=\"hljs-keyword\">var</span> <span class=\"hljs-title function_\">fragmentHandler</span> = <span class=\"hljs-keyword\">async</span> (<span class=\"hljs-params\"></span>) => {\n    <span class=\"hljs-keyword\">while</span> (<span class=\"hljs-keyword\">await</span> _channel.<span class=\"hljs-property\">Reader</span>.<span class=\"hljs-title class_\">WaitToReadAsync</span>()) {\n      <span class=\"hljs-keyword\">if</span> (_channel.<span class=\"hljs-property\">Reader</span>.<span class=\"hljs-title class_\">TryRead</span>(out <span class=\"hljs-keyword\">var</span> fragment)) {\n        <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">handler</span>(fragment);\n      }\n    }\n  };\n\n  <span class=\"hljs-keyword\">var</span> completion = <span class=\"hljs-title function_\">fragmentHandler</span>();\n\n  <span class=\"hljs-comment\">// 👇 (4) 이제 세대 프롬프트를 동시에 실행해요</span>\n  <span class=\"hljs-title class_\">Task</span>.<span class=\"hljs-title class_\">WaitAll</span>([\n    <span class=\"hljs-title function_\">handler</span>(<span class=\"hljs-keyword\">new</span> (<span class=\"hljs-string\">\"alt\"</span>, alternates)),\n    <span class=\"hljs-title class_\">GenerateIngredientsAsync</span>(recipe, ingredientsOnHand, request.<span class=\"hljs-property\">PrepTime</span>, cancellation),\n    <span class=\"hljs-title class_\">GenerateIntroAsync</span>(recipe, cancellation),\n    <span class=\"hljs-title class_\">GenerateIngredientIntroAsync</span>(ingredientsOnHand, cancellation),\n    <span class=\"hljs-title class_\">GenerateSidesAsync</span>(recipe, cancellation)\n  ]);\n\n  <span class=\"hljs-comment\">// 👇 (5) 그리고 모든 작업이 완료될 때까지 기다려요.</span>\n  _channel.<span class=\"hljs-property\">Writer</span>.<span class=\"hljs-title class_\">Complete</span>();\n\n  <span class=\"hljs-keyword\">await</span> completion;\n}\n</code></pre>\n<p>여기서 Task.WaitAll의 중요한 차이점은 JavaScript의 Promise.all과 개념적으로 비슷하지만, .NET에서는 멀티 스레드인 .NET 런타임 때문에 동시성과 병렬로 실행될 수 있어요. 이 경우 스레드 풀 스케줄러가 각 작업이 다른 스레드에서 실행될지 여부를 결정할 거에요. 채널을 사용하면 출력을 하나의 스레드에 바인딩된 리더에 병합하여 동기화된 액세스가 필요 없어졌어요.</p>\n<p>각 세대 작업은 비슷한 패턴을 따라가요:</p>\n<pre><code class=\"hljs language-js\">private <span class=\"hljs-keyword\">async</span> <span class=\"hljs-title class_\">Task</span> <span class=\"hljs-title class_\">GenerateIntroAsync</span>(\n  <span class=\"hljs-title class_\">RecipeSummary</span> recipe,\n  <span class=\"hljs-title class_\">CancellationToken</span> cancellation = <span class=\"hljs-keyword\">default</span>\n) {\n  <span class=\"hljs-keyword\">var</span> prompt = <span class=\"hljs-string\">\"...\"</span>;\n\n  <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">ExecutePromptAsync</span>(\n    <span class=\"hljs-string\">\"int\"</span>, <span class=\"hljs-comment\">// 👈 이것은 프론트엔드 출력 대상의 ID와 일치합니다</span>\n    prompt,\n    <span class=\"hljs-keyword\">new</span> () {\n      <span class=\"hljs-title class_\">MaxTokens</span> = <span class=\"hljs-number\">250</span>,\n      <span class=\"hljs-title class_\">Temperature</span> = <span class=\"hljs-number\">0.55</span>,\n      <span class=\"hljs-title class_\">TopP</span> = <span class=\"hljs-number\">0</span>\n    },\n    <span class=\"hljs-attr\">cancellation</span>: cancellation\n  );\n}\n</code></pre>\n<p>그리고 프롬프트를 실행하는 메서드:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">/// &#x3C;summary></span>\n<span class=\"hljs-comment\">/// 프롬프트를 실행하고 결과를 채널에 작성합니다.</span>\n<span class=\"hljs-comment\">/// &#x3C;/summary></span>\nprivate <span class=\"hljs-keyword\">async</span> <span class=\"hljs-title class_\">Task</span> <span class=\"hljs-title class_\">ExecutePromptAsync</span>(\n  string part,\n  string prompt,\n  <span class=\"hljs-title class_\">OpenAIPromptExecutionSettings</span> settings,\n  <span class=\"hljs-title class_\">Action</span>&#x3C;string>? resultHandler = <span class=\"hljs-literal\">null</span>,\n  string? modelOverride = <span class=\"hljs-literal\">null</span>,\n  <span class=\"hljs-title class_\">CancellationToken</span> cancellation = <span class=\"hljs-keyword\">default</span>\n) {\n  <span class=\"hljs-comment\">// 👇 대화를 초기화합니다</span>\n  <span class=\"hljs-keyword\">var</span> chat = _kernel.<span class=\"hljs-property\">GetRequiredService</span>&#x3C;<span class=\"hljs-title class_\">IChatCompletionService</span>>(\n    modelOverride ?? <span class=\"hljs-string\">\"70b\"</span> <span class=\"hljs-comment\">// 명시된 오버라이드가 없으면 70b를 사용합니다.</span>\n  );\n\n  <span class=\"hljs-keyword\">var</span> history = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">ChatHistory</span>();\n  <span class=\"hljs-keyword\">var</span> buffer = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">StringBuilder</span>();\n\n  history.<span class=\"hljs-title class_\">AddUserMessage</span>(prompt);\n\n  <span class=\"hljs-comment\">// 👇 응답을 스트리밍하고 각 부분을 채널에 작성합니다</span>\n  <span class=\"hljs-keyword\">await</span> foreach (<span class=\"hljs-keyword\">var</span> message <span class=\"hljs-keyword\">in</span> chat.<span class=\"hljs-title class_\">GetStreamingChatMessageContentsAsync</span>(\n      history, settings, _kernel, cancellation\n    )\n  ) {\n      <span class=\"hljs-keyword\">await</span> _channel.<span class=\"hljs-property\">Writer</span>.<span class=\"hljs-title class_\">WriteAsync</span>( <span class=\"hljs-comment\">// 👈 채널의 라이터 엔드</span>\n        <span class=\"hljs-title function_\">new</span>(part, message.<span class=\"hljs-property\">Content</span> ?? <span class=\"hljs-string\">\"\"</span>),\n        cancellation\n      );\n\n      buffer.<span class=\"hljs-title class_\">Append</span>(message.<span class=\"hljs-property\">Content</span>); <span class=\"hljs-comment\">// 👈 전체 출력을 보유하는 버퍼</span>\n  }\n\n  <span class=\"hljs-keyword\">var</span> output = buffer.<span class=\"hljs-title class_\">ToString</span>();\n\n  <span class=\"hljs-comment\">// 👇 호출자가 전체 결과를 원하는 경우 여기에서 사용할 수 있습니다</span>\n  resultHandler?.<span class=\"hljs-title class_\">Invoke</span>(output);\n}\n</code></pre>\n<p>애플리케이션 실행 중 의존성 주입을 통해 커널 인스턴스가 구성됩니다:```</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">// Program.cs</span>\n<span class=\"hljs-keyword\">var</span> builder = <span class=\"hljs-title class_\">WebApplication</span>.<span class=\"hljs-title class_\">CreateBuilder</span>(args);\n\n<span class=\"hljs-keyword\">var</span> fireworksEndpoint = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">Uri</span>(<span class=\"hljs-string\">\"https://api.fireworks.ai/inference/v1/chat/completions\"</span>);\n<span class=\"hljs-keyword\">var</span> groqEndpoint = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">Uri</span>(<span class=\"hljs-string\">\"https://api.groq.com/openai/v1/chat/completions\"</span>);\n\n<span class=\"hljs-keyword\">var</span> config = builder.<span class=\"hljs-property\">Configuration</span>\n  .<span class=\"hljs-title class_\">GetSection</span>(<span class=\"hljs-title function_\">nameof</span>(<span class=\"hljs-title class_\">RecipesConfig</span>))\n  .<span class=\"hljs-property\">Get</span>&#x3C;<span class=\"hljs-title class_\">RecipesConfig</span>>();\n\n<span class=\"hljs-comment\">// Semantic Kernel을 설정하여 필요한만큼의 LLM을 등록합니다.</span>\n<span class=\"hljs-keyword\">var</span> kernelBuilder = <span class=\"hljs-title class_\">Kernel</span>.<span class=\"hljs-title class_\">CreateBuilder</span>();\n<span class=\"hljs-keyword\">var</span> kernel = kernelBuilder\n  .<span class=\"hljs-title class_\">AddOpenAIChatCompletion</span>(\n    <span class=\"hljs-attr\">modelId</span>: <span class=\"hljs-string\">\"accounts/fireworks/models/llama-v3-70b-instruct\"</span>,\n    <span class=\"hljs-attr\">apiKey</span>: config!.<span class=\"hljs-property\">FireworksKey</span>,\n    <span class=\"hljs-attr\">endpoint</span>: fireworksEndpoint,\n    <span class=\"hljs-attr\">serviceId</span>: <span class=\"hljs-string\">\"70b\"</span> <span class=\"hljs-comment\">// 👈 더 나은 결과를 위해 기본적으로 이 serviceId를 사용합니다</span>\n  )\n  .<span class=\"hljs-title class_\">AddOpenAIChatCompletion</span>(\n    <span class=\"hljs-attr\">modelId</span>: <span class=\"hljs-string\">\"accounts/fireworks/models/llama-v3-8b-instruct\"</span>,\n    <span class=\"hljs-attr\">apiKey</span>: config!.<span class=\"hljs-property\">FireworksKey</span>,\n    <span class=\"hljs-attr\">endpoint</span>: fireworksEndpoint,\n    <span class=\"hljs-attr\">serviceId</span>: <span class=\"hljs-string\">\"8b\"</span> <span class=\"hljs-comment\">// 👈 더 빠른 속도가 필요한 경우 이 serviceId를 사용합니다</span>\n  )\n  .<span class=\"hljs-title class_\">AddOpenAIChatCompletion</span>(\n    <span class=\"hljs-attr\">modelId</span>: <span class=\"hljs-string\">\"llama3-8b-8192\"</span>,\n    <span class=\"hljs-attr\">apiKey</span>: config!.<span class=\"hljs-property\">GroqKey</span>,\n    <span class=\"hljs-attr\">endpoint</span>: groqEndpoint,\n    <span class=\"hljs-attr\">serviceId</span>: <span class=\"hljs-string\">\"groq-8b\"</span> <span class=\"hljs-comment\">// 👈 최대 처리량을 위해 이 serviceId를 사용합니다</span>\n  )\n  <span class=\"hljs-comment\">// 다른 LLM을 여기에 등록합니다.</span>\n  .<span class=\"hljs-title class_\">Build</span>();\n\nbuilder.<span class=\"hljs-property\">Services</span>\n  .<span class=\"hljs-property\">Configure</span>&#x3C;<span class=\"hljs-title class_\">RecipesConfig</span>>(\n    builder.<span class=\"hljs-property\">Configuration</span>.<span class=\"hljs-title class_\">GetSection</span>(<span class=\"hljs-title function_\">nameof</span>(<span class=\"hljs-title class_\">RecipesConfig</span>))\n  )\n  .<span class=\"hljs-title class_\">AddCors</span>()\n  .<span class=\"hljs-title class_\">AddSingleton</span>(kernel)  <span class=\"hljs-comment\">// 👈 설정된 kernel을 싱글톤으로 추가합니다</span>\n  .<span class=\"hljs-property\">AddScoped</span>&#x3C;<span class=\"hljs-title class_\">RecipeGenerator</span>>();\n</code></pre>\n<p>Semantic Kernel을 통해 여러 LLM 엔드포인트를 구성할 수 있습니다. 이를 사용하여 작은 빠른 LLM이 프로세스를 가속화할 수 있는 플로 구현을 단순화할 수 있습니다.</p>\n<h1>SSE를 활용한 동시 스트림</h1>\n<p>컨텐츠가 생성되면 백엔드는 즉시 프론트엔드로 스트리밍하여 매우 반응이 뛰어난 사용자 경험을 제공합니다. 이 과정은 동시에(그리고 스레드 풀 스케줄러에 따라 확장하여 병렬로) 발생하며, 채널에 수집되어 클라이언트에서 소비될 응답 스트림으로 작성됩니다.```</p>\n<p>이 흐름을 시각화하기 위해 아래 다이어그램을 확인해보세요:</p>\n<p><img src=\"/assets/img/2024-05-13-NeedforSpeedLLMsBeyondOpenAIwithCNET8SSEChannelsLlama3andFireworksai_7.png\" alt=\"다이어그램\"></p>\n<p>Task.WaitAll 코드 블록은 채널의 공유 가능하고 스레드 안전한 writer 엔드를 전달받은 상태이며, reader 엔드는 HTTP 응답 스트림과 콜백을 통해 연결됩니다.</p>\n<p>해당 콜백은 간단히 EventSource의 필요한 형식 명세에 따라 Fragment를 서식화합니다.</p>\n<p>이 경우:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-attr\">data</span>: ing|tomatoes\n\n<span class=\"hljs-attr\">data</span>: ing|basil\n\n<span class=\"hljs-attr\">data</span>: ste|<span class=\"hljs-number\">3.</span> <span class=\"hljs-title class_\">Chop</span> the\n</code></pre>\n<p>프론트엔드는 이러한 메시지 스트림을 받아 UI의 서로 다른 섹션에 누적합니다.</p>\n<ul>\n<li>첫 번째 부분인 ing은 이 내용이 속하는 프론트엔드 부분을 식별합니다 (이 경우에는 \"재료\")</li>\n<li>| 이후의 텍스트는 LLM에 의해 작성된 출력 토큰 세트를 의미합니다.</li>\n</ul>\n<p>프론트엔드에서 @microsoft/fetch-event-source는 기본 EventSource를 대체하여 POST 사용을 가능하게 하는 폴리필(polyfill)로 사용됩니다.</p>\n<p>수신자는 각 메시지를 가져와 디코드합니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-attr\">onmessage</span>: <span class=\"hljs-function\">(<span class=\"hljs-params\">msg</span>) =></span> {\n  <span class=\"hljs-keyword\">var</span> payload = msg.<span class=\"hljs-property\">data</span>\n\n  <span class=\"hljs-keyword\">var</span> [part, content] = payload.<span class=\"hljs-title function_\">split</span>(<span class=\"hljs-string\">'|'</span>)\n\n  <span class=\"hljs-keyword\">if</span> (!part || !$el(<span class=\"hljs-string\">`#<span class=\"hljs-subst\">${part}</span>`</span>)) {\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-comment\">// 이 메시지는 버립니다</span>\n  }\n\n  <span class=\"hljs-comment\">// 👇 이 부분은 새 줄을 인코딩하고 여기서 대체하는 해킹입니다.</span>\n  content = content.<span class=\"hljs-title function_\">replace</span>(<span class=\"hljs-regexp\">/⮑/gi</span>, <span class=\"hljs-string\">\"\\n\"</span>)\n\n  $el(<span class=\"hljs-string\">`#<span class=\"hljs-subst\">${part}</span>`</span>).<span class=\"hljs-property\">innerHTML</span> += content\n},\n</code></pre>\n<p>text/event-stream의 특이점은 이중 줄바꿈이 메시지 블록의 끝을 나타낸다는 것입니다. 그래서 줄바꿈은 어떤 방식으로든 인코딩되어야 합니다 (다양한 방법이 있습니다). 이 경우, 단일 문자 ⮑을 사용하여 해당 문자를 찾아 \\n으로 대체하는 것이 간답습니다.</p>\n<p>CSS는 그냥 이 부분을 고려하면 됩니다:</p>\n<pre><code class=\"hljs language-js\">#add, #ing, #ste {\n  white-<span class=\"hljs-attr\">space</span>: pre-line;\n}\n</code></pre>\n<p>HTML 자체는 간단합니다:</p>\n<pre><code class=\"hljs language-js\">&#x3C;!-- 이 블록은 추가 재료를 보관합니다 -->\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"additional\"</span>></span>\n  <span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">h2</span>></span>필요한 재료<span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">h2</span>></span>\n  <span class=\"hljs-comment\">&#x3C;!-- 👇 이 ID는 Fragment.Part와 일치합니다 --></span>\n  <span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">\"add\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span>\n<span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n\n&#x3C;!-- 이 블록은 단계를 보관합니다 -->\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"recipe\"</span>></span>\n  <span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">h2</span>></span>조리 단계<span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">h2</span>></span>\n  <span class=\"hljs-comment\">&#x3C;!-- 👇 이 ID는 Fragment.Part와 일치합니다 --></span>\n  <span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">div</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">\"ste\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span>\n<span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">div</span>></span></span>\n</code></pre>\n<h1>모두가 준비되었으니 이제 앱을 실행하면 다음과 같은 경험을 할 수 있습니다:</h1>\n<p><img src=\"https://miro.medium.com/v2/resize:fit:1400/0*uCMJGy8UoyaC4rFX.gif\" alt=\"recipe app\"></p>\n<p>레시피 목록을 생성하는 호출이 차단되므로 약간의 초기 지연이 있습니다.</p>\n<p>그러나 한 번 목록이 생성되고 무작위로 선택된 후, 추가적인 생성은 전체 재료 목록에 의해 차단되는 단계만 동시에 발생합니다. (전체 재료 목록을 사용하여 정확한 단계를 생성해야 하기 때문입니다).</p>\n<h1>결론</h1>\n<p>사용자 경험(UX)이 높은 처리량을 필요로 하며 작은 컨텍스트 창을 통해 작동할 수 있는 애플리케이션의 경우, Fireworks.ai와 Llama-3 8B/70B는 절대적으로 게임 체인저입니다. 그것은 팀이 OpenAI의 GPT 모델의 높은 지연 때문에 전반적인 UX를 희생시키지 않고 사용 사례에 대해 빌드할 수 있도록 해줍니다.</p>\n<p>System.Threading.Channels를 사용한 .NET 8 웹 API에 그것을 플러그인하고 SSE와 결합하면, 여러 콘텐츠 청크를 동시에 생성하고, 상호작용적인 생성 AI 경험을 더 많이 구축하거나 생성적인 워크플로우를 간단히 가속화하는 새로운 가능성을 열 수 있습니다.</p>\n<p>동일한 기술을 사용하면 (SSE를 제외하고) 낮은 지연 시간 + 높은 처리량 모델 및 플랫폼을 사용하여 여러 프롬프트를 병렬로 처리하여 서버 생성 워크로드의 처리량을 늘릴 수 있습니다.</p>\n<p>전체 repository:</p>\n</body>\n</html>\n"},"__N_SSG":true}