{"pageProps":{"post":{"title":"íŒŒì´í† ì¹˜ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë§Œë“¤ì–´ë³´ê¸° GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ í¬í•¨","description":"","date":"2024-05-15 10:33","slug":"2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation","content":"\n\n## C/C++, CUDA, ë° Pythonì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ ìœ ì˜ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í•´ ë³´ì„¸ìš”. GPU ì§€ì›ê³¼ ìë™ ë¯¸ë¶„ì„ ì œê³µí•©ë‹ˆë‹¤\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png)\n\n# ì†Œê°œ\n\nì—¬ëŸ¬ í•´ ë™ì•ˆ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•´ ì™”ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ê·¸ ë¬¸ë²•ê³¼ ê·œì¹™ì„ ìµíˆê³ ë„, ì œ ê¶ê¸ˆì¦ì„ ìê·¹í•˜ë˜ ê²ƒì´ ìˆì—ˆìŠµë‹ˆë‹¤: ì´ëŸ¬í•œ ì‘ì—… ì¤‘ì— ë‚´ë¶€ì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ” ê±¸ê¹Œìš”? ì´ ëª¨ë“  ê²ƒì´ ì–´ë–»ê²Œ ì‘ë™í• ê¹Œìš”?\n\n\n\nì—¬ê¸°ê¹Œì§€ ì˜¤ì…¨ë‹¤ë©´, ì•„ë§ˆë„ ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ ê°€ì§€ê³  ê³„ì‹¤ ê²ƒì…ë‹ˆë‹¤. íŒŒì´í† ì¹˜(PyTorch)ì—ì„œ ëª¨ë¸ì„ ìƒì„±í•˜ê³  í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ë¬¼ì–´ë³¸ë‹¤ë©´ ì•„ë§ˆë„ ì•„ë˜ ì½”ë“œì™€ ë¹„ìŠ·í•œ ê²ƒì„ ìƒê°í•´ë³¼ ê²ƒì…ë‹ˆë‹¤:\n\n```js\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(1, 10)\n        self.sigmoid = nn.Sigmoid()\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        \n        return out\n\n...\n\nmodel = MyModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(epochs):\n    for x, y in ...\n        \n        x = x.to(device)\n        y = y.to(device)\n\n        outputs = model(x)\n        loss = criterion(outputs, y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\ní•˜ì§€ë§Œ ì´ë²ˆì— ì—­ì „íŒŒ(backward) ë‹¨ê³„ê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë¬¼ì–´ë³¸ë‹¤ë©´ ì–´ë–¨ê¹Œìš”? ë˜ëŠ” ì˜ˆë¥¼ ë“¤ì–´, í…ì„œë¥¼ ì¬êµ¬ì„±í•  ë•Œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ìš”? ë‚´ë¶€ì—ì„œ ë°ì´í„°ê°€ ì¬ë°°ì¹˜ë˜ë‚˜ìš”? ê·¸ëŸ° ì¼ì´ ì–´ë–»ê²Œ ì¼ì–´ë‚˜ë‚˜ìš”? ì™œ PyTorchëŠ” ë¹ ë¥¸ê°€ìš”? PyTorchê°€ GPU ì—°ì‚°ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ìš”? ì´ëŸ° ì§ˆë¬¸ë“¤ì´ í•­ìƒ ì €ë¥¼ í˜¸ê¸°ì‹¬ ê°€ë“í•˜ê²Œ ë§Œë“¤ì—ˆê³ , ì—¬ëŸ¬ë¶„ë„ ë§ˆì°¬ê°€ì§€ë¡œ í˜¸ê¸°ì‹¬ì´ ë“œì‹¤ ê²ƒì´ë¼ê³  ìƒìƒí•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ëŸ¬í•œ ê°œë…ì„ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì²˜ìŒë¶€í„° êµ¬ì¶•í•´ë³´ëŠ” ê²ƒì´ ë¬´ì—‡ë³´ë‹¤ ì¢‹ì„ê¹Œìš”? ì´ ê¸€ì—ì„œ ì—¬ëŸ¬ë¶„ì´ ë°°ìš°ê²Œ ë  ê²ƒì´ ë°”ë¡œ ê·¸ê²ë‹ˆë‹¤!\n\n## #1 â€” í…ì„œ\n\n\n\ní…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ì € ì•Œì•„ì•¼ í•  ê°œë…ì€ ë¬´ì—‡ì´ í…ì„œì¸ì§€ì— ëŒ€í•œ ëª…ë°±í•œ ê°œë…ì…ë‹ˆë‹¤.\n\ní…ì„œëŠ” ëª‡ ê°€ì§€ ìˆ«ìë¥¼ í¬í•¨í•˜ëŠ” nì°¨ì› ë°ì´í„° êµ¬ì¡°ì˜ ìˆ˜í•™ì  ê°œë…ì´ë¼ëŠ” ì§ê´€ì ì¸ ìƒê°ì„ ê°€ì§€ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—¬ê¸°ì„œëŠ” ì´ ë°ì´í„° êµ¬ì¡°ë¥¼ ê³„ì‚°ì  ê´€ì ì—ì„œ ì–´ë–»ê²Œ ëª¨ë¸ë§í• ì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. í…ì„œëŠ” ë°ì´í„° ìì²´ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ì–‘ì´ë‚˜ í…ì„œê°€ ìˆëŠ” ì¥ì¹˜(ì˜ˆ: CPU ë©”ëª¨ë¦¬, GPU ë©”ëª¨ë¦¬)ì™€ ê°™ì€ ì¸¡ë©´ì„ ì„¤ëª…í•˜ëŠ” ë©”íƒ€ë°ì´í„°ë¡œ êµ¬ì„±ëœë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\ní…ì„œì˜ ë‚´ë¶€ë¥¼ ì´í•´í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ì¸ strideë¼ëŠ” ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ ë©”íƒ€ë°ì´í„°ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í…ì„œ ë°ì´í„° ì¬ë°°ì—´ì˜ ë‚´ë¶€ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì•½ê°„ ë” ì´ì— ëŒ€í•´ ë…¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n\n\n\n2-D í…ì„œì˜ ëª¨ì–‘ì´ [4, 8]ì¸ ê²½ìš°ë¥¼ ìƒìƒí•´ë³´ì„¸ìš”.\n\n![í…ì„œ](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_2.png)\n\ní…ì„œì˜ ë°ì´í„°(ì¦‰, ë¶€ë™ ì†Œìˆ˜ì  ìˆ˜)ëŠ” ì‹¤ì œë¡œ ë©”ëª¨ë¦¬ì— 1ì°¨ì› ë°°ì—´ë¡œ ì €ì¥ë©ë‹ˆë‹¤.\n\n![ë°ì´í„°](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_3.png)\n\n\n\nê·¸ëŸ¬ë©´ ì´ 1ì°¨ì› ë°°ì—´ì„ Nì°¨ì› í…ì„œë¡œ ë‚˜íƒ€ë‚´ë ¤ë©´ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n4í–‰ 8ì—´ì˜ í–‰ë ¬ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ í–‰ë ¬ì˜ ëª¨ë“  ì›ì†Œê°€ 1ì°¨ì› ë°°ì—´ì˜ í–‰ì— ì˜í•´ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•  ë•Œ, ìœ„ì¹˜ [2, 3]ì˜ ê°’ì„ ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´ 2í–‰(ê° í–‰ì— 8ê°œì˜ ìš”ì†Œ)ì„ íš¡ë‹¨í•´ì•¼ í•˜ë©° ì¶”ê°€ë¡œ 3ê°œì˜ ìœ„ì¹˜ë¥¼ ì§€ë‚˜ì•¼ í•©ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ë©´ 1ì°¨ì› ë°°ì—´ì—ì„œ 3 + 2 * 8 ìš”ì†Œë¥¼ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.\n\në”°ë¼ì„œ, '8'ì€ ë‘ ë²ˆì§¸ ì°¨ì›ì˜ ìŠ¤íŠ¸ë¼ì´ë“œì…ë‹ˆë‹¤. ì´ ê²½ìš°, ë°°ì—´ì—ì„œ ë‹¤ë¥¸ ìœ„ì¹˜ë¡œ \"ì í”„\"í•˜ê¸° ìœ„í•´ ëª‡ ê°œì˜ ìš”ì†Œë¥¼ íš¡ë‹¨í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´ì…ë‹ˆë‹¤.\n\n\n\në”°ë¼ì„œ, ëª¨ì–‘ì´ [shape_0, shape_1]ì¸ 2ì°¨ì› í…ì„œì˜ ìš”ì†Œ [i, j]ì— ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ j + i * shape_1 ìœ„ì¹˜ì— ìˆëŠ” ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n\nì´ì œ 3ì°¨ì› í…ì„œë¥¼ ìƒìƒí•´ë³´ê² ìŠµë‹ˆë‹¤:\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_5.png)\n\nì´ 3ì°¨ì› í…ì„œë¥¼ í–‰ë ¬ì˜ ì‹œí€€ìŠ¤ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ [5, 4, 8] í…ì„œë¥¼ [4, 8] ëª¨ì–‘ì˜ 5ê°œ í–‰ë ¬ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\nì´ì œ [1, 3, 7] ìœ„ì¹˜ì— ìˆëŠ” ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•˜ê¸° ìœ„í•´ [4,8] í˜•íƒœì˜ í–‰ë ¬ì„ 1ê°œ ì™„ì „íˆ íš¡ë‹¨í•˜ê³ , [8] í˜•íƒœì˜ í–‰ì„ 2ê°œ, [1] í˜•íƒœì˜ ì—´ì„ 7ê°œ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ 1ì°¨ì› ë°°ì—´ì—ì„œ (1 * 4 * 8) + (2 * 8) + (7 * 1) ìœ„ì¹˜ë¥¼ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_6.png)\n\në”°ë¼ì„œ, [shape_0, shape_1, shape_2] ëª¨ì–‘ì˜ 3ì°¨ì› í…ì„œì—ì„œ 1ì°¨ì› ë°ì´í„° ë°°ì—´ì—ì„œ [i][j][k] ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_7.png)\n\n\n\nì´ shape_1 * shape_2ê°€ ì²« ë²ˆì§¸ ì°¨ì›ì˜ strideì´ê³ , shape_2ëŠ” ë‘ ë²ˆì§¸ ì°¨ì›ì˜ strideì´ë©° 1ì€ ì„¸ ë²ˆì§¸ ì°¨ì›ì˜ strideì…ë‹ˆë‹¤.\n\nê·¸ëŸ° ë‹¤ìŒ, ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ì„œëŠ”:\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_8.png)\n\nê° ì°¨ì›ì˜ strideëŠ” ë‹¤ìŒ ì°¨ì› í…ì„œ ëª¨ì–‘ì˜ ê³±ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n\n\n<img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_9.png\" />\n\nê·¸ëŸ° ë‹¤ìŒ stride[n-1] = 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n\nìš°ë¦¬ì˜ í˜•íƒœì˜ í…ì„œ ì˜ˆì œ [5, 4, 8]ì—ì„œ strides = [4*8, 8, 1] = [32, 8, 1]ì¼ ê²ƒì…ë‹ˆë‹¤.\n\nì—¬ëŸ¬ë¶„ë“¤ë„ ì§ì ‘ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆì–´ìš”:\n\n\n\n```js\nimport torch\n\ntorch.rand([5, 4, 8]).stride()\n#(32, 8, 1)\n```\n\nì•Œê² ì–´ìš”, ê·¸ëŸ°ë° ì™œ ëª¨ì–‘ê³¼ ìŠ¤íŠ¸ë¼ì´ë“œê°€ í•„ìš”í•œ ê±´ê°€ìš”? Nì°¨ì› í…ì„œì˜ ìš”ì†Œì— ì ‘ê·¼í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ì´ ê°œë…ì€ í…ì„œ ë°°ì—´ì„ ë§¤ìš° ì‰½ê²Œ ì¡°ì‘í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆì–´ìš”.\n\nì˜ˆë¥¼ ë“¤ì–´, í…ì„œë¥¼ ì¬êµ¬ì„±í•˜ë ¤ë©´ ìƒˆë¡œìš´ ëª¨ì–‘ì„ ì„¤ì •í•˜ê³  ìƒˆë¡œìš´ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤! (ìƒˆë¡œìš´ ëª¨ì–‘ì€ ë™ì¼í•œ ìš”ì†Œ ìˆ˜ë¥¼ ë³´ì¥í•˜ë¯€ë¡œ)\n\n```js\nimport torch\n\nt = torch.rand([5, 4, 8])\n\nprint(t.shape)\n# [5, 4, 8]\n\nprint(t.stride())\n# [32, 8, 1]\n\nnew_t = t.reshape([4, 5, 2, 2, 2])\n\nprint(new_t.shape)\n# [4, 5, 2, 2, 2]\n\nprint(new_t.stride())\n# [40, 8, 4, 2, 1]\n``` \n\n\n\n\ní…ì„œ ë‚´ë¶€ì—ì„œëŠ” ì—¬ì „íˆ ë™ì¼í•œ 1ì°¨ì› ë°°ì—´ë¡œ ì €ì¥ë©ë‹ˆë‹¤. reshape ë©”ì„œë“œëŠ” ë°°ì—´ ë‚´ ìš”ì†Œì˜ ìˆœì„œë¥¼ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! ëŒ€ë‹¨í•˜ì§€ ì•Šë‚˜ìš”? ğŸ˜\n\në‹¤ìŒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorchì—ì„œ ë‚´ë¶€ 1ì°¨ì› ë°°ì—´ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```js\nimport ctypes\n\ndef print_internal(t: torch.Tensor):\n    print(\n        torch.frombuffer(\n            ctypes.string_at(t.data_ptr(), t.storage().nbytes()), dtype=t.dtype\n        )\n    )\n\nprint_internal(t)\n# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...\n\nprint_internal(new_t)\n# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...\n```\n\nì˜ˆë¥¼ ë“¤ì–´ ë‘ ì¶•ì„ ì „ì¹˜í•˜ë ¤ë©´ ë‚´ë¶€ì ìœ¼ë¡œ í•´ë‹¹ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ë‹¨ìˆœíˆ ë°”ê¾¸ì–´ ì£¼ë©´ ë©ë‹ˆë‹¤!\n\n\n\n```js\nt = torch.arange(0, 24).reshape(2, 3, 4)\nprint(t)\n# [[[ 0,  1,  2,  3],\n#   [ 4,  5,  6,  7],\n#   [ 8,  9, 10, 11]],\n \n#  [[12, 13, 14, 15],\n#   [16, 17, 18, 19],\n#   [20, 21, 22, 23]]]\n\nprint(t.shape)\n# [2, 3, 4]\n\nprint(t.stride())\n# [12, 4, 1]\n\nnew_t = t.transpose(0, 1)\nprint(new_t)\n# [[[ 0,  1,  2,  3],\n#   [12, 13, 14, 15]],\n\n#  [[ 4,  5,  6,  7],\n#   [16, 17, 18, 19]],\n\n#  [[ 8,  9, 10, 11],\n#   [20, 21, 22, 23]]]\n\nprint(new_t.shape)\n# [3, 2, 4]\n\nprint(new_t.stride())\n# [4, 12, 1]\n```\n\në‚´ë¶€ ë°°ì—´ì„ ì¶œë ¥í•˜ë©´ ë‘ ê°’ ëª¨ë‘ ë™ì¼í•©ë‹ˆë‹¤:\n\n```js\nprint_internal(t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n\nprint_internal(new_t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n```\n\nê·¸ëŸ¬ë‚˜ new_tì˜ ìŠ¤íŠ¸ë¼ì´ë“œëŠ” ì´ì œ ìœ„ì˜ ì‹ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ê²ƒì€ í…ì„œê°€ ì´ì œ ì—°ì†ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°œìƒí•©ë‹ˆë‹¤. ì¦‰, ë‚´ë¶€ ë°°ì—´ì€ ë™ì¼í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ë‚´ì˜ ê°’ì˜ ìˆœì„œê°€ í…ì„œì˜ ì‹¤ì œ ìˆœì„œì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.```\n\n\n\n```js\nt.is_contiguous()\n# True\n\nnew_t.is_contiguous()\n# False\n```\n\nì´ëŠ” ì—°ì†ë˜ì§€ ì•ŠëŠ” ìš”ì†Œì— ì—°ì†ì ìœ¼ë¡œ ì•¡ì„¸ìŠ¤í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤ (ì‹¤ì œ í…ì„œ ìš”ì†ŒëŠ” ë©”ëª¨ë¦¬ ìƒì—ì„œ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤). ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```js\nnew_t_contiguous = new_t.contiguous()\n\nprint(new_t_contiguous.is_contiguous())\n# True\n```\n\në‚´ë¶€ ë°°ì—´ì„ ë¶„ì„í•˜ë©´ ì´ì œ ìˆœì„œê°€ ì‹¤ì œ í…ì„œ ìˆœì„œì™€ ì¼ì¹˜í•˜ì—¬ ë” ë‚˜ì€ ë©”ëª¨ë¦¬ ì•¡ì„¸ìŠ¤ íš¨ìœ¨ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:```\n\n\n\n```js\nprint(new_t)\n# [[[ 0,  1,  2,  3],\n#   [12, 13, 14, 15]],\n\n#  [[ 4,  5,  6,  7],\n#   [16, 17, 18, 19]],\n\n#  [[ 8,  9, 10, 11],\n#   [20, 21, 22, 23]]]\n\nprint_internal(new_t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n\nprint_internal(new_t_contiguous)\n# [ 0,  1,  2,  3, 12, 13, 14, 15,  4,  5,  6,  7, 16, 17, 18, 19,  8,  9, 10, 11, 20, 21, 22, 23]\n```\n\nì´ì œ ìš°ë¦¬ëŠ” í…ì„œê°€ ì–´ë–»ê²Œ ëª¨ë¸ë§ë˜ëŠ”ì§€ ì´í•´í–ˆìœ¼ë‹ˆ, ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±ì„ ì‹œì‘í•´ ë´…ì‹œë‹¤!\n\në‚´ê°€ ë§Œë“¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ë¦„ì€ Norchì…ë‹ˆë‹¤. PyTorchê°€ ì•„ë‹Œ (NOT PyTorch)ì„ ì˜ë¯¸í•˜ë©°, ì„±(Nogueira)ì„ ì•”ì‹œí•˜ê¸°ë„ í•©ë‹ˆë‹¤. ğŸ˜\n\nì²« ë²ˆì§¸ë¡œ ì•Œì•„ì•¼ í•  ê²ƒì€ PyTorchê°€ Pythonì„ í†µí•´ ì‚¬ìš©ë˜ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œëŠ” C/C++ë¡œ ì‹¤í–‰ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë¨¼ì € ë‚´ë¶€ C/C++ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.\n\n\n\n\në¨¼ì € í…ì„œë¥¼ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” êµ¬ì¡°ì²´ë¡œ ì •ì˜í•˜ê³  ì´ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```js\n//norch/csrc/tensor.cpp\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\ntypedef struct {\n    float* data;\n    int* strides;\n    int* shape;\n    int ndim;\n    int size;\n    char* device;\n} Tensor;\n\nTensor* create_tensor(float* data, int* shape, int ndim) {\n    \n    Tensor* tensor = (Tensor*)malloc(sizeof(Tensor));\n    if (tensor == NULL) {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(1);\n    }\n    tensor->data = data;\n    tensor->shape = shape;\n    tensor->ndim = ndim;\n\n    tensor->size = 1;\n    for (int i = 0; i < ndim; i++) {\n        tensor->size *= shape[i];\n    }\n\n    tensor->strides = (int*)malloc(ndim * sizeof(int));\n    if (tensor->strides == NULL) {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(1);\n    }\n    int stride = 1;\n    for (int i = ndim - 1; i >= 0; i--) {\n        tensor->strides[i] = stride;\n        stride *= shape[i];\n    }\n    \n    return tensor;\n}\n```\n\nì¼ë¶€ ìš”ì†Œì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ì„œëŠ” ì•ì„œ ë°°ì› ë˜ ìŠ¤íŠ¸ë¼ì´ë“œ(strides)ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```js\n//norch/csrc/tensor.cpp\n\nfloat get_item(Tensor* tensor, int* indices) {\n    int index = 0;\n    for (int i = 0; i < tensor->ndim; i++) {\n        index += indices[i] * tensor->strides[i];\n    }\n\n    float result;\n    result = tensor->data[index];\n\n    return result;\n}\n```\n\n\n\nì´ì œ í…ì„œ ì‘ì—…ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ ë³´ì—¬ë“œë¦¬ê² ê³ , ì´ ê¸€ ëì— ë§í¬ëœ ì €ì¥ì†Œì—ì„œ ì™„ì „í•œ ë²„ì „ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```js\n//norch/csrc/cpu.cpp\n\nvoid add_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    for (int i = 0; i < tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] + tensor2->data[i];\n    }\n}\n\nvoid sub_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    for (int i = 0; i < tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] - tensor2->data[i];\n    }\n}\n\nvoid elementwise_mul_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    for (int i = 0; i < tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] * tensor2->data[i];\n    }\n}\n\nvoid assign_tensor_cpu(Tensor* tensor, float* result_data) {\n\n    for (int i = 0; i < tensor->size; i++) {\n        result_data[i] = tensor->data[i];\n    }\n}\n\n...\n```\n\nê·¸ ë‹¤ìŒì—, ì´ëŸ¬í•œ ì‘ì—…ë“¤ì„ í˜¸ì¶œí•  í…ì„œ ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```js\n//norch/csrc/tensor.cpp\n\nTensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {\n    if (tensor1->ndim != tensor2->ndim) {\n        fprintf(stderr, \"ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œëŠ” ë™ì¼í•œ ì°¨ì› ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ %d ì™€ %d\\n\", tensor1->ndim, tensor2->ndim);\n        exit(1);\n    }\n\n    int ndim = tensor1->ndim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i < ndim; i++) {\n        if (tensor1->shape[i] != tensor2->shape[i]) {\n            fprintf(stderr, \"ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œëŠ” ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤ %d ì™€ %d ì¸ë±ìŠ¤ %dì—ì„œ\\n\", tensor1->shape[i], tensor2->shape[i], i);\n            exit(1);\n        }\n        shape[i] = tensor1->shape[i];\n    }        \n    float* result_data = (float*)malloc(tensor1->size * sizeof(float));\n    if (result_data == NULL) {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(1);\n    }\n    add_tensor_cpu(tensor1, tensor2, result_data);\n    \n    return create_tensor(result_data, shape, ndim, device);\n}\n```\n\n\n\nì´ì „ì— ì–¸ê¸‰í•œ ëŒ€ë¡œ, í…ì„œ ì¬êµ¬ì„±ì€ ë‚´ë¶€ ë°ì´í„° ë°°ì—´ì„ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n```js\n//norch/csrc/tensor.cpp\n\nTensor* reshape_tensor(Tensor* tensor, int* new_shape, int new_ndim) {\n\n    int ndim = new_ndim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i < ndim; i++) {\n        shape[i] = new_shape[i];\n    }\n\n    // ìƒˆ ëª¨ì–‘ì˜ ìš”ì†Œ ì´ ìˆ˜ ê³„ì‚°\n    int size = 1;\n    for (int i = 0; i < new_ndim; i++) {\n        size *= shape[i];\n    }\n\n    // ì´ ìš”ì†Œ ìˆ˜ê°€ í˜„ì¬ í…ì„œì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸\n    if (size != tensor->size) {\n        fprintf(stderr, \"í…ì„œë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ëª¨ì–‘ì˜ ìš”ì†Œ ì´ ìˆ˜ê°€ í˜„ì¬ í…ì„œì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\\n\");\n        exit(1);\n    }\n\n    float* result_data = (float*)malloc(tensor->size * sizeof(float));\n    if (result_data == NULL) {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(1);\n    }\n    assign_tensor_cpu(tensor, result_data);\n    return create_tensor(result_data, shape, ndim, device);\n}\n```\n\nì´ì œ ì¼ë¶€ í…ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì§€ë§Œ, ëˆ„êµ¬ë‚˜ C/C++ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì´ì œ Python ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤!\n\nPythonì„ ì‚¬ìš©í•˜ì—¬ C/C++ ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤. Pybind11ê³¼ Cython ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì‹œì—ì„œëŠ” ctypesë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n\n\n\nì•„ë˜ëŠ” ctypesì˜ ê¸°ë³¸ì ì¸ êµ¬ì¡°ì…ë‹ˆë‹¤:\n\n```js\n//C ì½”ë“œ\n#include <stdio.h>\n\nfloat add_floats(float a, float b) {\n    return a + b;\n}\n```\n\n```js\n# ì»´íŒŒì¼\ngcc -shared -o add_floats.so -fPIC add_floats.c\n```\n\n```js\n# Python ì½”ë“œ\nimport ctypes\n\n# ê³µìœ  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\nlib = ctypes.CDLL('./add_floats.so')\n\n# í•¨ìˆ˜ì˜ ì¸ìì™€ ë°˜í™˜ ìœ í˜• ì •ì˜\nlib.add_floats.argtypes = [ctypes.c_float, ctypes.c_float]\nlib.add_floats.restype = ctypes.c_float\n\n# íŒŒì´ì¬ float ê°’ì„ c_float ìœ í˜•ìœ¼ë¡œ ë³€í™˜\na = ctypes.c_float(3.5)\nb = ctypes.c_float(2.2)\n\n# C í•¨ìˆ˜ í˜¸ì¶œ\nresult = lib.add_floats(a, b)\nprint(result)\n# 5.7\n```\n\n\n\në³´ì‹œë‹¤ì‹œí”¼ ë§¤ìš° ì§ê´€ì ì…ë‹ˆë‹¤. C/C++ ì½”ë“œë¥¼ ì»´íŒŒì¼í•œ í›„ Pythonì—ì„œ ctypesë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ ë° ë°˜í™˜ c_typesë¥¼ ì •ì˜í•˜ê³ , ë³€ìˆ˜ë¥¼ í•´ë‹¹ c_typesë¡œ ë³€í™˜í•˜ê³  í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ë°°ì—´(ë¶€ë™ ì†Œìˆ˜ì  ëª©ë¡)ê³¼ ê°™ì€ ë³´ë‹¤ ë³µì¡í•œ ìœ í˜•ì˜ ê²½ìš° í¬ì¸í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```js\ndata = [1.0, 2.0, 3.0]\ndata_ctype = (ctypes.c_float * len(data))(*data)\n\nlib.some_array_func.argstypes = [ctypes.POINTER(ctypes.c_float)]\n\n...\n\nlib.some_array_func(data)\n```\n\nê·¸ë¦¬ê³  êµ¬ì¡°ì²´ ìœ í˜•ì˜ ê²½ìš° ì§ì ‘ c_typeì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```js\nclass CustomType(ctypes.Structure):\n    _fields_ = [\n        ('field1', ctypes.POINTER(ctypes.c_float)),\n        ('field2', ctypes.POINTER(ctypes.c_int)),\n        ('field3', ctypes.c_int),\n    ]\n\n# ctypes.POINTER(CustomType)ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n```\n\n\n\nê°„ë‹¨íˆ ì„¤ëª…í•˜ê³ , í…ì„œ C/C++ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìœ„í•œ Python ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤!\n\n```js\n# norch/tensor.py\n\nimport ctypes\n\nclass CTensor(ctypes.Structure):\n    _fields_ = [\n        ('data', ctypes.POINTER(ctypes.c_float)),\n        ('strides', ctypes.POINTER(ctypes.c_int)),\n        ('shape', ctypes.POINTER(ctypes.c_int)),\n        ('ndim', ctypes.c_int),\n        ('size', ctypes.c_int),\n    ]\n\nclass Tensor:\n    os.path.abspath(os.curdir)\n    _C = ctypes.CDLL(\"COMPILED_LIB.so\")\n\n    def __init__(self):\n        \n        data, shape = self.flatten(data)\n        self.data_ctype = (ctypes.c_float * len(data))(*data)\n        self.shape_ctype = (ctypes.c_int * len(shape))(*shape)\n        self.ndim_ctype = ctypes.c_int(len(shape))\n       \n        self.shape = shape\n        self.ndim = len(shape)\n\n        Tensor._C.create_tensor.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n        Tensor._C.create_tensor.restype = ctypes.POINTER(CTensor)\n\n        self.tensor = Tensor._C.create_tensor(\n            self.data_ctype,\n            self.shape_ctype,\n            self.ndim_ctype,\n        )\n        \n    def flatten(self, nested_list):\n        \"\"\"\n        This method simply convert a list type tensor to a flatten tensor with its shape\n        \n        Example:\n        \n        Arguments:  \n            nested_list: [[1, 2, 3], [-5, 2, 0]]\n        Return:\n            flat_data: [1, 2, 3, -5, 2, 0]\n            shape: [2, 3]\n        \"\"\"\n        def flatten_recursively(nested_list):\n            flat_data = []\n            shape = []\n            if isinstance(nested_list, list):\n                for sublist in nested_list:\n                    inner_data, inner_shape = flatten_recursively(sublist)\n                    flat_data.extend(inner_data)\n                shape.append(len(nested_list))\n                shape.extend(inner_shape)\n            else:\n                flat_data.append(nested_list)\n            return flat_data, shape\n        \n        flat_data, shape = flatten_recursively(nested_list)\n        return flat_data, shape\n```\n\nì´ì œ Python í…ì„œ ì‘ì—…ì„ í¬í•¨í•˜ì—¬ C/C++ ì‘ì—…ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```js\n# norch/tensor.py\n\ndef __getitem__(self, indices):\n    \"\"\"\n    index í…ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ì„œì— ì•¡ì„¸ìŠ¤ tensor[i, j, k...]\n    \"\"\"\n\n    if len(indices) != self.ndim:\n        raise ValueError(\"ì¸ë±ìŠ¤ ìˆ˜ê°€ ì°¨ì› ìˆ˜ì™€ ì¼ì¹˜í•´ì•¼ í•¨\")\n    \n    Tensor._C.get_item.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int)]\n    Tensor._C.get_item.restype = ctypes.c_float\n                                       \n    indices = (ctypes.c_int * len(indices))(*indices)\n    value = Tensor._C.get_item(self.tensor, indices)  \n    \n    return value\n\ndef reshape(self, new_shape):\n    \"\"\"\n    í…ì„œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤\n    result = tensor.reshape([1,2])\n    \"\"\"\n    new_shape_ctype = (ctypes.c_int * len(new_shape))(*new_shape)\n    new_ndim_ctype = ctypes.c_int(len(new_shape))\n    \n    Tensor._C.reshape_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n    Tensor._C.reshape_tensor.restype = ctypes.POINTER(CTensor)\n    result_tensor_ptr = Tensor._C.reshape_tensor(self.tensor, new_shape_ctype, new_ndim_ctype)   \n\n    result_data = Tensor()\n    result_data.tensor = result_tensor_ptr\n    result_data.shape = new_shape.copy()\n    result_data.ndim = len(new_shape)\n    result_data.device = self.device\n\n    return result_data\n\ndef __add__(self, other):\n    \"\"\"\n    í…ì„œë¥¼ ë”í•©ë‹ˆë‹¤\n    result = tensor1 + tensor2\n    \"\"\"\n  \n    if self.shape != other.shape:\n        raise ValueError(\"ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œë“¤ì€ ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•¨\")\n    \n    Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n    Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n\n    result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n\n    result_data = Tensor()\n    result_data.tensor = result_tensor_ptr\n    result_data.shape = self.shape.copy()\n    result_data.ndim = self.ndim\n    result_data.device = self.device\n\n    return result_data\n\n# ê¸°íƒ€ ì—°ì‚° í¬í•¨:\n# __str__\n# __sub__ (-)\n# __mul__ (*)\n# __matmul__ (@)\n# __pow__ (**)\n# __truediv__ (/)\n# log\n# ...\n```\n\n\n\nì—¬ê¸°ê¹Œì§€ ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ì´ì œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  í…ì„œ ì‘ì—…ì„ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ìƒê²¼ìŠµë‹ˆë‹¤!\n\n```js\nimport norch\n\ntensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]])\ntensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]])\n\nresult = tensor1 + tensor2\nprint(result[0, 0])\n# 4 \n```\n\n# #2 â€” GPU ì§€ì›\n\nìš°ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë§Œë“  í›„, ì´ì œ ìƒˆë¡œìš´ ìˆ˜ì¤€ìœ¼ë¡œ ëŒì–´ì˜¬ë¦´ ê²ƒì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ GPUë¡œ ì „ì†¡í•˜ê³  ìˆ˜í•™ ì—°ì‚°ì„ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ `.to(\"cuda\")`ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ì˜ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. CUDAê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê¸°ë³¸ ì§€ì‹ì´ ìˆì„ ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° ë‹¤ë¥¸ ê¸°ì‚¬ì¸ 'CUDA íŠœí† ë¦¬ì–¼'ì„ ì½ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦´ê²Œìš”. ğŸ˜Š\n\n\n\n...\n\nê¸‰í•œ ì‚¬ëŒë“¤ì„ ìœ„í•´, ê°„ë‹¨í•œ ì†Œê°œê°€ ì—¬ê¸° ìˆì–´ìš”:\n\nê¸°ë³¸ì ìœ¼ë¡œ, ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ì½”ë“œëŠ” CPU ë©”ëª¨ë¦¬ì—ì„œ ì‹¤í–‰ë˜ê³  ìˆì–´ìš”. í•˜ë‚˜ì˜ ì‘ì—…ì— ëŒ€í•´ì„œëŠ” CPUê°€ ë¹ ë¥´ì§€ë§Œ, GPUì˜ ì¥ì ì€ ë³‘ë ¬í™” ëŠ¥ë ¥ì— ìˆì–´ìš”. CPU ë””ìì¸ì€ ì—°ì‚°(ìŠ¤ë ˆë“œ)ì„ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ë„ë¡ ëª©í‘œë¥¼ í•œ ë°˜ë©´, GPU ë””ìì¸ì€ ìˆ˜ë°±ë§Œ ê°œì˜ ì—°ì‚°ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ë„ë¡ ëª©í‘œë¥¼ í•´ìš” (ê°œë³„ ìŠ¤ë ˆë“œì˜ ì„±ëŠ¥ì„ í¬ìƒí•˜ë©°).\n\nê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì´ ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë³‘ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´, ë°±ë§Œ ê°œì˜ ìš”ì†Œë¡œ êµ¬ì„±ëœ í…ì„œë¥¼ ì¶”ê°€í•  ë•Œ, ë°˜ë³µë¬¸ ë‚´ì—ì„œ ê° ìƒ‰ì¸ì˜ ìš”ì†Œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ëŒ€ì‹ , GPUë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêº¼ë²ˆì— ëª¨ë‘ë¥¼ ë³‘ë ¬ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆì–´ìš”. ì´ë¥¼ ìœ„í•´ NVIDIAì—ì„œ ê°œë°œí•œ ê°œë°œìë“¤ì´ GPU ì§€ì›ì„ ì†Œí”„íŠ¸ì›¨ì–´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” í”Œë«í¼ì¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”.\n\n\n\nê·¸ê±¸ í•˜ë ¤ë©´, íŠ¹ì • GPU ì‘ì—…(ì˜ˆ: CPU ë©”ëª¨ë¦¬ì—ì„œ GPU ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ë³µì‚¬)ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ê°„ë‹¨í•œ C/C++ ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤ ì¸ CUDA C/C++ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPUì—ì„œ GPUë¡œ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ê³  ë°°ì—´ì˜ ê° ìš”ì†Œë¥¼ ì¶”ê°€í•˜ëŠ” AddTwoArrays í•¨ìˆ˜(ì»¤ë„ì´ë¼ê³ ë„ í•¨)ë¥¼ Nê°œì˜ GPU ìŠ¤ë ˆë“œì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ëª‡ ê°€ì§€ CUDA C/C++ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n```c\n#include <stdio.h>\n\n// CPU ë²„ì „(ë¹„êµìš©)\nvoid AddTwoArrays_CPU(flaot A[], float B[], float C[]) {\n    for (int i = 0; i < N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n\n// ì»¤ë„ ì •ì˜\n__global__ void AddTwoArrays_GPU(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // ë°°ì—´ í¬ê¸°\n    float A[N], B[N], C[N]; // ë°°ì—´ A, B, C\n\n    ...\n\n    float *d_A, *d_B, *d_C; // ë°°ì—´ A, B, Cì˜ ì¥ì¹˜ í¬ì¸í„°\n\n    // ë°°ì—´ A, B, Cì— ëŒ€í•œ ì¥ì¹˜ì—ì„œì˜ ë©”ëª¨ë¦¬ í• ë‹¹\n    cudaMalloc((void **)&d_A, N * sizeof(float));\n    cudaMalloc((void **)&d_B, N * sizeof(float));\n    cudaMalloc((void **)&d_C, N * sizeof(float));\n\n    // í˜¸ìŠ¤íŠ¸ì—ì„œ ì¥ì¹˜ë¡œ ë°°ì—´ A ë° B ë³µì‚¬\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Nê°œì˜ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ë„ í˜¸ì¶œ\n    AddTwoArrays_GPU<<<1, N>>>(d_A, d_B, d_C);\n    \n    // ì¥ì¹˜ì—ì„œ í˜¸ìŠ¤íŠ¸ë¡œ ë²¡í„° C ë³µì‚¬\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n}\n```\n\nì£¼ëª©í•  ì ì€ ê° ìš”ì†Œ ìŒì„ ê°ê° ì¶”ê°€í•˜ëŠ” ëŒ€ì‹  ëª¨ë“  ë§ì…ˆ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë£¨í”„ ëª…ë ¹ì„ ì œê±°í•œ ê²ƒì…ë‹ˆë‹¤.\n\n\n\nê°„ë‹¨í•œ ì†Œê°œ ì´í›„ì—, í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆì–´ìš”.\n\nì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” CPUì—ì„œ GPUë¡œ í…ì„œ ë°ì´í„°ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n```js\n//norch/csrc/tensor.cpp\n\nvoid to_device(Tensor* tensor, char* target_device) {\n    if ((strcmp(target_device, \"cuda\") == 0) && (strcmp(tensor->device, \"cpu\") == 0)) {\n        cpu_to_cuda(tensor);\n    }\n\n    else if ((strcmp(target_device, \"cpu\") == 0) && (strcmp(tensor->device, \"cuda\") == 0)) {\n        cuda_to_cpu(tensor);\n    }\n}\n```\n\n```js\n//norch/csrc/cuda.cu\n\n__host__ void cpu_to_cuda(Tensor* tensor) {\n    \n    float* data_tmp;\n    cudaMalloc((void **)&data_tmp, tensor->size * sizeof(float));\n    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyHostToDevice);\n\n    tensor->data = data_tmp;\n\n    const char* device_str = \"cuda\";\n    tensor->device = (char*)malloc(strlen(device_str) + 1);\n    strcpy(tensor->device, device_str); \n\n    printf(\"í…ì„œê°€ ì„±ê³µì ìœ¼ë¡œ %së¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\", tensor->device);\n}\n\n__host__ void cuda_to_cpu(Tensor* tensor) {\n    float* data_tmp = (float*)malloc(tensor->size * sizeof(float));\n\n    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyDeviceToHost);\n    cudaFree(tensor->data);\n\n    tensor->data = data_tmp;\n\n    const char* device_str = \"cpu\";\n    tensor->device = (char*)malloc(strlen(device_str) + 1);\n    strcpy(tensor->device, device_str); \n\n    printf(\"í…ì„œê°€ ì„±ê³µì ìœ¼ë¡œ %së¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\", tensor->device);\n}\n```\n\n\n\níŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„ëœ ë˜í¼:\n\n```js\n# norch/tensor.py\n\ndef to(self, device):\n    self.device = device\n    self.device_ctype = self.device.encode('utf-8')\n  \n    Tensor._C.to_device.argtypes = [ctypes.POINTER(CTensor), ctypes.c_char_p]\n    Tensor._C.to_device.restype = None\n    Tensor._C.to_device(self.tensor, self.device_ctype)\n  \n    return self\n```\n\në‹¤ìŒìœ¼ë¡œ, ëª¨ë“  í…ì„œ ì—°ì‚°ì— ëŒ€í•´ GPU ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ì…ˆê³¼ ëº„ì…ˆì— ëŒ€í•œ ì˜ˆì œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤:\n\n```js\n//norch/csrc/cuda.cu\n\n#define THREADS_PER_BLOCK 128\n\n__global__ void add_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {\n    \n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        result_data[i] = data1[i] + data2[i];\n    }\n}\n\n__host__ void add_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n    add_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);\n\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    cudaDeviceSynchronize();\n}\n\n__global__ void sub_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {\n   \n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        result_data[i] = data1[i] - data2[i];\n    }\n}\n\n__host__ void sub_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n    sub_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);\n\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    cudaDeviceSynchronize();\n}\n\n...\n```\n\n\n\nê·¸ëŸ° ë‹¤ìŒ, í…ì„œ.cppì— ìƒˆë¡œìš´ í…ì„œ ì†ì„± char* deviceë¥¼ ì¶”ê°€í•˜ê³  ì‘ì—…ì„ ì‹¤í–‰í•  ìœ„ì¹˜(CPU ë˜ëŠ” GPU)ë¥¼ ì„ íƒí•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```js\n//norch/csrc/tensor.cpp\n\nTensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {\n    if (tensor1->ndim != tensor2->ndim) {\n        fprintf(stderr, \"ë§ì…ˆì„ ìœ„í•´ í…ì„œê°€ ë™ì¼í•œ ì°¨ì› ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ %d and %d\\n\", tensor1->ndim, tensor2->ndim);\n        exit(1);\n    }\n\n    if (strcmp(tensor1->device, tensor2->device) != 0) {\n        fprintf(stderr, \"í…ì„œëŠ” ë™ì¼í•œ ì¥ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤: %s and %s\\n\", tensor1->device, tensor2->device);\n        exit(1);\n    }\n\n    char* device = (char*)malloc(strlen(tensor1->device) + 1);\n    if (device != NULL) {\n        strcpy(device, tensor1->device);\n    } else {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(-1);\n    }\n    int ndim = tensor1->ndim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i < ndim; i++) {\n        if (tensor1->shape[i] != tensor2->shape[i]) {\n            fprintf(stderr, \"ë§ì…ˆì„ ìœ„í•´ í…ì„œë“¤ì€ ìƒ‰ì¸ %dì—ì„œ ë™ì¼í•œ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤ %d and %d\\n\", i, tensor1->shape[i], tensor2->shape[i]);\n            exit(1);\n        }\n        shape[i] = tensor1->shape[i];\n    }        \n\n    if (strcmp(tensor1->device, \"cuda\") == 0) {\n\n        float* result_data;\n        cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));\n        add_tensor_cuda(tensor1, tensor2, result_data);\n        return create_tensor(result_data, shape, ndim, device);\n    } \n    else {\n        float* result_data = (float*)malloc(tensor1->size * sizeof(float));\n        if (result_data == NULL) {\n            fprintf(stderr, \"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\");\n            exit(1);\n        }\n        add_tensor_cpu(tensor1, tensor2, result_data);\n        return create_tensor(result_data, shape, ndim, device);\n    }     \n}\n```\n\nì´ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ GPU ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤!\n\n```js\nimport norch\n\ntensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]]).to(\"cuda\")\ntensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]]).to(\"cuda\")\n\nresult = tensor1 + tensor2\n```\n\n\n\n# #3 â€” Automatic Differentiation (Autograd)\n\níŒŒì´í† ì¹˜ê°€ ì¸ê¸°ë¥¼ ì–»ê²Œ ëœ ì£¼ìš” ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” Autograd ëª¨ë“ˆ ë•Œë¬¸ì…ë‹ˆë‹¤. Autograd ëª¨ë“ˆì€ ìë™ ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤ (ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ê°™ì€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤). .backward()ë¼ëŠ” ë‹¨ì¼ ë©”ì„œë“œ í˜¸ì¶œë¡œ ì´ì „ í…ì„œ ì—°ì‚°ì—ì„œ ëª¨ë“  ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:\n\n```js\nx = torch.tensor([[1., 2, 3], [3., 2, 1]], requires_grad=True)\n# [[1,  2,  3],\n#  [3,  2., 1]]\n\ny = torch.tensor([[3., 2, 1], [1., 2, 3]], requires_grad=True)\n# [[3,  2, 1],\n#  [1,  2, 3]]\n\nL = ((x - y) ** 3).sum()\n\nL.backward()\n\n# xì™€ yì˜ ê¸°ìš¸ê¸°ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\nprint(x.grad)\n# [[12, 0, 12],\n#  [12, 0, 12]]\n\nprint(y.grad)\n# [[-12, 0, -12],\n#  [-12, 0, -12]]\n\n# zë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n# x = x - í•™ìŠµë¥  * x.grad\n# y = y - í•™ìŠµë¥  * y.grad\n```\n\në¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ì§€ ì´í•´í•˜ê¸° ìœ„í•´ ë™ì¼í•œ ì ˆì°¨ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•´ë³´ê² ìŠµë‹ˆë‹¤:\n\n\n\n<img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_10.png\" />\n\nìš°ì„  ê³„ì‚°í•´ ë´…ì‹œë‹¤:\n\n<img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_11.png\" />\n\nxê°€ í–‰ë ¬ì´ë¼ëŠ” ê²ƒì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê° ìš”ì†Œì— ëŒ€í•œ Lì˜ ë¯¸ë¶„ì„ ê°œë³„ì ìœ¼ë¡œ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. ê²Œë‹¤ê°€, Lì€ ëª¨ë“  ìš”ì†Œì— ëŒ€í•œ í•©ì´ì§€ë§Œ ê° ìš”ì†Œì— ëŒ€í•œ ë¯¸ë¶„ì—ì„œ ë‹¤ë¥¸ ìš”ì†Œë“¤ì€ ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•­ì„ ì–»ìŠµë‹ˆë‹¤:\n\n\n\n\n![ì´ë¯¸ì§€](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_12.png)\n\nê° í•­ì— ëŒ€í•´ ì—°ì‡„ ë²•ì¹™ì„ ì ìš©í•˜ì—¬ ì™¸ë¶€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ê³  ë‚´ë¶€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•œ ê°’ì„ ê³±í•©ë‹ˆë‹¤:\n\n![ì´ë¯¸ì§€](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_13.png)\n\nWhere:\n\n\n\n\në§ˆì¹¨ë‚´:\n\n![ì´ë¯¸ì§€](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_14.png)\n\nê·¸ëŸ¬ë¯€ë¡œ, xì— ê´€í•œ Lì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ëŠ” ìµœì¢… ë°©ì •ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n\n\nì•„ë˜ëŠ” Markdown í˜•ì‹ìœ¼ë¡œ ë³€ê²½ëœ ë‚´ìš©ì…ë‹ˆë‹¤.\n\n\n![Image 1](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_16.png)\n\nSubstituting the values into the equation:\n\n![Image 2](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_17.png)\n\nCalculating the result, we get the same values we obtained with PyTorch:\n\n\n\n\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_18.png)\n\nNow, letâ€™s analyze what we just did:\n\nBasically, we observed all the operations involved in reverse order: a summation, a power of 3, and a subtraction. Then, we applied the chain rule, calculating the derivative of each operation and recursively calculated the derivative for the next operation. So, first we need an implementation of the derivative for different math operations:\n\nFor addition:\n\n\n\n\n\n![Image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_19.png)\n\n```js\n# norch/autograd/functions.py\n\nclass AddBackward:\n    def __init__(self, x, y):\n        self.input = [x, y]\n\n    def backward(self, gradient):\n        return [gradient, gradient]\n```\n\nFor sin:\n\n![Image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_20.png)\n\n\n\n\n```js\n# norch/autograd/functions.py\n\nclass SinBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        return [x.cos() * gradient]\n```\n\nì½”ì‚¬ì¸ì— ëŒ€í•´:\n\n![2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21.png)\n\n```js\n# norch/autograd/functions.py\n\nclass CosBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        return [- x.sin() * gradient]\n```\n\n\n\nìš”ì†Œë³„ ê³±ì…ˆì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”:\n\n![element-wise multiplication](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_22.png)\n\n```python\n# norch/autograd/functions.py\n\nclass ElementwiseMulBackward:\n    def __init__(self, x, y):\n        self.input = [x, y]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        y = self.input[1]\n        return [y * gradient, x * gradient]\n```\n\ní•©ì‚°ì— ëŒ€í•´ì„œ:\n\n\n\n\n# norch/autograd/functions.py\n\n```python\nclass SumBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        # sum í•¨ìˆ˜ëŠ” í…ì„œë¥¼ ìŠ¤ì¹¼ë¼ë¡œ ì¤„ì´ë¯€ë¡œ ê¸°ìš¸ê¸°ë¥¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ ë¸Œë¡œë“œìºìŠ¤íŠ¸ë©ë‹ˆë‹¤.\n        return [float(gradient.tensor.contents.data[0]) * self.input[0].ones_like()]\n```\n\në‹¤ë¥¸ ì—°ì‚°ì„ ì‚´í´ë³¼ ìˆ˜ ìˆëŠ” GitHub ì €ì¥ì†Œ ë§í¬ë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì´ì œ ê° ì‘ì—…ì— ëŒ€í•œ ë„í•¨ìˆ˜ ì‹ì„ ê°€ì¡Œìœ¼ë‹ˆ, ì¬ê·€ì ìœ¼ë¡œ ì—­ì „íŒŒ ì²´ì¸ ê·œì¹™ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ì„œì— requires_grad ì¸ìë¥¼ ì„¤ì •í•˜ì—¬ ì´ í…ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ì €ì¥í•˜ë ¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Trueì´ë©´ ê° í…ì„œ ì‘ì—…ì˜ ê¸°ìš¸ê¸°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\n\n```python\n# norch/tensor.py\n\ndef __add__(self, other):\n\n  if self.shape != other.shape:\n      raise ValueError(\"ë§ì…ˆì„ ìœ„í•´ í…ì„œëŠ” ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n  \n  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n  \n  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n  \n  result_data = Tensor()\n  result_data.tensor = result_tensor_ptr\n  result_data.shape = self.shape.copy()\n  result_data.ndim = self.ndim\n  result_data.device = self.device\n  \n  result_data.requires_grad = self.requires_grad or other.requires_grad\n  if result_data.requires_grad:\n      result_data.grad_fn = AddBackward(self, other)\n```\n\n\n\nê·¸ëŸ¼, `.backward()` ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”:\n\n```python\n# norch/tensor.py\n\ndef backward(self, gradient=None):\n    if not self.requires_grad:\n        return\n    \n    if gradient is None:\n        if self.shape == [1]:\n            gradient = Tensor([1]) # dx/dx = 1 case\n        else:\n            raise RuntimeError(\"Gradient argument must be specified for non-scalar tensors.\")\n\n    if self.grad is None:\n        self.grad = gradient\n\n    else:\n        self.grad += gradient\n\n    if self.grad_fn is not None: # not a leaf\n        grads = self.grad_fn.backward(gradient) # call the operation backward\n        for tensor, grad in zip(self.grad_fn.input, grads):\n            if isinstance(tensor, Tensor):\n                tensor.backward(grad) # recursively call the backward again for the gradient expression (chain rule)\n```\n\në§ˆì§€ë§‰ìœ¼ë¡œ, í…ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì œë¡œí™”í•˜ëŠ” `.zero_grad()`ì™€ í…ì„œì˜ ì˜¤í† ê·¸ë˜ë“œ íˆìŠ¤í† ë¦¬ë¥¼ ì œê±°í•˜ëŠ” `.detach()`ë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”:\n\n```python\n# norch/tensor.py\n\ndef zero_grad(self):\n    self.grad = None\n\ndef detach(self):\n    self.grad = None\n    self.grad_fn = None\n```\n\n\n\nì¶•í•˜í•©ë‹ˆë‹¤! GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ì´ ìˆëŠ” ì™„ì „í•œ í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§Œë“œì…¨êµ°ìš”! ì´ì œ nn ë° optim ëª¨ë“ˆì„ ë§Œë“¤ì–´ ëª‡ ê°€ì§€ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ ë” ì‰½ê²Œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## #4 â€” nn ë° optim ëª¨ë“ˆ\n\nnnì€ ì‹ ê²½ë§ ë° ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ëª¨ë“ˆì´ë©°, optimì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì„ ì¬í˜„í•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” Parameterë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ParameterëŠ” ê°„ë‹¨íˆ ë§í•´ í•­ìƒ Trueë¡œ ì„¤ì •ëœ requires_grad ì†ì„±ì„ ê°–ëŠ” í›ˆë ¨ ê°€ëŠ¥í•œ í…ì„œë¡œ, ì¼ë¶€ ì„ì˜ì˜ ì´ˆê¸°í™” ê¸°ë²•ì„ ì‚¬ìš©í•´ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n\n```js\n# norch/nn/parameter.py\n\nfrom norch.tensor import Tensor\nfrom norch.utils import utils\nimport random\n\nclass Parameter(Tensor):\n    \"\"\"\n    A parameter is a trainable tensor.\n    \"\"\"\n    def __init__(self, shape):\n        data = utils.generate_random_list(shape=shape)\n        super().__init__(data, requires_grad=True)\n```\n\n\n\n```js\n# norch/utisl/utils.py\n\ndef generate_random_list(shape):\n    \"\"\"\n    ëœë¤í•œ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ 'shape' í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n    [4, 2] --> [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]\n    \"\"\"\n    if len(shape) == 0:\n        return []\n    else:\n        inner_shape = shape[1:]\n        if len(inner_shape) == 0:\n            return [random.uniform(-1, 1) for _ in range(shape[0])]\n        else:\n            return [generate_random_list(inner_shape) for _ in range(shape[0])]\n```\n\níŒŒë¼ë¯¸í„°ë¥¼ í™œìš©í•˜ë©´ ëª¨ë“ˆì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```js\n# norch/nn/module.py\n\nfrom .parameter import Parameter\nfrom collections import OrderedDict\nfrom abc import ABC\nimport inspect\n\nclass Module(ABC):\n    \"\"\"\n    ëª¨ë“ˆì„ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤\n    \"\"\"\n    def __init__(self):\n        self._modules = OrderedDict()\n        self._params = OrderedDict()\n        self._grads = OrderedDict()\n        self.training = True\n\n    def forward(self, *inputs, **kwargs):\n        raise NotImplementedError\n\n    def __call__(self, *inputs, **kwargs):\n        return self.forward(*inputs, **kwargs)\n\n    def train(self):\n        self.training = True\n        for param in self.parameters():\n            param.requires_grad = True\n\n    def eval(self):\n        self.training = False\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def parameters(self):\n        for name, value in inspect.getmembers(self):\n            if isinstance(value, Parameter):\n                yield self, name, value\n            elif isinstance(value, Module):\n                yield from value.parameters()\n\n    def modules(self):\n        yield from self._modules.values()\n\n    def gradients(self):\n        for module in self.modules():\n            yield module._grads\n\n    def zero_grad(self):\n        for _, _, parameter in self.parameters():\n            parameter.zero_grad()\n\n    def to(self, device):\n        for _, _, parameter in self.parameters():\n            parameter.to(device)\n\n        return self\n    \n    def inner_repr(self):\n        return \"\"\n\n    def __repr__(self):\n        string = f\"{self.get_name()}(\"\n        tab = \"   \"\n        modules = self._modules\n        if modules == {}:\n            string += f'\\n{tab}(parameters): {self.inner_repr()}'\n        else:\n            for key, module in modules.items():\n                string += f\"\\n{tab}({key}): {module.get_name()}({module.inner_repr()})\"\n        return f'{string}\\n)'\n    \n    def get_name(self):\n        return self.__class__.__name__\n    \n    def __setattr__(self, key, value):\n        self.__dict__[key] = value\n\n        if isinstance(value, Module):\n            self._modules[key] = value\n        elif isinstance(value, Parameter):\n            self._params[key] = value\n```\n\nì˜ˆë¥¼ ë“¤ì–´, nn.Moduleì„ ìƒì†í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆì„ ë§Œë“¤ê±°ë‚˜, ì´ì „ì— ìƒì„±ëœ ëª¨ë“ˆ ì¤‘ í•˜ë‚˜ì¸ ì„ í˜• ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ y = Wx + b ì‘ì—…ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\n```js\n# norch/nn/modules/linear.py\n\nfrom ..module import Module\nfrom ..parameter import Parameter\n\nclass Linear(Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.weight = Parameter(shape=[self.output_dim, self.input_dim])\n        self.bias = Parameter(shape=[self.output_dim, 1])\n\n    def forward(self, x):\n        z = self.weight @ x + self.bias\n        return z\n\n    def inner_repr(self):\n        return f\"input_dim={self.input_dim}, output_dim={self.output_dim}, \" \\\n               f\"bias={True if self.bias is not None else False}\"\n```\n\nì´ì œ ëª‡ ê°€ì§€ ì†ì‹¤ ë° í™œì„±í™” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í‰ê·  ì œê³± ì˜¤ì°¨ ì†ì‹¤ ë° ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜:\n\n```js\n# norch/nn/loss.py\n\nfrom .module import Module\n \nclass MSELoss(Module):\n    def __init__(self):\n      pass\n\n    def forward(self, predictions, labels):\n        assert labels.shape == predictions.shape, \\\n            \"Labels and predictions shape does not match: {} and {}\".format(labels.shape, predictions.shape)\n        \n        return ((predictions - labels) ** 2).sum() / predictions.numel\n\n    def __call__(self, *inputs):\n        return self.forward(*inputs)\n```\n\n```js\n# norch/nn/activation.py\n\nfrom .module import Module\nimport math\n\nclass Sigmoid(Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 1.0 / (1.0 + (math.e) ** (-x)) \n```\n\n\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤. ì˜ˆì‹œë¡œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent) ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤:\n\n```js\n# norch/optim/optimizer.py\n\nfrom abc import ABC\nfrom norch.tensor import Tensor\n\nclass Optimizer(ABC):\n    \"\"\"\n    ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤\n    \"\"\"\n\n    def __init__(self, parameters):\n        if isinstance(parameters, Tensor):\n            raise TypeError(\"parametersëŠ” ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ì´ì–´ì•¼ í•˜ì§€ë§Œ {} íƒ€ì…ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤\".format(type(parameters)))\n        elif isinstance(parameters, dict):\n            parameters = parameters.values()\n\n        self.parameters = list(parameters)\n\n    def step(self):\n        raise NotImplementedError\n    \n    def zero_grad(self):\n        for module, name, parameter in self.parameters:\n            parameter.zero_grad()\n\n\nclass SGD(Optimizer):\n    def __init__(self, parameters, lr=1e-1, momentum=0):\n        super().__init__(parameters)\n        self.lr = lr\n        self.momentum = momentum\n        self._cache = {'velocity': [p.zeros_like() for (_, _, p) in self.parameters]}\n\n    def step(self):\n        for i, (module, name, _) in enumerate(self.parameters):\n            parameter = getattr(module, name)\n\n            velocity = self._cache['velocity'][i]\n\n            velocity = self.momentum * velocity - self.lr * parameter.grad\n\n            updated_parameter = parameter + velocity\n\n            setattr(module, name, updated_parameter)\n\n            self._cache['velocity'][i] = velocity\n\n            parameter.detach()\n            velocity.detach()\n```\n\nê·¸ë¦¬ê³  ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤! ì´ì œ ìš°ë¦¬ë§Œì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ë§Œë“¤ì—ˆì–´ìš”! ğŸ¥³\n\nì´ì œ í•™ìŠµì„ ì‹œì‘í•´ë´…ì‹œë‹¤:\n\n\n\n```js\nimport norch\nimport norch.nn as nn\nimport norch.optim as optim\nimport random\nimport math\n\nrandom.seed(1)\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(1, 10)\n        self.sigmoid = nn.Sigmoid()\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        \n        return out\n\ndevice = \"cuda\"\nepochs = 10\n\nmodel = MyModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\nloss_list = []\n\nx_values = [0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ,\n        4.4,  4.8,  5.2,  5.6,  6. ,  6.4,  6.8,  7.2,  7.6,  8. ,  8.4,\n        8.8,  9.2,  9.6, 10. , 10.4, 10.8, 11.2, 11.6, 12. , 12.4, 12.8,\n       13.2, 13.6, 14. , 14.4, 14.8, 15.2, 15.6, 16. , 16.4, 16.8, 17.2,\n       17.6, 18. , 18.4, 18.8, 19.2, 19.6, 20.]\n\ny_true = []\nfor x in x_values:\n    y_true.append(math.pow(math.sin(x), 2))\n\n\nfor epoch in range(epochs):\n    for x, target in zip(x_values, y_true):\n        x = norch.Tensor([[x]]).T\n        target = norch.Tensor([[target]]).T\n\n        x = x.to(device)\n        target = target.to(device)\n\n        outputs = model(x)\n        loss = criterion(outputs, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}')\n    loss_list.append(loss[0])\n\n# Epoch [1/10], Loss: 1.7035\n# Epoch [2/10], Loss: 0.7193\n# Epoch [3/10], Loss: 0.3068\n# Epoch [4/10], Loss: 0.1742\n# Epoch [5/10], Loss: 0.1342\n# Epoch [6/10], Loss: 0.1232\n# Epoch [7/10], Loss: 0.1220\n# Epoch [8/10], Loss: 0.1241\n# Epoch [9/10], Loss: 0.1270\n# Epoch [10/10], Loss: 0.1297\n```\n\n<img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_23.png\" />\n\nì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±ë˜ê³  ì‚¬ìš©ì ì •ì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤!\n\nì „ì²´ ì½”ë“œëŠ” ì—¬ê¸°ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n# ê²°ë¡ \n\nì´ ê²Œì‹œë¬¼ì—ì„œëŠ” í…ì„œì™€ ê°™ì€ ê¸°ë³¸ ê°œë…, ì–´ë–»ê²Œ ëª¨ë¸ë§ë˜ëŠ”ì§€, CUDA ë° Autogradì™€ ê°™ì€ ê³ ê¸‰ ì£¼ì œ ë“±ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ì´ ê°€ëŠ¥í•œ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ ê²Œì‹œë¬¼ì´ ì—¬ëŸ¬ë¶„ì´ PyTorchê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê°„ëµíˆ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.\n\nì•ìœ¼ë¡œì˜ ê²Œì‹œë¬¼ì—ì„œëŠ” ë¶„ì‚° í›ˆë ¨(ë‹¤ì¤‘ ë…¸ë“œ/ë‹¤ì¤‘ GPU) ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ê°™ì€ ê³ ê¸‰ ì£¼ì œë¥¼ ë‹¤ë£¨ë ¤ê³  í•  ê²ƒì…ë‹ˆë‹¤. ì˜ê²¬ì´ ìˆê±°ë‚˜ ë‹¤ìŒì— ì–´ë–¤ ë‚´ìš©ì„ ë‹¤ë£¨ê¸¸ ì›í•˜ì‹œëŠ”ì§€ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”! ì½ì–´ ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š\n\në˜í•œ ìµœì‹  ê¸°ì‚¬ë¥¼ ë°›ì•„ë³´ê¸° ìœ„í•´ ì—¬ê¸°ì™€ ì œ LinkedIn í”„ë¡œí•„ì—ì„œ íŒ”ë¡œìš°í•´ ì£¼ì„¸ìš”!\n\n\n\n# ì°¸ê³  ìë£Œ\n\n- [PyNorch](https://github.com) - ì´ í”„ë¡œì íŠ¸ì˜ GitHub ì €ì¥ì†Œ \n- [CUDA íŠœí† ë¦¬ì–¼](https://www.example.com/tutorial-cuda) - CUDA ì‘ë™ ë°©ì‹ì— ëŒ€í•œ ê°„ë‹¨í•œ ì†Œê°œ\n- [PyTorch](https://pytorch.org/docs) - PyTorch ë¬¸ì„œ\n\n\n\n# MartinLwx's ë¸”ë¡œê·¸ - ìŠ¤íŠ¸ë¼ì´ë“œì— ê´€í•œ íŠœí† ë¦¬ì–¼.\n\n# ìŠ¤íŠ¸ë¼ì´ë“œ íŠœí† ë¦¬ì–¼ - ìŠ¤íŠ¸ë¼ì´ë“œì— ê´€í•œ ë˜ ë‹¤ë¥¸ íŠœí† ë¦¬ì–¼.\n\n# PyTorch ë‚´ë¶€ êµ¬ì¡° - PyTorch êµ¬ì¡°ì— ëŒ€í•œ ê°€ì´ë“œ.\n\n# ë„¤ì¸  - NumPyë¥¼ ì‚¬ìš©í•œ PyTorch ì¬êµ¬í˜„.\n\n\n\nMarkdownìœ¼ë¡œ í‘œ íƒœê·¸ë¥¼ ë³€ê²½í•˜ì‹­ì‹œì˜¤.","ogImage":{"url":"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png"},"coverImage":"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png","tag":["Tech"],"readingTime":40},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h2>C/C++, CUDA, ë° Pythonì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ ìœ ì˜ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í•´ ë³´ì„¸ìš”. GPU ì§€ì›ê³¼ ìë™ ë¯¸ë¶„ì„ ì œê³µí•©ë‹ˆë‹¤</h2>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png\" alt=\"image\"></p>\n<h1>ì†Œê°œ</h1>\n<p>ì—¬ëŸ¬ í•´ ë™ì•ˆ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•´ ì™”ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ê·¸ ë¬¸ë²•ê³¼ ê·œì¹™ì„ ìµíˆê³ ë„, ì œ ê¶ê¸ˆì¦ì„ ìê·¹í•˜ë˜ ê²ƒì´ ìˆì—ˆìŠµë‹ˆë‹¤: ì´ëŸ¬í•œ ì‘ì—… ì¤‘ì— ë‚´ë¶€ì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ” ê±¸ê¹Œìš”? ì´ ëª¨ë“  ê²ƒì´ ì–´ë–»ê²Œ ì‘ë™í• ê¹Œìš”?</p>\n<p>ì—¬ê¸°ê¹Œì§€ ì˜¤ì…¨ë‹¤ë©´, ì•„ë§ˆë„ ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ ê°€ì§€ê³  ê³„ì‹¤ ê²ƒì…ë‹ˆë‹¤. íŒŒì´í† ì¹˜(PyTorch)ì—ì„œ ëª¨ë¸ì„ ìƒì„±í•˜ê³  í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ë¬¼ì–´ë³¸ë‹¤ë©´ ì•„ë§ˆë„ ì•„ë˜ ì½”ë“œì™€ ë¹„ìŠ·í•œ ê²ƒì„ ìƒê°í•´ë³¼ ê²ƒì…ë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span> torch.<span class=\"hljs-property\">nn</span> <span class=\"hljs-keyword\">as</span> nn\n<span class=\"hljs-keyword\">import</span> torch.<span class=\"hljs-property\">optim</span> <span class=\"hljs-keyword\">as</span> optim\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyModel</span>(nn.<span class=\"hljs-property\">Module</span>):\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n        <span class=\"hljs-variable language_\">super</span>(<span class=\"hljs-title class_\">MyModel</span>, self).<span class=\"hljs-title function_\">__init__</span>()\n        self.<span class=\"hljs-property\">fc1</span> = nn.<span class=\"hljs-title class_\">Linear</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">10</span>)\n        self.<span class=\"hljs-property\">sigmoid</span> = nn.<span class=\"hljs-title class_\">Sigmoid</span>()\n        self.<span class=\"hljs-property\">fc2</span> = nn.<span class=\"hljs-title class_\">Linear</span>(<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">1</span>)\n\n    def <span class=\"hljs-title function_\">forward</span>(self, x):\n        out = self.<span class=\"hljs-title function_\">fc1</span>(x)\n        out = self.<span class=\"hljs-title function_\">sigmoid</span>(out)\n        out = self.<span class=\"hljs-title function_\">fc2</span>(out)\n        \n        <span class=\"hljs-keyword\">return</span> out\n\n...\n\nmodel = <span class=\"hljs-title class_\">MyModel</span>().<span class=\"hljs-title function_\">to</span>(device)\ncriterion = nn.<span class=\"hljs-title class_\">MSELoss</span>()\noptimizer = optim.<span class=\"hljs-title function_\">SGD</span>(model.<span class=\"hljs-title function_\">parameters</span>(), lr=<span class=\"hljs-number\">0.001</span>)\n\n<span class=\"hljs-keyword\">for</span> epoch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(epochs):\n    <span class=\"hljs-keyword\">for</span> x, y <span class=\"hljs-keyword\">in</span> ...\n        \n        x = x.<span class=\"hljs-title function_\">to</span>(device)\n        y = y.<span class=\"hljs-title function_\">to</span>(device)\n\n        outputs = <span class=\"hljs-title function_\">model</span>(x)\n        loss = <span class=\"hljs-title function_\">criterion</span>(outputs, y)\n        \n        optimizer.<span class=\"hljs-title function_\">zero_grad</span>()\n        loss.<span class=\"hljs-title function_\">backward</span>()\n        optimizer.<span class=\"hljs-title function_\">step</span>()\n</code></pre>\n<p>í•˜ì§€ë§Œ ì´ë²ˆì— ì—­ì „íŒŒ(backward) ë‹¨ê³„ê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë¬¼ì–´ë³¸ë‹¤ë©´ ì–´ë–¨ê¹Œìš”? ë˜ëŠ” ì˜ˆë¥¼ ë“¤ì–´, í…ì„œë¥¼ ì¬êµ¬ì„±í•  ë•Œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ìš”? ë‚´ë¶€ì—ì„œ ë°ì´í„°ê°€ ì¬ë°°ì¹˜ë˜ë‚˜ìš”? ê·¸ëŸ° ì¼ì´ ì–´ë–»ê²Œ ì¼ì–´ë‚˜ë‚˜ìš”? ì™œ PyTorchëŠ” ë¹ ë¥¸ê°€ìš”? PyTorchê°€ GPU ì—°ì‚°ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ìš”? ì´ëŸ° ì§ˆë¬¸ë“¤ì´ í•­ìƒ ì €ë¥¼ í˜¸ê¸°ì‹¬ ê°€ë“í•˜ê²Œ ë§Œë“¤ì—ˆê³ , ì—¬ëŸ¬ë¶„ë„ ë§ˆì°¬ê°€ì§€ë¡œ í˜¸ê¸°ì‹¬ì´ ë“œì‹¤ ê²ƒì´ë¼ê³  ìƒìƒí•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ëŸ¬í•œ ê°œë…ì„ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ìŠ¤ìŠ¤ë¡œ í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì²˜ìŒë¶€í„° êµ¬ì¶•í•´ë³´ëŠ” ê²ƒì´ ë¬´ì—‡ë³´ë‹¤ ì¢‹ì„ê¹Œìš”? ì´ ê¸€ì—ì„œ ì—¬ëŸ¬ë¶„ì´ ë°°ìš°ê²Œ ë  ê²ƒì´ ë°”ë¡œ ê·¸ê²ë‹ˆë‹¤!</p>\n<h2>#1 â€” í…ì„œ</h2>\n<p>í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ì € ì•Œì•„ì•¼ í•  ê°œë…ì€ ë¬´ì—‡ì´ í…ì„œì¸ì§€ì— ëŒ€í•œ ëª…ë°±í•œ ê°œë…ì…ë‹ˆë‹¤.</p>\n<p>í…ì„œëŠ” ëª‡ ê°€ì§€ ìˆ«ìë¥¼ í¬í•¨í•˜ëŠ” nì°¨ì› ë°ì´í„° êµ¬ì¡°ì˜ ìˆ˜í•™ì  ê°œë…ì´ë¼ëŠ” ì§ê´€ì ì¸ ìƒê°ì„ ê°€ì§€ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—¬ê¸°ì„œëŠ” ì´ ë°ì´í„° êµ¬ì¡°ë¥¼ ê³„ì‚°ì  ê´€ì ì—ì„œ ì–´ë–»ê²Œ ëª¨ë¸ë§í• ì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. í…ì„œëŠ” ë°ì´í„° ìì²´ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ì–‘ì´ë‚˜ í…ì„œê°€ ìˆëŠ” ì¥ì¹˜(ì˜ˆ: CPU ë©”ëª¨ë¦¬, GPU ë©”ëª¨ë¦¬)ì™€ ê°™ì€ ì¸¡ë©´ì„ ì„¤ëª…í•˜ëŠ” ë©”íƒ€ë°ì´í„°ë¡œ êµ¬ì„±ëœë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<p>í…ì„œì˜ ë‚´ë¶€ë¥¼ ì´í•´í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ì¸ strideë¼ëŠ” ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ ë©”íƒ€ë°ì´í„°ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í…ì„œ ë°ì´í„° ì¬ë°°ì—´ì˜ ë‚´ë¶€ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì•½ê°„ ë” ì´ì— ëŒ€í•´ ë…¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.</p>\n<p>2-D í…ì„œì˜ ëª¨ì–‘ì´ [4, 8]ì¸ ê²½ìš°ë¥¼ ìƒìƒí•´ë³´ì„¸ìš”.</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_2.png\" alt=\"í…ì„œ\"></p>\n<p>í…ì„œì˜ ë°ì´í„°(ì¦‰, ë¶€ë™ ì†Œìˆ˜ì  ìˆ˜)ëŠ” ì‹¤ì œë¡œ ë©”ëª¨ë¦¬ì— 1ì°¨ì› ë°°ì—´ë¡œ ì €ì¥ë©ë‹ˆë‹¤.</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_3.png\" alt=\"ë°ì´í„°\"></p>\n<p>ê·¸ëŸ¬ë©´ ì´ 1ì°¨ì› ë°°ì—´ì„ Nì°¨ì› í…ì„œë¡œ ë‚˜íƒ€ë‚´ë ¤ë©´ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>\n<p>4í–‰ 8ì—´ì˜ í–‰ë ¬ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ í–‰ë ¬ì˜ ëª¨ë“  ì›ì†Œê°€ 1ì°¨ì› ë°°ì—´ì˜ í–‰ì— ì˜í•´ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•  ë•Œ, ìœ„ì¹˜ [2, 3]ì˜ ê°’ì„ ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´ 2í–‰(ê° í–‰ì— 8ê°œì˜ ìš”ì†Œ)ì„ íš¡ë‹¨í•´ì•¼ í•˜ë©° ì¶”ê°€ë¡œ 3ê°œì˜ ìœ„ì¹˜ë¥¼ ì§€ë‚˜ì•¼ í•©ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ë©´ 1ì°¨ì› ë°°ì—´ì—ì„œ 3 + 2 * 8 ìš”ì†Œë¥¼ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.</p>\n<p>ë”°ë¼ì„œ, '8'ì€ ë‘ ë²ˆì§¸ ì°¨ì›ì˜ ìŠ¤íŠ¸ë¼ì´ë“œì…ë‹ˆë‹¤. ì´ ê²½ìš°, ë°°ì—´ì—ì„œ ë‹¤ë¥¸ ìœ„ì¹˜ë¡œ \"ì í”„\"í•˜ê¸° ìœ„í•´ ëª‡ ê°œì˜ ìš”ì†Œë¥¼ íš¡ë‹¨í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´ì…ë‹ˆë‹¤.</p>\n<p>ë”°ë¼ì„œ, ëª¨ì–‘ì´ [shape_0, shape_1]ì¸ 2ì°¨ì› í…ì„œì˜ ìš”ì†Œ [i, j]ì— ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ j + i * shape_1 ìœ„ì¹˜ì— ìˆëŠ” ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•´ì•¼ í•©ë‹ˆë‹¤.</p>\n<p>ì´ì œ 3ì°¨ì› í…ì„œë¥¼ ìƒìƒí•´ë³´ê² ìŠµë‹ˆë‹¤:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_5.png\" alt=\"image\"></p>\n<p>ì´ 3ì°¨ì› í…ì„œë¥¼ í–‰ë ¬ì˜ ì‹œí€€ìŠ¤ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ [5, 4, 8] í…ì„œë¥¼ [4, 8] ëª¨ì–‘ì˜ 5ê°œ í–‰ë ¬ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<p>ì´ì œ [1, 3, 7] ìœ„ì¹˜ì— ìˆëŠ” ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•˜ê¸° ìœ„í•´ [4,8] í˜•íƒœì˜ í–‰ë ¬ì„ 1ê°œ ì™„ì „íˆ íš¡ë‹¨í•˜ê³ , [8] í˜•íƒœì˜ í–‰ì„ 2ê°œ, [1] í˜•íƒœì˜ ì—´ì„ 7ê°œ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ 1ì°¨ì› ë°°ì—´ì—ì„œ (1 * 4 * 8) + (2 * 8) + (7 * 1) ìœ„ì¹˜ë¥¼ íš¡ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_6.png\" alt=\"image\"></p>\n<p>ë”°ë¼ì„œ, [shape_0, shape_1, shape_2] ëª¨ì–‘ì˜ 3ì°¨ì› í…ì„œì—ì„œ 1ì°¨ì› ë°ì´í„° ë°°ì—´ì—ì„œ [i][j][k] ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_7.png\" alt=\"image\"></p>\n<p>ì´ shape_1 * shape_2ê°€ ì²« ë²ˆì§¸ ì°¨ì›ì˜ strideì´ê³ , shape_2ëŠ” ë‘ ë²ˆì§¸ ì°¨ì›ì˜ strideì´ë©° 1ì€ ì„¸ ë²ˆì§¸ ì°¨ì›ì˜ strideì…ë‹ˆë‹¤.</p>\n<p>ê·¸ëŸ° ë‹¤ìŒ, ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ì„œëŠ”:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_8.png\" alt=\"image\"></p>\n<p>ê° ì°¨ì›ì˜ strideëŠ” ë‹¤ìŒ ì°¨ì› í…ì„œ ëª¨ì–‘ì˜ ê³±ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>\n<p>ê·¸ëŸ° ë‹¤ìŒ stride[n-1] = 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.</p>\n<p>ìš°ë¦¬ì˜ í˜•íƒœì˜ í…ì„œ ì˜ˆì œ [5, 4, 8]ì—ì„œ strides = [4*8, 8, 1] = [32, 8, 1]ì¼ ê²ƒì…ë‹ˆë‹¤.</p>\n<p>ì—¬ëŸ¬ë¶„ë“¤ë„ ì§ì ‘ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆì–´ìš”:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> torch\n\ntorch.<span class=\"hljs-title function_\">rand</span>([<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">8</span>]).<span class=\"hljs-title function_\">stride</span>()\n#(<span class=\"hljs-number\">32</span>, <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">1</span>)\n</code></pre>\n<p>ì•Œê² ì–´ìš”, ê·¸ëŸ°ë° ì™œ ëª¨ì–‘ê³¼ ìŠ¤íŠ¸ë¼ì´ë“œê°€ í•„ìš”í•œ ê±´ê°€ìš”? Nì°¨ì› í…ì„œì˜ ìš”ì†Œì— ì ‘ê·¼í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ì´ ê°œë…ì€ í…ì„œ ë°°ì—´ì„ ë§¤ìš° ì‰½ê²Œ ì¡°ì‘í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆì–´ìš”.</p>\n<p>ì˜ˆë¥¼ ë“¤ì–´, í…ì„œë¥¼ ì¬êµ¬ì„±í•˜ë ¤ë©´ ìƒˆë¡œìš´ ëª¨ì–‘ì„ ì„¤ì •í•˜ê³  ìƒˆë¡œìš´ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤! (ìƒˆë¡œìš´ ëª¨ì–‘ì€ ë™ì¼í•œ ìš”ì†Œ ìˆ˜ë¥¼ ë³´ì¥í•˜ë¯€ë¡œ)</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> torch\n\nt = torch.<span class=\"hljs-title function_\">rand</span>([<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">8</span>])\n\n<span class=\"hljs-title function_\">print</span>(t.<span class=\"hljs-property\">shape</span>)\n# [<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">8</span>]\n\n<span class=\"hljs-title function_\">print</span>(t.<span class=\"hljs-title function_\">stride</span>())\n# [<span class=\"hljs-number\">32</span>, <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">1</span>]\n\nnew_t = t.<span class=\"hljs-title function_\">reshape</span>([<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>])\n\n<span class=\"hljs-title function_\">print</span>(new_t.<span class=\"hljs-property\">shape</span>)\n# [<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>]\n\n<span class=\"hljs-title function_\">print</span>(new_t.<span class=\"hljs-title function_\">stride</span>())\n# [<span class=\"hljs-number\">40</span>, <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>]\n</code></pre>\n<p>í…ì„œ ë‚´ë¶€ì—ì„œëŠ” ì—¬ì „íˆ ë™ì¼í•œ 1ì°¨ì› ë°°ì—´ë¡œ ì €ì¥ë©ë‹ˆë‹¤. reshape ë©”ì„œë“œëŠ” ë°°ì—´ ë‚´ ìš”ì†Œì˜ ìˆœì„œë¥¼ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! ëŒ€ë‹¨í•˜ì§€ ì•Šë‚˜ìš”? ğŸ˜</p>\n<p>ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorchì—ì„œ ë‚´ë¶€ 1ì°¨ì› ë°°ì—´ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> ctypes\n\ndef <span class=\"hljs-title function_\">print_internal</span>(<span class=\"hljs-attr\">t</span>: torch.<span class=\"hljs-property\">Tensor</span>):\n    <span class=\"hljs-title function_\">print</span>(\n        torch.<span class=\"hljs-title function_\">frombuffer</span>(\n            ctypes.<span class=\"hljs-title function_\">string_at</span>(t.<span class=\"hljs-title function_\">data_ptr</span>(), t.<span class=\"hljs-title function_\">storage</span>().<span class=\"hljs-title function_\">nbytes</span>()), dtype=t.<span class=\"hljs-property\">dtype</span>\n        )\n    )\n\n<span class=\"hljs-title function_\">print_internal</span>(t)\n# [<span class=\"hljs-number\">0.0752</span>, <span class=\"hljs-number\">0.5898</span>, <span class=\"hljs-number\">0.3930</span>, <span class=\"hljs-number\">0.9577</span>, <span class=\"hljs-number\">0.2276</span>, <span class=\"hljs-number\">0.9786</span>, <span class=\"hljs-number\">0.1009</span>, <span class=\"hljs-number\">0.138</span>, ...\n\n<span class=\"hljs-title function_\">print_internal</span>(new_t)\n# [<span class=\"hljs-number\">0.0752</span>, <span class=\"hljs-number\">0.5898</span>, <span class=\"hljs-number\">0.3930</span>, <span class=\"hljs-number\">0.9577</span>, <span class=\"hljs-number\">0.2276</span>, <span class=\"hljs-number\">0.9786</span>, <span class=\"hljs-number\">0.1009</span>, <span class=\"hljs-number\">0.138</span>, ...\n</code></pre>\n<p>ì˜ˆë¥¼ ë“¤ì–´ ë‘ ì¶•ì„ ì „ì¹˜í•˜ë ¤ë©´ ë‚´ë¶€ì ìœ¼ë¡œ í•´ë‹¹ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ë‹¨ìˆœíˆ ë°”ê¾¸ì–´ ì£¼ë©´ ë©ë‹ˆë‹¤!</p>\n<pre><code class=\"hljs language-js\">t = torch.<span class=\"hljs-title function_\">arange</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">24</span>).<span class=\"hljs-title function_\">reshape</span>(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">4</span>)\n<span class=\"hljs-title function_\">print</span>(t)\n# [[[ <span class=\"hljs-number\">0</span>,  <span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>],\n#   [ <span class=\"hljs-number\">4</span>,  <span class=\"hljs-number\">5</span>,  <span class=\"hljs-number\">6</span>,  <span class=\"hljs-number\">7</span>],\n#   [ <span class=\"hljs-number\">8</span>,  <span class=\"hljs-number\">9</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>]],\n \n#  [[<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>],\n#   [<span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">17</span>, <span class=\"hljs-number\">18</span>, <span class=\"hljs-number\">19</span>],\n#   [<span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">23</span>]]]\n\n<span class=\"hljs-title function_\">print</span>(t.<span class=\"hljs-property\">shape</span>)\n# [<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">4</span>]\n\n<span class=\"hljs-title function_\">print</span>(t.<span class=\"hljs-title function_\">stride</span>())\n# [<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">1</span>]\n\nnew_t = t.<span class=\"hljs-title function_\">transpose</span>(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">1</span>)\n<span class=\"hljs-title function_\">print</span>(new_t)\n# [[[ <span class=\"hljs-number\">0</span>,  <span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>],\n#   [<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>]],\n\n#  [[ <span class=\"hljs-number\">4</span>,  <span class=\"hljs-number\">5</span>,  <span class=\"hljs-number\">6</span>,  <span class=\"hljs-number\">7</span>],\n#   [<span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">17</span>, <span class=\"hljs-number\">18</span>, <span class=\"hljs-number\">19</span>]],\n\n#  [[ <span class=\"hljs-number\">8</span>,  <span class=\"hljs-number\">9</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>],\n#   [<span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">23</span>]]]\n\n<span class=\"hljs-title function_\">print</span>(new_t.<span class=\"hljs-property\">shape</span>)\n# [<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">4</span>]\n\n<span class=\"hljs-title function_\">print</span>(new_t.<span class=\"hljs-title function_\">stride</span>())\n# [<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">1</span>]\n</code></pre>\n<p>ë‚´ë¶€ ë°°ì—´ì„ ì¶œë ¥í•˜ë©´ ë‘ ê°’ ëª¨ë‘ ë™ì¼í•©ë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">print_internal</span>(t)\n# [ <span class=\"hljs-number\">0</span>,  <span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>,  <span class=\"hljs-number\">4</span>,  <span class=\"hljs-number\">5</span>,  <span class=\"hljs-number\">6</span>,  <span class=\"hljs-number\">7</span>,  <span class=\"hljs-number\">8</span>,  <span class=\"hljs-number\">9</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>, <span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">17</span>, <span class=\"hljs-number\">18</span>, <span class=\"hljs-number\">19</span>, <span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">23</span>]\n\n<span class=\"hljs-title function_\">print_internal</span>(new_t)\n# [ <span class=\"hljs-number\">0</span>,  <span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>,  <span class=\"hljs-number\">4</span>,  <span class=\"hljs-number\">5</span>,  <span class=\"hljs-number\">6</span>,  <span class=\"hljs-number\">7</span>,  <span class=\"hljs-number\">8</span>,  <span class=\"hljs-number\">9</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>, <span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">17</span>, <span class=\"hljs-number\">18</span>, <span class=\"hljs-number\">19</span>, <span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">23</span>]\n</code></pre>\n<p>ê·¸ëŸ¬ë‚˜ new_tì˜ ìŠ¤íŠ¸ë¼ì´ë“œëŠ” ì´ì œ ìœ„ì˜ ì‹ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ê²ƒì€ í…ì„œê°€ ì´ì œ ì—°ì†ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°œìƒí•©ë‹ˆë‹¤. ì¦‰, ë‚´ë¶€ ë°°ì—´ì€ ë™ì¼í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ë‚´ì˜ ê°’ì˜ ìˆœì„œê°€ í…ì„œì˜ ì‹¤ì œ ìˆœì„œì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.```</p>\n<pre><code class=\"hljs language-js\">t.<span class=\"hljs-title function_\">is_contiguous</span>()\n# <span class=\"hljs-title class_\">True</span>\n\nnew_t.<span class=\"hljs-title function_\">is_contiguous</span>()\n# <span class=\"hljs-title class_\">False</span>\n</code></pre>\n<p>ì´ëŠ” ì—°ì†ë˜ì§€ ì•ŠëŠ” ìš”ì†Œì— ì—°ì†ì ìœ¼ë¡œ ì•¡ì„¸ìŠ¤í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤ (ì‹¤ì œ í…ì„œ ìš”ì†ŒëŠ” ë©”ëª¨ë¦¬ ìƒì—ì„œ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤). ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\">new_t_contiguous = new_t.<span class=\"hljs-title function_\">contiguous</span>()\n\n<span class=\"hljs-title function_\">print</span>(new_t_contiguous.<span class=\"hljs-title function_\">is_contiguous</span>())\n# <span class=\"hljs-title class_\">True</span>\n</code></pre>\n<p>ë‚´ë¶€ ë°°ì—´ì„ ë¶„ì„í•˜ë©´ ì´ì œ ìˆœì„œê°€ ì‹¤ì œ í…ì„œ ìˆœì„œì™€ ì¼ì¹˜í•˜ì—¬ ë” ë‚˜ì€ ë©”ëª¨ë¦¬ ì•¡ì„¸ìŠ¤ íš¨ìœ¨ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:```</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">print</span>(new_t)\n# [[[ <span class=\"hljs-number\">0</span>,  <span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>],\n#   [<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>]],\n\n#  [[ <span class=\"hljs-number\">4</span>,  <span class=\"hljs-number\">5</span>,  <span class=\"hljs-number\">6</span>,  <span class=\"hljs-number\">7</span>],\n#   [<span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">17</span>, <span class=\"hljs-number\">18</span>, <span class=\"hljs-number\">19</span>]],\n\n#  [[ <span class=\"hljs-number\">8</span>,  <span class=\"hljs-number\">9</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>],\n#   [<span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">23</span>]]]\n\n<span class=\"hljs-title function_\">print_internal</span>(new_t)\n# [ <span class=\"hljs-number\">0</span>,  <span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>,  <span class=\"hljs-number\">4</span>,  <span class=\"hljs-number\">5</span>,  <span class=\"hljs-number\">6</span>,  <span class=\"hljs-number\">7</span>,  <span class=\"hljs-number\">8</span>,  <span class=\"hljs-number\">9</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>, <span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">17</span>, <span class=\"hljs-number\">18</span>, <span class=\"hljs-number\">19</span>, <span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">23</span>]\n\n<span class=\"hljs-title function_\">print_internal</span>(new_t_contiguous)\n# [ <span class=\"hljs-number\">0</span>,  <span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">13</span>, <span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">15</span>,  <span class=\"hljs-number\">4</span>,  <span class=\"hljs-number\">5</span>,  <span class=\"hljs-number\">6</span>,  <span class=\"hljs-number\">7</span>, <span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">17</span>, <span class=\"hljs-number\">18</span>, <span class=\"hljs-number\">19</span>,  <span class=\"hljs-number\">8</span>,  <span class=\"hljs-number\">9</span>, <span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">11</span>, <span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">21</span>, <span class=\"hljs-number\">22</span>, <span class=\"hljs-number\">23</span>]\n</code></pre>\n<p>ì´ì œ ìš°ë¦¬ëŠ” í…ì„œê°€ ì–´ë–»ê²Œ ëª¨ë¸ë§ë˜ëŠ”ì§€ ì´í•´í–ˆìœ¼ë‹ˆ, ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±ì„ ì‹œì‘í•´ ë´…ì‹œë‹¤!</p>\n<p>ë‚´ê°€ ë§Œë“¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ë¦„ì€ Norchì…ë‹ˆë‹¤. PyTorchê°€ ì•„ë‹Œ (NOT PyTorch)ì„ ì˜ë¯¸í•˜ë©°, ì„±(Nogueira)ì„ ì•”ì‹œí•˜ê¸°ë„ í•©ë‹ˆë‹¤. ğŸ˜</p>\n<p>ì²« ë²ˆì§¸ë¡œ ì•Œì•„ì•¼ í•  ê²ƒì€ PyTorchê°€ Pythonì„ í†µí•´ ì‚¬ìš©ë˜ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œëŠ” C/C++ë¡œ ì‹¤í–‰ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë¨¼ì € ë‚´ë¶€ C/C++ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.</p>\n<p>ë¨¼ì € í…ì„œë¥¼ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” êµ¬ì¡°ì²´ë¡œ ì •ì˜í•˜ê³  ì´ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/tensor.cpp</span>\n\n#include &#x3C;stdio.<span class=\"hljs-property\">h</span>>\n#include &#x3C;stdlib.<span class=\"hljs-property\">h</span>>\n#include &#x3C;string.<span class=\"hljs-property\">h</span>>\n#include &#x3C;math.<span class=\"hljs-property\">h</span>>\n\ntypedef struct {\n    float* data;\n    int* strides;\n    int* shape;\n    int ndim;\n    int size;\n    char* device;\n} <span class=\"hljs-title class_\">Tensor</span>;\n\n<span class=\"hljs-title class_\">Tensor</span>* <span class=\"hljs-title function_\">create_tensor</span>(<span class=\"hljs-params\">float* data, int* shape, int ndim</span>) {\n    \n    <span class=\"hljs-title class_\">Tensor</span>* tensor = (<span class=\"hljs-title class_\">Tensor</span>*)<span class=\"hljs-title function_\">malloc</span>(<span class=\"hljs-title function_\">sizeof</span>(<span class=\"hljs-title class_\">Tensor</span>));\n    <span class=\"hljs-keyword\">if</span> (tensor == <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n    tensor->data = data;\n    tensor->shape = shape;\n    tensor->ndim = ndim;\n\n    tensor->size = <span class=\"hljs-number\">1</span>;\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; ndim; i++) {\n        tensor->size *= shape[i];\n    }\n\n    tensor->strides = (int*)<span class=\"hljs-title function_\">malloc</span>(ndim * <span class=\"hljs-title function_\">sizeof</span>(int));\n    <span class=\"hljs-keyword\">if</span> (tensor->strides == <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n    int stride = <span class=\"hljs-number\">1</span>;\n    <span class=\"hljs-keyword\">for</span> (int i = ndim - <span class=\"hljs-number\">1</span>; i >= <span class=\"hljs-number\">0</span>; i--) {\n        tensor->strides[i] = stride;\n        stride *= shape[i];\n    }\n    \n    <span class=\"hljs-keyword\">return</span> tensor;\n}\n</code></pre>\n<p>ì¼ë¶€ ìš”ì†Œì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ì„œëŠ” ì•ì„œ ë°°ì› ë˜ ìŠ¤íŠ¸ë¼ì´ë“œ(strides)ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/tensor.cpp</span>\n\nfloat <span class=\"hljs-title function_\">get_item</span>(<span class=\"hljs-params\">Tensor* tensor, int* indices</span>) {\n    int index = <span class=\"hljs-number\">0</span>;\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; tensor->ndim; i++) {\n        index += indices[i] * tensor->strides[i];\n    }\n\n    float result;\n    result = tensor->data[index];\n\n    <span class=\"hljs-keyword\">return</span> result;\n}\n</code></pre>\n<p>ì´ì œ í…ì„œ ì‘ì—…ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ ë³´ì—¬ë“œë¦¬ê² ê³ , ì´ ê¸€ ëì— ë§í¬ëœ ì €ì¥ì†Œì—ì„œ ì™„ì „í•œ ë²„ì „ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/cpu.cpp</span>\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">add_tensor_cpu</span>(<span class=\"hljs-params\">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {\n    \n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] + tensor2->data[i];\n    }\n}\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">sub_tensor_cpu</span>(<span class=\"hljs-params\">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {\n    \n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] - tensor2->data[i];\n    }\n}\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">elementwise_mul_tensor_cpu</span>(<span class=\"hljs-params\">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {\n    \n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] * tensor2->data[i];\n    }\n}\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">assign_tensor_cpu</span>(<span class=\"hljs-params\">Tensor* tensor, float* result_data</span>) {\n\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; tensor->size; i++) {\n        result_data[i] = tensor->data[i];\n    }\n}\n\n...\n</code></pre>\n<p>ê·¸ ë‹¤ìŒì—, ì´ëŸ¬í•œ ì‘ì—…ë“¤ì„ í˜¸ì¶œí•  í…ì„œ ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/tensor.cpp</span>\n\n<span class=\"hljs-title class_\">Tensor</span>* <span class=\"hljs-title function_\">add_tensor</span>(<span class=\"hljs-params\">Tensor* tensor1, Tensor* tensor2</span>) {\n    <span class=\"hljs-keyword\">if</span> (tensor1->ndim != tensor2->ndim) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œëŠ” ë™ì¼í•œ ì°¨ì› ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ %d ì™€ %d\\n\"</span>, tensor1->ndim, tensor2->ndim);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n\n    int ndim = tensor1->ndim;\n    int* shape = (int*)<span class=\"hljs-title function_\">malloc</span>(ndim * <span class=\"hljs-title function_\">sizeof</span>(int));\n    <span class=\"hljs-keyword\">if</span> (shape == <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; ndim; i++) {\n        <span class=\"hljs-keyword\">if</span> (tensor1->shape[i] != tensor2->shape[i]) {\n            <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œëŠ” ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤ %d ì™€ %d ì¸ë±ìŠ¤ %dì—ì„œ\\n\"</span>, tensor1->shape[i], tensor2->shape[i], i);\n            <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n        }\n        shape[i] = tensor1->shape[i];\n    }        \n    float* result_data = (float*)<span class=\"hljs-title function_\">malloc</span>(tensor1->size * <span class=\"hljs-title function_\">sizeof</span>(float));\n    <span class=\"hljs-keyword\">if</span> (result_data == <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n    <span class=\"hljs-title function_\">add_tensor_cpu</span>(tensor1, tensor2, result_data);\n    \n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">create_tensor</span>(result_data, shape, ndim, device);\n}\n</code></pre>\n<p>ì´ì „ì— ì–¸ê¸‰í•œ ëŒ€ë¡œ, í…ì„œ ì¬êµ¬ì„±ì€ ë‚´ë¶€ ë°ì´í„° ë°°ì—´ì„ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/tensor.cpp</span>\n\n<span class=\"hljs-title class_\">Tensor</span>* <span class=\"hljs-title function_\">reshape_tensor</span>(<span class=\"hljs-params\">Tensor* tensor, int* new_shape, int new_ndim</span>) {\n\n    int ndim = new_ndim;\n    int* shape = (int*)<span class=\"hljs-title function_\">malloc</span>(ndim * <span class=\"hljs-title function_\">sizeof</span>(int));\n    <span class=\"hljs-keyword\">if</span> (shape == <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; ndim; i++) {\n        shape[i] = new_shape[i];\n    }\n\n    <span class=\"hljs-comment\">// ìƒˆ ëª¨ì–‘ì˜ ìš”ì†Œ ì´ ìˆ˜ ê³„ì‚°</span>\n    int size = <span class=\"hljs-number\">1</span>;\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; new_ndim; i++) {\n        size *= shape[i];\n    }\n\n    <span class=\"hljs-comment\">// ì´ ìš”ì†Œ ìˆ˜ê°€ í˜„ì¬ í…ì„œì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸</span>\n    <span class=\"hljs-keyword\">if</span> (size != tensor->size) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"í…ì„œë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ëª¨ì–‘ì˜ ìš”ì†Œ ì´ ìˆ˜ê°€ í˜„ì¬ í…ì„œì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n\n    float* result_data = (float*)<span class=\"hljs-title function_\">malloc</span>(tensor->size * <span class=\"hljs-title function_\">sizeof</span>(float));\n    <span class=\"hljs-keyword\">if</span> (result_data == <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n    <span class=\"hljs-title function_\">assign_tensor_cpu</span>(tensor, result_data);\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">create_tensor</span>(result_data, shape, ndim, device);\n}\n</code></pre>\n<p>ì´ì œ ì¼ë¶€ í…ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì§€ë§Œ, ëˆ„êµ¬ë‚˜ C/C++ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì´ì œ Python ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤!</p>\n<p>Pythonì„ ì‚¬ìš©í•˜ì—¬ C/C++ ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤. Pybind11ê³¼ Cython ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì‹œì—ì„œëŠ” ctypesë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.</p>\n<p>ì•„ë˜ëŠ” ctypesì˜ ê¸°ë³¸ì ì¸ êµ¬ì¡°ì…ë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//C ì½”ë“œ</span>\n#include &#x3C;stdio.<span class=\"hljs-property\">h</span>>\n\nfloat <span class=\"hljs-title function_\">add_floats</span>(<span class=\"hljs-params\">float a, float b</span>) {\n    <span class=\"hljs-keyword\">return</span> a + b;\n}\n</code></pre>\n<pre><code class=\"hljs language-js\"># ì»´íŒŒì¼\ngcc -shared -o add_floats.<span class=\"hljs-property\">so</span> -fPIC add_floats.<span class=\"hljs-property\">c</span>\n</code></pre>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">Python</span> ì½”ë“œ\n<span class=\"hljs-keyword\">import</span> ctypes\n\n# ê³µìœ  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\nlib = ctypes.<span class=\"hljs-title function_\">CDLL</span>(<span class=\"hljs-string\">'./add_floats.so'</span>)\n\n# í•¨ìˆ˜ì˜ ì¸ìì™€ ë°˜í™˜ ìœ í˜• ì •ì˜\nlib.<span class=\"hljs-property\">add_floats</span>.<span class=\"hljs-property\">argtypes</span> = [ctypes.<span class=\"hljs-property\">c_float</span>, ctypes.<span class=\"hljs-property\">c_float</span>]\nlib.<span class=\"hljs-property\">add_floats</span>.<span class=\"hljs-property\">restype</span> = ctypes.<span class=\"hljs-property\">c_float</span>\n\n# íŒŒì´ì¬ float ê°’ì„ c_float ìœ í˜•ìœ¼ë¡œ ë³€í™˜\na = ctypes.<span class=\"hljs-title function_\">c_float</span>(<span class=\"hljs-number\">3.5</span>)\nb = ctypes.<span class=\"hljs-title function_\">c_float</span>(<span class=\"hljs-number\">2.2</span>)\n\n# C í•¨ìˆ˜ í˜¸ì¶œ\nresult = lib.<span class=\"hljs-title function_\">add_floats</span>(a, b)\n<span class=\"hljs-title function_\">print</span>(result)\n# <span class=\"hljs-number\">5.7</span>\n</code></pre>\n<p>ë³´ì‹œë‹¤ì‹œí”¼ ë§¤ìš° ì§ê´€ì ì…ë‹ˆë‹¤. C/C++ ì½”ë“œë¥¼ ì»´íŒŒì¼í•œ í›„ Pythonì—ì„œ ctypesë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ ë° ë°˜í™˜ c_typesë¥¼ ì •ì˜í•˜ê³ , ë³€ìˆ˜ë¥¼ í•´ë‹¹ c_typesë¡œ ë³€í™˜í•˜ê³  í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ë°°ì—´(ë¶€ë™ ì†Œìˆ˜ì  ëª©ë¡)ê³¼ ê°™ì€ ë³´ë‹¤ ë³µì¡í•œ ìœ í˜•ì˜ ê²½ìš° í¬ì¸í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\">data = [<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">2.0</span>, <span class=\"hljs-number\">3.0</span>]\ndata_ctype = (ctypes.<span class=\"hljs-property\">c_float</span> * <span class=\"hljs-title function_\">len</span>(data))(*data)\n\nlib.<span class=\"hljs-property\">some_array_func</span>.<span class=\"hljs-property\">argstypes</span> = [ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_float</span>)]\n\n...\n\nlib.<span class=\"hljs-title function_\">some_array_func</span>(data)\n</code></pre>\n<p>ê·¸ë¦¬ê³  êµ¬ì¡°ì²´ ìœ í˜•ì˜ ê²½ìš° ì§ì ‘ c_typeì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CustomType</span>(ctypes.<span class=\"hljs-property\">Structure</span>):\n    _fields_ = [\n        (<span class=\"hljs-string\">'field1'</span>, ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_float</span>)),\n        (<span class=\"hljs-string\">'field2'</span>, ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_int</span>)),\n        (<span class=\"hljs-string\">'field3'</span>, ctypes.<span class=\"hljs-property\">c_int</span>),\n    ]\n\n# ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CustomType</span>)ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n</code></pre>\n<p>ê°„ë‹¨íˆ ì„¤ëª…í•˜ê³ , í…ì„œ C/C++ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìœ„í•œ Python ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤!</p>\n<pre><code class=\"hljs language-js\"># norch/tensor.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">import</span> ctypes\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CTensor</span>(ctypes.<span class=\"hljs-property\">Structure</span>):\n    _fields_ = [\n        (<span class=\"hljs-string\">'data'</span>, ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_float</span>)),\n        (<span class=\"hljs-string\">'strides'</span>, ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_int</span>)),\n        (<span class=\"hljs-string\">'shape'</span>, ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_int</span>)),\n        (<span class=\"hljs-string\">'ndim'</span>, ctypes.<span class=\"hljs-property\">c_int</span>),\n        (<span class=\"hljs-string\">'size'</span>, ctypes.<span class=\"hljs-property\">c_int</span>),\n    ]\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Tensor</span>:\n    os.<span class=\"hljs-property\">path</span>.<span class=\"hljs-title function_\">abspath</span>(os.<span class=\"hljs-property\">curdir</span>)\n    _C = ctypes.<span class=\"hljs-title function_\">CDLL</span>(<span class=\"hljs-string\">\"COMPILED_LIB.so\"</span>)\n\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n        \n        data, shape = self.<span class=\"hljs-title function_\">flatten</span>(data)\n        self.<span class=\"hljs-property\">data_ctype</span> = (ctypes.<span class=\"hljs-property\">c_float</span> * <span class=\"hljs-title function_\">len</span>(data))(*data)\n        self.<span class=\"hljs-property\">shape_ctype</span> = (ctypes.<span class=\"hljs-property\">c_int</span> * <span class=\"hljs-title function_\">len</span>(shape))(*shape)\n        self.<span class=\"hljs-property\">ndim_ctype</span> = ctypes.<span class=\"hljs-title function_\">c_int</span>(<span class=\"hljs-title function_\">len</span>(shape))\n       \n        self.<span class=\"hljs-property\">shape</span> = shape\n        self.<span class=\"hljs-property\">ndim</span> = <span class=\"hljs-title function_\">len</span>(shape)\n\n        <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">create_tensor</span>.<span class=\"hljs-property\">argtypes</span> = [ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_float</span>), ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_int</span>), ctypes.<span class=\"hljs-property\">c_int</span>]\n        <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">create_tensor</span>.<span class=\"hljs-property\">restype</span> = ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>)\n\n        self.<span class=\"hljs-property\">tensor</span> = <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-title function_\">create_tensor</span>(\n            self.<span class=\"hljs-property\">data_ctype</span>,\n            self.<span class=\"hljs-property\">shape_ctype</span>,\n            self.<span class=\"hljs-property\">ndim_ctype</span>,\n        )\n        \n    def <span class=\"hljs-title function_\">flatten</span>(self, nested_list):\n        <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n        This method simply convert a list type tensor to a flatten tensor with its shape\n        \n        Example:\n        \n        Arguments:  \n            nested_list: [[1, 2, 3], [-5, 2, 0]]\n        Return:\n            flat_data: [1, 2, 3, -5, 2, 0]\n            shape: [2, 3]\n        \"</span><span class=\"hljs-string\">\"\"</span>\n        def <span class=\"hljs-title function_\">flatten_recursively</span>(nested_list):\n            flat_data = []\n            shape = []\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">isinstance</span>(nested_list, list):\n                <span class=\"hljs-keyword\">for</span> sublist <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">nested_list</span>:\n                    inner_data, inner_shape = <span class=\"hljs-title function_\">flatten_recursively</span>(sublist)\n                    flat_data.<span class=\"hljs-title function_\">extend</span>(inner_data)\n                shape.<span class=\"hljs-title function_\">append</span>(<span class=\"hljs-title function_\">len</span>(nested_list))\n                shape.<span class=\"hljs-title function_\">extend</span>(inner_shape)\n            <span class=\"hljs-attr\">else</span>:\n                flat_data.<span class=\"hljs-title function_\">append</span>(nested_list)\n            <span class=\"hljs-keyword\">return</span> flat_data, shape\n        \n        flat_data, shape = <span class=\"hljs-title function_\">flatten_recursively</span>(nested_list)\n        <span class=\"hljs-keyword\">return</span> flat_data, shape\n</code></pre>\n<p>ì´ì œ Python í…ì„œ ì‘ì—…ì„ í¬í•¨í•˜ì—¬ C/C++ ì‘ì—…ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"># norch/tensor.<span class=\"hljs-property\">py</span>\n\ndef <span class=\"hljs-title function_\">__getitem__</span>(self, indices):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    index í…ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ì„œì— ì•¡ì„¸ìŠ¤ tensor[i, j, k...]\n    \"</span><span class=\"hljs-string\">\"\"</span>\n\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">len</span>(indices) != self.<span class=\"hljs-property\">ndim</span>:\n        raise <span class=\"hljs-title class_\">ValueError</span>(<span class=\"hljs-string\">\"ì¸ë±ìŠ¤ ìˆ˜ê°€ ì°¨ì› ìˆ˜ì™€ ì¼ì¹˜í•´ì•¼ í•¨\"</span>)\n    \n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">get_item</span>.<span class=\"hljs-property\">argtypes</span> = [ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>), ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_int</span>)]\n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">get_item</span>.<span class=\"hljs-property\">restype</span> = ctypes.<span class=\"hljs-property\">c_float</span>\n                                       \n    indices = (ctypes.<span class=\"hljs-property\">c_int</span> * <span class=\"hljs-title function_\">len</span>(indices))(*indices)\n    value = <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-title function_\">get_item</span>(self.<span class=\"hljs-property\">tensor</span>, indices)  \n    \n    <span class=\"hljs-keyword\">return</span> value\n\ndef <span class=\"hljs-title function_\">reshape</span>(self, new_shape):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    í…ì„œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤\n    result = tensor.reshape([1,2])\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    new_shape_ctype = (ctypes.<span class=\"hljs-property\">c_int</span> * <span class=\"hljs-title function_\">len</span>(new_shape))(*new_shape)\n    new_ndim_ctype = ctypes.<span class=\"hljs-title function_\">c_int</span>(<span class=\"hljs-title function_\">len</span>(new_shape))\n    \n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">reshape_tensor</span>.<span class=\"hljs-property\">argtypes</span> = [ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>), ctypes.<span class=\"hljs-title function_\">POINTER</span>(ctypes.<span class=\"hljs-property\">c_int</span>), ctypes.<span class=\"hljs-property\">c_int</span>]\n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">reshape_tensor</span>.<span class=\"hljs-property\">restype</span> = ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>)\n    result_tensor_ptr = <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-title function_\">reshape_tensor</span>(self.<span class=\"hljs-property\">tensor</span>, new_shape_ctype, new_ndim_ctype)   \n\n    result_data = <span class=\"hljs-title class_\">Tensor</span>()\n    result_data.<span class=\"hljs-property\">tensor</span> = result_tensor_ptr\n    result_data.<span class=\"hljs-property\">shape</span> = new_shape.<span class=\"hljs-title function_\">copy</span>()\n    result_data.<span class=\"hljs-property\">ndim</span> = <span class=\"hljs-title function_\">len</span>(new_shape)\n    result_data.<span class=\"hljs-property\">device</span> = self.<span class=\"hljs-property\">device</span>\n\n    <span class=\"hljs-keyword\">return</span> result_data\n\ndef <span class=\"hljs-title function_\">__add__</span>(self, other):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    í…ì„œë¥¼ ë”í•©ë‹ˆë‹¤\n    result = tensor1 + tensor2\n    \"</span><span class=\"hljs-string\">\"\"</span>\n  \n    <span class=\"hljs-keyword\">if</span> self.<span class=\"hljs-property\">shape</span> != other.<span class=\"hljs-property\">shape</span>:\n        raise <span class=\"hljs-title class_\">ValueError</span>(<span class=\"hljs-string\">\"ë§ì…ˆì„ ìœ„í•´ì„œ í…ì„œë“¤ì€ ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•¨\"</span>)\n    \n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">add_tensor</span>.<span class=\"hljs-property\">argtypes</span> = [ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>), ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>)]\n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">add_tensor</span>.<span class=\"hljs-property\">restype</span> = ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>)\n\n    result_tensor_ptr = <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-title function_\">add_tensor</span>(self.<span class=\"hljs-property\">tensor</span>, other.<span class=\"hljs-property\">tensor</span>)\n\n    result_data = <span class=\"hljs-title class_\">Tensor</span>()\n    result_data.<span class=\"hljs-property\">tensor</span> = result_tensor_ptr\n    result_data.<span class=\"hljs-property\">shape</span> = self.<span class=\"hljs-property\">shape</span>.<span class=\"hljs-title function_\">copy</span>()\n    result_data.<span class=\"hljs-property\">ndim</span> = self.<span class=\"hljs-property\">ndim</span>\n    result_data.<span class=\"hljs-property\">device</span> = self.<span class=\"hljs-property\">device</span>\n\n    <span class=\"hljs-keyword\">return</span> result_data\n\n# ê¸°íƒ€ ì—°ì‚° í¬í•¨:\n# __str__\n# __sub__ (-)\n# __mul__ (*)\n# __matmul__ (@)\n# __pow__ (**)\n# __truediv__ (/)\n# log\n# ...\n</code></pre>\n<p>ì—¬ê¸°ê¹Œì§€ ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ì´ì œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  í…ì„œ ì‘ì—…ì„ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ìƒê²¼ìŠµë‹ˆë‹¤!</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> norch\n\ntensor1 = norch.<span class=\"hljs-title class_\">Tensor</span>([[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>], [<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>]])\ntensor2 = norch.<span class=\"hljs-title class_\">Tensor</span>([[<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>], [<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>]])\n\nresult = tensor1 + tensor2\n<span class=\"hljs-title function_\">print</span>(result[<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>])\n# <span class=\"hljs-number\">4</span> \n</code></pre>\n<h1>#2 â€” GPU ì§€ì›</h1>\n<p>ìš°ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë§Œë“  í›„, ì´ì œ ìƒˆë¡œìš´ ìˆ˜ì¤€ìœ¼ë¡œ ëŒì–´ì˜¬ë¦´ ê²ƒì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ GPUë¡œ ì „ì†¡í•˜ê³  ìˆ˜í•™ ì—°ì‚°ì„ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ <code>.to(\"cuda\")</code>ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ì˜ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. CUDAê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê¸°ë³¸ ì§€ì‹ì´ ìˆì„ ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° ë‹¤ë¥¸ ê¸°ì‚¬ì¸ 'CUDA íŠœí† ë¦¬ì–¼'ì„ ì½ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦´ê²Œìš”. ğŸ˜Š</p>\n<p>...</p>\n<p>ê¸‰í•œ ì‚¬ëŒë“¤ì„ ìœ„í•´, ê°„ë‹¨í•œ ì†Œê°œê°€ ì—¬ê¸° ìˆì–´ìš”:</p>\n<p>ê¸°ë³¸ì ìœ¼ë¡œ, ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ì½”ë“œëŠ” CPU ë©”ëª¨ë¦¬ì—ì„œ ì‹¤í–‰ë˜ê³  ìˆì–´ìš”. í•˜ë‚˜ì˜ ì‘ì—…ì— ëŒ€í•´ì„œëŠ” CPUê°€ ë¹ ë¥´ì§€ë§Œ, GPUì˜ ì¥ì ì€ ë³‘ë ¬í™” ëŠ¥ë ¥ì— ìˆì–´ìš”. CPU ë””ìì¸ì€ ì—°ì‚°(ìŠ¤ë ˆë“œ)ì„ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ë„ë¡ ëª©í‘œë¥¼ í•œ ë°˜ë©´, GPU ë””ìì¸ì€ ìˆ˜ë°±ë§Œ ê°œì˜ ì—°ì‚°ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ë„ë¡ ëª©í‘œë¥¼ í•´ìš” (ê°œë³„ ìŠ¤ë ˆë“œì˜ ì„±ëŠ¥ì„ í¬ìƒí•˜ë©°).</p>\n<p>ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì´ ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë³‘ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´, ë°±ë§Œ ê°œì˜ ìš”ì†Œë¡œ êµ¬ì„±ëœ í…ì„œë¥¼ ì¶”ê°€í•  ë•Œ, ë°˜ë³µë¬¸ ë‚´ì—ì„œ ê° ìƒ‰ì¸ì˜ ìš”ì†Œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ëŒ€ì‹ , GPUë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêº¼ë²ˆì— ëª¨ë‘ë¥¼ ë³‘ë ¬ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆì–´ìš”. ì´ë¥¼ ìœ„í•´ NVIDIAì—ì„œ ê°œë°œí•œ ê°œë°œìë“¤ì´ GPU ì§€ì›ì„ ì†Œí”„íŠ¸ì›¨ì–´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” í”Œë«í¼ì¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”.</p>\n<p>ê·¸ê±¸ í•˜ë ¤ë©´, íŠ¹ì • GPU ì‘ì—…(ì˜ˆ: CPU ë©”ëª¨ë¦¬ì—ì„œ GPU ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ë³µì‚¬)ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ê°„ë‹¨í•œ C/C++ ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤ ì¸ CUDA C/C++ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<p>ì•„ë˜ ì½”ë“œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPUì—ì„œ GPUë¡œ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ê³  ë°°ì—´ì˜ ê° ìš”ì†Œë¥¼ ì¶”ê°€í•˜ëŠ” AddTwoArrays í•¨ìˆ˜(ì»¤ë„ì´ë¼ê³ ë„ í•¨)ë¥¼ Nê°œì˜ GPU ìŠ¤ë ˆë“œì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ëª‡ ê°€ì§€ CUDA C/C++ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-meta\">#<span class=\"hljs-keyword\">include</span> <span class=\"hljs-string\">&#x3C;stdio.h></span></span>\n\n<span class=\"hljs-comment\">// CPU ë²„ì „(ë¹„êµìš©)</span>\n<span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">AddTwoArrays_CPU</span><span class=\"hljs-params\">(flaot A[], <span class=\"hljs-type\">float</span> B[], <span class=\"hljs-type\">float</span> C[])</span> {\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &#x3C; N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n\n<span class=\"hljs-comment\">// ì»¤ë„ ì •ì˜</span>\n__global__ <span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">AddTwoArrays_GPU</span><span class=\"hljs-params\">(<span class=\"hljs-type\">float</span> A[], <span class=\"hljs-type\">float</span> B[], <span class=\"hljs-type\">float</span> C[])</span> {\n    <span class=\"hljs-type\">int</span> i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\n<span class=\"hljs-type\">int</span> <span class=\"hljs-title function_\">main</span><span class=\"hljs-params\">()</span> {\n\n    <span class=\"hljs-type\">int</span> N = <span class=\"hljs-number\">1000</span>; <span class=\"hljs-comment\">// ë°°ì—´ í¬ê¸°</span>\n    <span class=\"hljs-type\">float</span> A[N], B[N], C[N]; <span class=\"hljs-comment\">// ë°°ì—´ A, B, C</span>\n\n    ...\n\n    <span class=\"hljs-type\">float</span> *d_A, *d_B, *d_C; <span class=\"hljs-comment\">// ë°°ì—´ A, B, Cì˜ ì¥ì¹˜ í¬ì¸í„°</span>\n\n    <span class=\"hljs-comment\">// ë°°ì—´ A, B, Cì— ëŒ€í•œ ì¥ì¹˜ì—ì„œì˜ ë©”ëª¨ë¦¬ í• ë‹¹</span>\n    cudaMalloc((<span class=\"hljs-type\">void</span> **)&#x26;d_A, N * <span class=\"hljs-keyword\">sizeof</span>(<span class=\"hljs-type\">float</span>));\n    cudaMalloc((<span class=\"hljs-type\">void</span> **)&#x26;d_B, N * <span class=\"hljs-keyword\">sizeof</span>(<span class=\"hljs-type\">float</span>));\n    cudaMalloc((<span class=\"hljs-type\">void</span> **)&#x26;d_C, N * <span class=\"hljs-keyword\">sizeof</span>(<span class=\"hljs-type\">float</span>));\n\n    <span class=\"hljs-comment\">// í˜¸ìŠ¤íŠ¸ì—ì„œ ì¥ì¹˜ë¡œ ë°°ì—´ A ë° B ë³µì‚¬</span>\n    cudaMemcpy(d_A, A, N * <span class=\"hljs-keyword\">sizeof</span>(<span class=\"hljs-type\">float</span>), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * <span class=\"hljs-keyword\">sizeof</span>(<span class=\"hljs-type\">float</span>), cudaMemcpyHostToDevice);\n\n    <span class=\"hljs-comment\">// Nê°œì˜ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ë„ í˜¸ì¶œ</span>\n    AddTwoArrays_GPU&#x3C;&#x3C;&#x3C;<span class=\"hljs-number\">1</span>, N>>>(d_A, d_B, d_C);\n    \n    <span class=\"hljs-comment\">// ì¥ì¹˜ì—ì„œ í˜¸ìŠ¤íŠ¸ë¡œ ë²¡í„° C ë³µì‚¬</span>\n    cudaMemcpy(C, d_C, N * <span class=\"hljs-keyword\">sizeof</span>(<span class=\"hljs-type\">float</span>), cudaMemcpyDeviceToHost);\n\n}\n</code></pre>\n<p>ì£¼ëª©í•  ì ì€ ê° ìš”ì†Œ ìŒì„ ê°ê° ì¶”ê°€í•˜ëŠ” ëŒ€ì‹  ëª¨ë“  ë§ì…ˆ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë£¨í”„ ëª…ë ¹ì„ ì œê±°í•œ ê²ƒì…ë‹ˆë‹¤.</p>\n<p>ê°„ë‹¨í•œ ì†Œê°œ ì´í›„ì—, í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆì–´ìš”.</p>\n<p>ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” CPUì—ì„œ GPUë¡œ í…ì„œ ë°ì´í„°ë¥¼ ë³´ë‚´ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/tensor.cpp</span>\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">to_device</span>(<span class=\"hljs-params\">Tensor* tensor, char* target_device</span>) {\n    <span class=\"hljs-keyword\">if</span> ((<span class=\"hljs-title function_\">strcmp</span>(target_device, <span class=\"hljs-string\">\"cuda\"</span>) == <span class=\"hljs-number\">0</span>) &#x26;&#x26; (<span class=\"hljs-title function_\">strcmp</span>(tensor->device, <span class=\"hljs-string\">\"cpu\"</span>) == <span class=\"hljs-number\">0</span>)) {\n        <span class=\"hljs-title function_\">cpu_to_cuda</span>(tensor);\n    }\n\n    <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> ((<span class=\"hljs-title function_\">strcmp</span>(target_device, <span class=\"hljs-string\">\"cpu\"</span>) == <span class=\"hljs-number\">0</span>) &#x26;&#x26; (<span class=\"hljs-title function_\">strcmp</span>(tensor->device, <span class=\"hljs-string\">\"cuda\"</span>) == <span class=\"hljs-number\">0</span>)) {\n        <span class=\"hljs-title function_\">cuda_to_cpu</span>(tensor);\n    }\n}\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/cuda.cu</span>\n\n__host__ <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">cpu_to_cuda</span>(<span class=\"hljs-params\">Tensor* tensor</span>) {\n    \n    float* data_tmp;\n    <span class=\"hljs-title function_\">cudaMalloc</span>((<span class=\"hljs-keyword\">void</span> **)&#x26;data_tmp, tensor->size * <span class=\"hljs-title function_\">sizeof</span>(float));\n    <span class=\"hljs-title function_\">cudaMemcpy</span>(data_tmp, tensor->data, tensor->size * <span class=\"hljs-title function_\">sizeof</span>(float), cudaMemcpyHostToDevice);\n\n    tensor->data = data_tmp;\n\n    <span class=\"hljs-keyword\">const</span> char* device_str = <span class=\"hljs-string\">\"cuda\"</span>;\n    tensor->device = (char*)<span class=\"hljs-title function_\">malloc</span>(<span class=\"hljs-title function_\">strlen</span>(device_str) + <span class=\"hljs-number\">1</span>);\n    <span class=\"hljs-title function_\">strcpy</span>(tensor->device, device_str); \n\n    <span class=\"hljs-title function_\">printf</span>(<span class=\"hljs-string\">\"í…ì„œê°€ ì„±ê³µì ìœ¼ë¡œ %së¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\"</span>, tensor->device);\n}\n\n__host__ <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">cuda_to_cpu</span>(<span class=\"hljs-params\">Tensor* tensor</span>) {\n    float* data_tmp = (float*)<span class=\"hljs-title function_\">malloc</span>(tensor->size * <span class=\"hljs-title function_\">sizeof</span>(float));\n\n    <span class=\"hljs-title function_\">cudaMemcpy</span>(data_tmp, tensor->data, tensor->size * <span class=\"hljs-title function_\">sizeof</span>(float), cudaMemcpyDeviceToHost);\n    <span class=\"hljs-title function_\">cudaFree</span>(tensor->data);\n\n    tensor->data = data_tmp;\n\n    <span class=\"hljs-keyword\">const</span> char* device_str = <span class=\"hljs-string\">\"cpu\"</span>;\n    tensor->device = (char*)<span class=\"hljs-title function_\">malloc</span>(<span class=\"hljs-title function_\">strlen</span>(device_str) + <span class=\"hljs-number\">1</span>);\n    <span class=\"hljs-title function_\">strcpy</span>(tensor->device, device_str); \n\n    <span class=\"hljs-title function_\">printf</span>(<span class=\"hljs-string\">\"í…ì„œê°€ ì„±ê³µì ìœ¼ë¡œ %së¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\"</span>, tensor->device);\n}\n</code></pre>\n<p>íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„ëœ ë˜í¼:</p>\n<pre><code class=\"hljs language-js\"># norch/tensor.<span class=\"hljs-property\">py</span>\n\ndef <span class=\"hljs-title function_\">to</span>(self, device):\n    self.<span class=\"hljs-property\">device</span> = device\n    self.<span class=\"hljs-property\">device_ctype</span> = self.<span class=\"hljs-property\">device</span>.<span class=\"hljs-title function_\">encode</span>(<span class=\"hljs-string\">'utf-8'</span>)\n  \n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">to_device</span>.<span class=\"hljs-property\">argtypes</span> = [ctypes.<span class=\"hljs-title function_\">POINTER</span>(<span class=\"hljs-title class_\">CTensor</span>), ctypes.<span class=\"hljs-property\">c_char_p</span>]\n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-property\">to_device</span>.<span class=\"hljs-property\">restype</span> = <span class=\"hljs-title class_\">None</span>\n    <span class=\"hljs-title class_\">Tensor</span>.<span class=\"hljs-property\">_C</span>.<span class=\"hljs-title function_\">to_device</span>(self.<span class=\"hljs-property\">tensor</span>, self.<span class=\"hljs-property\">device_ctype</span>)\n  \n    <span class=\"hljs-keyword\">return</span> self\n</code></pre>\n<p>ë‹¤ìŒìœ¼ë¡œ, ëª¨ë“  í…ì„œ ì—°ì‚°ì— ëŒ€í•´ GPU ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ì…ˆê³¼ ëº„ì…ˆì— ëŒ€í•œ ì˜ˆì œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/cuda.cu</span>\n\n#define <span class=\"hljs-variable constant_\">THREADS_PER_BLOCK</span> <span class=\"hljs-number\">128</span>\n\n__global__ <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">add_tensor_cuda_kernel</span>(<span class=\"hljs-params\">float* data1, float* data2, float* result_data, int size</span>) {\n    \n    int i = blockIdx.<span class=\"hljs-property\">x</span> * blockDim.<span class=\"hljs-property\">x</span> + threadIdx.<span class=\"hljs-property\">x</span>;\n    <span class=\"hljs-keyword\">if</span> (i &#x3C; size) {\n        result_data[i] = data1[i] + data2[i];\n    }\n}\n\n__host__ <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">add_tensor_cuda</span>(<span class=\"hljs-params\">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {\n    \n    int number_of_blocks = (tensor1->size + <span class=\"hljs-variable constant_\">THREADS_PER_BLOCK</span> - <span class=\"hljs-number\">1</span>) / <span class=\"hljs-variable constant_\">THREADS_PER_BLOCK</span>;\n    add_tensor_cuda_kernel&#x3C;&#x3C;&#x3C;number_of_blocks, <span class=\"hljs-variable constant_\">THREADS_PER_BLOCK</span>>>>(tensor1->data, tensor2->data, result_data, tensor1->size);\n\n    cudaError_t error = <span class=\"hljs-title function_\">cudaGetLastError</span>();\n    <span class=\"hljs-keyword\">if</span> (error != cudaSuccess) {\n        <span class=\"hljs-title function_\">printf</span>(<span class=\"hljs-string\">\"CUDA error: %s\\n\"</span>, <span class=\"hljs-title function_\">cudaGetErrorString</span>(error));\n        <span class=\"hljs-title function_\">exit</span>(-<span class=\"hljs-number\">1</span>);\n    }\n\n    <span class=\"hljs-title function_\">cudaDeviceSynchronize</span>();\n}\n\n__global__ <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">sub_tensor_cuda_kernel</span>(<span class=\"hljs-params\">float* data1, float* data2, float* result_data, int size</span>) {\n   \n    int i = blockIdx.<span class=\"hljs-property\">x</span> * blockDim.<span class=\"hljs-property\">x</span> + threadIdx.<span class=\"hljs-property\">x</span>;\n    <span class=\"hljs-keyword\">if</span> (i &#x3C; size) {\n        result_data[i] = data1[i] - data2[i];\n    }\n}\n\n__host__ <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">sub_tensor_cuda</span>(<span class=\"hljs-params\">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {\n    \n    int number_of_blocks = (tensor1->size + <span class=\"hljs-variable constant_\">THREADS_PER_BLOCK</span> - <span class=\"hljs-number\">1</span>) / <span class=\"hljs-variable constant_\">THREADS_PER_BLOCK</span>;\n    sub_tensor_cuda_kernel&#x3C;&#x3C;&#x3C;number_of_blocks, <span class=\"hljs-variable constant_\">THREADS_PER_BLOCK</span>>>>(tensor1->data, tensor2->data, result_data, tensor1->size);\n\n    cudaError_t error = <span class=\"hljs-title function_\">cudaGetLastError</span>();\n    <span class=\"hljs-keyword\">if</span> (error != cudaSuccess) {\n        <span class=\"hljs-title function_\">printf</span>(<span class=\"hljs-string\">\"CUDA error: %s\\n\"</span>, <span class=\"hljs-title function_\">cudaGetErrorString</span>(error));\n        <span class=\"hljs-title function_\">exit</span>(-<span class=\"hljs-number\">1</span>);\n    }\n\n    <span class=\"hljs-title function_\">cudaDeviceSynchronize</span>();\n}\n\n...\n</code></pre>\n<p>ê·¸ëŸ° ë‹¤ìŒ, í…ì„œ.cppì— ìƒˆë¡œìš´ í…ì„œ ì†ì„± char* deviceë¥¼ ì¶”ê°€í•˜ê³  ì‘ì—…ì„ ì‹¤í–‰í•  ìœ„ì¹˜(CPU ë˜ëŠ” GPU)ë¥¼ ì„ íƒí•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-comment\">//norch/csrc/tensor.cpp</span>\n\n<span class=\"hljs-title class_\">Tensor</span>* <span class=\"hljs-title function_\">add_tensor</span>(<span class=\"hljs-params\">Tensor* tensor1, Tensor* tensor2</span>) {\n    <span class=\"hljs-keyword\">if</span> (tensor1->ndim != tensor2->ndim) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë§ì…ˆì„ ìœ„í•´ í…ì„œê°€ ë™ì¼í•œ ì°¨ì› ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ %d and %d\\n\"</span>, tensor1->ndim, tensor2->ndim);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n\n    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-title function_\">strcmp</span>(tensor1->device, tensor2->device) != <span class=\"hljs-number\">0</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"í…ì„œëŠ” ë™ì¼í•œ ì¥ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤: %s and %s\\n\"</span>, tensor1->device, tensor2->device);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n\n    char* device = (char*)<span class=\"hljs-title function_\">malloc</span>(<span class=\"hljs-title function_\">strlen</span>(tensor1->device) + <span class=\"hljs-number\">1</span>);\n    <span class=\"hljs-keyword\">if</span> (device != <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">strcpy</span>(device, tensor1->device);\n    } <span class=\"hljs-keyword\">else</span> {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(-<span class=\"hljs-number\">1</span>);\n    }\n    int ndim = tensor1->ndim;\n    int* shape = (int*)<span class=\"hljs-title function_\">malloc</span>(ndim * <span class=\"hljs-title function_\">sizeof</span>(int));\n    <span class=\"hljs-keyword\">if</span> (shape == <span class=\"hljs-variable constant_\">NULL</span>) {\n        <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n        <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n    }\n\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; ndim; i++) {\n        <span class=\"hljs-keyword\">if</span> (tensor1->shape[i] != tensor2->shape[i]) {\n            <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë§ì…ˆì„ ìœ„í•´ í…ì„œë“¤ì€ ìƒ‰ì¸ %dì—ì„œ ë™ì¼í•œ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤ %d and %d\\n\"</span>, i, tensor1->shape[i], tensor2->shape[i]);\n            <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n        }\n        shape[i] = tensor1->shape[i];\n    }        \n\n    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-title function_\">strcmp</span>(tensor1->device, <span class=\"hljs-string\">\"cuda\"</span>) == <span class=\"hljs-number\">0</span>) {\n\n        float* result_data;\n        <span class=\"hljs-title function_\">cudaMalloc</span>((<span class=\"hljs-keyword\">void</span> **)&#x26;result_data, tensor1->size * <span class=\"hljs-title function_\">sizeof</span>(float));\n        <span class=\"hljs-title function_\">add_tensor_cuda</span>(tensor1, tensor2, result_data);\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">create_tensor</span>(result_data, shape, ndim, device);\n    } \n    <span class=\"hljs-keyword\">else</span> {\n        float* result_data = (float*)<span class=\"hljs-title function_\">malloc</span>(tensor1->size * <span class=\"hljs-title function_\">sizeof</span>(float));\n        <span class=\"hljs-keyword\">if</span> (result_data == <span class=\"hljs-variable constant_\">NULL</span>) {\n            <span class=\"hljs-title function_\">fprintf</span>(stderr, <span class=\"hljs-string\">\"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨\\n\"</span>);\n            <span class=\"hljs-title function_\">exit</span>(<span class=\"hljs-number\">1</span>);\n        }\n        <span class=\"hljs-title function_\">add_tensor_cpu</span>(tensor1, tensor2, result_data);\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">create_tensor</span>(result_data, shape, ndim, device);\n    }     \n}\n</code></pre>\n<p>ì´ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ GPU ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤!</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> norch\n\ntensor1 = norch.<span class=\"hljs-title class_\">Tensor</span>([[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>], [<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>]]).<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">\"cuda\"</span>)\ntensor2 = norch.<span class=\"hljs-title class_\">Tensor</span>([[<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>], [<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>]]).<span class=\"hljs-title function_\">to</span>(<span class=\"hljs-string\">\"cuda\"</span>)\n\nresult = tensor1 + tensor2\n</code></pre>\n<h1>#3 â€” Automatic Differentiation (Autograd)</h1>\n<p>íŒŒì´í† ì¹˜ê°€ ì¸ê¸°ë¥¼ ì–»ê²Œ ëœ ì£¼ìš” ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” Autograd ëª¨ë“ˆ ë•Œë¬¸ì…ë‹ˆë‹¤. Autograd ëª¨ë“ˆì€ ìë™ ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤ (ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ê°™ì€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤). .backward()ë¼ëŠ” ë‹¨ì¼ ë©”ì„œë“œ í˜¸ì¶œë¡œ ì´ì „ í…ì„œ ì—°ì‚°ì—ì„œ ëª¨ë“  ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\">x = torch.<span class=\"hljs-title function_\">tensor</span>([[<span class=\"hljs-number\">1.</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>], [<span class=\"hljs-number\">3.</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>]], requires_grad=<span class=\"hljs-title class_\">True</span>)\n# [[<span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>,  <span class=\"hljs-number\">3</span>],\n#  [<span class=\"hljs-number\">3</span>,  <span class=\"hljs-number\">2.</span>, <span class=\"hljs-number\">1</span>]]\n\ny = torch.<span class=\"hljs-title function_\">tensor</span>([[<span class=\"hljs-number\">3.</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>], [<span class=\"hljs-number\">1.</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>]], requires_grad=<span class=\"hljs-title class_\">True</span>)\n# [[<span class=\"hljs-number\">3</span>,  <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">1</span>],\n#  [<span class=\"hljs-number\">1</span>,  <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>]]\n\nL = ((x - y) ** <span class=\"hljs-number\">3</span>).<span class=\"hljs-title function_\">sum</span>()\n\nL.<span class=\"hljs-title function_\">backward</span>()\n\n# xì™€ yì˜ ê¸°ìš¸ê¸°ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n<span class=\"hljs-title function_\">print</span>(x.<span class=\"hljs-property\">grad</span>)\n# [[<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">12</span>],\n#  [<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">12</span>]]\n\n<span class=\"hljs-title function_\">print</span>(y.<span class=\"hljs-property\">grad</span>)\n# [[-<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">0</span>, -<span class=\"hljs-number\">12</span>],\n#  [-<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">0</span>, -<span class=\"hljs-number\">12</span>]]\n\n# zë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n# x = x - í•™ìŠµë¥  * x.<span class=\"hljs-property\">grad</span>\n# y = y - í•™ìŠµë¥  * y.<span class=\"hljs-property\">grad</span>\n</code></pre>\n<p>ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ì§€ ì´í•´í•˜ê¸° ìœ„í•´ ë™ì¼í•œ ì ˆì°¨ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•´ë³´ê² ìŠµë‹ˆë‹¤:</p>\n<p>ìš°ì„  ê³„ì‚°í•´ ë´…ì‹œë‹¤:</p>\n<p>xê°€ í–‰ë ¬ì´ë¼ëŠ” ê²ƒì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê° ìš”ì†Œì— ëŒ€í•œ Lì˜ ë¯¸ë¶„ì„ ê°œë³„ì ìœ¼ë¡œ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. ê²Œë‹¤ê°€, Lì€ ëª¨ë“  ìš”ì†Œì— ëŒ€í•œ í•©ì´ì§€ë§Œ ê° ìš”ì†Œì— ëŒ€í•œ ë¯¸ë¶„ì—ì„œ ë‹¤ë¥¸ ìš”ì†Œë“¤ì€ ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•­ì„ ì–»ìŠµë‹ˆë‹¤:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_12.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<p>ê° í•­ì— ëŒ€í•´ ì—°ì‡„ ë²•ì¹™ì„ ì ìš©í•˜ì—¬ ì™¸ë¶€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ê³  ë‚´ë¶€ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•œ ê°’ì„ ê³±í•©ë‹ˆë‹¤:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_13.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<p>Where:</p>\n<p>ë§ˆì¹¨ë‚´:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_14.png\" alt=\"ì´ë¯¸ì§€\"></p>\n<p>ê·¸ëŸ¬ë¯€ë¡œ, xì— ê´€í•œ Lì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ëŠ” ìµœì¢… ë°©ì •ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>\n<p>ì•„ë˜ëŠ” Markdown í˜•ì‹ìœ¼ë¡œ ë³€ê²½ëœ ë‚´ìš©ì…ë‹ˆë‹¤.</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_16.png\" alt=\"Image 1\"></p>\n<p>Substituting the values into the equation:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_17.png\" alt=\"Image 2\"></p>\n<p>Calculating the result, we get the same values we obtained with PyTorch:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_18.png\" alt=\"image\"></p>\n<p>Now, letâ€™s analyze what we just did:</p>\n<p>Basically, we observed all the operations involved in reverse order: a summation, a power of 3, and a subtraction. Then, we applied the chain rule, calculating the derivative of each operation and recursively calculated the derivative for the next operation. So, first we need an implementation of the derivative for different math operations:</p>\n<p>For addition:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_19.png\" alt=\"Image\"></p>\n<pre><code class=\"hljs language-js\"># norch/autograd/functions.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">AddBackward</span>:\n    def <span class=\"hljs-title function_\">__init__</span>(self, x, y):\n        self.<span class=\"hljs-property\">input</span> = [x, y]\n\n    def <span class=\"hljs-title function_\">backward</span>(self, gradient):\n        <span class=\"hljs-keyword\">return</span> [gradient, gradient]\n</code></pre>\n<p>For sin:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_20.png\" alt=\"Image\"></p>\n<pre><code class=\"hljs language-js\"># norch/autograd/functions.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">SinBackward</span>:\n    def <span class=\"hljs-title function_\">__init__</span>(self, x):\n        self.<span class=\"hljs-property\">input</span> = [x]\n\n    def <span class=\"hljs-title function_\">backward</span>(self, gradient):\n        x = self.<span class=\"hljs-property\">input</span>[<span class=\"hljs-number\">0</span>]\n        <span class=\"hljs-keyword\">return</span> [x.<span class=\"hljs-title function_\">cos</span>() * gradient]\n</code></pre>\n<p>ì½”ì‚¬ì¸ì— ëŒ€í•´:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21.png\" alt=\"2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21\"></p>\n<pre><code class=\"hljs language-js\"># norch/autograd/functions.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">CosBackward</span>:\n    def <span class=\"hljs-title function_\">__init__</span>(self, x):\n        self.<span class=\"hljs-property\">input</span> = [x]\n\n    def <span class=\"hljs-title function_\">backward</span>(self, gradient):\n        x = self.<span class=\"hljs-property\">input</span>[<span class=\"hljs-number\">0</span>]\n        <span class=\"hljs-keyword\">return</span> [- x.<span class=\"hljs-title function_\">sin</span>() * gradient]\n</code></pre>\n<p>ìš”ì†Œë³„ ê³±ì…ˆì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”:</p>\n<p><img src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_22.png\" alt=\"element-wise multiplication\"></p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># norch/autograd/functions.py</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">ElementwiseMulBackward</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, x, y</span>):\n        self.<span class=\"hljs-built_in\">input</span> = [x, y]\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self, gradient</span>):\n        x = self.<span class=\"hljs-built_in\">input</span>[<span class=\"hljs-number\">0</span>]\n        y = self.<span class=\"hljs-built_in\">input</span>[<span class=\"hljs-number\">1</span>]\n        <span class=\"hljs-keyword\">return</span> [y * gradient, x * gradient]\n</code></pre>\n<p>í•©ì‚°ì— ëŒ€í•´ì„œ:</p>\n<h1>norch/autograd/functions.py</h1>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">SumBackward</span>:\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, x</span>):\n        self.<span class=\"hljs-built_in\">input</span> = [x]\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self, gradient</span>):\n        <span class=\"hljs-comment\"># sum í•¨ìˆ˜ëŠ” í…ì„œë¥¼ ìŠ¤ì¹¼ë¼ë¡œ ì¤„ì´ë¯€ë¡œ ê¸°ìš¸ê¸°ë¥¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ ë¸Œë¡œë“œìºìŠ¤íŠ¸ë©ë‹ˆë‹¤.</span>\n        <span class=\"hljs-keyword\">return</span> [<span class=\"hljs-built_in\">float</span>(gradient.tensor.contents.data[<span class=\"hljs-number\">0</span>]) * self.<span class=\"hljs-built_in\">input</span>[<span class=\"hljs-number\">0</span>].ones_like()]\n</code></pre>\n<p>ë‹¤ë¥¸ ì—°ì‚°ì„ ì‚´í´ë³¼ ìˆ˜ ìˆëŠ” GitHub ì €ì¥ì†Œ ë§í¬ë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<p>ì´ì œ ê° ì‘ì—…ì— ëŒ€í•œ ë„í•¨ìˆ˜ ì‹ì„ ê°€ì¡Œìœ¼ë‹ˆ, ì¬ê·€ì ìœ¼ë¡œ ì—­ì „íŒŒ ì²´ì¸ ê·œì¹™ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ì„œì— requires_grad ì¸ìë¥¼ ì„¤ì •í•˜ì—¬ ì´ í…ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ì €ì¥í•˜ë ¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Trueì´ë©´ ê° í…ì„œ ì‘ì—…ì˜ ê¸°ìš¸ê¸°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># norch/tensor.py</span>\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__add__</span>(<span class=\"hljs-params\">self, other</span>):\n\n  <span class=\"hljs-keyword\">if</span> self.shape != other.shape:\n      <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">\"ë§ì…ˆì„ ìœ„í•´ í…ì„œëŠ” ë™ì¼í•œ ëª¨ì–‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\"</span>)\n  \n  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n  \n  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n  \n  result_data = Tensor()\n  result_data.tensor = result_tensor_ptr\n  result_data.shape = self.shape.copy()\n  result_data.ndim = self.ndim\n  result_data.device = self.device\n  \n  result_data.requires_grad = self.requires_grad <span class=\"hljs-keyword\">or</span> other.requires_grad\n  <span class=\"hljs-keyword\">if</span> result_data.requires_grad:\n      result_data.grad_fn = AddBackward(self, other)\n</code></pre>\n<p>ê·¸ëŸ¼, <code>.backward()</code> ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># norch/tensor.py</span>\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self, gradient=<span class=\"hljs-literal\">None</span></span>):\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> self.requires_grad:\n        <span class=\"hljs-keyword\">return</span>\n    \n    <span class=\"hljs-keyword\">if</span> gradient <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-keyword\">if</span> self.shape == [<span class=\"hljs-number\">1</span>]:\n            gradient = Tensor([<span class=\"hljs-number\">1</span>]) <span class=\"hljs-comment\"># dx/dx = 1 case</span>\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-keyword\">raise</span> RuntimeError(<span class=\"hljs-string\">\"Gradient argument must be specified for non-scalar tensors.\"</span>)\n\n    <span class=\"hljs-keyword\">if</span> self.grad <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n        self.grad = gradient\n\n    <span class=\"hljs-keyword\">else</span>:\n        self.grad += gradient\n\n    <span class=\"hljs-keyword\">if</span> self.grad_fn <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>: <span class=\"hljs-comment\"># not a leaf</span>\n        grads = self.grad_fn.backward(gradient) <span class=\"hljs-comment\"># call the operation backward</span>\n        <span class=\"hljs-keyword\">for</span> tensor, grad <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">zip</span>(self.grad_fn.<span class=\"hljs-built_in\">input</span>, grads):\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(tensor, Tensor):\n                tensor.backward(grad) <span class=\"hljs-comment\"># recursively call the backward again for the gradient expression (chain rule)</span>\n</code></pre>\n<p>ë§ˆì§€ë§‰ìœ¼ë¡œ, í…ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì œë¡œí™”í•˜ëŠ” <code>.zero_grad()</code>ì™€ í…ì„œì˜ ì˜¤í† ê·¸ë˜ë“œ íˆìŠ¤í† ë¦¬ë¥¼ ì œê±°í•˜ëŠ” <code>.detach()</code>ë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># norch/tensor.py</span>\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">zero_grad</span>(<span class=\"hljs-params\">self</span>):\n    self.grad = <span class=\"hljs-literal\">None</span>\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">detach</span>(<span class=\"hljs-params\">self</span>):\n    self.grad = <span class=\"hljs-literal\">None</span>\n    self.grad_fn = <span class=\"hljs-literal\">None</span>\n</code></pre>\n<p>ì¶•í•˜í•©ë‹ˆë‹¤! GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ì´ ìˆëŠ” ì™„ì „í•œ í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§Œë“œì…¨êµ°ìš”! ì´ì œ nn ë° optim ëª¨ë“ˆì„ ë§Œë“¤ì–´ ëª‡ ê°€ì§€ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ ë” ì‰½ê²Œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<h2>#4 â€” nn ë° optim ëª¨ë“ˆ</h2>\n<p>nnì€ ì‹ ê²½ë§ ë° ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ëª¨ë“ˆì´ë©°, optimì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì„ ì¬í˜„í•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” Parameterë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ParameterëŠ” ê°„ë‹¨íˆ ë§í•´ í•­ìƒ Trueë¡œ ì„¤ì •ëœ requires_grad ì†ì„±ì„ ê°–ëŠ” í›ˆë ¨ ê°€ëŠ¥í•œ í…ì„œë¡œ, ì¼ë¶€ ì„ì˜ì˜ ì´ˆê¸°í™” ê¸°ë²•ì„ ì‚¬ìš©í•´ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"># norch/nn/parameter.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">from</span> norch.<span class=\"hljs-property\">tensor</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Tensor</span>\n<span class=\"hljs-keyword\">from</span> norch.<span class=\"hljs-property\">utils</span> <span class=\"hljs-keyword\">import</span> utils\n<span class=\"hljs-keyword\">import</span> random\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Parameter</span>(<span class=\"hljs-title class_\">Tensor</span>):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    A parameter is a trainable tensor.\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    def <span class=\"hljs-title function_\">__init__</span>(self, shape):\n        data = utils.<span class=\"hljs-title function_\">generate_random_list</span>(shape=shape)\n        <span class=\"hljs-variable language_\">super</span>().<span class=\"hljs-title function_\">__init__</span>(data, requires_grad=<span class=\"hljs-title class_\">True</span>)\n</code></pre>\n<pre><code class=\"hljs language-js\"># norch/utisl/utils.<span class=\"hljs-property\">py</span>\n\ndef <span class=\"hljs-title function_\">generate_random_list</span>(shape):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    ëœë¤í•œ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ 'shape' í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n    [4, 2] --> [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">len</span>(shape) == <span class=\"hljs-number\">0</span>:\n        <span class=\"hljs-keyword\">return</span> []\n    <span class=\"hljs-attr\">else</span>:\n        inner_shape = shape[<span class=\"hljs-number\">1</span>:]\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">len</span>(inner_shape) == <span class=\"hljs-number\">0</span>:\n            <span class=\"hljs-keyword\">return</span> [random.<span class=\"hljs-title function_\">uniform</span>(-<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>) <span class=\"hljs-keyword\">for</span> _ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(shape[<span class=\"hljs-number\">0</span>])]\n        <span class=\"hljs-attr\">else</span>:\n            <span class=\"hljs-keyword\">return</span> [<span class=\"hljs-title function_\">generate_random_list</span>(inner_shape) <span class=\"hljs-keyword\">for</span> _ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(shape[<span class=\"hljs-number\">0</span>])]\n</code></pre>\n<p>íŒŒë¼ë¯¸í„°ë¥¼ í™œìš©í•˜ë©´ ëª¨ë“ˆì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"># norch/nn/<span class=\"hljs-variable language_\">module</span>.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">from</span> .<span class=\"hljs-property\">parameter</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Parameter</span>\n<span class=\"hljs-keyword\">from</span> collections <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">OrderedDict</span>\n<span class=\"hljs-keyword\">from</span> abc <span class=\"hljs-keyword\">import</span> <span class=\"hljs-variable constant_\">ABC</span>\n<span class=\"hljs-keyword\">import</span> inspect\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Module</span>(<span class=\"hljs-variable constant_\">ABC</span>):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    ëª¨ë“ˆì„ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤\n    \"</span><span class=\"hljs-string\">\"\"</span>\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n        self.<span class=\"hljs-property\">_modules</span> = <span class=\"hljs-title class_\">OrderedDict</span>()\n        self.<span class=\"hljs-property\">_params</span> = <span class=\"hljs-title class_\">OrderedDict</span>()\n        self.<span class=\"hljs-property\">_grads</span> = <span class=\"hljs-title class_\">OrderedDict</span>()\n        self.<span class=\"hljs-property\">training</span> = <span class=\"hljs-title class_\">True</span>\n\n    def <span class=\"hljs-title function_\">forward</span>(self, *inputs, **kwargs):\n        raise <span class=\"hljs-title class_\">NotImplementedError</span>\n\n    def <span class=\"hljs-title function_\">__call__</span>(self, *inputs, **kwargs):\n        <span class=\"hljs-keyword\">return</span> self.<span class=\"hljs-title function_\">forward</span>(*inputs, **kwargs)\n\n    def <span class=\"hljs-title function_\">train</span>(self):\n        self.<span class=\"hljs-property\">training</span> = <span class=\"hljs-title class_\">True</span>\n        <span class=\"hljs-keyword\">for</span> param <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-title function_\">parameters</span>():\n            param.<span class=\"hljs-property\">requires_grad</span> = <span class=\"hljs-title class_\">True</span>\n\n    def <span class=\"hljs-built_in\">eval</span>(self):\n        self.<span class=\"hljs-property\">training</span> = <span class=\"hljs-title class_\">False</span>\n        <span class=\"hljs-keyword\">for</span> param <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-title function_\">parameters</span>():\n            param.<span class=\"hljs-property\">requires_grad</span> = <span class=\"hljs-title class_\">False</span>\n\n    def <span class=\"hljs-title function_\">parameters</span>(self):\n        <span class=\"hljs-keyword\">for</span> name, value <span class=\"hljs-keyword\">in</span> inspect.<span class=\"hljs-title function_\">getmembers</span>(self):\n            <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">isinstance</span>(value, <span class=\"hljs-title class_\">Parameter</span>):\n                <span class=\"hljs-keyword\">yield</span> self, name, value\n            elif <span class=\"hljs-title function_\">isinstance</span>(value, <span class=\"hljs-title class_\">Module</span>):\n                <span class=\"hljs-keyword\">yield</span> <span class=\"hljs-keyword\">from</span> value.<span class=\"hljs-title function_\">parameters</span>()\n\n    def <span class=\"hljs-title function_\">modules</span>(self):\n        <span class=\"hljs-keyword\">yield</span> <span class=\"hljs-keyword\">from</span> self.<span class=\"hljs-property\">_modules</span>.<span class=\"hljs-title function_\">values</span>()\n\n    def <span class=\"hljs-title function_\">gradients</span>(self):\n        <span class=\"hljs-keyword\">for</span> <span class=\"hljs-variable language_\">module</span> <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-title function_\">modules</span>():\n            <span class=\"hljs-keyword\">yield</span> <span class=\"hljs-variable language_\">module</span>.<span class=\"hljs-property\">_grads</span>\n\n    def <span class=\"hljs-title function_\">zero_grad</span>(self):\n        <span class=\"hljs-keyword\">for</span> _, _, parameter <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-title function_\">parameters</span>():\n            parameter.<span class=\"hljs-title function_\">zero_grad</span>()\n\n    def <span class=\"hljs-title function_\">to</span>(self, device):\n        <span class=\"hljs-keyword\">for</span> _, _, parameter <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-title function_\">parameters</span>():\n            parameter.<span class=\"hljs-title function_\">to</span>(device)\n\n        <span class=\"hljs-keyword\">return</span> self\n    \n    def <span class=\"hljs-title function_\">inner_repr</span>(self):\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"\"</span>\n\n    def <span class=\"hljs-title function_\">__repr__</span>(self):\n        string = f<span class=\"hljs-string\">\"{self.get_name()}(\"</span>\n        tab = <span class=\"hljs-string\">\"   \"</span>\n        modules = self.<span class=\"hljs-property\">_modules</span>\n        <span class=\"hljs-keyword\">if</span> modules == {}:\n            string += f<span class=\"hljs-string\">'\\n{tab}(parameters): {self.inner_repr()}'</span>\n        <span class=\"hljs-attr\">else</span>:\n            <span class=\"hljs-keyword\">for</span> key, <span class=\"hljs-variable language_\">module</span> <span class=\"hljs-keyword\">in</span> modules.<span class=\"hljs-title function_\">items</span>():\n                string += f<span class=\"hljs-string\">\"\\n{tab}({key}): {module.get_name()}({module.inner_repr()})\"</span>\n        <span class=\"hljs-keyword\">return</span> f<span class=\"hljs-string\">'{string}\\n)'</span>\n    \n    def <span class=\"hljs-title function_\">get_name</span>(self):\n        <span class=\"hljs-keyword\">return</span> self.<span class=\"hljs-property\">__class__</span>.<span class=\"hljs-property\">__name__</span>\n    \n    def <span class=\"hljs-title function_\">__setattr__</span>(self, key, value):\n        self.<span class=\"hljs-property\">__dict__</span>[key] = value\n\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">isinstance</span>(value, <span class=\"hljs-title class_\">Module</span>):\n            self.<span class=\"hljs-property\">_modules</span>[key] = value\n        elif <span class=\"hljs-title function_\">isinstance</span>(value, <span class=\"hljs-title class_\">Parameter</span>):\n            self.<span class=\"hljs-property\">_params</span>[key] = value\n</code></pre>\n<p>ì˜ˆë¥¼ ë“¤ì–´, nn.Moduleì„ ìƒì†í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆì„ ë§Œë“¤ê±°ë‚˜, ì´ì „ì— ìƒì„±ëœ ëª¨ë“ˆ ì¤‘ í•˜ë‚˜ì¸ ì„ í˜• ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ y = Wx + b ì‘ì—…ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<pre><code class=\"hljs language-js\"># norch/nn/modules/linear.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">from</span> ..<span class=\"hljs-property\">module</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Module</span>\n<span class=\"hljs-keyword\">from</span> ..<span class=\"hljs-property\">parameter</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Parameter</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Linear</span>(<span class=\"hljs-title class_\">Module</span>):\n    def <span class=\"hljs-title function_\">__init__</span>(self, input_dim, output_dim):\n        <span class=\"hljs-variable language_\">super</span>().<span class=\"hljs-title function_\">__init__</span>()\n        self.<span class=\"hljs-property\">input_dim</span> = input_dim\n        self.<span class=\"hljs-property\">output_dim</span> = output_dim\n        self.<span class=\"hljs-property\">weight</span> = <span class=\"hljs-title class_\">Parameter</span>(shape=[self.<span class=\"hljs-property\">output_dim</span>, self.<span class=\"hljs-property\">input_dim</span>])\n        self.<span class=\"hljs-property\">bias</span> = <span class=\"hljs-title class_\">Parameter</span>(shape=[self.<span class=\"hljs-property\">output_dim</span>, <span class=\"hljs-number\">1</span>])\n\n    def <span class=\"hljs-title function_\">forward</span>(self, x):\n        z = self.<span class=\"hljs-property\">weight</span> @ x + self.<span class=\"hljs-property\">bias</span>\n        <span class=\"hljs-keyword\">return</span> z\n\n    def <span class=\"hljs-title function_\">inner_repr</span>(self):\n        <span class=\"hljs-keyword\">return</span> f<span class=\"hljs-string\">\"input_dim={self.input_dim}, output_dim={self.output_dim}, \"</span> \\\n               f<span class=\"hljs-string\">\"bias={True if self.bias is not None else False}\"</span>\n</code></pre>\n<p>ì´ì œ ëª‡ ê°€ì§€ ì†ì‹¤ ë° í™œì„±í™” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í‰ê·  ì œê³± ì˜¤ì°¨ ì†ì‹¤ ë° ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜:</p>\n<pre><code class=\"hljs language-js\"># norch/nn/loss.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">from</span> .<span class=\"hljs-property\">module</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Module</span>\n \n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MSELoss</span>(<span class=\"hljs-title class_\">Module</span>):\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n      pass\n\n    def <span class=\"hljs-title function_\">forward</span>(self, predictions, labels):\n        assert labels.<span class=\"hljs-property\">shape</span> == predictions.<span class=\"hljs-property\">shape</span>, \\\n            <span class=\"hljs-string\">\"Labels and predictions shape does not match: {} and {}\"</span>.<span class=\"hljs-title function_\">format</span>(labels.<span class=\"hljs-property\">shape</span>, predictions.<span class=\"hljs-property\">shape</span>)\n        \n        <span class=\"hljs-keyword\">return</span> ((predictions - labels) ** <span class=\"hljs-number\">2</span>).<span class=\"hljs-title function_\">sum</span>() / predictions.<span class=\"hljs-property\">numel</span>\n\n    def <span class=\"hljs-title function_\">__call__</span>(self, *inputs):\n        <span class=\"hljs-keyword\">return</span> self.<span class=\"hljs-title function_\">forward</span>(*inputs)\n</code></pre>\n<pre><code class=\"hljs language-js\"># norch/nn/activation.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">from</span> .<span class=\"hljs-property\">module</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Module</span>\n<span class=\"hljs-keyword\">import</span> math\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Sigmoid</span>(<span class=\"hljs-title class_\">Module</span>):\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n        <span class=\"hljs-variable language_\">super</span>().<span class=\"hljs-title function_\">__init__</span>()\n\n    def <span class=\"hljs-title function_\">forward</span>(self, x):\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">1.0</span> / (<span class=\"hljs-number\">1.0</span> + (math.<span class=\"hljs-property\">e</span>) ** (-x)) \n</code></pre>\n<p>ë§ˆì§€ë§‰ìœ¼ë¡œ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤. ì˜ˆì‹œë¡œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent) ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤:</p>\n<pre><code class=\"hljs language-js\"># norch/optim/optimizer.<span class=\"hljs-property\">py</span>\n\n<span class=\"hljs-keyword\">from</span> abc <span class=\"hljs-keyword\">import</span> <span class=\"hljs-variable constant_\">ABC</span>\n<span class=\"hljs-keyword\">from</span> norch.<span class=\"hljs-property\">tensor</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Tensor</span>\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Optimizer</span>(<span class=\"hljs-variable constant_\">ABC</span>):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤\n    \"</span><span class=\"hljs-string\">\"\"</span>\n\n    def <span class=\"hljs-title function_\">__init__</span>(self, parameters):\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">isinstance</span>(parameters, <span class=\"hljs-title class_\">Tensor</span>):\n            raise <span class=\"hljs-title class_\">TypeError</span>(<span class=\"hljs-string\">\"parametersëŠ” ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ì´ì–´ì•¼ í•˜ì§€ë§Œ {} íƒ€ì…ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤\"</span>.<span class=\"hljs-title function_\">format</span>(<span class=\"hljs-title function_\">type</span>(parameters)))\n        elif <span class=\"hljs-title function_\">isinstance</span>(parameters, dict):\n            parameters = parameters.<span class=\"hljs-title function_\">values</span>()\n\n        self.<span class=\"hljs-property\">parameters</span> = <span class=\"hljs-title function_\">list</span>(parameters)\n\n    def <span class=\"hljs-title function_\">step</span>(self):\n        raise <span class=\"hljs-title class_\">NotImplementedError</span>\n    \n    def <span class=\"hljs-title function_\">zero_grad</span>(self):\n        <span class=\"hljs-keyword\">for</span> <span class=\"hljs-variable language_\">module</span>, name, parameter <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-property\">parameters</span>:\n            parameter.<span class=\"hljs-title function_\">zero_grad</span>()\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">SGD</span>(<span class=\"hljs-title class_\">Optimizer</span>):\n    def <span class=\"hljs-title function_\">__init__</span>(self, parameters, lr=<span class=\"hljs-number\">1e-1</span>, momentum=<span class=\"hljs-number\">0</span>):\n        <span class=\"hljs-variable language_\">super</span>().<span class=\"hljs-title function_\">__init__</span>(parameters)\n        self.<span class=\"hljs-property\">lr</span> = lr\n        self.<span class=\"hljs-property\">momentum</span> = momentum\n        self.<span class=\"hljs-property\">_cache</span> = {<span class=\"hljs-string\">'velocity'</span>: [p.<span class=\"hljs-title function_\">zeros_like</span>() <span class=\"hljs-keyword\">for</span> (_, _, p) <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-property\">parameters</span>]}\n\n    def <span class=\"hljs-title function_\">step</span>(self):\n        <span class=\"hljs-keyword\">for</span> i, (<span class=\"hljs-variable language_\">module</span>, name, _) <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">enumerate</span>(self.<span class=\"hljs-property\">parameters</span>):\n            parameter = <span class=\"hljs-title function_\">getattr</span>(<span class=\"hljs-variable language_\">module</span>, name)\n\n            velocity = self.<span class=\"hljs-property\">_cache</span>[<span class=\"hljs-string\">'velocity'</span>][i]\n\n            velocity = self.<span class=\"hljs-property\">momentum</span> * velocity - self.<span class=\"hljs-property\">lr</span> * parameter.<span class=\"hljs-property\">grad</span>\n\n            updated_parameter = parameter + velocity\n\n            <span class=\"hljs-title function_\">setattr</span>(<span class=\"hljs-variable language_\">module</span>, name, updated_parameter)\n\n            self.<span class=\"hljs-property\">_cache</span>[<span class=\"hljs-string\">'velocity'</span>][i] = velocity\n\n            parameter.<span class=\"hljs-title function_\">detach</span>()\n            velocity.<span class=\"hljs-title function_\">detach</span>()\n</code></pre>\n<p>ê·¸ë¦¬ê³  ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤! ì´ì œ ìš°ë¦¬ë§Œì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ë§Œë“¤ì—ˆì–´ìš”! ğŸ¥³</p>\n<p>ì´ì œ í•™ìŠµì„ ì‹œì‘í•´ë´…ì‹œë‹¤:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> norch\n<span class=\"hljs-keyword\">import</span> norch.<span class=\"hljs-property\">nn</span> <span class=\"hljs-keyword\">as</span> nn\n<span class=\"hljs-keyword\">import</span> norch.<span class=\"hljs-property\">optim</span> <span class=\"hljs-keyword\">as</span> optim\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\">import</span> math\n\nrandom.<span class=\"hljs-title function_\">seed</span>(<span class=\"hljs-number\">1</span>)\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyModel</span>(nn.<span class=\"hljs-property\">Module</span>):\n    def <span class=\"hljs-title function_\">__init__</span>(self):\n        <span class=\"hljs-variable language_\">super</span>(<span class=\"hljs-title class_\">MyModel</span>, self).<span class=\"hljs-title function_\">__init__</span>()\n        self.<span class=\"hljs-property\">fc1</span> = nn.<span class=\"hljs-title class_\">Linear</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">10</span>)\n        self.<span class=\"hljs-property\">sigmoid</span> = nn.<span class=\"hljs-title class_\">Sigmoid</span>()\n        self.<span class=\"hljs-property\">fc2</span> = nn.<span class=\"hljs-title class_\">Linear</span>(<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">1</span>)\n\n    def <span class=\"hljs-title function_\">forward</span>(self, x):\n        out = self.<span class=\"hljs-title function_\">fc1</span>(x)\n        out = self.<span class=\"hljs-title function_\">sigmoid</span>(out)\n        out = self.<span class=\"hljs-title function_\">fc2</span>(out)\n        \n        <span class=\"hljs-keyword\">return</span> out\n\ndevice = <span class=\"hljs-string\">\"cuda\"</span>\nepochs = <span class=\"hljs-number\">10</span>\n\nmodel = <span class=\"hljs-title class_\">MyModel</span>().<span class=\"hljs-title function_\">to</span>(device)\ncriterion = nn.<span class=\"hljs-title class_\">MSELoss</span>()\noptimizer = optim.<span class=\"hljs-title function_\">SGD</span>(model.<span class=\"hljs-title function_\">parameters</span>(), lr=<span class=\"hljs-number\">0.001</span>)\nloss_list = []\n\nx_values = [<span class=\"hljs-number\">0.</span> ,  <span class=\"hljs-number\">0.4</span>,  <span class=\"hljs-number\">0.8</span>,  <span class=\"hljs-number\">1.2</span>,  <span class=\"hljs-number\">1.6</span>,  <span class=\"hljs-number\">2.</span> ,  <span class=\"hljs-number\">2.4</span>,  <span class=\"hljs-number\">2.8</span>,  <span class=\"hljs-number\">3.2</span>,  <span class=\"hljs-number\">3.6</span>,  <span class=\"hljs-number\">4.</span> ,\n        <span class=\"hljs-number\">4.4</span>,  <span class=\"hljs-number\">4.8</span>,  <span class=\"hljs-number\">5.2</span>,  <span class=\"hljs-number\">5.6</span>,  <span class=\"hljs-number\">6.</span> ,  <span class=\"hljs-number\">6.4</span>,  <span class=\"hljs-number\">6.8</span>,  <span class=\"hljs-number\">7.2</span>,  <span class=\"hljs-number\">7.6</span>,  <span class=\"hljs-number\">8.</span> ,  <span class=\"hljs-number\">8.4</span>,\n        <span class=\"hljs-number\">8.8</span>,  <span class=\"hljs-number\">9.2</span>,  <span class=\"hljs-number\">9.6</span>, <span class=\"hljs-number\">10.</span> , <span class=\"hljs-number\">10.4</span>, <span class=\"hljs-number\">10.8</span>, <span class=\"hljs-number\">11.2</span>, <span class=\"hljs-number\">11.6</span>, <span class=\"hljs-number\">12.</span> , <span class=\"hljs-number\">12.4</span>, <span class=\"hljs-number\">12.8</span>,\n       <span class=\"hljs-number\">13.2</span>, <span class=\"hljs-number\">13.6</span>, <span class=\"hljs-number\">14.</span> , <span class=\"hljs-number\">14.4</span>, <span class=\"hljs-number\">14.8</span>, <span class=\"hljs-number\">15.2</span>, <span class=\"hljs-number\">15.6</span>, <span class=\"hljs-number\">16.</span> , <span class=\"hljs-number\">16.4</span>, <span class=\"hljs-number\">16.8</span>, <span class=\"hljs-number\">17.2</span>,\n       <span class=\"hljs-number\">17.6</span>, <span class=\"hljs-number\">18.</span> , <span class=\"hljs-number\">18.4</span>, <span class=\"hljs-number\">18.8</span>, <span class=\"hljs-number\">19.2</span>, <span class=\"hljs-number\">19.6</span>, <span class=\"hljs-number\">20.</span>]\n\ny_true = []\n<span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">x_values</span>:\n    y_true.<span class=\"hljs-title function_\">append</span>(math.<span class=\"hljs-title function_\">pow</span>(math.<span class=\"hljs-title function_\">sin</span>(x), <span class=\"hljs-number\">2</span>))\n\n\n<span class=\"hljs-keyword\">for</span> epoch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(epochs):\n    <span class=\"hljs-keyword\">for</span> x, target <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">zip</span>(x_values, y_true):\n        x = norch.<span class=\"hljs-title class_\">Tensor</span>([[x]]).<span class=\"hljs-property\">T</span>\n        target = norch.<span class=\"hljs-title class_\">Tensor</span>([[target]]).<span class=\"hljs-property\">T</span>\n\n        x = x.<span class=\"hljs-title function_\">to</span>(device)\n        target = target.<span class=\"hljs-title function_\">to</span>(device)\n\n        outputs = <span class=\"hljs-title function_\">model</span>(x)\n        loss = <span class=\"hljs-title function_\">criterion</span>(outputs, target)\n        \n        optimizer.<span class=\"hljs-title function_\">zero_grad</span>()\n        loss.<span class=\"hljs-title function_\">backward</span>()\n        optimizer.<span class=\"hljs-title function_\">step</span>()\n\n    <span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}'</span>)\n    loss_list.<span class=\"hljs-title function_\">append</span>(loss[<span class=\"hljs-number\">0</span>])\n\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">1</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">1.7035</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">2</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.7193</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">3</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.3068</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">4</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.1742</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">5</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.1342</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">6</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.1232</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">7</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.1220</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">8</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.1241</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">9</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.1270</span>\n# <span class=\"hljs-title class_\">Epoch</span> [<span class=\"hljs-number\">10</span>/<span class=\"hljs-number\">10</span>], <span class=\"hljs-title class_\">Loss</span>: <span class=\"hljs-number\">0.1297</span>\n</code></pre>\n<p>ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±ë˜ê³  ì‚¬ìš©ì ì •ì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤!</p>\n<p>ì „ì²´ ì½”ë“œëŠ” ì—¬ê¸°ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>\n<h1>ê²°ë¡ </h1>\n<p>ì´ ê²Œì‹œë¬¼ì—ì„œëŠ” í…ì„œì™€ ê°™ì€ ê¸°ë³¸ ê°œë…, ì–´ë–»ê²Œ ëª¨ë¸ë§ë˜ëŠ”ì§€, CUDA ë° Autogradì™€ ê°™ì€ ê³ ê¸‰ ì£¼ì œ ë“±ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” GPU ì§€ì› ë° ìë™ ë¯¸ë¶„ì´ ê°€ëŠ¥í•œ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ ê²Œì‹œë¬¼ì´ ì—¬ëŸ¬ë¶„ì´ PyTorchê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê°„ëµíˆ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.</p>\n<p>ì•ìœ¼ë¡œì˜ ê²Œì‹œë¬¼ì—ì„œëŠ” ë¶„ì‚° í›ˆë ¨(ë‹¤ì¤‘ ë…¸ë“œ/ë‹¤ì¤‘ GPU) ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ê°™ì€ ê³ ê¸‰ ì£¼ì œë¥¼ ë‹¤ë£¨ë ¤ê³  í•  ê²ƒì…ë‹ˆë‹¤. ì˜ê²¬ì´ ìˆê±°ë‚˜ ë‹¤ìŒì— ì–´ë–¤ ë‚´ìš©ì„ ë‹¤ë£¨ê¸¸ ì›í•˜ì‹œëŠ”ì§€ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”! ì½ì–´ ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š</p>\n<p>ë˜í•œ ìµœì‹  ê¸°ì‚¬ë¥¼ ë°›ì•„ë³´ê¸° ìœ„í•´ ì—¬ê¸°ì™€ ì œ LinkedIn í”„ë¡œí•„ì—ì„œ íŒ”ë¡œìš°í•´ ì£¼ì„¸ìš”!</p>\n<h1>ì°¸ê³  ìë£Œ</h1>\n<ul>\n<li><a href=\"https://github.com\" rel=\"nofollow\" target=\"_blank\">PyNorch</a> - ì´ í”„ë¡œì íŠ¸ì˜ GitHub ì €ì¥ì†Œ</li>\n<li><a href=\"https://www.example.com/tutorial-cuda\" rel=\"nofollow\" target=\"_blank\">CUDA íŠœí† ë¦¬ì–¼</a> - CUDA ì‘ë™ ë°©ì‹ì— ëŒ€í•œ ê°„ë‹¨í•œ ì†Œê°œ</li>\n<li><a href=\"https://pytorch.org/docs\" rel=\"nofollow\" target=\"_blank\">PyTorch</a> - PyTorch ë¬¸ì„œ</li>\n</ul>\n<h1>MartinLwx's ë¸”ë¡œê·¸ - ìŠ¤íŠ¸ë¼ì´ë“œì— ê´€í•œ íŠœí† ë¦¬ì–¼.</h1>\n<h1>ìŠ¤íŠ¸ë¼ì´ë“œ íŠœí† ë¦¬ì–¼ - ìŠ¤íŠ¸ë¼ì´ë“œì— ê´€í•œ ë˜ ë‹¤ë¥¸ íŠœí† ë¦¬ì–¼.</h1>\n<h1>PyTorch ë‚´ë¶€ êµ¬ì¡° - PyTorch êµ¬ì¡°ì— ëŒ€í•œ ê°€ì´ë“œ.</h1>\n<h1>ë„¤ì¸  - NumPyë¥¼ ì‚¬ìš©í•œ PyTorch ì¬êµ¬í˜„.</h1>\n<p>Markdownìœ¼ë¡œ í‘œ íƒœê·¸ë¥¼ ë³€ê²½í•˜ì‹­ì‹œì˜¤.</p>\n</body>\n</html>\n"},"__N_SSG":true}