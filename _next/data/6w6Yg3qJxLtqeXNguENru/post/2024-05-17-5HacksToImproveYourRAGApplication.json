{"pageProps":{"post":{"title":"여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁","description":"","date":"2024-05-17 04:20","slug":"2024-05-17-5HacksToImproveYourRAGApplication","content":"\n\nRAG는 기업 및 비즈니스에서 Gen AI 기능을 사용자 지정 데이터와 통합하는 데 중요한 도구가 되었습니다.\n\n![image](/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png)\n\n다음은 RAG 애플리케이션을 개선하는 몇 가지 팁입니다.\n\n- 쿼리 보강\n- 문서 청킹\n- 결과 재랭킹\n- 임베딩 어댑터\n- 가상 문서 임베딩\n\n<div class=\"content-ad\"></div>\n\n쿼리 확장:\n\n관련 데이터를 검색하고 정확한 응답을 얻기 위해 프롬프트와 함께 보강하는 것이 중요합니다.\n\n단계:\n\n- 코사인 유사도나 유클리드 거리를 사용하여 벡터 임베딩 데이터베이스를 사용하여 사용자 쿼리를 기반으로 문서를 검색합니다.\n- 검색된 데이터/문서와 프롬프트를 결합합니다.\n- LLM(언어 모델)을 사용하여 하이브리드 데이터로부터 데이터를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nimport chromadb\nimport openai\n\ndef augmented_query_creator(user_query, retrieved_documents):\n    information = \"\\n\\n\".join(retrieved_documents)\n    prompt = (f'You are a movie critic.\\n'\n    f'Your users are asking questions about movie review.\\n'\n    f'You will be shown the user\\'s question, and the relevant information from the movie.\\n'\n    f'Answer the user\\'s question using only this information.\\n\\n'\n    f'Question: {query}. \\n Information: {information}')\n    return prompt\n\ndef generate_answer(prompt):\n    openai.api_key = \"YOUR_OPENAI_API_KEY\"  \n    response = openai.Completion.create(\n        engine=\"text-davinci-003\",  \n        prompt=prompt,\n        max_tokens=1024, \n        n=1,\n        stop=None,\n        temperature=0.7\n    )\n    return response.choices[0].text.strip()\n\nif __name__ == \"__main__\":\n    query = \"What is the review of the movie?\"\n    \n    # 1 Retrive relevant documents\n    results = chroma_collection.query(query_texts=[query], n_results=5)\n    retrieved_documents = results['documents'][0]\n    \n    # 2 Augmented query generation\n    augmented_query = augmented_query_creator(query,retrieved_documents)\n\n    # 3 Response for augmented query\n    result = generate_answer(augmented_query)\n```\n\n문서 청크 데이터 중복 :\n\n다양한 문서에 대한 벡터 데이터베이스를 구축할 때, 토큰 제한으로 인해 데이터 손실이 발생할 수 있습니다. 이 문제를 해결하기 위해 데이터를 작은 세그먼트로 분할하는 것이 해결책입니다.\n\n하지만 이러한 청크를 사용하더라도 한 문서와 다른 문서 사이의 의미와 연속성 손실이 발생할 수 있습니다. 이 문제를 완화하기 위해 데이터의 일관성과 흐름을 유지하기 위해 청크 사이에 중첩을 도입하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings  # 귀하의 선택한 임베딩 모델로 대체하십시오\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import TextLoader\n\n# 문서 경로와 임베딩 모델 정의 (귀하의 것으로 대체하십시오)\ndocument_path = \"your_document.txt\"\nembedding_model = OpenAIEmbeddings\n\n# 청크 크기 및 선택적인 오버랩 설정\nchunk_size = 500\nchunk_overlap = 100\n\n# 문서를 로드하고 RecursiveCharacterTextSplitter로 분할합니다.\ntext_loader = TextLoader(document_path)\ndocuments = text_loader.load()\nsplitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\ntexts = splitter.split_documents(documents)\n\n# 임베딩을 사용하여 ChromaDB 생성\npersist_directory = \"chroma_db\"\nchroma_collection = Chroma.from_documents(\n    documents=texts, embedding=embedding_model(), persist_directory=persist_directory\n)\n```\n\n재랭킹\n\n결과를 재랭킹하는 것은 검색된 문서를 검색기에 의해 검색된 후 특정 기준에 따라 다시 정렬하는 것을 의미합니다. 응답을 생성하기 전에 검색된 문서의 관련성을 더욱 정제하는 데 유용할 수 있습니다.\n\n우리는 문서의 관련성 순서를 변경하기 위해 코사인 유사성 대신 크로스 인코더 모델을 사용합니다.```\n\n<div class=\"content-ad\"></div>\n\nStep 1: 크로스-인코더 모델을 로드합니다.\n\nStep 2: 관련성 점수를 변경하는 재랭크 함수입니다.\n\nStep 3: 문서를 정렬하고 반환합니다.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\n# ChromaDB와 크로스-인코더 모델을 로드합니다.\nchromadb = Chroma.load(\"chroma_db\")  \ncross_encoder = SentenceTransformer(\"all-mpnet-base-v2\")  \n\ndef re_rank_results(query, retrieved_chunks, k=3):\n  \n  scored_chunks = []\n  for chunk in retrieved_chunks:\n    score = cross_encoder.compute_similarity([query], [chunk])[0][0]\n    scored_chunks.append({\"chunk\": chunk, \"score\": score})\n\n  # 점수를 기준으로 (내림차순으로) 정렬하고 상위 k개 결과를 반환합니다.\n  sorted_chunks = sorted(scored_chunks, key=lambda x: x[\"score\"], reverse=True)\n  return sorted_chunks[:k]\n```\n\n<div class=\"content-ad\"></div>\n\n임베딩 어댑터:\n\n임베딩 어댑터는 초기 임베딩 프로세스와 검색 단계 간의 세세한 조정 단계로 작용하는 소규모 신경망 모듈입니다. 그 목적은 쿼리의 임베딩과 지식베이스에 저장된 문서 표현의 정렬을 개선하는 것입니다.\n\n단계 1: 임베딩 생성\n\n단계 2: 임베딩 어댑터로 섬세하게 조정하기\n\n<div class=\"content-ad\"></div>\n\n3단계: 개선된 검색\n\n```js\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_embeddings import SentenceTransformerEmbeddings\nfrom langchain.text_encoders import IdentityEncoder  # 원본 텍스트 보존\nfrom langchain.document_loaders import TextLoader\n\n# 문서 경로 및 임베딩 모델 정의\ndocument_path = \"your_document.txt\"\nembedding_model = SentenceTransformerEmbeddings(\"all-mpnet-base-v2\")\n\n# 문서 로드\ntext_loader = TextLoader(document_path)\ndocuments = text_loader.load()\n\n# 텍스트 인코더를 사용하여 ChromaDB 생성 (선택 사항)\npersist_directory = \"chroma_db\"  \ntext_encoder = IdentityEncoder()  \n\nvectordb = Chroma.from_documents(\n    documents=documents,\n    embedding=embedding_model(),\n    text_encoder=text_encoder,\n    persist_directory=persist_directory\n)\n\n# 선택적 지속성\nvectordb.persist()\n\n# 텍스트 검색 예시\nquery = \"북극 해는 어디에 있나요?\"\n\n# 인코딩된 텍스트를 기반으로 검색 (임베딩 아님)\nresults = vectordb.search(query, k=5)  # 상위 5개 결과 가져오기\n\n# 검색된 문서 출력\nfor doc in results:\n    print(doc)\n\nprint(\"ChromaDB 검색 완료!\")\n```\n\n가상 문서 임베딩:\n\nHyDE는 대형 언어 모델(Large Language Models, LLMs)을 활용하여 문서로부터 정보 검색을 개선하는 기술입니다.\n\n<div class=\"content-ad\"></div>\n\nStep 1: Query 이해하기: 모든 것은 사용자 쿼리로 시작됩니다. HyDE는 이 쿼리를 입력으로 받습니다.\n\nStep 2: 가상 문서 생성: HyDE는 GPT-3과 같은 LLM을 사용하여 사용자 쿼리에 완벽한 답변이 될 것으로 믿는 가상 문서를 생성합니다. 이 문서에는 사실적인 정보 뿐만 아니라 창의적인 요소나 사용자 의도에 부합하는 설명이 포함될 수 있습니다.\n\nStep 3: 가설 인코딩: 가상 문서가 생성된 후, HyDE는 문서 자체를 사용하지 않습니다. 대신, 문서의 의미를 수학적 벡터 표현으로 인코딩합니다. 이 벡터는 가상 답변 내의 핵심 개념과 정보를 포착합니다.\n\nStep 4: 유사 문서 찾기: 이제 검색 과정이 시작됩니다. HyDE는 가상 문서를 나타내는 벡터를 사용하여 방대한 문서 컬렉션(보통 미리 인코딩된)을 검색합니다. 이것은 가상 문서의 벡터와 유사한 실제 문서를 탐색합니다. 유사성은 이 실제 문서들이 가상 답변과 유사한 방법으로 사용자 쿼리에 대응한다는 것을 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n5단계: 검색된 문서를 활용하기: HyDE 프로세스를 기반으로 가장 관련성 높은 것으로 간주된 이러한 검색된 문서는 이후 RAG 시스템에 공급됩니다. RAG 내의 LLM은 이 문서들을 사용하여 사용자의 초기 쿼리에 대한 더 포괄적이고 유익한 응답을 생성할 수 있습니다.\n\n참고 자료:\n\n- https://platform.openai.com/docs/assistants/overview\n- LinkedIn GitHub","ogImage":{"url":"/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png"},"coverImage":"/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png","tag":["Tech"],"readingTime":6},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>RAG는 기업 및 비즈니스에서 Gen AI 기능을 사용자 지정 데이터와 통합하는 데 중요한 도구가 되었습니다.</p>\n<p><img src=\"/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png\" alt=\"image\"></p>\n<p>다음은 RAG 애플리케이션을 개선하는 몇 가지 팁입니다.</p>\n<ul>\n<li>쿼리 보강</li>\n<li>문서 청킹</li>\n<li>결과 재랭킹</li>\n<li>임베딩 어댑터</li>\n<li>가상 문서 임베딩</li>\n</ul>\n<p>쿼리 확장:</p>\n<p>관련 데이터를 검색하고 정확한 응답을 얻기 위해 프롬프트와 함께 보강하는 것이 중요합니다.</p>\n<p>단계:</p>\n<ul>\n<li>코사인 유사도나 유클리드 거리를 사용하여 벡터 임베딩 데이터베이스를 사용하여 사용자 쿼리를 기반으로 문서를 검색합니다.</li>\n<li>검색된 데이터/문서와 프롬프트를 결합합니다.</li>\n<li>LLM(언어 모델)을 사용하여 하이브리드 데이터로부터 데이터를 생성합니다.</li>\n</ul>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> chromadb\n<span class=\"hljs-keyword\">import</span> openai\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">augmented_query_creator</span>(<span class=\"hljs-params\">user_query, retrieved_documents</span>):\n    information = <span class=\"hljs-string\">\"\\n\\n\"</span>.join(retrieved_documents)\n    prompt = (<span class=\"hljs-string\">f'You are a movie critic.\\n'</span>\n    <span class=\"hljs-string\">f'Your users are asking questions about movie review.\\n'</span>\n    <span class=\"hljs-string\">f'You will be shown the user\\'s question, and the relevant information from the movie.\\n'</span>\n    <span class=\"hljs-string\">f'Answer the user\\'s question using only this information.\\n\\n'</span>\n    <span class=\"hljs-string\">f'Question: <span class=\"hljs-subst\">{query}</span>. \\n Information: <span class=\"hljs-subst\">{information}</span>'</span>)\n    <span class=\"hljs-keyword\">return</span> prompt\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">generate_answer</span>(<span class=\"hljs-params\">prompt</span>):\n    openai.api_key = <span class=\"hljs-string\">\"YOUR_OPENAI_API_KEY\"</span>  \n    response = openai.Completion.create(\n        engine=<span class=\"hljs-string\">\"text-davinci-003\"</span>,  \n        prompt=prompt,\n        max_tokens=<span class=\"hljs-number\">1024</span>, \n        n=<span class=\"hljs-number\">1</span>,\n        stop=<span class=\"hljs-literal\">None</span>,\n        temperature=<span class=\"hljs-number\">0.7</span>\n    )\n    <span class=\"hljs-keyword\">return</span> response.choices[<span class=\"hljs-number\">0</span>].text.strip()\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n    query = <span class=\"hljs-string\">\"What is the review of the movie?\"</span>\n    \n    <span class=\"hljs-comment\"># 1 Retrive relevant documents</span>\n    results = chroma_collection.query(query_texts=[query], n_results=<span class=\"hljs-number\">5</span>)\n    retrieved_documents = results[<span class=\"hljs-string\">'documents'</span>][<span class=\"hljs-number\">0</span>]\n    \n    <span class=\"hljs-comment\"># 2 Augmented query generation</span>\n    augmented_query = augmented_query_creator(query,retrieved_documents)\n\n    <span class=\"hljs-comment\"># 3 Response for augmented query</span>\n    result = generate_answer(augmented_query)\n</code></pre>\n<p>문서 청크 데이터 중복 :</p>\n<p>다양한 문서에 대한 벡터 데이터베이스를 구축할 때, 토큰 제한으로 인해 데이터 손실이 발생할 수 있습니다. 이 문제를 해결하기 위해 데이터를 작은 세그먼트로 분할하는 것이 해결책입니다.</p>\n<p>하지만 이러한 청크를 사용하더라도 한 문서와 다른 문서 사이의 의미와 연속성 손실이 발생할 수 있습니다. 이 문제를 완화하기 위해 데이터의 일관성과 흐름을 유지하기 위해 청크 사이에 중첩을 도입하는 것이 중요합니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">vectorstores</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Chroma</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">embeddings</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">OpenAIEmbeddings</span>  # 귀하의 선택한 임베딩 모델로 대체하십시오\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">text_splitter</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">RecursiveCharacterTextSplitter</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">document_loaders</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">TextLoader</span>\n\n# 문서 경로와 임베딩 모델 정의 (귀하의 것으로 대체하십시오)\ndocument_path = <span class=\"hljs-string\">\"your_document.txt\"</span>\nembedding_model = <span class=\"hljs-title class_\">OpenAIEmbeddings</span>\n\n# 청크 크기 및 선택적인 오버랩 설정\nchunk_size = <span class=\"hljs-number\">500</span>\nchunk_overlap = <span class=\"hljs-number\">100</span>\n\n# 문서를 로드하고 <span class=\"hljs-title class_\">RecursiveCharacterTextSplitter</span>로 분할합니다.\ntext_loader = <span class=\"hljs-title class_\">TextLoader</span>(document_path)\ndocuments = text_loader.<span class=\"hljs-title function_\">load</span>()\nsplitter = <span class=\"hljs-title class_\">RecursiveCharacterTextSplitter</span>(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\ntexts = splitter.<span class=\"hljs-title function_\">split_documents</span>(documents)\n\n# 임베딩을 사용하여 <span class=\"hljs-title class_\">ChromaDB</span> 생성\npersist_directory = <span class=\"hljs-string\">\"chroma_db\"</span>\nchroma_collection = <span class=\"hljs-title class_\">Chroma</span>.<span class=\"hljs-title function_\">from_documents</span>(\n    documents=texts, embedding=<span class=\"hljs-title function_\">embedding_model</span>(), persist_directory=persist_directory\n)\n</code></pre>\n<p>재랭킹</p>\n<p>결과를 재랭킹하는 것은 검색된 문서를 검색기에 의해 검색된 후 특정 기준에 따라 다시 정렬하는 것을 의미합니다. 응답을 생성하기 전에 검색된 문서의 관련성을 더욱 정제하는 데 유용할 수 있습니다.</p>\n<p>우리는 문서의 관련성 순서를 변경하기 위해 코사인 유사성 대신 크로스 인코더 모델을 사용합니다.```</p>\n<p>Step 1: 크로스-인코더 모델을 로드합니다.</p>\n<p>Step 2: 관련성 점수를 변경하는 재랭크 함수입니다.</p>\n<p>Step 3: 문서를 정렬하고 반환합니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> sentence_transformers <span class=\"hljs-keyword\">import</span> SentenceTransformer\n\n<span class=\"hljs-comment\"># ChromaDB와 크로스-인코더 모델을 로드합니다.</span>\nchromadb = Chroma.load(<span class=\"hljs-string\">\"chroma_db\"</span>)  \ncross_encoder = SentenceTransformer(<span class=\"hljs-string\">\"all-mpnet-base-v2\"</span>)  \n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">re_rank_results</span>(<span class=\"hljs-params\">query, retrieved_chunks, k=<span class=\"hljs-number\">3</span></span>):\n  \n  scored_chunks = []\n  <span class=\"hljs-keyword\">for</span> chunk <span class=\"hljs-keyword\">in</span> retrieved_chunks:\n    score = cross_encoder.compute_similarity([query], [chunk])[<span class=\"hljs-number\">0</span>][<span class=\"hljs-number\">0</span>]\n    scored_chunks.append({<span class=\"hljs-string\">\"chunk\"</span>: chunk, <span class=\"hljs-string\">\"score\"</span>: score})\n\n  <span class=\"hljs-comment\"># 점수를 기준으로 (내림차순으로) 정렬하고 상위 k개 결과를 반환합니다.</span>\n  sorted_chunks = <span class=\"hljs-built_in\">sorted</span>(scored_chunks, key=<span class=\"hljs-keyword\">lambda</span> x: x[<span class=\"hljs-string\">\"score\"</span>], reverse=<span class=\"hljs-literal\">True</span>)\n  <span class=\"hljs-keyword\">return</span> sorted_chunks[:k]\n</code></pre>\n<p>임베딩 어댑터:</p>\n<p>임베딩 어댑터는 초기 임베딩 프로세스와 검색 단계 간의 세세한 조정 단계로 작용하는 소규모 신경망 모듈입니다. 그 목적은 쿼리의 임베딩과 지식베이스에 저장된 문서 표현의 정렬을 개선하는 것입니다.</p>\n<p>단계 1: 임베딩 생성</p>\n<p>단계 2: 임베딩 어댑터로 섬세하게 조정하기</p>\n<p>3단계: 개선된 검색</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">vectorstores</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Chroma</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">text_embeddings</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SentenceTransformerEmbeddings</span>\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">text_encoders</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">IdentityEncoder</span>  # 원본 텍스트 보존\n<span class=\"hljs-keyword\">from</span> langchain.<span class=\"hljs-property\">document_loaders</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">TextLoader</span>\n\n# 문서 경로 및 임베딩 모델 정의\ndocument_path = <span class=\"hljs-string\">\"your_document.txt\"</span>\nembedding_model = <span class=\"hljs-title class_\">SentenceTransformerEmbeddings</span>(<span class=\"hljs-string\">\"all-mpnet-base-v2\"</span>)\n\n# 문서 로드\ntext_loader = <span class=\"hljs-title class_\">TextLoader</span>(document_path)\ndocuments = text_loader.<span class=\"hljs-title function_\">load</span>()\n\n# 텍스트 인코더를 사용하여 <span class=\"hljs-title class_\">ChromaDB</span> 생성 (선택 사항)\npersist_directory = <span class=\"hljs-string\">\"chroma_db\"</span>  \ntext_encoder = <span class=\"hljs-title class_\">IdentityEncoder</span>()  \n\nvectordb = <span class=\"hljs-title class_\">Chroma</span>.<span class=\"hljs-title function_\">from_documents</span>(\n    documents=documents,\n    embedding=<span class=\"hljs-title function_\">embedding_model</span>(),\n    text_encoder=text_encoder,\n    persist_directory=persist_directory\n)\n\n# 선택적 지속성\nvectordb.<span class=\"hljs-title function_\">persist</span>()\n\n# 텍스트 검색 예시\nquery = <span class=\"hljs-string\">\"북극 해는 어디에 있나요?\"</span>\n\n# 인코딩된 텍스트를 기반으로 검색 (임베딩 아님)\nresults = vectordb.<span class=\"hljs-title function_\">search</span>(query, k=<span class=\"hljs-number\">5</span>)  # 상위 <span class=\"hljs-number\">5</span>개 결과 가져오기\n\n# 검색된 문서 출력\n<span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">results</span>:\n    <span class=\"hljs-title function_\">print</span>(doc)\n\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">\"ChromaDB 검색 완료!\"</span>)\n</code></pre>\n<p>가상 문서 임베딩:</p>\n<p>HyDE는 대형 언어 모델(Large Language Models, LLMs)을 활용하여 문서로부터 정보 검색을 개선하는 기술입니다.</p>\n<p>Step 1: Query 이해하기: 모든 것은 사용자 쿼리로 시작됩니다. HyDE는 이 쿼리를 입력으로 받습니다.</p>\n<p>Step 2: 가상 문서 생성: HyDE는 GPT-3과 같은 LLM을 사용하여 사용자 쿼리에 완벽한 답변이 될 것으로 믿는 가상 문서를 생성합니다. 이 문서에는 사실적인 정보 뿐만 아니라 창의적인 요소나 사용자 의도에 부합하는 설명이 포함될 수 있습니다.</p>\n<p>Step 3: 가설 인코딩: 가상 문서가 생성된 후, HyDE는 문서 자체를 사용하지 않습니다. 대신, 문서의 의미를 수학적 벡터 표현으로 인코딩합니다. 이 벡터는 가상 답변 내의 핵심 개념과 정보를 포착합니다.</p>\n<p>Step 4: 유사 문서 찾기: 이제 검색 과정이 시작됩니다. HyDE는 가상 문서를 나타내는 벡터를 사용하여 방대한 문서 컬렉션(보통 미리 인코딩된)을 검색합니다. 이것은 가상 문서의 벡터와 유사한 실제 문서를 탐색합니다. 유사성은 이 실제 문서들이 가상 답변과 유사한 방법으로 사용자 쿼리에 대응한다는 것을 나타냅니다.</p>\n<p>5단계: 검색된 문서를 활용하기: HyDE 프로세스를 기반으로 가장 관련성 높은 것으로 간주된 이러한 검색된 문서는 이후 RAG 시스템에 공급됩니다. RAG 내의 LLM은 이 문서들을 사용하여 사용자의 초기 쿼리에 대한 더 포괄적이고 유익한 응답을 생성할 수 있습니다.</p>\n<p>참고 자료:</p>\n<ul>\n<li><a href=\"https://platform.openai.com/docs/assistants/overview\" rel=\"nofollow\" target=\"_blank\">https://platform.openai.com/docs/assistants/overview</a></li>\n<li>LinkedIn GitHub</li>\n</ul>\n</body>\n</html>\n"},"__N_SSG":true}