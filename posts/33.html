<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>allround-coder</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///posts/33" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="allround-coder" data-gatsby-head="true"/><meta property="og:title" content="allround-coder" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///posts/33" data-gatsby-head="true"/><meta name="twitter:title" content="allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/960f1fe994a0ab5c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/960f1fe994a0ab5c.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-b692b09f2b5275a4.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/6w6Yg3qJxLtqeXNguENru/_buildManifest.js" defer=""></script><script src="/_next/static/6w6Yg3qJxLtqeXNguENru/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="RocksDB를 사용한 서버 부팅 속도 최적화" href="/post/2024-05-16-ServerbootstrapoptimizationusingRocksDB"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="RocksDB를 사용한 서버 부팅 속도 최적화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="RocksDB를 사용한 서버 부팅 속도 최적화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">RocksDB를 사용한 서버 부팅 속도 최적화</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로" href="/post/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="흑자 블랙 바스타의 코발트 스트라이크를 추적하기" href="/post/2024-05-15-HuntingBlackBastasCobaltStrike"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="흑자 블랙 바스타의 코발트 스트라이크를 추적하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="흑자 블랙 바스타의 코발트 스트라이크를 추적하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">흑자 블랙 바스타의 코발트 스트라이크를 추적하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지" href="/post/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="마이크로소프트 Phi-3-Mini는 정말 인상적이에요" href="/post/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="마이크로소프트 Phi-3-Mini는 정말 인상적이에요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="마이크로소프트 Phi-3-Mini는 정말 인상적이에요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">마이크로소프트 Phi-3-Mini는 정말 인상적이에요</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN" href="/post/2024-05-15-InterpretablekNNikNN"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-InterpretablekNNikNN_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="안녕하세요, 월드, OpenAI GPT-4o" href="/post/2024-05-15-HelloWorldtoOpenAIGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="안녕하세요, 월드, OpenAI GPT-4o" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="안녕하세요, 월드, OpenAI GPT-4o" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">안녕하세요, 월드, OpenAI GPT-4o</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다" href="/post/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="콜모고로프-아놀드 네트워크 신경망 발전의 포괄적 가이드" href="/post/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="콜모고로프-아놀드 네트워크 신경망 발전의 포괄적 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="콜모고로프-아놀드 네트워크 신경망 발전의 포괄적 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">콜모고로프-아놀드 네트워크 신경망 발전의 포괄적 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇 조립의 미래" href="/post/2024-05-15-TheFutureofRoboticAssembly"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇 조립의 미래" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-TheFutureofRoboticAssembly_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇 조립의 미래" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">로봇 조립의 미래</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link posts_-active__YVJEi" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"RocksDB를 사용한 서버 부팅 속도 최적화","description":"","date":"2024-05-16 03:02","slug":"2024-05-16-ServerbootstrapoptimizationusingRocksDB","content":"\n\n# 소개\n\nFlipkart의 검색 인프라에서 Mustang은 SOLR 인덱스를 관리하는 필수 서비스입니다. 현재, 우리는 Flipkart, Grocery, Hyperlocal 및 Shopsy와 같은 다양한 비즈니스 단위에 대응하는 다양한 샤드에서 운영하고 있습니다. 각 샤드는 해당 샤드로 직접 전송되는 데이터 양과 요청 양에 따라 결정된 다양한 복제본을 호스팅합니다.\n\n각 복제본에는 디스크에 저장된 데이터(제품 관련 데이터, SOLR에서 제공)와 판매자별 목록 데이터에 대한 메모리에 저장된 데이터가 포함되어 있습니다. 빠르게 변화하는 속성에 대한 NRT(Near Real-Time) 데이터로, 애플리케이션 구동 시 중앙 집중식 Redis 클러스터에서 데이터를 가져와 메모리에 있는 데이터 구조체가 구축됩니다. 이러한 메모리에 있는 데이터 구조체는 Kafka 파이프라인을 통해 업데이트되어 Redis와 동기화되어 유지됩니다.\n\n![이미지](/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png)\n\n\n\n평균적으로 각 복제본은 약 15 백만 개의 리스트에 대한 데이터를 보유합니다. 부팅 중에 이러한 인메모리 데이터 구조를 구축하는 프로세스에는 약 30~40분이 소요됩니다. 이 프로세스에서 주요 병목 현상은 Redis인데, 배포 중 동시 요청의 증가를 처리하는 데 어려움을 겪습니다 (이 클러스터의 크기가 각 데이터 센터에 거의 400대의 가상 머신에 가깝기 때문).\n\n이 전체 절차는 배포를 크게 늦추어 최소 2일 이상 소요됩니다. 개발자의 생산성뿐만 아니라 적시에 버그 수정을 배포하는 데도 어려움을 겪게 됩니다.\n\n이 블로그에서는 RocksDB를 사용하여 Mustang의 부팅 시간을 최적화하는 방법에 대해 논의합니다.\n\n# 문제 분석\n\n\n\n우리 Redis 클러스터는 Mustang 서버가 다시 시작될 때마다 막혔었습니다. 10%의 롤아웃 요인조차도 약 40개의 Mustang 서버가 Redis에 대해 300K 이상의 동시 호출을 하는 결과를 가져왔습니다. 이러한 대규모 동시 요청의 이유는 각 서버의 폴러 스레드 수 및 배치 크기 때문이었습니다.\n\n또한, Redis에서 목록에 대한 데이터를 가져오는 것이 단순한 Redis GET 작업이 아니었습니다. 우리는 Redis에서 목록 POJO를 작성하는 논리를 추상화한 라이브러리를 작성했는데, 이 라이브러리는 다중 동시 호출을 통해 각 속성의 데이터를 얻기 위해 내부적으로 Redis에 대한 요청을 여러 번 보내고 이를 병합하여 단일 POJO를 생성합니다.\n\n예를 들어, 목록과 관련된 제공은 Redis 내에 SET으로 저장되었으며, 서비스 가능 영역의 가용성 데이터는 BITFIELD로 저장되었습니다. 두 데이터를 가져오려면 Redis에 대해 구별된 쿼리가 필요했으며, 응답 구문 분석도 그에 따라 달랐습니다.\n\n문제점을 찾기 위해 아래에서 위로 가는 방식으로 병목 현상을 탐색하는 것은 좋은 선택처럼 보였습니다. 따라서 우리는 Redis부터 시작했습니다.\n\n\n\n# Redis 튜닝\n\n각 애플리케이션 서버의 폴러 스레드의 일괄 처리 크기와 스레드 수를 조정해 보았지만 전반적인 성능에는 개선이 없었습니다. 각 일괄 처리의 지연 시간은 조금 줄었지만 전체적으로 처리해야 하는 일괄 처리가 더 많아져 얻는 이점이 상쇄되었습니다.\n\n사용되지 않는 몇 가지 상품 속성을 찾았지만 여전히 상품 POJO의 일부였습니다. 이러한 속성을 POJO에서 제거하니 성능이 조금 향상되었지만 충분하지는 않았습니다.\n\nRedis 클러스터의 각 샤드에 레플리카 수를 늘리는 옵션도 검토했습니다. 이를 통해 더 나은 부하 분산을 얻을 수 있지만 클러스터가 대부분 유휴 상태이고 Mustang 배포 중에만 사용되기 때문에 실용적이지 않았습니다. 여기에 더 많은 리소스를 추가하는 것에 가치가 없다고 판단했습니다.\n\n\n\nRedis를 더 최적화하는 것이 어려워서, 우리는 응용 프로그램의 서버 쪽 옵션을 탐색했어요.\n\n# 파일 캐시 생성\n\n아이디어는 Redis로부터 데이터를 한 번만 검색하여 인메모리 데이터 구조를 구축하고, 그 후에는 미래 배포를 위해 로컬로 캐시하는 것이었습니다.\n\n같은 작업을 위한 빠른 POC를 진행하기 위해 우리는 셧다운 후크를 작성했어요. 이 후크는 모든 인메모리 데이터 구조를 로컬 디스크의 별도 파일에 직렬화할 거에요. 시작 시, 응용 프로그램은 로컬에 저장된 데이터를 역직렬화하고 인메모리 데이터 구조를 로드할 거예요.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_1.png\" /\u003e\n\n시작할 때는 아이디어가 유망해 보였지만 나중에 다양한 문제가 발생했습니다. 이 중 일부는 아래와 같습니다:\n\n- 저희의 인메모리 데이터 구조는 사실상 코드의 다른 세그먼트(그리고 메모리에서)입니다. 그러나 이들은 모두 상품 목록 데이터에서 파생되었습니다. 한 세그먼트의 직렬화 중에 문제가 발생하면 해당 세그먼트를 다시 불러오지 않고는 복구할 수 없습니다. 이는 단 하나의 파일이 손상되어도 직렬화된 데이터를 모두 폐기하고 Redis로부터 전체 데이터를 다시 부트스트랩해야 한다는 것을 의미합니다.\n- 상품 목록 데이터는 각 샤드마다 동일하지만, 이러한 인메모리 데이터 구조는 응용 프로그램 서버 간에 심지어 같은 샤드 내에서도 다를 수 있습니다. 이 차이는 이러한 데이터 구조 내에서 상품 목록을 식별할 때 사용하는 서수의 무작위적인 특성에서 비롯됩니다. 이 상품 목록 서수는 SOLR 색인 파일을 비동기적으로 로드하는 프로세스 동안 먼저 도착한 사람이 우선적으로 생성되기 때문에 결정론적이지 않습니다. 따라서 동일한 샤드의 다른 복제본들 사이에서 직렬화된 파일을 공유하는 것이 불가능합니다.\n- 코드가 지저분해 보였던 이유는 직렬화 및 역직렬화를 위해 Jackson을 사용했기 때문입니다. Jackson은 우리 코드에서 정상적으로 작동하기 위해 특정 getter 및 setter를 필요로 합니다. 이는 특히 상속을 다룰 때 또는 간단히 속성을 반환하는 대신 사용자 정의 논리를 포함하는 getter가 이미 있는 경우에 복잡함을 야기했습니다.\n\n이러한 제약으로 인해 더 견고하고 우아한 솔루션이 필요했습니다. POC를 기반으로 내장 데이터베이스 접근 방식이 유망해 보였고 우리는 결국 RocksDB를 선택하게 되었습니다.\n\n\n\n# RocksDB가 당신을 구해줍니다\n\n## 왜 RocksDB를 선택했나요?\n\n우리가 RocksDB를 선택한 이유를 이해해 봅시다.\n\n- 이는 내장형 데이터베이스입니다. 이는 중앙 서버에서 실행할 필요가 없음을 의미합니다. 라이브러리로 직접 코드에 사용할 수 있습니다. 우리는 중앙 집중형 솔루션에서 벗어나려고 했기 때문에, 이것은 우리에게 완벽했습니다.\n- 당신의 요구에 맞게 튜닝할 수 있는 구성 옵션을 제공합니다.\n- 다양한 작업 부하에서 테스트할 때 유망한 결과를 보았으며, 이는 그 성능에 대한 우리의 신뢰를 높였습니다.\n- 이는 매우 인기 있는 데이터베이스이며 다양한 산업에서 사용되고 있습니다 (X가 비슷한 문제를 해결하기 위해 어떻게 사용하는지, Cloudflare가 어떻게 사용하는지, MySQL과의 통합하여 저장 엔진으로 사용하는 방법 확인 등)\n- 페이스북이 유지 관리하는 매우 활발한 커뮤니티 지원이 있습니다.\n\n\n\n이 데이터 포인트들은 좋은 임베디드 데이터베이스를 선택하는 데 도움이 많이 되었어요. \n\n## RocksDB의 저장 스키마\n\n임베디드 데이터베이스 선택을 결정하고 나면 코드에 통합하는 작업을 진행했어요. 이 작업은 간단했어요. 우리는 Redis에서 데이터를 한 번 가져와서 이후 배포에서 로컬로 저장된 데이터를 사용할 수 있도록 RocksDB에 저장하고 싶었어요. \n\n![RocksDB를 사용한 서버 부트스트랩 최적화](/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_2.png)\n\n\n\n간단하게 유지하기 위해 우리는 간단한 저장 스키마를 설계했습니다. RocksDB의 각 행은 단일 목록의 데이터를 포함했습니다. 각 행의 키는 목록의 ID이었고 값은 직렬화된 목록 POJO였습니다. 다음과 같이 보였습니다:\n\n```js\n“LISTING_1\": “{\\\"attribute_1\\\": \\\"value_1\\\", \\\"attribute_2\\\": \\\"value_2\\\"}\"\n```\n\n단순함 외에도 이러한 종류의 스키마를 선택한 주된 이유 중 하나는 작업의 세분화였습니다. 이 스키마를 사용하면 필요에 따라 단일 목록 또는 그룹의 목록을 업설 또는 가져올 수 있었습니다. RocksDB에 일부 목록의 데이터가 없는 경우 Redis에서 누락된 레코드를 다음 부트스트랩 단계에서 가져와 RocksDB를 업데이트할 수 있습니다. 시스템은 자가 치유 기능을 제공합니다.\n\n이것이 우리 솔루션의 기초를 놓은 것이지만, 아직 해결해야 할 많은 문제가 있었습니다. 다음 섹션에서 몇 가지 다른 어려움에 대해 이야기하겠습니다.\n\n\n\n## 고요한 데이터 문제 해결\n\nRedis에서 데이터를 한 번 가져와 로컬에 저장하는 것만으로는 비즈니스의 빠르게 변화하는 성격 때문에 충분하지 않았습니다. 플립카트의 대부분 비즈니스 요구 사항은 목록 속성에 일부 변경이 필요했으며(인메모리 데이터 구조에 대한 비슷한 스키마 변경 포함), 그래서 그럴 때마다 RocksDB에 저장된 데이터는 애플리케이션의 스키마와 호환되지 않게 되었습니다.\n\n이 문제를 해결하기 위해 RocksDB에 인메모리 데이터 구조의 스키마 해시를 리스트 데이터와 함께 저장했습니다. 이 스키마는 배포가 발생할 때 최신 스키마(코드에 저장된)와 비교됩니다. 스키마 불일치가 발생하면 RocksDB 데이터를 단순히 폐기하고 Redis에서 최신 데이터로 다시 채웁니다.\n\n이 방법은 괜찮았지만, 결과적으로 서면 배포의 60%가 데이터 구조 변경을 포함했다는 것이 밝혀졌습니다. 이는 여전히 대부분의 배포에서 Redis에 의존하고 있다는 것을 의미했으며, 이는 이상적이지 않았습니다.\n\n\n\n우리는 더 나은 계획을 세웠어요. 동일한 샤드 내의 모든 레플리카가 동일한 목록 데이터를 가지고 있다는 사실을 알고 있었죠. 그래서 왜 각 샤드의 단일 무작위 레플리카로부터 RocksDB 데이터를 생성하여 GCS에 저장한 다음 해당 샤드의 다른 레플리카에 사용하지 않을까요?\n\n이 방법을 구현하는 것은 쉬웠어요. 우리는 코드에 유효성 검사 레이어를 작성해뒀는데, Mustang 시작 시 스키마 해시를 비교할 수 있도록 했어요. 동일하다면 응용 프로그램은 RocksDB에서 데이터를 로드하지만 다르다면 해당 응용 프로그램은 로컬 RocksDB 데이터를 삭제하고 GCS에서 최신 데이터를 가져와서 계속해서 RocksDB에서 데이터를 로드해요.\n\nGCS에 데이터가 없을 경우 Redis로 데이터를 부트스트랩합니다. 배포 파이프라인도 이러한 유형의 배포를 처리하도록 변경되었어요. 스키마 변경이 있을 때마다 CI 파이프라인이 각 샤드에서 무작위 레플리카를 선택하여 최신 코드를 배포하고 Redis에서 부트스트랩한 다음 로컬 RocksDB 데이터를 GCS에 업로드합니다. 나머지 레플리카는 로컬 데이터가 무효화될 때 자동으로 GCS에서 데이터를 가져와요.\n\n## Kafka 업데이트 처리\n\n\n\n카프카에서 업데이트를 다룰 때, RocksDB의 데이터가 Redis에 있는 것과 함께 최신 상태를 유지해야 했습니다. 우리는 이 프로세스를 최적화하기 위해 카프카 업데이트를 가로채고 그것을 RocksDB에 추가한 후 메모리 데이터 구조의 변경 사항을 반영하는 전용 클래스를 개발했습니다.\n\n우리는 전체 POJO를 RocksDB에 저장했기 때문에 데이터를 업데이트하기 위해 읽기-수정-업데이트 작업을 수행해야 했습니다. 주요 기술적 장벽은 RocksDB 내에서 잠재적인 업데이트 실패를 관리하여 Mustang이 다음 재시작 시 RocksDB에서 가장 최신 데이터를 검색할 수 있도록 보장하는 데 있었습니다.\n\n이를 완화하기 위해 우리는 try-catch 블록 내에서 오류 처리를 구현했습니다. 어떤 이유로든 업데이트가 실패하면 해당 목록을 간단히 RocksDB에서 삭제했습니다. 삭제 실패의 드문 경우(다시 시도 후에도)를 대비하여 종료시 전체 RocksDB 데이터 세트를 지우기로 선택했습니다. RocksDB 업데이트가 실패하더라도 사용자에 대한 데이터 불일치가 없도록 메모리 데이터 구조를 업데이트했습니다.\n\n해결해야 할 또 다른 문제가 있었습니다. RocksDB의 부트스트랩 후 카프카 이벤트를 가장 최신 오프셋에서 읽기 시작하는 Mustang의 경우(기존 시스템의 기본 동작 방식)에는 데이터 손실이 발생할 수 있습니다. 이는 RocksDB가 Mustang이 재시작하는 동안 업데이트를 받지 못하기 때문입니다. 그 전에는 Mustang이 소스의 진실인 Redis에서 부트스트랩을 했기 때문에 가장 최신 카프카 오프셋에서 읽기를 안전하게 시작할 수 있었던 것이었습니다.\n\n\n\n이 문제를 해결하기 위해 우리는 종료할 때 모든 파티션의 Kafka 오프셋을 RocksDB에 저장하기 시작했습니다. 그런 다음 시작할 때 Mustang은 해당 파티션의 저장된 오프셋으로 다시 이동하였습니다. 응용 프로그램 충돌과 같은 이유로 RocksDB에서 오프셋을 찾을 수 없는 경우, 로컬 덤프를 폐기합니다. 또한 현재 오프셋과 저장된 오프셋 간의 차이가 상당히 큰 경우 로컬 덤프도 폐기합니다. 이 결정은 이러한 중요한 데이터 갭을 다시 채우는 데 상당한 시간이 걸릴 것으로 이해하고 기준이 됩니다. 이 차이에 대한 임계값은 Mustang이 일반적으로 다섯 분 동안 처리할 수 있는 업데이트 양을 추정하여 결정됩니다.\n\n## 사용 사례에 맞게 RocksDB 조정\n\n이 시점까지 우리는 RocksDB를 우리의 코드베이스에 성공적으로 통합할 수 있었습니다. Mustang이 인메모리 데이터 구조를 부팅하는 데 걸리는 시간은 30분에서 15분으로 줄었습니다. 우리는 우리의 액세스 패턴을 더 잘 이해하고 RocksDB의 내부를 깊이 파고들면 RocksDB에서 더 많은 성과를 얻을 수 있다고 믿었습니다.\n\n우리의 워크로드는 쓰기와 읽기의 혼합물이었습니다. Redis에서 데이터를 부팅하고 RocksDB에 삽입하는 동안은 완전히 쓰기 중심이지만 이후 배포에서는 항상 읽기 중심입니다. 이러한 패턴 중 하나에만 최적화할 수 있었고, 우리는 읽기를 최적화하기로 결정했습니다 (당연한 이유로).\n\n\n\n아래는 저희 케이스에 작동한 몇 가지 최적화 내용입니다:\n\n- 캐시 비활성화: LRU 기반 블록 캐시는 모든 RocksDB 사용 사례에서 사용하는 것이 좋지만 심한 락 경합이 발생합니다. 부팅 중에 한 번만 목록을 읽는 경우에는 데이터 블록의 캐시를 비활성화했습니다.\n- 레벨 컴팩션 사용: 레벨 컴팩션 전략은 크기 단위(일명 유니버설) 컴팩션 전략보다 읽기 및 공간 증폭면에서 더 나은 결과를 줍니다.\n- LSM 트리의 레벨 수 줄임: LSM 트리의 레벨 수는 RocksDB에서 중요한 속성입니다. LSM 트리에 존재할 레벨 수를 결정합니다. 데이터 중 일부가 핫하게 액세스되는 경우 레벨이 많으면 유익하지만, 액세스 패턴이 무작위인 경우 읽기 대기 시간에 영향을 줄 수 있습니다. 우리의 경우에는 레벨 수를 기본값인 7에서 3으로 줄였습니다.\n- 주기적인 전체 컴팩션 트리거: RocksDB는 최상태일 때 가장 잘 작동합니다. 읽기 성능을 최적화하기 위해 카프카 업데이트로 인한 읽기(및 공간) 증폭을 줄이기 위해 매일 취침 시간에 전체 컴팩션을 수동으로 트리거하는 비동기 스레드를 작성했습니다.\n- WAL 비활성화: 기본적으로 RocksDB는 모든 쓰기를 WAL에 메모리 테이블과 함께 저장합니다. 데이터가 손실되지 않는 자가 치유하는 성격의 경우 WAL을 비활성화했습니다.\n- multiGet()을 사용하여 데이터 읽기: RocksDB는 DB에서 데이터를 읽는 다양한 방법을 제공합니다. get() 명령 또는 multiGet() 명령을 실행할 수 있습니다. multiGet()은 다중 get() 호출에 비해 더 효율적이며, 필터/인덱스 캐시에서 적은 스레드 경합, 내부 메서드 호출 수 감소, 다른 데이터 블록에 대한 IO에 대한 더 나은 병렬화 등 여러 이유로 선호됩니다.\n- 목록 정렬 및 일괄 처리: 전체 목록 세트를 정렬한 다음 RocksDB에서 데이터를 가져오기 전에 이를 작은 일괄 처리로 만들었습니다. 정렬된 목록은 디스크에서 동일한 또는 근접한 페이지에 있을 가능성이 크기 때문에 랜덤 디스크 IO를 줄였습니다.\n\n이러한 최적화로 부팅 시간을 15분에서 약 6분으로 대폭 단축하여 효율을 높이는 우리의 노력에서 중요한 성과를 도출했습니다.\n\n자세한 내용은 이 튜닝 안내서를 읽어보세요.\n\n\n\n# RocksDB를 운영 환경으로 이끌기\n\n모든 것이 준비된 상태에서 우리는 RocksDB를 운영 환경으로 적용하고 싶었습니다. 그러나 스택에 새로운 기술을 도입하는 것은 어떠한 중단도 방지하기 위해 조심스럽게 진행해야 했습니다. Mustang이 RocksDB와 어떻게 상호 작용하는지 모니터링하기 위한 적절한 메트릭을 도입하고 위험을 완화하는 방법을 구현했습니다. 그런 다음, 각 샤드 내의 일부 레플리카에 대한 제한적인 배포를 시작했습니다.\n\n## 작은 결함\n\n몇 개의 Mustang 서버에 최신 코드를 배포한 직후, 응답 시간이 저하되는 현상이 발생하기 시작했습니다. 더 깊게 파고들어보니, 몇 일 동안 전체 VM의 메모리가 천천히 소비되는 것을 발견했습니다. 이 과도한 메모리 사용은 Solr의 인덱스 관련 파일을 캐싱하는 능력을 방해하여 런타임 중 과도한 디스크 이용률을 초래하여 지연 시간을 증가시켰습니다. Java 어플리케이션이 이 메모리 누수를 발생시킬 수 없다는 것은 명백했습니다. 왜냐하면 Java 어플리케이션은 시작 시에 고정된 메모리 청크(힙 메모리 형태로 할당)만 할당되고, 그 위에 JVM이 기능하기 위해 약간의 추가 네이티브 메모리만 사용하는 것 뿐이기 때문입니다. (JVM이 사용하는 네이티브 메모리를 확인하는 방법은 이 가이드를 참조하세요)\n\n\n\n의심이 RocksDB로 향했는데, 내장 데이터베이스로 분류되었지만 Java 라이브러리뿐만 아니라 C++ 구성 요소도 포함되어 있고 JNI를 통해 Java 애플리케이션과 상호 작용하는 것으로 확인되었습니다. Java에서 RocksDB와 상호 작용하는 클래스들은 내부적으로 RocksDB의 C++ 대응물을 호출합니다. C++의 메모리 관리는 자동 가비지 콜렉션 시스템이 없어서 어려워요. 우리는 초반에 RocksDB에 버그가 있을거라고 생각했어요.\n\n그때부터 우리는 네이티브 메모리 누수를 디버깅할 수 있는 옵션을 탐색하기 시작했어요. 네이티브 메모리 누수를 찾아가는 우리의 여정은 블로그로서 충분히 소개할 만한 주제였지만, 간결함을 위해 이 자리에서 우리가 도움을 받은 내용에 대해서만 이야기할 예정이에요.\n\n이러한 유형의 메모리 누수를 디버깅하기 위해 우리는 Jeprof라는 도구를 발견했어요. 이 도구는 JNI 호출과 같은 이유로 발생하는 네이티브 메모리 할당을 추적하는 데 사용됩니다. 그래서 우리는 Jeprof를 구성하여 애플리케이션의 네이티브 메모리 할당에 대한 메모리 덤프를 수 분마다 수행하도록 설정했어요. 나중에 우리는 몇 시간 간격으로 생성된 2개의 무작위 덤프를 비교하여 어떤 객체가 크기가 커지고 있는지 확인했어요. 같은 내용의 일부를 아래에 기술한 내용을 참고해 주세요:\n\n![이미지](/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_3.png)\n\n\n\n스크린샷을 보면, RocksDB에서 제공된 ReadOptions()와 WriteOptions() 객체들이 많이 할당되었음을 확인할 수 있었습니다.\n\n코드를 확인해본 결과, 이 두 클래스에 대한 새로운 객체 할당이 발생하는 곳은 하나뿐이었습니다. 우리 코드에서 RocksDB 관련 구성을 처리하기 위해, ReadOptions()와 WriteOptions()와 같은 다양한 객체를 보유하고 이를 맵에 다양한 열 패밀리 이름에 매핑하는 새 클래스를 만들었습니다. 이러한 객체들은 일반적으로 Mustang 시작 시 제공되며 다시 생성되지 않습니다.\n\n그러나 안전을 위해, 열 패밀리에 요청되었을 때 이미 사용 가능한 객체들이 없는 상황을 처리하기 위한 해결책을 마련했습니다. 우리는 Map의 getOrDefault() 메서드를 활용하여, 미리 구성된 객체를 검색하거나 지정된 열 패밀리에 대해 실행 중에 새로운 객체를 생성할 수 있도록 했습니다. 이 구현에 대한 자바 코드는 아래와 유사했습니다:\n\n```java\n...\npublic ReadOptions getReadOptionsByColumnFamily(String columnFamilyName) {\n  return this.readOptions.getOrDefault(columnFamilyName, new ReadOptions());\n}\n\npublic WriteOptions getWriteOptionsByColumnFamily(String columnFamilyName) {\n  return this.writeOptions.getOrDefault(columnFamilyName, new WriteOptions());\n}\n...\n```\n\n\n\n## 그럼 왜 메모리 누수를 발생시킬까요?\n\nRocksDB의 메모리 관리에 관한 문서를 읽어보니, RocksDB의 각 클래스가 Autocloseable을 직간접적으로 구현한다는 것을 알게 되었습니다. RocksDB의 자바 객체에서 실제 메모리를 해제하기 위해 사용이 완료되면 명시적으로 close()를 호출해야 합니다 (또는 try-with-resources를 사용). 이를 하지 않으면 메모리 누수가 발생할 수 있습니다.\n\n위의 코드 조각에서 언급된 로직을 구현하는 과정에서, Map의 getOrDefault() 내에서 새 객체가 생성되고 있음을 인지하지 못했습니다. 그 함수가 호출될 때마다 사전 구성된 객체가 주어진 칼럼 패밀리를 위해 존재하는지 여부와 관계없이 이러한 새로 생성된 객체는 닫히지 않고 메모리 누수에 기여했습니다.\n\n메모리 누수의 근본 원인을 이해한 후, 문제 해결은 쉬웠습니다. 이 구성 클래스를 인스턴스화하는 동안 기본 ReadOptions 및 WriteOptions 객체를 한 번만 만들어서 Map의 getOrDefault()에서 새 객체를 생성하는 대신 이들을 사용했습니다.\n\n\n\n영향을 받은 Mustang 서버에 대한 수정 사항을 신속하게 배포하고 메모리 안정성을 확인하기 위해 한동안 모니터링했어요.\n\n## 배포 계속 진행\n\nRocksDB의 부정적인 영향이 없고 모든 것이 원활히 작동하는 것을 확인한 후, 시스템 자원에 특별히 주의를 기울이면서 남은 Mustang 서버를 배포하기로 했어요. 이 점진적인 접근법을 통해 우리는 원활한 전환을 보장하고 제품 환경의 안정성을 유지할 수 있었어요.\n\n## 결론\n\n\n\n요약하자면, RocksDB는 Mustang의 배포 과정에서 현저한 향상을 이끌어냈습니다. Redis에 의존했던 이전 방식 대비 인메모리 데이터 구조를 부트스트랩하는 데 필요한 시간을 크게 줄였습니다. \n\n다양한 샤드를 통해 방대한 테스트를 거친 결과, 부트스트랩 시간이 상당히 줄어들었으며 평균 6분으로 안정화되었습니다 (이전 30~40분). 서버 당 부트스트랩 시간뿐만 아니라 배포 속도도 향상되었으며 이제 더 많은 서버를 병렬로 배포할 수 있게 되었습니다.\n\n그 결과, 전체 샤드에 걸친 Mustang의 전체 배포 과정은 이제 3시간 미만이 소요되며, RocksDB 도입을 통해 달성한 효율성 향상의 증거가 되었습니다. 이 성공은 저희 인프라 내 지속적인 개선과 최적화에 대한 헌신을 강조합니다.","ogImage":{"url":"/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png"},"coverImage":"/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png","tag":["Tech"],"readingTime":12},{"title":"규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로","description":"","date":"2024-05-15 16:45","slug":"2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost","content":"\n\n기업 엔지니어링 팀들은 대량의 비정형 텍스트를 처리하기 위해 OpenAI 및 다른 생성 AI 서비스의 파워를 활용하여 생산성을 크게 향상시키고 있습니다. 그들은 고객, 직원, 및 하위 비즈니스를 위한 생산성 도구를 만들고 평가하고 있습니다. 비즈니스 중요 텍스트를 활용하는 프로덕션급 도구들은 엄격한 데이터 준수 가이드라인이 필요합니다. 대부분의 기업은 고객에게 표시할 수 있는 텍스트 유형 및 OpenAI GPT-4와 같은 제3자 AI API 서비스로 보낼 수 있는 텍스트 유형에 대한 준수 정책을 가지고 있습니다.\n\n![그림](/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png)\n\n![그림](/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_1.png)\n\n## 원시 비정형 텍스트 청크의 프라이버시 보호 필요성이 커지고 있습니다.\n\n\n\n사용자에게 보여지거나 안전한 환경을 벗어난 텍스트에는 SSN, 신용 카드 번호 또는 이메일 등의 PII(개인 식별 정보)가 포함되지 않도록 해야 합니다. 예를 들어 외부 API와 같은 임베딩 서비스나 GPT-4 프롬프팅을 사용하기 전에는 안전한 환경을 벗어나는 텍스트에서 PII를 삭제하여 규정을 준수해야 합니다.\n\n## 기존 솔루션: 높은 GPU 사용량과 증가된 대기 시간\n\nNamed Entity Recognition (NER)은 텍스트 분석에서 잘 알려진 문제이며, Hugging Face와 같은 회사에서 제공하는 것처럼 이를 해결하기 위한 여러 오픈 소스 모델이 있습니다. 그러나 이러한 모델을 확장하려면 상당한 GPU 자원이 필요합니다. 예를 들어, BERT와 같은 기본 모델은 리소스 소모가 많으며, 세부 조정을 위해 4대의 V100 GPU에서 몇 시간이 필요합니다(표 1 참조). 또한 배포 중에 느려지는 경우가 있어, 애플리케이션 응답 시간이 약 50배 이상 소요될 수 있습니다(그림 1 참조). 반면, spaCy와 같은 저렴한 대안은 리소스 소비가 적지만, 더 정교한 모델의 93%에 미치지 못하는 65%의 정확도를 제공합니다. 성능 차이는 NER 솔루션을 배포할 때 계산 요구 사항과 정확도 사이의 교환이 강조됩니다.\n\n## ThirdAI의 CPU 전용 NER 모델: 신속한 세부 조정, 초고속 대기 시간, SOTA 정확도. GPU 사이클을 해방하세요.\n\n\n\n저희는 NER을 위해 특별히 제작된 사전 훈련된 기본 모델을 자랑스럽게 소개합니다. 최신 기술의 정확성을 제공하며 BERT나 DistillBERT보다 30~50배 빠른 레이턴시로 작동합니다. 놀랍게도 최대 1,000개의 토큰에서도 우리 모델은 단일 CPU 코어에서 1,000개의 라벨 예측에 대해 약 50ms의 레이턴시를 유지합니다. (비교 자료는 도표 1과 테이블 1을 참고하세요)\n\n저렴한 CPU 전용 인스턴스에서 세밀한 조정 가능: 저희 모델은 수백만 개의 라벨이 지정된 샘플을 몇 분 내에 저렴하고 쉽게 구할 수 있는 CPU 전용 인스턴스에서 세밀하게 조정할 수 있습니다. 추론 및 세밀 조정 중에 다양한 프로세서 간에 일관된 성능을 유지합니다.\n\n저희 기술을 활용하고자 하는 개발자들을 위해 사전 훈련된 NER 모델을 배포하는 간단한 스크립트를 제공해드립니다. 이 모델은 여러 언어에서 표준 NER 범주를 식별할 수 있는 능력이 있습니다. 언어 및 지원되는 범주에 대한 자세한 내용은 안내에서 확인할 수 있습니다. 특화된 다국어 데이터셋에 대해 세밀한 조정을 원하시는 경우에는 다음 대체 스크립트를 사용하십시오.\n\n## 순서 개선된 총소유비용(TCO)\n\n\n\nTable 1은 세밀 조정 비용을 요약하며 Figure 1의 차이는 배치 비용으로 직접 변환됩니다. ThirdAI의 기술적 차이로 인해 전용 GPU 리소스 제약 조건을 없애고 비용을 100배 절감할 수 있습니다. ThirdAI를 사용하여 언제 어디서나 손쉽게 데이터를 사적으로 보호하세요.\n\n## 중요한 링크\n\n모든 파이썬 노트북 및 지침서 링크: https://github.com/ThirdAILabs/Demos/tree/main/named_entity_recognition","ogImage":{"url":"/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png"},"coverImage":"/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png","tag":["Tech"],"readingTime":3},{"title":"흑자 블랙 바스타의 코발트 스트라이크를 추적하기","description":"","date":"2024-05-15 16:44","slug":"2024-05-15-HuntingBlackBastasCobaltStrike","content":"\n\n\u003cimg src=\"/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png\" /\u003e\n\n지난 주 FBI와 CISA가 Black Basta 랜섬웨어 그룹을 위한 #StopRansomware 경보를 발표했습니다.2024년 중에 이 그룹은 LockBit와 Play 다음으로 총 랜섬웨어 피해자가 세 번째로 많은 것으로 나타났습니다. 특히, 그 그룹이 Cobalt Strike를 사용한다는 점이 강조되었습니다(표 10 - \"알려진 Black Basta Cobalt Strike 서버 도메인\" 참조).\n\nIntel-Ops는 Black Basta가 배포한 Cobalt Strike 서버를 포함하여 야생에서 Cobalt Strike 서버를 적극적으로 추적하고 있습니다. 이 게시물에서는 FBI/CISA 경보에 포함된 C2 서버 및 해당 경보에 미포함된 C2 서버 또는 악의적이거나 Black Basta와 관련된 서버로 공개적으로 추적되지 않은 서버에 대한 분석 결과 중 일부를 강조하겠습니다.\n\n\n\n블랙 바스타의 코발트 스트라이크 그래프:\n\n발리딘의 \"Bulk Analyzer\" 도구를 사용하여 공지사항의 도메인을 빠르게 활성 IP 주소로 해결하고 시간에 따른 호스팅 패턴을 이해하며, 말테고 그래프에 대한 상관 관계를 생성할 수 있습니다:\n\n![그래프 이미지](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_1.png)\n\n# 결과\n\n\n\n- 블랙 바스타 인프라는 명확한 클러스터로 그룹화될 수 있으며, 이 중 일부를 아래에서 강조하겠습니다.\n- 블랙 바스타 인프라에서 관찰된 주요 워터마크는 1357776117 및 1158277545입니다.\n- 코발트 스트라이크 서버의 대부분은 Vult Hosting LLC (AS-CHOOPA), JW Lucasweg 35, Digital Ocean 및 Servinga에서 호스팅됩니다.\n\n# 클러스터 1\n\n블랙 바스타가 운영하는 대다수의 코발트 스트라이크 서버는 DNS 비컨을 활용합니다: https://hstechdocs.helpsystems.com/manuals/cobaltstrike/current/userguide/content/topics/listener-infrastructue_beacon-dns.htm. 우리의 분석에 따르면, 공지서에서의 IOC는 거의 전적으로 Vultr, Lucasweg 및 Digital Ocean에서 호스팅됩니다.\n\n우리의 조사에 따르면, 적어도 6개의 추가 DNS 코발트 스트라이크 비컨이 이러한 제공업체에서 호스팅되었습니다. Intel-Ops는 다른 제공업체에서도 유사한 C2를 식별했습니다. \"thenewbees[.]org\"와 같은 새로 식별된 도메인은 공지서에 기재된 다른 DNS 비컨들의 네이밍 컨벤션과 일치합니다.\n\n\n\n인텔-옵스 클러스터 1의 예시 DNS 비콘:\n\n![Example DNS Beacon](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_2.png)\n\n## 클러스터 2\n\n도메인 \"usaglobalnews[.]org\"은 Cobalt Strike 비콘의 일부인 것으로 보입니다. 인텔-옵스 클러스터 2에서는 두 개의 도메인이 이전에 이미 Pikabot을 포함한 Black Basta 사건과 관련하여 Trend Micro에 의해 2023년 12월에 보고되었습니다. \"ruggioil[.]com\"과 \"bluenetworking[.]net\"의 도메인은 모두 해당 사건과 연결되었습니다.\n\n\n\nhttps://www.trendmicro.com/content/dam/trendmicro/global/ko/research/24/a/a-look-into-pikabot-spam-wave-campaign/ioc-pikabot-spam-campaign.txt\n\nIntel-Ops Cluster 2에서의 Cobalt Strike 서버 예시:\n\n![Cobalt Strike 3](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_3.png)\n\n![Cobalt Strike 4](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_4.png)\n\n\n\n# 클러스터 3\n\n인텔-옵스 클러스터 3 내에서는 3개의 자문 도메인이 관찰되었으며, 이 중 일부는 중국 호스팅 업체 및 중국 DNS 레코드의 분포가 더 큰 코발트 스트라이크 서버와 클러스터링되어 있습니다. 추가로, \"dfir-delight\"의 2024년 4월 보고서에서 적어도 3개의 추가 C2 서버와 자문 도메인 중 하나가 공개적으로 블랙 바스타 활동에 속한다고 확인되었습니다: https://dfir-delight.de/p/black-basta-iocs/\n\n인텔-옵스 클러스터 3 코발트 스트라이크 C2 서버 예시:\n\n![HuntingBlackBastasCobaltStrike_5](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_5.png)\n\n\n\n# 클러스터 4\n\nIntel-Ops는 워터마크가 있는 여러 개의 Cobalt Strike 비콘을 식별했습니다: 1357776117. 이 워터마크가 있는 IP 주소의 비교적 작은 클러스터가 있습니다: 우리는 Hunt.io를 사용하여 지난 30일 동안 이 워터마크가 있는 IP의 수를 식별할 수 있습니다. 특히, 해결 도메인 중 일부가 공고서의 도메인 이름 규약과 일치하는 것을 확인했습니다:\n\n![이미지](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_6.png)\n\n이 클러스터 내에서, 최근의 Black Basta 사건에 공개적으로 속한 다른 IP 주소가 있습니다. 또한, 워터마크만을 분석했을 때 호스팅이 Black Basta 클러스터의 다른 패턴과 거의 일치하지 않았습니다, 예를 들어 DNS 비콘이요.\n\n\n\n예시 인텔-옵스 클러스터 4 내의 Cobalt Strike C2 서버:\n\n![Alt text](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_7.png)\n\n# 결론\n\n블랙 바스터 공고로부터 추론할 수 있는 추가적인 클러스터/상관 관계가 있습니다. 이 분석은 보고서에 나와 있는 알려진 지표만 다루고 있습니다. Cobalt Strike를 이용하는 그룹에 대한 보호를 강화하고 추가 정보를 얻으려면:\n\n\n\n**Hunting Adversary Infrastructure Course**\n\n우리의 \"Hunting Adversary Infrastructure\" 과정에서 가르치는 기술을 활용하면 보안 분석가들이 모두 레벨에서 활동을 클러스터링하고 소속을 판단하는 등 보고서를 더욱 풍부하게 할 수 있습니다. 이러한 모든 지표들은 Intel-Ops에서 적극 추적되고 곧 C2 위협 피드를 통해 제공될 예정입니다.\n\nCobalt Strike와 Black Basta와 같은 그룹과 같은 프레임워크를 추적하는 방법을 배우고 싶다면, 저희 과정에 등록해보시기 바랍니다. 여기에서 등록하면 학생들은 배우는 데 도움을 줄 추가적인 쿼리 및 API 크레딧이 포함된 Validin 플랫폼을 위한 특별한 Intel-Ops 계정을 획들할 수 있습니다.\n\nC2 Feed\n\n\n\n저희 C2 피드에 관한 정보가 필요하시면 LinkedIn, Twitter 또는 이메일(contact@intel-ops.io)로 연락해주세요.","ogImage":{"url":"/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png"},"coverImage":"/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png","tag":["Tech"],"readingTime":4},{"title":"AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지","description":"","date":"2024-05-15 16:41","slug":"2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny","content":"\n\n## 공정한 숫자 분석가의 장단점\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*Aun9fXyvK-JxqCHKhQyxrg.gif)\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*j7df-zSIAOBOo6rKwFgT1Q.gif)\n\n위 멋진 애니메이션들은 최고의 AI 기반 비디오 생성기인 Sora가 OpenAI로부터 나온 것 중에서도 가장 진보된 것입니다. 이것들은 의심의 여지없이 인상적입니다. 인공지능 분야에서 일하는 사람들도 지난 몇십 년 동안 많은 발전을 본 사람에게도 놀랄 만한 것입니다. 이 비디오를 관람하면 AI가 클립의 다양한 테마와 객체를 얼마나 매끄럽게 섞어내는지, 그 결과물이 얼마나 일관되고 물리적으로 타당해 보이는지에 놀라게 될 것입니다. 이 비디오를 보면 평범한 사람은 AI 기술이 이렇게 현실적인 수준에 도달했다는 사실에 당황할 것입니다.\n\n\n\n잠시 지나면 새로움이 사라지고 다른 것을 느끼기 시작할 지도 모릅니다. 이 비디오들에는 이상하고 최면적인 느낌이 있습니다. 충분히 오랫동안 보게 되면 눈빛 — 아마도 마음까지 — 흐릿해질 것입니다. 비디오가 미묘하고 일정한 압력으로 당신의 두뇌에 가해지는 것 같은 느낌이 듭니다. 이 경험에는 뭔가 기묘한 것이 있는데, 특정한 방식으로 말하자면 눈뒤쪽이나 다른 어딘가에 숨겨져 있는 것처럼 영원히 손에 쓸 수 없는 느낌이 듭니다.\n\n이 경험은 크래프트가 완벽하게 집중되어 있어서 불안할 수 있습니다. 그들의 표현은 섬세하고 부드럽고 윤득한, 내용에 관계 없이 가파른 모서리가 없습니다. 그러나 확신컨대 저는 그들에게 사로잡혔음에도 불구하고 뇌 속에 흐릿함이 남는 느낌을 떠났습니다. 이것은 비디오의 초현실적인 내용 때문인 것이 부분적으로는 사실입니다. 그러나 이것만으로 현상을 완전히 설명할 수는 없습니다 — 많은 인간이 만든 비디오가 초현실적이며, 똑같은 기이하면서 인상적인 효과를 내지는 않습니다. 이 현상을 직간접적으로 기술로서의 편견으로 이해할 수도 없습니다. 사실, 이 클립들이 인간에 의해 촬영되었다고 상상하면 기이함을 더 심화시키고 희석시키는 것보다 더 매료되는 느낌을 주곤 합니다.\n\n이 클립들은 눈동자를 즐겁게 해줄 수 있으며, 그들의 즐거움은 선명하고 생생한 영상의 시각적 즐거움에서 나옵니다. 단지 사탕처럼, 그들은 궁극적으로 만족할 만한 느낌을 주지는 못할 수도 있습니다. 항상 무언가를 \"전달\"하려고 하는 것 같지만 결코 그렇지 않습니다. 다시 말해, 그들은 아무것도 전달하려는 의도가 없으며 순간적인 경험 자체가 그 요점입니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*OIsAJ4ZB-YvQ34PDvyHswA.gif)\n\n\n\n그들을 관찰하는 것은 분수를 관찰하는 것처럼 매혹적입니다: 그 자체로 또한 발표 방식으로 매혹적입니다. 아니면 클라이맥스가 없는 이야기를 듣는 것과 같을지도 모릅니다. 설정이 해결되었다고 생각하는 즉시, 그것이 약간 변화하고 다른 이야기를 드러내며, 지금은 자신의 결말을 예측하며, 그리고 그렇게 이어지는 것은 하나의 문장처럼. 비디오는 항상 깨달음 직전인데, 그것들은 결코 넘어가지 않습니다. — 그리고 당신 역시 그것을 기대하지 않습니다. 그것들은 한계 상태이며, 그 상태를 유지하는 것을 선호합니다.\n\n![GIF 이미지](https://miro.medium.com/v2/resize:fit:400/1*xX03li5aQruQGNI5fwfcqg.gif)\n\n최면술 효과는 비디오가 어딘가로 향하고 있다는 느낌에서 온다. 그리고 모퉁이마다 새로운 약속을 하며 당신을 긴장시킵니다. 시청자는 따라가고, 무언가가 제공될 때까지 놓칠 수 없다고 느낍니다. 마침내 그것이 제공되지 않을 것으로 깨닫고 단지 그 순간의 경험을 즐깁니다. 그래서 그것은 목표에서 목표로 방황하며, 사회적 미디어에서 스크롤하는 것만큼 방향성이 없습니다.\n\n명확히 말하자면, 비디오에는 기저적인 아이디어가 있지만, 시작할 때 공개되고 일관성 있게 유지됩니다. 설정과 전달의 곡선은 없으며, 단지 지속적인 압력만 있는 것입니다. 이것이 비디오 콘텐츠가 시각적으로 매력적이어야 하는 이유이며, 종종 슬로우 모션으로 제시됩니다. 눈 얼음 위에서 노는 귀여운 강아지, 컵 속의 보트, 고원을 걸어 다니는 털족. 그들이 교육되는 재고 영상처럼, 그들의 시각적 매력은 그들의 요점 전부입니다. 클립의 메시지는 매체와 일치합니다.\n\n\n\n\n[![image](https://miro.medium.com/v2/resize:fit:640/1*KlbQB5xH8kI5G0FWXOWL1g.gif)](https://miro.medium.com/v2/resize:fit:640/1*KlbQB5xH8kI5G0FWXOWL1g.gif)\n\n아마도 이 현상을 이해하는 가장 좋은 방법은 다음과 같이 물어보는 것입니다: 클립은 무엇을 말하려고 하는 걸까요? 그 의도는 무엇이며, 주제는 무엇이며, 논문은 무엇일까요? 창조자가 우리가 이것을 보게 하려고 했던 이유, 특히 이것을 보게 하려고 한 이유는 무엇인가요? 어떤 비디오를 보든지, 생성의 의도나 목적을 빨리 파악할 것을 기대합니다. AI로 생성된 비디오의 경우 답을 찾기 어렵지 않을 것입니다. 왜냐하면 생성 모델이 한 번 훈련되면 모든 지정된 내용은 생성된 텍스트 프롬프트에서 나오기 때문이죠. 따라서 어떤 독특한 메시지라도 해당 비디오가 같은 모델의 다른 비디오와 비교하여 가지고 있는 것은 오로지 해당 프롬프트에서 온다는 데서 오게 됩니다. 텍스트 프롬프트는 궁극적으로 논문을 요약해야 하지만... 그렇지 않습니다. 프롬프트는 내용을 대표하지만 목적은 대변하지 못합니다.\n\n내가 무엇을 말하는지 이해하려면 AI에게 제공하는 모든 프롬프트가 어디서나 나온 것은 아니라는 것을 기억해야 합니다. 작가로서 이 프롬프트를 통해 표현하려는 동기, 감정, 목표 및 의도의 역사가 있습니다. 이것들이 당신의 숨겨진 목적이며, 글자 그대로 요청하고 있는 구체적인 내용이 아닙니다. 그 백그라운드 중 어느 것도 AI로 전달되지 않습니다. AI는 모든 사실을 받지 못하므로 속여주고 있다고 할 수 있습니다.\n\n[![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png)](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png)\n\n\n\n\nAI에게 \"가난한 사람들을 돕고 싶게 만들고 싶어\" 또는 \"세계를 탐험하는 것에 흥분하게 만들고 싶어\" 또는 \"아이처럼 궁금해하게 만들고 싶어\"와 같은 것을 전달하는 것은 어려운 일입니다. 종종 실제로 당신이 정말 무엇을 말하려고 하는 지에 대해 완전히 인식하지 못합니다 - 적어도 영어 단어로 표현할 정도로 충분히 인식하지 못합니다.\n\n따라서 AI의 결과물이 기대에 미치지 못하는 경우, 그것이 우연히 악한 요정처럼 당신이 요청한 대로 제공하지만 실제로 원하는 것은 제공하지 않기 때문입니다. AI가 범하는 실수는 사용자의 숨은 의도에서의 소외감을 드러냅니다. AI는 직역적이어서 당신이 강조하거나 억압해야 할 부분을 식별하지 못하며 실제로 있어야 하는 누락된 부분을 추가해야 하는 타이밍조차도 파악하지 못합니다.\n\n사용자로서 당신은 결과가 의도한 대로 가까워지도록 프롬프트를 수정해야 합니다만 실제로 말하지 않은 것을 수정해야 합니다. 그 동안 AI는 밀려오는 방향, 프롬프트별로 여러 번에 걸쳐 사용자의 비전 쪽으로 밀리며 움직입니다. 당신이 관심을 갖고 있다는 이유로 AI를 끌어당기고 있는 것이 아니라 밀어내고 있는 것입니다. 마치 모래성을 계속 다듬어야 하는 것처럼 그렇지 않은 부분은 천천히 무너지는 것입니다.\n\n![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_1.png)\n\n\n\n아티스트 제너레이터를 사용하여 미술을 만드는 경험은 종종 영화 롤을 위해 서영장의 서막이 필요한 침팬지 촬영을 하는 것과 비슷합니다. 촬영장에서의 동물 트레이너와 카메라맨은 충분한 샷을 촬영하여 결과물이 침팬지가 원하는대로 보이도록 편집할 수 있도록 해야 합니다. 동물은 슬픈 상호작용을 보고도 그것이 슬픔스러워 보이도록 통찰할 확률이 낮습니다. 이 모든 것은 다른 훈련된 행동들을 조각조각으로 모아서 마치 침팬지가 의도했던 것처럼 보이도록 해야 합니다.\n\nAI가 당신의 의도와 일치하려면 자신의 의도를 가져야 하거나, 사용자의 프롬프트에서 그 의도를 추출해내야 합니다. 후자는 매우 어렵습니다. 반면에 인간 예술가에게 작품을 의뢰할 때는 요청으로부터 당신의 의도를 이해할 것을 기대합니다. 그렇지 않으면 감정적이거나 은유적인 언어를 통해 설명할 수 있습니다.\n\nAI가 표현하는 \"의도\"나 의미는 프롬프트 자체에서 비롯된 것이 아니라, 훈련 비디오에서 빌려온 단편에서 나옵니다. 오직 즐거운 비디오에서만 훈련하면 결과물은 즐거울 것입니다. AI는 표현하는 대상이나 그들의 예술이 전달하는 것에 대해 의견을 갖거나 깐깐해하지 않습니다. 그들은 훈련 중 주어진 이미지를 사용하여 완전히 개방적이라고 말할 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*vZHKB5bRVRcQmiCef7DB2w.gif)\n\n\n\nAI 생성기는 다양한 콘텐츠를 일관된 전체로 통합하는 능력이 우수합니다. 그들이 하는 일은 본질적으로 그겁니다. 생성기는 사용자가 제공한 프롬프트를 받아들인 다음, 교육 중에 보았던 다른 클립들과 관련이 있는 방향으로 여러 방향으로 분기합니다.\n\n![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_2.png)\n\n반면에, 인간 창작자들은 상당히 제한적이며, 예술가가 특이한 표현 스타일에서 벗어나는 것이 어려울 수 있습니다. 이는 강점이자 약점입니다. 인간이 만든 작품의 제한된 스타일은 집중을 보여주며, 무엇을 중요시 여기는지를 보여줍니다. 의도적인 인간 행동은 그 안에 포함된 가치를 보여줍니다. 예술가에게 이러한 가치가 작품 속 주제들을 유발시키며 그들이 무엇을 그리거나 그림을 그릴지를 선택적으로 만듭니다.\n\n인간 커뮤니케이션은 일관성 이상을 추구합니다. 그것은 시청자를 특정 방향으로 이끌거나, 특정한 생각이나 감정을 일으키기 위해 노력합니다. 이것은 결과적인 표현에 내재되어 있습니다. 결과적으로, AI가 훈련한 각 인간 예술 작품은 다른 의도가 있습니다. 그리고 그들이 큰 냄비에 섞는다면 결과를 혼란스럽게 할 수밖에 없습니다. 이것이 생성 인공지능이 한 특정 예술가의 스타일을 복사할 때 더 성공적인 이유입니다. 그들은 해당 예술가의 암시적 동기를 빌려오며, 무시할 것, 강조할 것, 무시할 것을 비교적 오염되지 않은 채로 가져옵니다.\n\n\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:640/1*3ysK_fx8qcIdKakh78C9SQ.gif\" /\u003e\n\n여러 관점을 결합할 때 그 결과물은 특정한 것을 말하지 않을 뿐더러 \"의견\"을 내세울 수도 없습니다. AI 생성 비디오는 우리가 만들어낸 문화 유물을 통해 서로 조화롭게 발음하는 열두 명의 인간 목소리와 같습니다. 결과물을 통합하는 데 결합된 의지나 목적이라는 단일한 가치는 없습니다. AI는 콘텐츠를 효율적으로 결합하는 데 탁월하지만 의도를 결합하지는 않습니다. 단순히 의도를 결합해서는 안 되기 때문입니다. \"의도\"는 항상 원하는 대로 편향됩니다. 양보할 여지가 없습니다. AI 예술에서 의도를 혼합하는 것을 바로잡기 위한 유일한 방법은 AI가 먼저 선택들이 관람자에게 미치는 효과를 배우고, 그것을 활용해 예술을 자신이 원하는 방향으로 이끌어내는 것입니다.\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:640/1*A2kk9SXCjVUHD3iHU9Hz9g.gif\" /\u003e\n\n현재, 생성적 AI는 일종의 깨끗한 판이며 편향되지 않은 데이터 분석기입니다. 그것에는 매력이 있습니다. 인간과 함께 일할 때의 많은 성가시움, 특히 고집부리기와 편견이 없는 것을 들 수 있습니다. 그것은 순순하고 공손하게 사회 유물을 흉내내는 것입니다. 그러나 이것은 실패가 혼란스러울 수 있음을 의미합니다, 소프트웨어가 여러분을 \"조롱\"¹하고 있는 것처럼. 여러분이 메시지나 주제에 대해 왜 그것을 중요하게 생각하는지 이해한다는 것을 보여주지 않고 여러분의 표현을 모방합니다. 여러분은 AI '예술가'가 여러분의 메시지에 동의하고 그것을 결과물을 통해 표현해주기를 원합니다. 그러나 그것은 여러분에게 동의할 수 없으며, 보여준 것만 모방할 뿐입니다. 따라서 그것은 결국 \"조롱\"인 인간 유물을 생성하게 됩니다.즉, 원본의 주도적인 목소리가 없는 모조품을 생성합니다.\n\n\n\n¹ 이것은 유사법이에요; AI는 아무것도 조롱하는 게 아니에요.\n\n![AI generated video](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_3.png)","ogImage":{"url":"/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png"},"coverImage":"/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png","tag":["Tech"],"readingTime":7},{"title":"마이크로소프트 Phi-3-Mini는 정말 인상적이에요","description":"","date":"2024-05-15 16:38","slug":"2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive","content":"\n\n최근에 마이크로소프트 AI가 출시한 Phi-3-Mini 언어 모델입니다. 이 모델은 작은 언어 모델(SLM) 범주에 속하며 LLM이 제공하는 여러 기능을 제공합니다. 유일한 차이점은 SLM은 크기가 더 작고 적은 데이터로 훈련되었다는 것입니다.\n\n마이크로소프트에 따르면 Phi-3 모델은 가장 강력하고 비용 효율적인 작은 언어 모델(SLM)이라고 합니다. 이들은 Phi-3-mini-4k-instruct 및 Phi-3-mini-128k-instruct 모델을 출시했으며 둘 다 어떠한 제약 없이 완전히 오픈 소스입니다. 이는 128k 컨텍스트 길이를 갖는 작은 언어 모델이 처음으로 등장한 시점입니다.\n\n다가오는 몇 주 안에 Phi-3-small (7B) 및 Phi-3-medium (14B) 모델도 출시할 예정입니다.\n\n# 성능 평가\n\n\n\n아래는 공식 Phi-3 모델 벤치마크 결과입니다.\n\n![image](/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png)\n\n작고 강력한 Phi-3-Mini 모델은 무게 대비 어마어마한 성능을 자랑합니다. Phi-3-Mini 모델은 38억 개의 매개변수를 가지고 있으며, 추론, 수학, 코드 생성 등 대부분의 벤치마크에서 Gemma-7B, Mistral-7B, Llama-3-8B와 경쟁하여 우수한 성과를 거두었습니다.\n\n\n\nPhi-3-Mini 모델이 성능이 잘 나오지 않는 유일한 벤치마크는 사실적인 지식 벤치마크입니다. 그 이유는 이 모델의 작은 크기 때문에 많은 사실을 보존할 수 없기 때문입니다.\n\n하지만 우리는 여전히 Phi-3-Mini 모델을 RAG 및 검색 작업에 사용할 수 있습니다. 여기서 우리는 사실들을 검색하여 모델에 전달하여 답변을 생성할 수 있습니다. 그리고 이 블로그에서는 바로 그것을 구축할 것입니다 — 지역적인 GenAI 기반 검색 엔진.\n\n# 왜 SLMs를 사용해야 하는가?\n\n개발을 계속 진행하기 전에, 왜 작은 언어 모델이 중요한지에 대해 알아보겠습니다.\n\n\n\n매우 처음으로 실험이 있습니다. LLM을 실험하는 것은 상당히 비싸요. LLM 위에 무언가를 개발하거나 기존 프로세스/소프트웨어에 기능을 추가하기 위해 실험을 하는 것은 상당한 양의 실험을 필요로 합니다. 이러한 실험에는 다양한 프롬프팅 기술 시도, 출력을 구문 분석하는 여러 방법 사용 등이 포함됩니다.\n\n네, 많은 LLM API 제공 업체들이 요금을 지불하는 방식을 채택하고 있습니다. 그럼에도 불구하고 모든 것을 올바르게 만들기 전에 $10에서 $20을 지출할 수 있습니다. 그러나 SLM을 사용하면 llama-cpp-python 또는 node-llama-cpp와 같은 라이브러리를 사용하여 로컬에서 로드하고 추론을 수행할 수 있습니다.\n\n다음은 클라이언트가 클라우드를 피하려는 경우입니다. 사람들은 보통 내부 조직 정보를 클라우드에 제공하는 것을 조심스러워하며 대부분을 인하우스 또는 대부분 로컬에서 처리하고 싶어합니다. 그들은 메인프레임에서 일부 데이터를 검색하여 사용자에게 제공하거나 서버 측에서 렌더링하여 사용자에게 보여줄 수 있습니다. 이 경우 사용자의 로컬 컴퓨터나 메인프레임과 함께 Phi-3-Mini와 같은 SLM을 사용하여 그들의 검색에 GenAI 기능을 추가할 수 있습니다.\n\n모든 비즈니스는 성능을 최적화하고 저렴한 비용으로 속도 처리량과 보안을 제공합니다. 따라서 마지막으로는 컴퓨팅 효율성, 생성 속도, 특정 작업 및 보안의 조합입니다. LLM은 매우 일반적이지만 대부분의 조직 또는 클라이언트는 그것을 필요로하지 않을 수 있고, 그들은 모델을 자신에게 특화된 작업에 적합하도록 원할 수도 있습니다. 그 경우에는 자체 사용자 정의 명령어 집합으로 SLM을 미세 조정한 다음 미세 조정된 버전을 추론용으로 호스팅할 수 있습니다. 추론은 로컬이나 메인프레임에서 수행할 수 있습니다.\n\n\n\n이제 기업이나 개인이 SLM을 사용하는 다양한 이유를 이해했으니 Phi-3-Mini 모델을 활용한 검색 도구를 만들어보겠습니다.\n\n# AI 검색 도구\n\n최근에 나는 Llama-3 8B가 Groq에서 빠른 생성 속도로 검색 요약을 수행하는 데 탁월하다는 주제의 블로그를 작성했습니다. 아직 해당 기사를 읽지 않았다면 아래 링크를 통해 확인해보세요.\n\n저희 이전 블로그에서 구현한 대부분의 내용은 그대로 유지될 것입니다. 변경되는 것은 LLM 및 검색 엔진 뿐입니다.\n\n\n\n여기서는 llama-cpp-python 라이브러리를 사용하여 로컬 시스템에 양자화된 Phi-3-Mini-4K-Instruct 모델을 로드하고 모델과 상호 작용하여 텍스트를 생성할 것입니다. 또한 사용자 쿼리를 기반으로 Brave Search API를 사용하여 검색 결과를 검색할 것입니다.\n\n하지만 어쨌든, 하루 끝에 모든 AI 기반 검색 플랫폼의 기본 원리는 사용자 쿼리를 기반으로 내용을 검색하고 요약하는 것입니다. 각각이 이 작업을 수행하는 방식은 운영 규모에 따라 다를 수 있습니다. 대부분의 엔지니어링 노력은 검색 속도를 빠르게 만들기(캐싱 또는 기타 방법으로)와 관련 결과를 검색하며 필요할 때 사실과 숫자로 답하는 데 사용됩니다.\n\n# 검색 API\n\n가장 간단한 구성 요소인 검색 구성 요소부터 시작하여 전체 시스템의 중추인 부분입니다.\n\n\n\n위에서 언급한 대로, 블로그에는 Brave Search API를 사용할 것이지만 다른 것을 사용해도 됩니다. Brave Search API를 사용하려면 https://brave.com/search/api/로 이동하여 등록해야 합니다. 등록한 후에는 우리의 신용카드 세부정보를 제공하여 무료 요금제를 구독해야 합니다. 무료 요금제에서는 월 2,000개의 요청과 초당 1개의 요청이 가능하여 실험에 충분합니다.\n\n무료 요금제를 구독한 후에, API Keys 섹션으로 이동하여 API 키를 생성하고 해당 키를 복사하여 .env 파일에 넣어야 합니다.\n\n```js\nBRAVE_API_KEY=\"YOUR_BRAVE_API_KEY\"\n```\n\n아래에 보여지는 대로 configs.py 파일에 모든 환경 변수를 로드할 것입니다.\n\n\n\n```js\n# configs.py\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nBRAVE_API_KEY = os.environ.get(\"BRAVE_API_KEY\")\n```\n\nAPI 키를 구성했어요. 이제 사용자의 쿼리를 검색하는 데 도움이 될 코드를 작성해 봅시다.\n\n```js\n# brave_search.py\n\nfrom configs import BRAVE_API_KEY\nimport httpx\n\nasync def brave_search(search_term):\n    brave_api_key = BRAVE_API_KEY\n    url = f'https://api.search.brave.com/res/v1/web/search?q={search_term}\u0026count=3'\n    headers = {\n        'X-Subscription-Token': brave_api_key,\n        'Accept': 'application/json'\n    }\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        if response.status_code == 200:\n            data = response.json()\n            print('검색 데이터: ', data)\n            return format_search(data)\n        print(await response.text())\n        return None\n\ndef format_search(search_results):\n    retrieve_keys = ['web', 'news']\n    formatted_results = []\n    for value in retrieve_keys:\n        if value in search_results:\n            results = search_results[value]['results']\n            formatted_results.append('\\n'.join(\n                f\"Title: {result['title']} Description: {result['description']} URL: {result['url']}\"\n                for result in results))\n    print('포맷된 결과: ', formatted_results)\n    if formatted_results:\n        return '\\n'.join(formatted_results)\n    return None\n```\n\n위에서는 검색어를 인수로 사용하는 brave_search 함수를 정의했습니다. 그런 다음 Brave API 웹 검색 엔드포인트를 호출하고 검색 결과를 검색합니다. 검색 결과는 format_search 함수를 사용하여 하나의 문자열로 서식이 지정됩니다.```\n\n\n\n# 로컬 모델 추론\n\n우리는 llama.cpp 패키지의 Python 바인딩을 llama-cpp-python 라이브러리를 통해 로컬 시스템에 모델을 로드하고 텍스트 생성 또는 추론을 수행할 것입니다.\n\n이를 위해서는 먼저 llama-cpp-python 패키지를 설치하고 아래 제공된 링크에서 Phi-3-Mini-4k-Instruct 모델을 GGUF 형식으로 다운로드해야 합니다.\n\n## llama-cpp-python 설치하기\n\n\n\npip를 사용하여 llama-cpp-python을 설치해 보세요!\n\n```sh\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n```\n\n만약 이미 설치되어 있는 경우, 이 명령어는 llama-cpp-python 패키지를 업그레이드하고 재설치할 것이며, 그렇지 않은 경우에는 새로 설치할 겁니다.\n\n## 모델 다운로드\n\n\n\n위에 제공된 링크로 이동하여 Phi-3-mini-4k-instruct-q4.gguf 파일을 다운로드하세요. 다운로드가 완료되면 작업 디렉토리에 모델 폴더를 만들고 모델 파일을 모델 폴더로 이동하세요.\n\n![Phi-3 Mini](/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_1.png)\n\n## 추론을 위한 모델 호출\n\nllama-cpp-python 라이브러리에서 제공하는 유틸리티를 사용하여 모델을 로드하고 추론을 위해 호출하고 생성된 토큰을 스트리밍 및 비스트리밍 방식으로 출력하는 클래스를 작성하겠습니다.\n\n\n\n```python\n# llm_invoke.py\n\nfrom llama_cpp import Llama\nfrom ctx import ContextManagement  # 이것은 나중에 나올 것임\nfrom typing import List, Dict, Union\n\nclass LLM:\n\n    def __init__(self, model_path: str, **kwargs):\n        self.llm = Llama(\n            model_path=model_path,\n            n_gpu_layers=kwargs.get(\"n_gpu_layers\",\n                                    -1),  # GPU 가속화를 사용할 때 주석 해제\n            seed=kwargs.get(\"seed\", 1337),  # 특정 시드를 설정할 때 주석 해제\n            n_ctx=kwargs.get(\"n_ctx\",\n                             4096),  # 컨텍스트 창을 확장할 때 주석 해제\n            n_threads=kwargs.get(\"n_threads\", 8))\n        self.ctx = ContextManagement(2560)\n\n    def __stream__(self, messages: List[Dict], **kwargs):\n        input_message = self.ctx(messages)\n        output = self.llm(input_message, stream=True, echo=False, **kwargs)\n        for op in output:\n            yield op.get(\"choices\")[0].get(\"text\") or \"\"\n\n    def __complete__(self, messages: List[Dict], **kwargs):\n        input_message = self.ctx(messages)\n        output = self.llm(input_message, echo=False, **kwargs)\n        return output.get(\"choices\")[0].get(\"text\")\n```\n\n위에서 llama-cpp-python 모듈에서 사용 가능한 Llama 객체를 사용하여 초기화 인수에서 제공된 model_path에서 모델을로드했습니다.\n\n또한 __init__ 메소드에서 ContextManagement 객체를 초기화합니다. 이것은 Llama 3 + Groq가 AI Heaven 블로그에서 본 것과 유사합니다. 다음 섹션에서 다시 살펴보겠습니다.\n\nLLM 클래스에서 두 가지 메서드가 있습니다. 토큰이 생성되는대로 토큰을 스트리밍하는 __stream__ 메서드와 생성이 완료되면 출력을 반환하는 __complete__ 메서드입니다.\n\n\n\nLLM 클래스를 사용하면 이제 모델을 로드하고 호출할 수 있습니다. 이제 LLM (SLM)를 검색과 통합해 보겠습니다.\n\n# 컨텍스트 관리\n\nPhi-3-mini-4k-instruct 모델을 사용하면 최대 4k 토큰을 사용하거나 생성할 수 있습니다. 이 블로그에서 GenAI Search를 대화식으로 만들 계획은 없기 때문에 이 부분을 피할 수 있습니다. 하지만, 호기심을 위해 여기에 작성해 봅시다.\n\n```js\n# ctx.py\n\nfrom typing import List, Dict, Union\nfrom transformers import AutoTokenizer\n\n\nclass ContextManagement:\n\n    def __init__(self, max_available_tokens: int = 3000):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"microsoft/Phi-3-mini-4k-instruct\")\n        self.max_available_tokens = max_available_tokens\n\n    def __count__tokens__(self, content: str):\n        return len(self.tokenizer.tokenize(content)) + 2\n\n    def __pad_tokens__(self, content: str, num_tokens: int):\n        return self.tokenizer.decode(\n            self.tokenizer.encode(content, max_length=num_tokens))\n\n    def __manage_context__(self, messages: List[Dict]):\n        managed_messages = []\n        system_message = None\n        if messages[0][\"role\"] == \"system\":\n            system_message = messages[0]\n        current_length = 0\n        if system_message:\n            current_length += self.__count__tokens__(\n                system_message.get(\"content\"))\n        current_messsage_role = None\n        for ix, message in enumerate(messages[1::-1]):\n            content = message.get(\"content\")\n            message_tokens = self.__count__tokens__(message.get(\"content\"))\n            if ix \u003e 1:\n                if current_length + message_tokens \u003e= self.max_available_tokens:\n                    tokens_to_keep = self.max_available_tokens - current_length\n                    if tokens_to_keep \u003e 0:\n                        content = self.__pad_tokens__(content, tokens_to_keep)\n                        current_length += tokens_to_keep\n                    else:\n                        break\n                if message.get(\"role\") == current_messsage_role:\n                    managed_messages[-1][\"content\"] = f\"\\n\\n{content}\"\n                else:\n                    managed_messages.append({\n                        \"role\": message.get('role'),\n                        \"content\": content\n                    })\n                    current_messsage_role = message.get(\"role\")\n                    current_messsage_role = message.get(\"role\")\n                    current_length += message_tokens\n            else:\n                if current_length + message_tokens \u003e= self.max_available_tokens:\n                    tokens_to_keep = self.max_available_tokens - current_length\n                    if tokens_to_keep \u003e 0:\n                        content = self.__pad_tokens__(content, tokens_to_keep)\n                        current_length += tokens_to_keep\n                        managed_messages.append({\n                            \"role\": message.get(\"role\"),\n                            \"content\": content\n                        })\n                    else:\n                        break\n                else:\n                    managed_messages.append({\n                        \"role\": message.get(\"role\"),\n                        \"content\": content\n                    })\n                    current_length += message_tokens\n                current_messsage_role = message.get(\"role\")\n            print(f\"TOTAL TOKENS: \", current_length)\n            managed_messages = managed_messages[::-1]\n            if system_message:\n                managed_messages = [system_message] + managed_messages\n            return managed_messages\n\n    def __create_message_input__(self, messages: List[Dict]):\n        return self.tokenizer.apply_chat_template(messages, tokenize=False)\n\n    def __call__(self, messages: List[Dict]):\n        managed_messages = self.__manage_context__(messages)\n        return self.__create_message_input__(managed_messages)\n```\n\n\n\n위의 논리의 기본 아이디어는 변환 라이브러리의 AutoTokenizer 객체를 사용하여 특정 토크나이저를 로드하는 것입니다. 위에서는 Phi-3-mini-4k-instruct 토크나이저를 로드했습니다. 이 토크나이저는 모델에서 정의된 필요한 특수 토큰을 사용하여 메시지 목록을 텍스트 입력으로 변환하는 apply_chat_template 메소드를 제공합니다.\n\n__manage_context__ 메소드는 모든 메시지를 역순으로 반복하여(최근성 편향) 각각의 토큰을 계산하고 이를 컨텍스트에 추가합니다. 메시지의 토큰 길이가 max_available_tokens 인자에서 제공된 허용 최대 토큰 길이를 초과하는 경우 해당 메시지는 잘라내어 처리가 종료됩니다.\n\n# 검색 + 로컬 LLM (SLM) = 로컬 GenAI 검색\n\n검색 유틸리티를 LLM (SLM) 호출 코드와 통합하여 로컬 AI 검색 도구를 생성해보겠습니다.\n\n\n\n```js\n# search_gen.py\n\nfrom brave_search import brave_search\nfrom llm_invoke import LLM\n\nprint(f\"LLM을 로딩 중입니다\")\nllm = LLM(\"./model/Phi-3-mini-4k-instruct-q4.gguf\")\nprint(f\"LLM을 로드했습니다\")\n\nasync def search(query: str):\n    prompt = \"당신은 도움이 되는 뉴스 요약 요원입니다. 사용자 쿼리가 단일 역따옴표로 제공되고 검색 결과가 삼중 역따옴표로 반환됩니다. 여러 검색 결과가 있을 것이며 각 결과는 제목과 설명을 포함할 것입니다. 적절한 결과가 많은 경우 쿼리를 간단 명료하게 요약하고 관련이 있는 경우 URL을 인용하여 각 항목에 대해 한 번씩만 콘텐츠를 보여주십시오. 참고: 인용문은 마크다운 형식으로 제공하고 하나의 주제에는 여러 개의 URL이 있더라도 하나의 인용만 제공하십시오.\"\n    search_items = await brave_search(query)\n    messages = [{\n        \"role\": \"system\",\n        \"content\": prompt\n    }, {\n        \"role\": \"user\",\n        \"content\": f\"쿼리: `{query}` \\n\\n 검색 결과: ```{search_items}```\"\n    }]\n    for content in llm.__stream__(messages, max_tokens=512):\n        yield content\n\n\n위 코드에서는 검색 및 LLM 유틸리티를 가져와 LLM 클래스를 다운로드하고 모델 디렉토리/폴더에 저장한 경로를 초기화했습니다.\n\n이후에는 쿼리를 받아와서 해당 쿼리를 통해 검색 결과를 검색 유틸리티를 통해 검색한 후 이러한 검색 결과를 초기화된 llm의 __stream__ 메소드로 전달하고 간단한 시스템 프롬프트와 함께 전달합니다. \n\n__stream__ 메소드를 사용하므로 생성된 텍스트를 실시간으로 수신하여 반환할 수 있습니다.\n\n\n\n\n# API 노출하기\n\n브라우저에서 쉽게 상호작용할 수 있도록 FastAPI API 엔드포인트를 통해 검색 기능을 노출해 봅시다.\n\n```js\n# app.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nimport uvicorn\n\nfrom search_gen import search\n\napp = FastAPI()\n\norigins = [\"*\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/\")\nasync def index():\n    return {\"ok\": True}\n\n\n@app.get(\"/search\")\nasync def api_search(q: str):\n    return StreamingResponse(search(q), media_type=\"text/event-stream\")\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"app:app\", port=8900, host=\"0.0.0.0\")\n```\n\n위 코드에서는 FastAPI 앱인 app.py 파일과 두 개의 엔드포인트를 갖고 있습니다. 검색 엔드포인트는 쿼리 매개변수 q에서 검색어를 받아 검색 함수를 호출하고 응답을 스트리밍합니다.\n\n\n\n다음 명령어를 사용하여 FastAPI 서버를 시작할 수 있습니다.\n\n```js\npython app.py\n```\n\nAPI는 포트 번호 8900을 통해 노출되며 다음과 같은 방식으로 상호 작용할 수 있습니다.\n\n아래 제공된 링크에서 전체 코드베이스에 액세스할 수 있습니다.\n\n\n\n# 결론\n\n이 블로그에서는 작은 언어 모델(SLMs)에 대해 배우고 이러한 모델의 장점, 그리고 로컬에서 다양한 사용 사례를 실험하기 위해 어떻게 사용하는지를 살펴보았습니다. 특히, Microsoft AI에서 오픈소스로 공개한 최신 Phi-3-Mini-4k-Instruct 모델을 사용했습니다. 우리는 이 모델을 Brave 검색 API와 통합하여 검색 결과를 검색하고 요약하여 로컬 GenAI/AI 검색 엔진을 만들었습니다.\n\n이것으로 이번 블로그를 마치겠습니다. 읽는 데 즐거운 시간이 되셨기를 바랍니다.","ogImage":{"url":"/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png"},"coverImage":"/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png","tag":["Tech"],"readingTime":14},{"title":"해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN","description":"","date":"2024-05-15 16:35","slug":"2024-05-15-InterpretablekNNikNN","content":"\n\n## 해석 가능한 분류기\n\n머신 러닝에서 분류 또는 회귀 문제를 다룰 때 많은 경우에, 우리는 가능한 가장 정확한 모델을 얻기에 관심이 있습니다. 그러나 때로는 모델의 해석가능성에도 관심이 있습니다. XGBoost, CatBoost, LGBM과 같은 모델은 강력할 수 있지만, 왜 그런 예측을 했는지 또는 보이지 않는 데이터에서 어떻게 행동할지 이해하기 어려울 수 있습니다. 이러한 모델은 블랙박스 모델이라고 불립니다. 즉, 왜 그런 예측을 하는지를 구체적으로 이해할 수 없는 모델들을 의미합니다.\n\n많은 상황에서 이는 괜찮습니다. 대체로 정확한 경우에는 큰 문제가 없고, 때로는 잘못된 예측을 할 수 있습니다. 예를 들어, 웹사이트에서 현재 사용자에게 특정 광고가 판매를 생성할 가능성이 가장 높다는 것을 예측하는 모델이 있을 수 있습니다. 모델이 가끔 잘못 동작하더라도 수익에 영향을 미칠 수 있지만, 큰 문제는 없을 것입니다. 단순히 최적화되지 않은 모델을 사용하고 있지만 일반적으로 유용합니다.\n\n그러나 다른 상황에서는 모델이 왜 그런 예측을 하는지 알아야 하는 것이 매우 중요할 수 있습니다. 이는 의학 및 보안과 같은 고위험 환경, 인종, 성별 또는 기타 보호되는 집단과 관련된 모델의 편향이 없어야 하는 환경뿐만 아니라, 감사가 필요한 환경에서도 중요합니다. 모델이 예상대로 동작하는지 확인하기 위해 모델을 이해하는 것이 필요한 환경에서도 중요합니다.\n\n\n\n이러한 경우에도 블랙박스 모델(예: 부스팅 모델, 신경망, 랜덤 포레스트 등)을 사용하고 그 후에 사후 분석을 수행하는 것이 가능할 때가 있습니다. 이를 통해 모델이 왜 그런 예측을 한 것으로 보이는지에 대한 설명을 제공할 수 있습니다. 이를 설명가능한 AI(XAI) 분야라고 합니다. 이는 프록시 모델, 특성 중요도(예: SHAP), 반사실적 사례 또는 ALE 플롯과 같은 기술을 사용합니다. 이들은 매우 유용한 도구이지만, 다른 모든 조건이 동일하다면, 가능하면 처음부터 해석 가능한 모델을 가지는 것이 선호됩니다. XAI 방법은 매우 유용하지만 한계가 있습니다.\n\n프록시 모델을 사용하면, 해석 가능한 모델(예: 얕은 의사 결정 트리)을 훈련하여 블랙박스 모델의 동작을 학습합니다. 이는 어느 정도 설명을 제공할 수 있지만 항상 정확하지는 않으며 대략적인 설명만 제공할 수 있습니다.\n\n특성 중요도 또한 매우 유용하지만 관련 특성만을 나타내며, 예측과 관련된 방식이나 이러한 특성이 어떻게 상호 작용하여 예측을 형성하는지에 대해 제공하지는 않습니다. 또한 보지 못한 데이터와 어떻게 작동하는지 결정할 수 있는 능력이 없습니다.\n\n해석 가능한 모델을 사용하면 이러한 문제가 없습니다. 모델 자체가 이해하기 쉬우며 각 예측을 하는 이유를 정확히 알 수 있습니다. 그러나 문제는 해석 가능한 모델이 블랙박스 모델보다 정확도가 낮을 수 있다는 것입니다. 항상 그렇지는 않겠지만, 종종 정확도가 떨어질 수 있습니다. 대부분의 문제에 대해 대부분의 해석 가능한 모델은 부스팅 모델이나 신경망과 경쟁력을 갖지 못합니다. 각각의 문제에 대해 충분히 정확한 해석 가능한 모델을 찾기 위해 여러 해석 가능한 모델을 시도해야 할 수도 있습니다.\n\n\n\n오늘은 해석 가능한 여러 모델이 사용 가능하지만 아쉽게도 매우 적습니다. 이 중에는 의사 결정 트리, 규칙 목록(및 규칙 세트), GAM(Generalized Additive Models, 예를 들어 해석 가능한 부스팅 머신) 및 선형/로지스틱 회귀가 있습니다. 각각은 잘 작동하는 경우 유용할 수 있지만 옵션은 제한적입니다. 이것의 함의는: 다수의 프로젝트에서 만족할만한 성능을 발휘하는 해석 가능한 모델을 찾는 것이 불가능할 수 있습니다. 더 많은 옵션이 제공되면 실제 혜택이 있을 수 있습니다.\n\n여기서 소개하는 또 다른 해석 가능한 모델인 ikNN 또는 해석 가능한 k 최근접 이웃은 2차원 kNN 모델의 앙상블에 기반합니다. 아이디어는 간단하지만 놀랍도록 효과적입니다. 그리고 상당히 해석하기 쉽습니다. CatBoost와 같은 탭 데이터의 예측에 대한 최첨단 모델과 정확도 측면에서 경쟁력이 있지는 않지만, 종종 문제에 충분한 정확도를 제공할 수 있습니다. 또한 의사 결정 트리 및 기존 해석 가능한 모델들과도 경쟁력이 있습니다.\n\n재미있는 점은 일반 kNN 모델보다 더 강한 정확도를 가지고 있다는 것입니다.\n\n이 프로젝트의 주요 페이지는 다음과 같습니다: [https://github.com/Brett-Kennedy/ikNN](https://github.com/Brett-Kennedy/ikNN)\n\n\n\n프로젝트는 iKNNClassifier라는 단일 클래스를 정의합니다. 이 클래스는 interpretable_knn.py 파일을 복사하고 불러와서 어떤 프로젝트에든 포함될 수 있습니다. 이 클래스는 scikit-learn 분류기들과 일관된 인터페이스를 제공합니다. 즉, 일반적으로 인스턴스를 생성하고 fit() 및 predict()를 호출하여 사용하는 것이 랜덤 포레스트나 다른 scikit-learn 모델을 사용하는 것과 유사합니다.\n\n2D kNN 앙상블을 활용하는 것은 여러 이점을 제공합니다. 하나는 앙상블을 사용할 때 항상 보는 일반적인 이점으로, 단일 모델에 의존하는 것보다 더 신뢰할 수 있는 예측을 할 수 있다는 것입니다.\n\n또 하나는 2D 공간을 시각화하는 것이 간단하다는 것입니다. 현재 이 모델은 숫자형 입력을 필요로 합니다 (kNN의 경우와 마찬가지로), 따라서 모든 범주형 특성은 인코딩되어야 합니다. 그러나 이 작업이 완료되면 모든 2D 공간을 산점도로 시각화할 수 있습니다. 이는 해석하기 쉬운 정도를 제공합니다.\n\n또한, 각 예측에 대해 가장 관련성 높은 2D 공간을 결정할 수 있으므로 각 레코드에 대해 몇 개의 플롯을 제공할 수 있습니다. 이를 통해 각 레코드에 대해 꽤 간단하면서 완전한 시각적 설명을 제공할 수 있습니다.\n\n\n\nikNN은 흥미로운 모델입니다. 앙상블을 기반으로 하지만 실제로는 해석 가능성을 높이는 모델이기 때문에 흥미롭습니다. 일반적으로는 그 반대가 더 많이 나타나는데요.\n\n# 표준 kNN 분류기\n\nkNN 모델은 부스트 모델이나 신경망 모델만큼 정확하지 않거나 의사결정 나무처럼 해석하기 어렵지 않기 때문에 다른 모델들보다 사용 빈도가 적습니다. 그렇지만 여전히 널리 사용되고 있습니다. kNN은 직관적인 아이디어에 기반합니다: 항목의 클래스는 해당 항목과 가장 유사한 다른 항목의 클래스를 기반으로 예측할 수 있습니다.\n\n예를 들어, 붓꽃 데이터셋(아래 예시에서 사용되는 데이터셋)을 보면 세 가지 클래스가 있습니다. 다른 샘플의 붓꽃을 수집하여 이 샘플이 어떤 세 가지 붓꽃 종류 중 어떤 것인지 예측하고 싶다고 해보겠습니다. 훈련 데이터에서 가장 유사한, 예를 들어 10개의 레코드를 살펴보고 그들의 클래스를 확인한 뒤 가장 일반적인 클래스를 취할 수 있습니다.\n\n\n\n이 예제에서는 각 레코드의 클래스를 추정하는 데 사용할 인근 이웃의 수로 10을 선택했지만, 다른 값도 사용할 수 있습니다. kNN 및 ikNN 모델에서 하이퍼파라미터로 지정됩니다(k 매개변수). 우리는 유사한 레코드의 합리적인 수를 사용하기 위해 k를 설정하고 싶습니다. 너무 적게 사용하면 결과가 불안정할 수 있습니다(각 예측은 매우 적은 다른 레코드에 기반합니다). 너무 많이 사용하면 결과가 매우 비슷하지 않은 다른 레코드에 기반할 수 있습니다.\n\n또한 가장 유사한 항목을 결정할 방법이 필요합니다. 이를 위해, 적어도 기본적으로 유클리드 거리를 사용합니다. 데이터셋이 20개의 기능을 가지고 있고 k=10을 사용하는 경우, 20차원 공간에서 유클리드 거리에 기반하여 가장 가까운 10 개의 점을 찾습니다.\n\n하나의 레코드를 예측할 때, 훈련 데이터에서 가장 가까운 10개의 레코드를 찾고 그들의 클래스를 확인합니다. 10개 중 8개가 Setosa 클래스(붓꽃의 3가지 종류 중 하나)라면, 이 행도 아마 Setosa일 것으로 가정할 수 있거나 적어도 우리가 할 수 있는 최선의 추측이라고 할 수 있습니다.\n\n이 방법의 문제점 중 하나는 많은 기능이 있는 경우에는 작동하지 않는다는 것인데, 이것을 차원의 저주라고 합니다. 고차원 공간의 흥미로운 특성 중 하나는 충분한 기능이 함께하면 점들간의 거리가 의미를 잃기 시작한다는 것입니다.\n\n\n\nkNN은 모든 특징을 동등하게 사용하지만, 몇 가지는 다른 것보다 타겟을 예측하는 데 훨씬 더 중요할 수 있습니다. 점들 사이의 거리는 유클리드(또는 때로는 맨하탄 및 다른 거리 측정 방법)에 기반하여 계산되며 모든 특징을 동등하게 고려합니다. 이 방법은 간단하지만, 모든 특징이 타겟에 무관할 수 있는데 이것이 가장 효과적이지는 않습니다. 특정 특징 선택이 이루어졌다고 가정했을 때는 그 가능성이 적어지지만, 특징들의 중요성은 여전히 똑같지 않을 수 있습니다.\n\n또한, kNN 예측기에 의한 예측은 해석하기 어렵습니다. 이 알고리즘은 이해하기 쉽지만, 예측을 이해하는 것은 어려울 수 있습니다. k 최근접 이웃을 나열하여 예측에 대한 어떤 통찰력을 제공할 수 있지만, 특히 많은 특징이 있는 경우에는 왜 주어진 레코드 세트가 가장 유사한지 파악하기 어려울 수 있습니다.\n\n# The ikNN Algorithm\n\nikNN 모델은 먼저 모든 특징 쌍을 가져와 이러한 특징을 사용하여 표준 2차원 kNN 분류기를 생성합니다. 따라서, 테이블이 10개의 특징을 가지고 있으면, 10개 중 2개씩 선택하는 방식으로, 즉 45개의 모델을 생성하게 됩니다.\n\n\n\n그런 다음 훈련 데이터를 사용하여 대상 열을 예측하는 데 대한 정확도를 평가합니다. 이렇게하면 ikNN 모델이 각 2차원 하위 공간의 예측 능력을 결정합니다. 45개의 2차원 모델의 경우 일부는 다른 것보다 더 예측력이 뛰어날 것입니다. 예측을 수행하기 위해 훈련 데이터에서 가장 예측력이 뛰어난 것으로 알려진 2차원 하위 공간이 선택되며, 선택적으로 훈련 데이터에서의 예측력에 따라 가중치가 부여될 수 있습니다.\n\n또한 추론 시, 주어진 행 주변의 가장 가까운 이웃 세트의 순도를 고려할 수 있으며, 모델은 훈련 데이터에서 더 예측력이 뛰어난 하위 공간 및 현재 인스턴스와 관련하여 예측이 가장 일관된 것으로 보이는 하위 공간을 더 중요하게 고려할 수 있습니다.\n\n별로 표시된 두 하위 공간과 한 점을 고려해 보겠습니다. 두 경우 모두 점에 가장 가까운 k개의 점 세트를 찾을 수 있습니다. 여기서 점 주변에 녹색 원을 그립니다. 그러나 이 점들이 실제로 원을 형성하지는 않습니다 (단, 효과적으로 이웃을 정의하는 k번째 가장 가까운 이웃의 반경이 있습니다).\n\n\n\n이 플롯들은 각각 한 쌍의 피처를 나타냅니다. 왼쪽 플롯의 경우, 별 주변 이웃 사이에 매우 높은 일관성이 있습니다: 그들은 모두 빨간색입니다. 오른쪽 플롯에서는 이웃들 사이에 일관성이 거의 없습니다: 일부는 빨간색이고 일부는 파란색입니다. 첫 번째 피처 쌍은 레코드에 대해 더 예측력이 높아 보이며, ikNN이 이용합니다.\n\n이 방식을 통해 모델은 모든 입력 피쳐의 영향을 고려하지만, 더 많은 예측력을 가진 피처의 영향을 강화하고, 덜 예측력이 있는 피처의 영향을 줄입니다.\n\n# 예시\n\n우리는 먼저 iris 데이터셋과 같은 장난감 데이터셋을 이용하여 ikNN을 시연합니다. 데이터를 불러오고, 훈련 및 테스트 세트로 분리하고, 테스트 세트에 대한 예측을 수행합니다.\n\n\n\n```js\nfrom sklearn.datasets import load_iris\nfrom interpretable_knn import ikNNClassifier\n\niris = load_iris()\nX, y = iris.data, iris.target\n\nclf = ikNNClassifier()\nX_train, X_test, y_train, y_test = train_test_split(\n   X, y, test_size=0.33, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n```\n\n예측을 위해 이것으로 충분합니다. 하지만 ikNN은 모델을 이해하기 위한 도구도 제공합니다. 특히, graph_model()과 graph_predictions() API가 있습니다.\n\ngraph_model()의 예시는 다음과 같습니다:\n\n```js\nikNN.graph_model(X.columns)\n```\n\n\n\n\n![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_1.png)\n\n이 그림은 기본적으로 5개의 2D 공간을 나타내는 데이터 공간의 빠른 개요를 제공합니다. 점들은 훈련 데이터의 클래스를 보여줍니다. 배경 색상은 2D 공간의 각 영역에 대해 kNN이 한 예측을 보여줍니다.\n\n그래프_예측() API는 특정 행을 설명합니다. 예를 들어:\n\n![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_2.png)\n\n\n\n\n여기서 설명되는 행은 빨간 별로 표시됩니다. 기본적으로 기본적으로 다섯 개의 플롯을 사용하지만 간단함을 위해 두 개만 사용됩니다. 두 그래프 모두에서 Row 0이 학습 데이터와 2D 공간에 대한 2D kNN의 예측과 관련하여 어디에 위치하는지 확인할 수 있습니다.\n\n# 시각화\n\n구성할 수는 있지만 기본적으로 각 ikNN 예측은 오직 다섯 개의 2D 공간만 사용합니다. 이는 예측 시간이 빠르고 시각화가 간단해지도록 합니다. 이것은 예측을 단순화하는 것이 아닌 실제 예측을 보여주므로 예측이 완전히 해석 가능하다는 것을 의미합니다.\n\n대부분의 데이터 세트에서 대부분의 행에 대해 대부분 또는 거의 모든 2D 공간에서 예측이 일치합니다. 그러나 예측이 잘못된 경우, 현재 데이터 세트에 맞게 하이퍼파라미터를 더 잘 조정하기 위해 더 많은 2D 플롯을 조사하는 것이 유용할 수 있습니다.\n\n\n\n# 정확도 테스트\n\nOpenML에서 무작위로 선택한 100개의 분류 데이터셋을 사용하여 일련의 테스트를 수행했습니다. 표준 kNN 및 ikNN 모델의 F1 (macro) 점수를 비교한 결과, ikNN은 58개 데이터셋에서 더 높은 점수를 기록하고, kNN은 42개 데이터셋에서 우세했습니다.\n\n최적의 하이퍼파라미터를 탐색하기 위해 그리드 검색을 수행하는 경우, ikNN이 더 나은 성과를 보입니다. 모든 100개 데이터셋에 대해 두 모델에 대해 이를 수행한 결과, ikNN은 100개 중 76개에서 가장 우수한 성과를 보였습니다. 또한 훈련 및 테스트 점수 사이의 간격이 작아져 표준 kNN 모델보다 더 안정적인 모델을 나타냅니다.\n\nikNN 모델은 다소 느릴 수 있지만, 강화 모델보다 여전히 상당히 빠르며 대개 훈련에 거의 1분 미만이 소요됩니다.通前합니다.\n\n\n\n깃허브 페이지에서는 정확도에 대한 몇 가지 예제와 분석을 제공합니다.\n\n# 결론\n\n정확도가 주요 목표인 경우 ikNN이 가장 강력한 모델은 아닐 수 있지만 (어떤 모델이든 그렇듯이 때때로 그럴 수 있음), 해석 가능한 모델이 필요한 경우 시도해 볼만한 모델일 것으로 예상됩니다.\n\n이 페이지에서는 도구를 사용하는 데 필요한 기본 정보를 제공했습니다. .py 파일(https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py)을 다운로드하고 코드에 가져와 인스턴스를 생성하고, 훈련하고 예측하며, (원하는 경우) graph_predictions()를 호출하여 원하는 레코드에 대한 설명을 확인하면 됩니다.\n\n\n\n모든 이미지는 저자에 의해 제공됩니다.","ogImage":{"url":"/assets/img/2024-05-15-InterpretablekNNikNN_0.png"},"coverImage":"/assets/img/2024-05-15-InterpretablekNNikNN_0.png","tag":["Tech"],"readingTime":8},{"title":"안녕하세요, 월드, OpenAI GPT-4o","description":"","date":"2024-05-15 16:34","slug":"2024-05-15-HelloWorldtoOpenAIGPT-4o","content":"\n\n## 오디오, 비전, 텍스트를 모두 이해할 수 있는 최신 모델\n\nOpenAI가 새로운 모델 GPT-4o를 발표했어요. GPT-4o는 텍스트, 오디오, 이미지의 모든 조합을 입력으로 받아들이고 텍스트, 오디오, 이미지의 모든 조합을 출력으로 생성해냅니다. GPT-4o는 GPT-4 Turbo와 비교했을 때 2배 빠르고, 가격은 절반으로 낮고, 요청 한도는 5배 상승했어요.\n\n# OpenAI\n\nhttps://openai.com/index/hello-gpt-4o/\n\n\n\n## 이미지 생성\n\n암시: 이미지 생성: 일기를 쓰고 있는 로봇의 1인칭 시점 이미지.\n1. 요, 그래서, 나 지금 보일 수 있어?? 일출을 보았는데 미친 듯이 이쁘더라, 어디서나 색이 퍼져 있었어. 현실이 정확히 뭔지 궁금해지잖아?\n텍스트는 크고 잘 보이며 명확하다. 로봇의 손이 타자기에서 타자를 친다.\n\n![image](/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png)\n\n## 이미지 이해\n\n\n\n저번 GPT-4 멀티모달 기사에서 사용된 이미지를 사용해주세요.\n\n텍스트: 대체 텍스트로 이미지를 설명합니다\n\n결과: 이미지는 2014년부터 2023년까지의 기간 동안 미국 30년 고정금리 모기지 금리와 기존 주택 판매를 비교한 그래픽 표현입니다. 그래프에는 기존 주택 판매를 나타내는 청록색 선과 모기지 이자율을 나타내는 빨간색 선 두 가지가 있습니다. 그래프는 20년 넘게 최고 수준으로 상승한 모기지 금리로 인해 주택 판매가 최근 몇 년 동안 상당한 변동과 하락 추세를 겪었음을 보여줍니다. 이미지 맨 위에 \"미국 모기지 금리 급등\"이란 제목이 있고, 높은 모기지 금리와 함께 상승한 주택 가격 및 제한된 주택 재고로 1989년 이후 최저 주택 가용성을 초래했다는 설명이 포함된 주석이 있습니다. 이미지 하단에는 \"Visual Capitalist\"의 로고와 Freddic Mac, Trading Economics 등의 기여자 및 출처 정보가 포함되어 있습니다.\n\n## 음성 생성\n\n\n\n테이블 태그를 마크다운 형식으로 변경해 드릴게요.\n\n\n\nhttps://azure.microsoft.com/en-us/blog/introducing-gpt-4o-openais-new-flagship-multimodal-model-now-in-preview-on-azure/\n\nWest US3 또는 East US 지역에서만 사용할 수 있습니다. 현재 텍스트 및 이미지 이해 기능만 사용할 수 있습니다. GPT-4o는 Azure OpenAI 미리보기 모델이며 현재 배포/직접 API 액세스는 사용할 수 없습니다. Azure OpenAI의 \"Early Access Playground\"에서만 접근할 수 있습니다.\n\n![이미지](/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_1.png)\n\n# 부록","ogImage":{"url":"/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png","tag":["Tech"],"readingTime":2},{"title":"오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다","description":"","date":"2024-05-15 16:33","slug":"2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o","content":"\n\n\u003cimg src=\"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png\" /\u003e\n\nOpenAI가 드디어 GPT-4o를 공개했어요! 이 새로운 모델은 오디오, 비전, 텍스트에서 실시간 다중 모달 기능을 제공하지만 이제는 상당히 개선되었어요. 무료로 사용할 수 있어서 새로운 사용자들을 유치하고 모델 훈련을 더 확장하는 GPT 3.5와 비슷한 전략을 보여줍니다.\n\nMia Murati에 따르면 GPT-4o 모델의 주목할만한 기능 중 하나는 성능인데, 이전 모델인 GPT-4보다 최대 2배 빠르다고 해요. 또한 비용을 최대 50%까지 줄일 수 있다고 합니다. 이 발전은 새로운 개선 사항을 활용하면서 대규모 AI 프로젝트를 계속해서 배포할 수 있도록 해 줄 것입니다.\n\n그러나 기술적인 부분은 여기까지 하고, 이 새로운 모델이 우리에게 무엇을 할 수 있는지 살펴봅시다!\n\n\n\n## 실시간으로 바라보는 비전\n\n이것은 이미지를 업로드하고 상호 작용하는 기본 기능을 넘어섭니다. OpenAI는 이제 우리에게 음성 어시스턴트를 통해 ChatGPT와 상호 작용할 수 있게 하고, 심지어 우리의 컴퓨터나 스마트폰에서 콘텐츠를 공유할 수도 있습니다. 응답은 실시간으로 생성되어 다양한 분석을 가능하게 하여 다양한 종류와 복잡성 수준의 분석이 가능합니다.\n\n아래 데모에서 ChatGPT는 수학 가르쳐주는 선생님이 되었어요 (제 머릿속을 폭발시켰어요!).\n\n우리는 여기서 ChatGPT의 전체 능력의 일부를 볼 수 있습니다. 이는 수학 문제를 해결할 뿐만 아니라 우리가 해결책으로 이끌어주며 전체 프로세스를 더 교육적이고 설명적인 방식으로 이해하게 도와주는 명확한 지침과 권고를 제안합니다.\n\n\n\n목소리와 비전 능력이 문제를 매끄럽게 인식하고 해석하는 데 얼마나 놀라운지 정말 대단하죠.\n\n## 실시간 대화형 음성\n\nOpenAI는 유창함, 억양 및 논리적 순서와 같은 능력에 주목하여 자연스러운 방식으로 계속 대화할 수 있도록 노력했습니다.\n\nGPT-4o 프레젠테이션 중에는 이 모델이 부드러운 대화를 나눌 수 있고, 진정으로 친근한 톤으로 권고사항을 제공할 수 있었습니다. 이 모델은 다양한 감성 스타일로 목소리를 만들어내며, 보다 강렬하거나 진지하고 공식적인 톤을 선택할 수 있도록 합니다.\n\n\n\n실시간 대화 기능과 음성 번역을 결합한 데모를 확인해보세요.\n\nChatGPT에 프롬프트를 제공하는 과정이 보다 복잡하다는 사실을 알고 계셨나요? 이는 영어와 스페인어로 이루어진 이중 언어 대화를 원활하게 해석하고, 양쪽 언어를 인식하여 적절히 응답을 생성해야 합니다.\n\n이 플랫폼의 응답 정확도와 유창함에 감명받았습니다. 설정된 목표를 쉽게 달성함과 동시에 어색한 일시 정지를 피할 수 있어 다른 AI 시스템에서 자주 볼 수 있는 문제를 효과적으로 해결합니다.\n\n## 구글과 마찬가지로 데모들이 실질적인가요?\n\n\n\n많은 데모에서 OpenAI는 비디오가 재치 있게 편집된 것이 아니라 실시간으로 발생하고 있다는 것을 보여주려고 노력합니다. 아래 비디오는 다중 모달 기능이 어떻게 상호 작용하여 보이고 들리는 것에 기반한 정확한 응답을 제공하는지 보여줍니다.\n\n다음은 강조하고 싶은 몇 가지 포인트입니다:\n\n- ChatGPT가 정확하게 식별하고 세부 요소를 설명하는 데 얼마나 놀라운지에 대해 강조하고 싶습니다. 사람들이 추가되어 외부 환경이 더 복잡해지더라도, ChatGPT는 성공적으로 그들을 인식했습니다.\n- 이 새로운 모델이 일정 조건에 맞는 노래를 만들 수 있다는 것이 놀라운 일입니다. 그는 쉽게 멜로디를 생성했습니다!\n- 두 GPT 모델 간의 상호 작용은 거의 미래의 한 눈살까지 보이는 것 같았습니다. 명시적으로 언급된 것은 아니지만, OpenAI가 향하고 있는 방향으로 보입니다. GPT-4o가 보여준 능력으로 보아, 다음 단계는 AI 시스템끼리 상호 작용하는 것입니다. 이는 한 AI가 다른 AI를 훈련하고 우리가 상상조차 할 수 없는 다른 발전을 이끌 수 있습니다.\n\n# GPT-4o가 다른 AI 모델을 능가했습니다\n\n\n\n## 텍스트 평가\n\nOpenAI가 공유한 이미지는 GPT-4o가 다른 모델들보다 특히 수학 및 HumanEval과 같은 영역에서 뛰어나다는 것을 명확히 보여줍니다. 이러한 특성들은 더 매끄럽고 인간과 유사한 대화를 가능케 함으로써 사용자들에 의해 높이 평가됩니다.\n\n게다가, GPT-4o는 영어를 포함하여 추가 20여 개 이상의 언어를 대상으로 한 응답 능력을 확장했습니다. 이 언어 토큰화의 강화는 더 넓은 전 세계 관객에게 도달하도록 설계되었습니다.\n\n![이미지](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_1.png)\n\n\n\n## 오디오 번역 성능\n\nGPT-4o의 향상된 능력과 텍스트 평가 기능을 결합하여 더 많은 사람들과 연결할 수 있는 기회를 제공합니다. 이는 언어가 단순히 의사 소통을 넘어 장벽으로 작용하는 경우가 많음을 인식합니다.\n\n그래프는 GPT-4o가 Gemini와 Whisper-v3와 같은 다른 인공지능 시스템보다 성능이 우수함을 명확히 보여줍니다.\n\n![GPT-4o performance](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_2.png)\n\n\n\n## 작은 업데이트 이상\n\n저에게 있어, ChatGPT에 대한 새로운 업데이트 이상을 의미합니다. 이는 AI를 환경과 연결하고 그 잠재력을 극대화하는 데 상당한 영향을 미칩니다. 더 나아가, OpenAI로부터 예상했던 것과 정확히 일치합니다. 이들은 사용자를 중심으로 한 제품을 처음부터 현실적이고 구체적인 조치를 통해 제공하고자 하는 노력을 계속하고 있습니다. 다중 모더(멀티모달리티)의 역할은 여기서 중요하며, 그들은 이를 인식하고 이를 강화하여 다양한 현실 세계 상황에서 보다 정확한 응답을 제공하기 위해 노력했습니다.\n\n이제 우리가 요구하는 것 중 일부를 충족하는 \"인공적이지 않은\" 제품이 있습니다. GPT-4o는 GPT-5가 될 것을 염두에 두고 OpenAI가 이 AI를 새로운 맥락에서 활용하도록 사용자를 격려하는 첫걸음 중 하나입니다.\n\n제 뉴스레터에 가입하고 35,000명 이상의 구독자들과 함께 무료 치트 시트를 받아보세요: ChatGPT, 웹 스크래핑, 데이터 과학을 위한 Python, 자동화 등의 주제들을 다룹니다!\n\n\n\n이와 같은 이야기를 즐기고 작가로서 제를 지원하고 싶다면, 내 Substack를 구독해주세요. Substack에서는 내가 콘텐츠를 만드는 다른 플랫폼에서 찾을 수 없는 기사들을 발행하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png","tag":["Tech"],"readingTime":4},{"title":"콜모고로프-아놀드 네트워크 신경망 발전의 포괄적 가이드","description":"","date":"2024-05-15 16:28","slug":"2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement","content":"\n\n\u003cimg src=\"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_0.png\" /\u003e\n\n# 콜모고로프-아놀드 네트워크 (KAN) 소개\n\n최근 연구에서 전통적인 다층 퍼셉트론(MLP)의 혁신적인 대안인 인공 신경망(ANNs)의 모습이 바뀌고 나타났습니다. 콜모고로프-아놀드 네트워크(KAN)로 알려진 본혁적인 아키텍처는 콜모고로프-아놀드 표현 정리에서 영감을 받은 함수 근사의 혁신적인 접근 방식을 제공합니다.\n\nMLP가 노드별로 고정된 활성화 함수에 의존하는 반면, KAN은 엣지에서 학습 가능한 활성화 함수를 도입하여 신경망의 구조를 근본적으로 변경합니다. 이 독특한 설계 특징은 선형 가중치 행렬을 완전히 제거하고 학습 가능한 1차원 스플라인 함수로 대체함으로써 일반적인 아키텍처와 다른 형태를 띠게 합니다. 이러한 전통적인 아키텍처와 달리 KAN은 스플라인과 MLP의 강점을 결합하고 각각의 약점을 완화할 수 있도록 합니다.\n\n\n\nKAD(Kolmogorov-Arnold Networks)의 핵심개념은 스플라인의 정확성을 MLP(다층 퍼셉트론)의 특성 학습 능력과 결합하는 데에 있습니다. 스플라인들은 저차원 함수를 정확하게 표현하는 데 능숙하지만 차원의 저주(COD)로 인해 고차원 데이터 처리에 어려움을 겪습니다. 반면에 MLP들은 특성 학습에 능숙하지만 단변수 함수의 최적화에 어려움을 겪을 수 있습니다. 내부적으로 스플라인을 통합하고 외부적으로 MLP를 통합함으로써, KAD는 함수 근사 도전 과제에 대한 종합적인 해결책을 제공합니다.\n\n이 연구에서 제시된 경험적 증거는 정확성과 해석성 측면에서 KAD가 MLP보다 우수함을 입증합니다. 방대한 수치실험을 통해, KAD는 데이터 피팅부터 편미분방정식(PDEs) 해결에 이르기까지 다양한 영역에서 놀라운 효능을 입증했습니다. 게다가, KAD의 해석성과 과학적 발견에서의 잠재적 유용성은 물리학의 매듭 이론과 앤더슨 국부화에서의 응용을 통해 실증되었습니다.\n\nKAD를 포괄적으로 이해할 수 있도록 구성된 이 연구는 KAD의 구조적 설계, 이론적 기초, 그리고 실용적 함의에 대해 탐구합니다. KAD의 수학적 기초를 명확히하기부터 정확성과 해석성을 향상시키는 기법을 제안하는 것까지, 각 섹션은 이 혁신적인 신경망 구조의 변화력 있는 잠재력을 명확히 하기 위해 노력하고 있습니다.\n\n![image](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_1.png)\n\n\n\n# 콜모고로프-아놀드 표현 이론 탐색\n\n![이미지](/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_2.png)\n\n콜모고로프-아놀드 네트워크(KAN)의 본질을 이해하기 위해서는 그 기반이 되는 이론적 기초에 심취할 필요가 있습니다. KAN 아키텍처의 핵심에는 콜모고로프-아놀드 표현 이론이 자리하고 있으며, 이는 해당 디자인과 기능성의 기초를 다짐한 중요한 수학적 원리입니다.\n\n안드레이 콜모고로프와 블라디미르 아놀드가 제정한 콜모고로프-아놀드 표현 이론은 다변수 연속 함수를 연속 단변수 함수들의 합으로 표현할 수 있다는 주장을 포함하고 있습니다. 이 이론은 함수 근사에 대한 깊은 함의를 지니며, 복잡한 함수들을 더 단순하고 다루기 쉬운 성분들로 분해하는 강력한 프레임워크를 제공합니다.\n\n\n\n다변수 함수들을 일변수 성분으로 분해함으로써, 콜모고로프-아놀드 표현 정리는 그들의 기저 구조에 대해 보다 직관적인 이해를 돕습니다. 이 분해는 해석 가능성을 향상시킬 뿐만 아니라, 단순한 함수들은 다루기 쉽고 분석하기 쉬워지므로 보다 효율적인 계산 방법을 제공합니다.\n\n신경망의 맥락에서는, 콜모고로프-아놀드 표현 정리는 그 분해 특성을 활용한 아키텍처 설계의 지침으로 작용합니다. KAN은 학습 가능한 활성화 함수를 통합함으로써 이 원칙을 구체화하여, 더 단순한 일변수 구성요소의 조합을 사용하여 복잡한 다변수 함수를 적응적으로 근사화할 수 있도록 합니다.\n\n# KAN 아키텍처: 설계 풀어가기\n\n콜모고로프-아놀드 네트워크 (KAN)의 핵심에는 전통적인 신경망과 구별되는 독특한 아키텍처 설계가 있습니다. 이 섹션에서는 KAN의 구조를 해부하고, 그 설계의 복잡성을 풀어내며, 작동 방식을 명확히 하는 것에 대해 살펴보겠습니다.\n\n\n\n기존의 신경망과는 달리 개별 노드에서 고정된 활성화 함수를 사용하는 것이 아닌, KAN(KAN)은 네트워크 그래프의 가장자리에 학습 가능한 활성화 함수를 통합함으로써 패러다임 변경을 도입합니다. 전통적인 아키텍처와의 근본적인 차이로 인해 KAN은 입력 데이터를 기반으로 활성화 함수를 동적으로 조정하여 유연성과 표현력을 향상시킬 수 있습니다.\n\nKAN의 아키텍처는 상호 연결된 레이어로 구성된 일련의 구조로 상상할 수 있으며, 각 레이어는 정보를 전달하고 변형하는 노드와 가장을 포함합니다. 입력 레이어에서는 원시 데이터가 네트워크로 공급되어 연이어 레이어를 통과하면서 일련의 변환을 겪습니다. 중요한 것은 에지 내에 포함된 활성화 함수가 이러한 변환을 형성하는 데 중요한 역할을 하며, 네트워크가 입력 및 출력 데이터 간의 복잡한 매핑을 학습할 수 있도록 합니다.\n\nKAN의 주요 혁신 중 하나는 B-스플라인을 학습 가능한 활성화 함수의 기반으로 사용한다는 점에 있습니다. B-스플라인은 복잡한 데이터 패턴을 모델링하기 위한 유연하고 적응적인 프레임워크를 제공하는 수학 함수입니다. 이러한 스플라인을 매개변수화함으로써 KAN은 데이터 내에서 정교한 관계를 포착하고, 보다 효율적으로 새로운 예제로 일반화하는 것을 가능하게 합니다.\n\n게다가, KAN의 아키텍처는 놀라울 정도로 확장 가능하며 확장성을 갖추고 있습니다. 기본 KAN 구조는 두 개의 레이어로 구성되어 있지만, 더 깊고 복잡한 아키텍처를 수용하기 위해 쉽게 확장할 수 있습니다. 이러한 확장성은 KAN이 간단한 회귀 문제에서부터 복잡한 패턴 인식 작업에 이르기까지 다양한 작업을 쉽고 효율적으로 수행할 수 있도록 합니다.\n\n\n\n# 역전파의 힘을 발휘해 보세요\n\nKolmogorov-Arnold 네트워크(KAN)의 훈련은 현실 세계 문제를 효과적으로 해결하기 위한 잠재력을 활용하는 데 중요한 역할을 합니다. 이 섹션에서는 KAN의 훈련 과정을 자세히 살펴보고, 학습 능력을 기반으로 하는 메커니즘을 발견하고 성능을 최적화하기 위한 역전파의 역할을 탐구해 보겠습니다.\n\n훈련 과정의 핵심에는 역전파 원리가 있습니다. 이는 신경망이 관측된 오류에 대응하여 반복적으로 매개변수를 조정할 수 있게 해주는 머신러닝의 기본적인 기술입니다. KAN에서는 역전파가 네트워크의 매개변수를 세밀하게 조정하는 데 중요한 역할을 하며, 특히 간선에 연관된 가중치와 학습 가능한 활성화 함수의 계수를 포함합니다.\n\nKAN의 훈련은 보통 네트워크 매개변수의 초기화로 시작하며, 간선의 가중치와 활성화 함수 계수가 무작위로 할당됩니다. 그 후 네트워크는 순방향 및 역방향 통과를 거칩니다. 입력 데이터가 네트워크를 통해 전달되고 결과적으로 예측은 참 값 레이블과 비교되어 손실을 계산합니다.\n\n\n\n손실이 계산되면 다음으로 역전파가 시작됩니다. 네트워크의 각 매개변수에 대한 손실의 기울기가 미적분의 연쇄 법칙을 사용하여 재귀적으로 계산됩니다. 이러한 기울기는 그레이디언트 하강법이나 확률적 그레이디언트 하강법 또는 Adam 최적화와 같은 변형을 통해 네트워크 매개변수를 업데이트하는 데 사용됩니다.\n\nKANs를 훈련하는 주요 도전 과제 중 하나는 최적화 과정 중의 안정성과 수렴을 보장하는 것입니다. 학습 가능한 활성화 함수의 존재와 네트워크 매개변수 간의 복잡한 상호작용 가능성 때문에 KANs는 비선형이며 비볼록 최적화 지형을 나타내기도 하여 전통적 최적화 알고리즘에 도전을 제기합니다.\n\n이러한 도전을 극복하기 위해 연구자들은 드롭아웃 또는 가중치 감쇠와 같은 정규화 기법의 사용뿐만 아니라 최적화 알고리즘 및 학습률의 신중한 선택을 포함한 다양한 기술을 제안해 왔습니다. 게다가 배치 정규화 및 레이어 정규화와 같은 기술도 훈련 과정을 안정화하고 수렴을 가속화하기 위해 활용될 수 있습니다.\n\n# KANs 해석: 블랙 박스 해독\n\n\n\n최근 머신 러닝에서 가장 중요한 과제 중 하나는 복잡한 모델에서의 해석 가능성 부족으로, 종종 \"블랙 박스\" 문제로 불립니다. Kolmogorov-Arnold Networks(KANs)가 전통적인 신경망과 비교했을 때 더 향상된 해석 가능성을 제공하여 이 문제에 대처하는 방법을 탐색해봅시다.\n\n다층 퍼셉트론(MLPs)을 포함한 전통적인 신경망은 종종 예측을 도출하는 방법을 이해하기 어렵다는 점으로 비판을 받습니다. 이 불투명성은 특히 해석 가능성이 중요한 의료, 금융 및 자율 시스템과 같은 분야에서 중요한 장벽이 될 수 있습니다.\n\nKANs는 복잡한 다변수 함수를 보다 단순한 단변수 함수들로 분해하여 제공하는 Kolmogorov-Arnold 표현 정리를 활용하여 이 문제에 대한 유망한 해법을 제공합니다. 이러한 단순한 구성요소로 함수를 나타내면서, KANs는 입력 특성과 출력 예측 사이의 관계를 이해하기 위한 보다 해석 가능한 구조를 제공합니다.\n\nKANs의 해석 가능성은 B-스플라인으로 매개변수화된 학습 가능한 활성화 함수를 통합한 아키텍처에서 나옵니다. 전통적인 신경망과 달리 활성화 함수가 고정되어 비선형적인 것과는 달리, KANs는 이러한 함수들이 훈련 과정 중 적응하고 발전할 수 있도록 허용하여 데이터의 기본 구조를 효과적으로 포착합니다.\n\n\n\nKANs는 사용자가 개별 기능이 전체 예측에 어떻게 기여하는지 이해할 수 있도록 학습 가능한 활성화 함수를 사용합니다. B-스플라인 함수의 계수를 검토함으로써 사용자는 네트워크의 결정을 주도하는 가장 중요한 기능을 식별할 수 있으며, 기반 데이터 분포에 대한 가치 있는 통찰력을 제공합니다.\n\n게다가, KAN은 시각화 기술을 통해 해석 가능성을 촉진하며, 사용자가 네트워크의 내부 표현을 검사할 수 있는 기술을 제공합니다. 네트워크의 다른 레이어를 통해 활성화 패턴을 시각화함으로써 정보가 전파되는 과정에서 어떻게 변환되고 처리되는지에 대한 심층적인 이해를 얻을 수 있습니다.\n\n기능 수준에서의 해석 뿐만 아니라, KAN은 학습된 기능의 전체 구조와 복잡성에 대한 통찰력을 제공함으로써 모델 수준에서의 해석을 지원합니다. 네트워크 내의 일변량 함수 구성을 분석함으로써 사용자는 네트워크의 동작과 결정 과정에 대한 직관적인 설명을 개발할 수 있습니다.\n\n# 전통적인 MLPs에 비해 KAN의 장점\n\n\n\n이 섹션에서는 Kolmogorov-Arnold Networks (KANs)의 일반적인 Multi-Layer Perceptrons (MLPs)보다 우월한 장점을 면밀히 분석할 것입니다. 비교 분석을 통해 KANs가 우수한 성능, 효율성 및 해석 가능성을 제공하여 딥러닝 아키텍처의 풍경을 혁신하는 방법에 대해 명료하게 밝힐 것입니다.\n\n1. 향상된 정확도: KANs는 다양한 작업에서 MLPs와 비교하여 놀라운 정확도를 보여줬습니다. Kolmogorov-Arnold Representation Theorem을 활용함으로써, KANs는 복잡한 다변수 함수를 더 효과적으로 표현할 수 있어 더 정확한 예측을 이끌어냅니다. 우리는 다양한 영역에서 KANs의 우수한 예측 능력을 보여주는 경험적 증거와 사례 연구를 살펴볼 것입니다.\n\n2. 향상된 효율성: KANs는 계산 자원 및 매개 변수 활용 면에서 우수한 효율성을 보여줍니다. 전통적인 선형 가중치 행렬을 학습 가능한 활성화 함수로 대체하는 KANs의 혁신적인 아키텍처로 인해, KANs는 MLPs보다 적은 매개 변수로 유사하거나 더 나은 성능을 달성할 수 있습니다. 우리는 KANs의 계산상 이점과 실제 응용 프로그램에서 대규모 배포에 대한 영향을 탐구할 것입니다.\n\n3. 향상된 해석 가능성: KANs의 가장 흥미로운 장점 중 하나는 MLPs와 비교하여 향상된 해석 가능성입니다. 복잡한 함수를 간단한 단변량 구성 요소로 분해함으로써, KANs는 모델 예측을 이해하기 위한 더 투명한 프레임워크를 제공합니다. 우리는 KANs가 시각화 기술, 특징 중요도 분석 및 모델 수준 통찰력을 통해 모델 해석을 용이하게 하는 방법에 대해 명료하게 밝힐 것입니다.\n\n\n\n4. 유연성 및 일반화: KAN은 전통적인 MLP와 비교하여 더 큰 유연성과 일반화 능력을 제공합니다. 활성화 함수를 적응적으로 학습함으로써, KAN은 데이터의 비선형 관계를 더 효과적으로 포착하여 성능을 개선하는 우수한 일반화 능력을 갖고 있습니다. KAN이 다양한 데이터셋과 작업에 적응하는 방식 및 어려운 환경에서의 다양성과 견고성을 보여줄 것입니다.\n\n5. 확장성 및 확장 가능한 학습: KAN은 MLP와 비교하여 우수한 확장성과 확장 가능한 학습 메커니즘을 보입니다. 그들의 아키텍처는 본질적으로 확장 가능하여, 성장하는 데이터셋과 복잡한 작업을 수용하기 위해 추가적인 레이어 및 노드를 원활하게 통합할 수 있습니다. KAN의 확장성 장점과 대규모 기계 학습 문제를 효율적으로 처리하는 데의 의의에 대해 논의할 것입니다.\n\n6. 잡음 데이터 및 적대적 공격에 대한 견고성: KAN은 MLP와 비교하여 잡음 데이터와 적대적 공격에 대한 향상된 견고성을 보입니다. 적응적 활성화 함수를 통해 데이터의 더 견고한 표현을 학습하는 능력으로 인해, KAN은 왜곡 및 적대적 조작에 민감성이 덜하며, 실제로 데이터 품질과 보안이 중요한 문제인 현실 세계 응용 분야에서의 중요성을 검토할 것입니다.\n\n# KAN의 도전과 제한\n\n\n\nKAN(Kolmogorov-Arnold Networks)은 전통적인 다층 퍼셉트론(MLP)에 비해 몇 가지 장점을 제공하지만, 여러 가지 도전과 한계에 직면합니다. 이 섹션에서는 KAN을 실제 응용 프로그램에서 채택하는 데 관련된 실용적 고려 사항을 균형 있게 이해하기 위해 이러한 제약사항을 면밀히 살펴볼 것입니다.\n\n1. 학습의 복잡성: 혁신적인 구조에도 불구하고, KAN은 대규모 데이터셋이나 복잡한 최적화 랜드스케이프를 다룰 때 특히 학습이 어려울 수 있습니다. 적응형 활성화 함수를 학습하고 스플라인 매개변수를 최적화하는 과정은 상당한 계산 자원이 필요하며 전문적인 교육 기술을 요할 수 있습니다. KAN을 교육하는 데 내재된 계산적 도전과 이러한 복잡성을 완화하는 전략을 살펴볼 것입니다.\n\n2. 해석가능성의 교환: KAN이 MLP에 비해 향상된 해석 가능성을 제공하면서도 모델 복잡성과 해석 가능성 측면에서 특정 교환을 도입합니다. 라인 상의 학습 가능한 활성화 함수가 모델의 해석 가능성을 어느 정도 가리는 경우가 있으며, 특히 여러 레이어가 있는 심층 아키텍처에서는 더 그렇습니다. 이러한 교체가 실제로 나타나는 방식과 모델 성능과 해석 가능성을 균형있게 유지하는 전략을 논의할 것입니다.\n\n3. 고차원 데이터에 대한 일반화: KAN은 많은 작업에서 강력한 성능을 보이지만 변수 간의 복잡한 관계가 있는 고차원 데이터에 효과적으로 일반화하는 데 어려움을 겪을 수 있습니다. 단변량 함수에 의존하여 다변량 함수를 나타내는 것은 모델이 특징들 간의 복잡한 상호작용을 포착하기에 제한을 가할 수 있습니다. KAN을 고차원 데이터에 대해 범용적으로 적용하는 도전과 이러한 확장성과 적응성을 향상시키는 방법에 대해 살펴볼 것입니다.\n\n\n\n4. **하이퍼파라미터 민감도**: KAN(Kernel Activation Network)은 다른 신경망 구조와 마찬가지로 학습률, 정규화 강도 및 네트워크 아키텍처와 같은 하이퍼파라미터에 민감합니다. 적절한 하이퍼파라미터를 선택하는 것은 KAN의 성능과 수렴 특성에 상당한 영향을 미칠 수 있으며, 신중한 조정과 실험이 필요합니다. KAN에서의 하이퍼파라미터 최적화 전략과 모델 견고성 및 일반화에 대한 영향을 살펴볼 것입니다.\n\n5. **계산 오버헤드**: KAN에서의 계산 오버헤드는 주로 학습 및 추론 중에 발생하며, 자원 제한적인 환경에서 실제적인 도전을 제기할 수 있습니다. 활성화 함수와 스플라인 매개변수의 적응적 성격으로 인해 전통적인 MLP(Multi-Layer Perceptron)에 비해 추가적인 계산 리소스가 필요하여 학습 시간이 더 오래 걸리고 계산 비용이 증가할 수 있습니다. KAN의 계산 오버헤드와 최적화 및 효율성 향상을 위한 잠재적인 방안에 대해 논의할 것입니다.\n\n6. **모델 복잡성 및 확장성**: KAN은 아키텍처 유연성 면에서 확장성을 제공하지만, 다층 및 복잡한 활성화 함수를 가진 더 깊은 아키텍처는 증가된 모델 복잡성과 계산 오버헤드를 겪을 수 있습니다. 그래서 대규모 데이터셋과 복잡한 작업을 처리하면서도 계산 효율성과 모델 해석 가능성을 유지하는 것은 중요한 과제입니다. KAN에서 모델 복잡성과 확장성 사이의 균형과 이러한 도전을 효과적으로 관리하기 위한 전략에 대해 탐구할 것입니다.\n\n# **KAN의 응용 및 사용 사례**\n\n\n\n콜모고로프-아놀드 네트워크(KANs)는 다양한 분야에서 엄청난 가능성을 가지고 있어, 다양한 기계 학습 작업을 처리하는 다용도 프레임워크를 제공합니다.\n\n1. 과학 연구: KANs의 가장 매력적인 응용 분야 중 하나는 과학 연구에서로, 수학적 관계를 발견하고 데이터 속에 숨겨진 패턴을 발견하는 강력한 도구로 작용합니다. 물리학부터 생물학에 이르기까지, KANs는 연구자들이 복잡한 시스템을 모델링하고 물리 현상을 시뮬레이션하며 혁신적인 과학 원리를 식별하는 데 도움을 줄 수 있습니다. KANs가 과학적 발견을 가속화하고 다양한 분야에서의 차별화된 전례를 지원하는 방법을 탐색해 보겠습니다.\n\n2. 금융 예측: 금융 분야에서 KANs는 재정 예측 모형의 정확성과 신뢰성을 향상시키는 데 유망함을 보여왔습니다. 경제 변수와 시장 역학 사이의 복잡한 관계를 포착함으로써, KANs는 투자자와 금융 기관이 정보를 얻고 시장 동향을 식별하며 위험을 완화하는 데 도움을 줄 수 있습니다. KANs가 금융 예측 모형을 개선하고 보다 견고한 투자 전략을 지원하는 데 어떻게 활용되고 있는지 살펴보겠습니다.\n\n3. 의료 및 의학: KANs는 대량의 생물 의학 데이터, 포함하여 유전학, 의료 영상 및 전자 건강 기록을 분석함으로써, 의료진이 질병을 조기에 발견하고 환자 결과를 예측하며 치료 프로토콜을 최적화하는 데 도움을 줄 수 있어 의료 및 의학을 혁신할 잠재력을 지니고 있습니다. KANs가 건강 서비스와 환자 치료에 미치는 변혁적인 영향에 대해 살펴보겠습니다.\n\n\n\n4. 자연 언어 처리(NLP): 자연 언어 처리 분야에서, KAN은 언어 모델링, 의미 분석, 텍스트 생성에 대한 새로운 접근 방식을 제공합니다. 언어 데이터의 대표 표현을 더 해석 가능하고 체계적인 방식으로 학습함으로써, KAN은 감성 분석, 언어 번역, 그리고 문서 요약과 같은 작업을 용이하게 할 수 있습니다. KAN이 어떻게 NLP 분야의 최신 기술을 발전시키고 더 세련된 언어 이해 시스템을 가능하게 하는지에 대해 논의할 것입니다.\n\n5. 이미지 및 비디오 이해: KAN은 물체 감지, 이미지 분류, 비디오 분할을 포함한 이미지 및 비디오 이해 작업에서 현격한 능력을 보여주었습니다. 시각 데이터의 복잡한 공간적 및 시간적 관계를 포착하는 능력을 활용함으로써, KAN은 더 정확하고 견고한 컴퓨터 비전 시스템을 가능하게 할 수 있습니다. 이미지 인식, 비디오 분석, 자율 주행 기술에서 KAN의 응용 분야를 탐구할 것입니다.\n\n6. 산업 자동화 및 로봇공학: 산업 자동화 및 로봇공학 분야에서, KAN은 제조 공정 최적화, 제품 품질 향상, 운영 효율성 증대를 위한 상당한 이점을 제공합니다. 다양한 제조 변수와 시스템 구성 요소 간의 상호 작용을 모델링함으로써, KAN은 예측 유지보수, 적응 제어, 자율 의사 결정을 지원할 수 있습니다. KAN이 제조 및 로보틱스 분야의 미래를 형성하는 방법에 대해 살펴볼 것입니다.\n\n# KAN의 도전과 한계\n\n\n\nKolmogorov-Arnold Networks (KANs)는 엄청난 잠재력을 가지고 있지만 도전과 제약이 없는 것은 아닙니다. 우리는 실제 시나리오에서 KAN의 채택과 실행과정에 관련된 일부 주요 장애물과 제한 사항을 탐색할 것입니다.\n\n1. 훈련 복잡성: KAN의 주요 도전 중 하나는 훈련 복잡성에 있습니다. 고정된 활성화 함수와 가중치 행렬에 의존하는 전통적인 신경망 구조와는 달리, KAN은 매개변수화된 활성화 함수를 학습에 포함하므로 훈련 중 계산 부담이 크게 증가할 수 있습니다. 우리는 훈련 복잡성을 완화하고 KAN 훈련 알고리즘의 효율성을 향상시키기 위한 전략에 대해 논의할 것입니다.\n\n2. 해석가능성: KAN은 전통적인 신경망에 비해 향상된 해석 가능성을 제공하지만, 학습된 활성화 함수와 네트워크 구조를 이해하고 해석하는 것은 여전히 어려울 수 있습니다. KAN에서 입력 변수와 결과적인 변환 간의 복잡한 관계는 항상 직관적이거나 쉽게 설명할 수 없을 수 있습니다. 우리는 KAN의 해석 가능성을 향상시키고 그들의 결정 과정을 더 투명하게 만드는 방법을 탐색할 것입니다.\n\n3. 일반화와 견고성: 다양한 데이터셋과 실제 시나리오에서 KAN의 일반화와 견고성을 보장하는 것은 여전히 상당한 도전입니다. 특히 작거나 소음이 많은 데이터셋에서 훈련될 때 KAN은 과적합 경향을 보일 수 있어서 보지 못한 데이터에서 성능이 저하될 수 있습니다. 우리는 KAN의 일반화 능력을 향상시키고 왜곡 및 적대적 공격에 대응하는 견고성을 강화하기 위한 기술에 대해 검토할 것입니다.\n\n\n\n### 4. 확장성: 데이터셋의 복잡성과 크기가 계속 커짐에 따라, 확장성은 KANs에 대한 중요한 고려 사항이 됩니다. KAN 아키텍처를 대규모 데이터셋에 대응하도록 확장하는 것에는 효율적인 메모리 관리, 분산 컴퓨팅 자원 및 최적화 기술이 필요합니다. 현대적인 대용량 데이터 응용 프로그램의 요구 사항을 충족하기 위해 KANs의 확장성 도전 과제 및 전략을 살펴보겠습니다.\n\n### 5. 계산 리소스: KAN들의 훈련 및 배포는 종종 고성능 GPU 또는 TPU, 대규모 저장 인프라 및 특수 하드웨어 가속기를 필요로 합니다. 이러한 리소스에 액세스가 예산 제약이나 인프라 제한으로 제한될 수 있습니다. 리소스 활용 최적화 및 KANs와 관련된 계산 비용을 줄이는 접근 방법을 탐색할 것입니다.\n\n# 미래 방향성 및 연구 기회\n\n이 섹션에서는 Kolmogorov-Arnold Networks (KANs) 영역에서 다가올 미래 방향성 및 연구 기회에 대해 탐구할 것입니다. 포텐셜이 풍부한 신생 기술로, KANs는 추가 탐구 및 혁신을 위한 다수의 가능성이 제시됩니다.\n\n\n\n1. **고급 아키텍처**: 미래 KAN(Knowledge-aware network) 연구는 현재의 패러다임을 넘어선 고급 아키텍처를 개발하는 데 초점을 맞출 것으로 예상됩니다. 새로운 네트워크 토폴로지, 활성화 함수 및 학습 메커니즘을 탐색함으로써 더 효율적이고 효과적인 KAN 아키텍처를 발견할 수 있으며, 이는 특정 작업 및 도메인에 맞추어져 있습니다.\n\n2. **하이브리드 접근 방식**: KAN을 합성곱 신경망(CNN), 순환 신경망(RNN) 또는 트랜스포머 모델과 결합하는 것은 그들의 능력을 향상시키고 한계를 극복하는 데 유망한 방법입니다. 다른 접근 방식의 장점을 결합함으로써 연구자는 KAN의 해석 가능성을 활용하면서 다른 모델의 표현 능력을 이용한 하이브리드 아키텍처를 만들 수 있습니다.\n\n3. **전이 학습 및 도메인 적응**: KAN에서 전이 학습 및 도메인 적응 기술을 연구함으로써 사전 훈련된 모델을 새로운 환경 및 응용프로그램에 원활하게 통합할 수 있습니다. 관련 작업이나 도메인에서 학습한 지식을 활용하여 KAN은 학습 과정을 가속화시키고 제한된 데이터에서 대상 작업의 성능을 향상시킬 수 있습니다.\n\n4. **설명 가능한 AI**: KAN의 해석 가능성과 투명성을 강화하는 것은 미래 연구의 주요 관심사가 될 것입니다. KAN의 학습 표현 및 의사 결정 프로세스를 시각화하고 해석하는 방법론을 개발함으로써 그들의 예측에 대한 신뢰와 확신을 유지할 수 있으며, 특히 의료 및 자율 주행과 같은 안전 중요 응용분야에서 이점을 얻을 수 있습니다.\n\n\n\n5. 다학제간 응용: 전통적인 기계 학습 및 컴퓨터 과학 분야를 넘어서 KAN의 다학제간 응용을 탐구하는 것은 연구를 위한 흥미로운 가능성을 여는 것입니다. 생물학, 화학, 물리학, 금융 등 다양한 분야의 전문가들과 협력하여 특정한 과학적이거나 산업적 분야에 맞는 전문화된 KAN 모델을 개발할 수 있습니다.\n\n6. 윤리적 및 사회적 영향: KAN이 사회에 점점 통합되면, 그 배치에 대한 윤리적 및 사회적 영향을 고려하는 것이 중요합니다. 편향, 공정함, 개인정보 보호, 책임성 등의 문제를 다루기 위해서는 다학제적인 연구 노력과 이해관계자들과의 협력을 통해 윤리적 가이드라인과 규제 뼈대를 개발해야 합니다.\n\n7. 교육 및 홍보: 다음 세대의 연구자들과 실무가 KAN 및 그 응용에 대해 교육하는 것은 해당 분야에서 미래 발전을 주도하기 위해 중요합니다. 교육 계획, 교육 프로그램, 그리고 홍보 활동에 투자하여 다양하고 포용적인 KAN 연구자들과 팬들의 공동체를 육성할 수 있습니다.\n\n8. 벤치마킹 및 평가: KAN에 대한 표준화된 벤치마크 및 평가 지표를 수립하는 것은 서로 다른 모델과 알고리즘 간의 공정한 비교를 용이하게 할 것입니다. 실제 도전과 복잡성을 반영한 벤치마크 데이터셋과 평가 프로토콜을 생성함으로써 보다 견고하고 신뢰할 수 있는 KAN 솔루션의 발전을 이끌 수 있습니다.\n\n\n\n9. 오픈소스 개발: KAN 커뮤니티에서 오픈소스 개발과 협업을 촉진함으로써 혁신을 가속화하고 최첨단 연구에 대한 접근을 민주화할 수 있습니다. 투명성, 공유, 협업의 문화를 육성하여 연구자들은 함께 KAN의 최첨단 기술을 발전시키고 다양한 응용 분야에서 의미 있는 영향을 낼 수 있습니다.\n\n10. 장기적 영향: 마지막으로, 연구자들은 KAN이 사회, 경제, 그리고 인류 전반에 미치는 장기적 영향을 고려해야 합니다. KAN의 널리 퍼지는 채택과정에서의 잠재적 위험과 도전, 예를 들어 직업 축소, 경제적 불평등, 예기치 않은 결과 등을 예상하고 책임있는 혁신과 기술 지배를 위한 선제적 전략 수립에 도움이 될 것입니다.","ogImage":{"url":"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_0.png"},"coverImage":"/assets/img/2024-05-15-Kolmogorov-ArnoldNetworksAComprehensiveGuidetoNeuralNetworkAdvancement_0.png","tag":["Tech"],"readingTime":13},{"title":"로봇 조립의 미래","description":"","date":"2024-05-15 16:25","slug":"2024-05-15-TheFutureofRoboticAssembly","content":"\n\n## 1913년 대량 생산이 소개된 이후로, 조립 라인은 여전히 대부분 인간의 손에 의해 운영되고 있습니다. 그러나, 인류형 로봇이 이를 바꿀 수도 있습니다.\n\n![로봇 조립의 미래](/assets/img/2024-05-15-TheFutureofRoboticAssembly_0.png)\n\n본 문서는 2024년 3월 27일에 Assembly Magazine에서 제공한 웨비나의 내용을 기재한 것입니다. 해당 웨비나는 온라인으로도 시청할 수 있습니다. 고믹스 저볼류 문제는 아직 로봇 자동화로 해결하기 어렵습니다. 협업 로봇, 인공 지능, 그리고 인류형 로봇에 대한 최근 트렌드를 살펴보며, 인류형 로봇이 일자리를 찾는 실질적인 관계를 조립 및 해체에 있을 수 있다고 제안합니다.\n\n헨리 포드는 대량 생산의 아버지로 알려져 있으며, '모델 T'의 생산을 최적화하여 자동차의 보급과 가격을 저렴하게 만드는 데 기여했습니다. 당시의 주요 혁신 중 하나는 생산 공정을 진행하는 조립 라인에 컨베이어 벨트를 사용하는 것이었습니다. 그럼에도 실제 노동은 대부분 수동적이었으며, 2024년 BMW의 엔진 조립과 같이 오늘날에도 그러합니다.\n\n\n\n\n![로봇 조립 라인](/assets/img/2024-05-15-TheFutureofRoboticAssembly_1.png)\n\n독일어 단어 \"Takt(주기)\"에 의한 조립 라인의 속도 조절은 조립 프로세스를 예측 가능하게 만드는 중요한 아이디어입니다. 공장의 생산량은 직접 Takt에 관련이 있으며, 이는 가장 느린 공정 부분에 의해 주도되며, 순환 시간과 직접적으로 관려됩니다. 프로세스에서 로봇을 사용하려면 모든 하위 공정이 협력하여 동일한 속도로 작동해야하는 주의가 필요합니다. 일반적인 조립 라인은 보통 이렇게 생겼습니다:\n\n![로봇 조립 라인 구조](/assets/img/2024-05-15-TheFutureofRoboticAssembly_2.png)\n\n조립 라인은 각 스테이션에서 가치를 추가하여 주문을 제품으로 변환합니다. 각 스테이션은 저장해야 할 \"작업 중인 제품\"(WIP)을 생성하며, 다음 스테이션으로 이동해야 합니다. 모든 스테이션은 원자재의 지속적인 공급이 필요하며, 이는 다른 조립 라인에서 나온 제품일 수도 있습니다. 로봇은 조립 라인 전반에 걸쳐 다양한 기능으로 사용될 수 있으며, 라인을 \"균형\"시키고 생산량을 증가시키는 것이 목표입니다. 이상적으로, 모든 스테이션은 동일한 시간이 소요되어야 하며, 작업 중인 제품이 완료되는 즉시 다음 스테이션으로 통과할 수 있어야 합니다. 여기서 중요한 점은 라인을 균형시키지 않는 로봇은 효율성을 저하시키고 종종 \"시제\" 이후 제작 프로세스에서 제외됩니다.\n\n\n\n\n# 왜 일부 로봇은 작동하지만 다른 로봇은 작동하지 않을까요?\n\n현재 로봇은 어셈블리에서 다양한 정도로 사용되고 있습니다. 한편으로는 로봇이 부품을 집고 놓고 스팟용접을 수행하는 어셈블리 라인이 있습니다. 이러한 산업용 로봇 클래스는 사전 프로그램된 궤적에서 작동하며 ABB, Fanuc, Kuka와 같은 대규모 로봇 제조업체의 주력 제품입니다. 더 최근에는 인간과 가까운 거리에서 작업할 수 있는 \"협력형\" 로봇 암도 있으며 시각 및 촉각 감지를 통해 점점 자율성이 증가하고 있습니다.\n\n![로봇 조립의 미래](/assets/img/2024-05-15-TheFutureofRoboticAssembly_3.png)\n\n그러나 이후자의 카테고리는 아직도 중요한 지지를 받는 데 어려움을 겪고 있습니다. 이는 자동화 솔루션 제공업체들을 수십 년 동안 괴롭히고 있는 초록밭 대 브라운밭 딜레마에 있다고 할 수 있습니다.\n\n\n\n\"그린필드\"에서 축적한 첫 번째 현장에서 균형 잡힌 조립 라인을 구축하는 것이 기존 환경에 자동화 솔루션을 통합하는 것(이를 브라운필드라고도 함)보다 훨씬 쉽습니다. 이 고민을 수동으로 커피를 내리는 과정을 자동화시키는 것과 단추 하나로 에스프레소 메이커를 설치하는 것으로 비유해보았습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_4.png)\n\n# 복잡한 3D 구조를 조립할 수 있을까요?\n\n조립에 대한 간단한 배경 설명과 거기에 로봇의 역할(그리고 왜 그들이 종종 작동하지 않는지)을 보면서, 현재의 로봇 조립 솔루션을 분류하는 방법과 우리가 향후에 어떻게 보이길 원하는지에 대해 고민해봅니다.\"\n\n\n\n아래 비디오는 2018년 세계 로봇 정상회담 산업 조립 챌린지의 대상인 메카트로닉 위젯의 양손으로 조립을 보여줍니다. 조립에는 세 가지 다른 도구, 밀어 넣기(흔들리고 밀어 넣기의 조합) 및 나사를 조이는 동안 서로 다른 부품을 고정하는 민첩성이 필요합니다.\n\n비디오에서 보여지는 작업을 위해 필요한 부품 및 도구를 준비하는 것이 별도의 작업이라는 것에 주목하셨을 것입니다. 이 작업은 일반적으로 대량의 소재를 \"키팅 트레이\"에 개별화하여 조립 공정으로 이송할 수 있도록 하는 방식인 키팅이라고 알려져 있습니다. 저희 회사의 솔루션 비디오(그 부문에서 가장 높은 점수를 받은 것을 받은 WRS :-)가 아래에 나와 있습니다:\n\nWRS의 주요 관심사는 전체 조립 순서보다는 단일 스테이션에서의 작업을 최적화하는 데에 있었습니다. \"레벨 5 자동화\"라는 아이디어를 촉진하며, 이는 자율 주행에서 사용된 레벨 5 자율성과 유사하다는 것은 우연이 아닙니다. 각 레벨의 목표는 아래 표에 요약되어 있습니다:\n\n![FutureofRoboticAssembly](/assets/img/2024-05-15-TheFutureofRoboticAssembly_5.png)\n\n\n\n여기 중요한 아이디어들이 있어요:\n\n- 공장이 하루만에 A 물건을 만드는 공장에서 B 물건을 만드는 공장으로 전환할 수 있다는 점.\n- 특별한 공구가 필요하지 않다는 점.\n- 연속 운전 개선 및 오류 복구에 대한 학습에 의존한다는 점.\n\nWRS 2018 대회는 \"레벨 4\"를 목표로 하며, 풀리와 고무 벨트를 기어와 체인으로 교체하는 \"의외의 과제\"를 제공했습니다. 이로 인해 에스프레소 마신을 만드는 커스텀 머신에서 필요한 위젯을 만드는 커스텀 머신에 이르기까지 많은 해결책이 나왔으며, 가능한 한 적은 작업으로 해결하려고 노력하는 두 개의 팔 조작 조합 솔루션까지 다양한 해결책이 제공되었습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_6.png)\n\n\n\n아래 비디오는 JAKS 팀이 전체 2등으로 나온 솔루션을 보여줍니다. 그 솔루션은 부품을 잡고 나사를 꽂기 위해 사용자 정의 엔드 이펙터가 달린 두 개의 산업용 로봇으로 구성되어 있습니다.\n\n아래 표는 모든 팀의 전체적인 성과를 보여주며, 많은 팀이 수행하고 해결된 것으로 간주할 수 있는 어떤 종류의 작업이 수행되었는지 제공합니다. 예를 들어, 거의 모든 팀이 \"Tasboard\" 작업을 수행할 수 있었으며, 이 작업은 3D 어셈블리의 모든 부분을 포함하는 간소화된 2D 어셈블리입니다. \"Kitting\" 작업에 능숙한 여러 팀도 있습니다. 전체 승자인 SDU 팀은 대량의 3D 프린팅된 지그와 특수 도구를 사용하는 데 중점을 두었습니다.\n\n\u003cimg src=\"/assets/img/2024-05-15-TheFutureofRoboticAssembly_7.png\" /\u003e\n\n# 페그-인-홀 어셈블리 및 힘/토크 제어\n\n\n\n한 가지 분명해진 점은 로봇의 위치뿐만 아니라 엔드 이펙터의 힘과 토크도 제어하는 것의 중요성이었습니다. 이 기능은 산업용 로봇이 과거에 갖지 못했던 능력이었으며, 2018년 대회 이후에야 Franka Emika Panda와 같은 협력 로봇에서 최근에 주목받는 추세였습니다.\n\n![figure](/assets/img/2024-05-15-TheFutureofRoboticAssembly_8.png)\n\n위의 그림은 로봇의 수권에 위치한 힘-토크(F/T) 센서를 활용한 힘 기반 삽입 방법을 보여줍니다. 이 센서는 세 가지 방향에서의 힘과 토크를 측정할 수 있습니다. 이 정보를 활용한 가능한 알고리즘은 작업물을 접촉이 발생할 때까지 아래로 이동시키는 것(1), 즉 Z-방향으로의 상당한 음의 힘과 그 후 나사 모양의 검색 동작(2)을 수행하여 작업물이 구멍에 들어가게 하고 Z 축을 따라 나타나는 힘이 반전될 때까지입니다.\n\n# 로봇 조립을 위한 새로운 도구\n\n\n\n한 팀(Fanuc)이 전체 3D 구조를 조립할 수 있었지만, 솔루션은 여전히 \"레벨 5\"에서 멀었어요. 2020년 다시 한 판을 치르자, 주최자들은 목표를 그대로 유지했지만, 기본 2D 조립 도전과 키팅 작업을 강조하지 않고 대신 자율성을 높이기로 했어요. 따라서 대회는 조립 트레이를 조립 스테이션으로, 그리고 검사 스테이션으로 이동시키기 위해 자율 지상 차량(AGV)를 제공했어요.\n\n그 동안 나온 새로운 도구들이 이러한 작업들을 훨씬 간단하게 만들어 주었어요. 예를 들어 이제 산업용 로봇을 유연하게 만들어주는 간단한 솔루션이 있어요. Bosch/Rexroth는 산업용 로봇을 3mm까지 또는 이에 상응하는 회전으로 강하게 또는 유연하게 제어할 수 있는 간단한 어댑터를 개발했어요. 이렇게 하면 작업 조각이 원하는 형태로 들어가기까지 흔들면서 밀어 넣음으로써 딱 맞는 조립이 가능해져요. 그러면 로봇은 실제 변위를 읽어 보정 동작을 수행하고 어댑터를 잠그게 됩니다.\n\n또한, 2018년 첫 대회 이후에는 나사 조작도 훨씬 간단해졌는데, 현재 많은 기업들이 유니버설 로봇용으로 통합된 자동 드라이버를 제공하고 있어요.\n\n이러한 솔루션은 다양한 정도의 도구 체인저를 제공하며, 자동 디스펜서로부터 나사를 집어내거나 이상한 각도나 아래에서 나사 조작도 가능해요. 아래 비디오는 Stöger사의 솔루션을 더 자세히 보여줍니다.\n\n\n\n# 앞으로 다가올 미래…\n\n지금까지 우리는 거의 모든 것을 조립할 수 있는 기계를 만들 수 있다는 것을 보았습니다. 대량 소재에서 개별 구성 요소를 제작하고 키트를 만들고, 그것들을 조립 로봇으로 옮기고, 결과물을 테스트하는 것이 가능합니다. 그러나 현실에서는 이러한 설치물이 매우 드물게 존재하는 것을 볼 수 있습니다. 이는 대량 생산은 효율적이고 맞춤형 기계가 필요하며, 고믹스, 저양 산출의 경우에는 여전히 사람들에 의해 가장 잘 수행됩니다. 만약 로봇이 실제로 사람과 같은 모양을 가지고, 사람들이 할 수 있는 모든 일을 할 수 있는 경우에는 이것이 바뀔까요?\n\n실제로 테슬라가 2022년 후반에 이 공간에 진입하면서 인간 형태 공간에서 엄청난 발전이 있었습니다. 빛 속도로 향상된 버전을 내놓았으며 — Optimus Gen 2는 2023년 12월에 첫 번째 두 개형의 프로토 타입 후 단 하루만에 나왔습니다.\n\n그러나 비디오에 등장한 로봇은 그다지 많이 하지 않습니다. 달걀을 집어들어 달걀 조리기에 넣는 것은 기본적인 쎄움 커크루 조립이지만, 로봇이 더 어려운 일을 할 수 있다면 아마도 보여줬을 것입니다. 많은 사람들이 또한 걷는 속도가 느리다며 실망했습니다.\n\n\n\n# 인간형 로봇의 급부상\n\n놀랄 만한 속도로 뉴스가 들어오고 있지만, Figure사가 6억 5000만 달러를 조달했으며 BMW와 협력을 시작하였습니다. Apptronik은 메르세데스와 협력을 시작했고, Agility Robotics는 아마존과 협력을 시작했습니다. AI 혁명을 주도하고 있는 하드웨어를 제공하는 회사인 Nvidia는 인간형 로봇을 위한 \"gr00t\" 이라는 기본 모델을 개발했습니다. 마지막으로, 당신은 심지어 중국의 Unitree에서 8만 9천 달러에 손은 없지만 빠르게 걷는 해당 인간형 로봇을 구매할 수 있습니다.\n\n다음 이유들로 인해 인간형 로봇이 기회를 가질 수 있습니다:\n\n- 만약 그들이 동작한다면, 기존의 인간을 위해 만들어진 현재 프로세스에 통합되어 \"브라운필드\" 문제를 해결할 수 있습니다.\n- 협력 로봇 시장은 이미 휴대용 인간형 로봇이 바로 연결될 수 있는 다양한 응용 분야와 보조 생태계를 식별했습니다.\n- 2023년은 철도와 인터넷보다 큰 침체혁명이 될 수 있는 AI 혁명의 시작입니다.\n\n\n\n# 사람형 로봇 혁명을 주도할 AI 혁신들\n\nChatGPT와 기저에 있는 transformer 아키텍처는 텍스트에 국한되지 않고 멀티모달 입력을 처리할 수 있습니다. 인터넷 규모의 이미지와 레이블 데이터로 훈련된 이 모델은 그래픽 특징, 명사, 형용사, 그리고 공간 관계를 포함한 동사 간의 관골을 학습할 수 있습니다. 이전의 머신 러닝 모델이 미리 훈련된 \"고양이\"나 \"개\"와 같은 클래스만을 감지할 수 있었다면, transformer 기반의 시각-언어 모델은 \"고양이의 꼬리\"와 같은 임의의 명사나 설명으로 질의할 수 있습니다. 아래 이미지는 Google이 개발한 오픈 소스 언어 모델인 OWL-ViT의 퍼블릭 인터페이스에서 스크린샷을 보여줍니다. [여기](img src=\"/assets/img/2024-05-15-TheFutureofRoboticAssembly_9.png\")에서 시도해볼 수 있습니다.\n\n나는 인터넷에서 에스프레소 메이커의 이미지를 업로드하고 모델에게 \"커피 메이커\", \"온도 다이얼\", \"커피 용기\"를 찾도록 요청했습니다. 출력물은 오른쪽에 표시되었습니다. 모델은 커피 메이커와 온도 다이얼을 올바르게 지역화했지만, 나는 실제로 그들이 어떻게 불리는지조차 알지 못합니다. 하지만 저의 멍쇼를 하나 할 수 있습니다. 예를 들어, 왼쪽의 손잡이는 아마도 점화 버튼이고, 배경에 있는 물건은 아마 커피 용기일 것입니다. 마지막으로, OWL-ViT는 왼쪽에 두 번째 커피 메이커를 놓쳤습니다. 출력물이 완벽하지는 않지만, 이러한 모델들은 상징적인 레이블을 물리적인 물체로 해석하면서 로봇이 스스로 야생을 탐험할 수 있는 문을 열어줍니다.\n\n\n\n또한, 단어에는 추가적인 의미와 연관된 맥락이 있기 때문에 대형 언어 모델은 로봇에게 상식적인 추론을 제공할 수도 있습니다. 예를 들어 물건을 집을 때 로봇은 과일 중 가장 익은 과일을 선택하거나 물건을 깨지 않고 집는 방법을 결정하는 데 상식적인 정보를 사용할 수 있을 것입니다. 본 연구는 샌드위치를 조립하는 데 초점을 맞추었지만, 이러한 기능은 플라스틱 부품이 섬세하게 다뤄져야 하거나, 프레스로 조립된 부품은 다른 접근 방식이 필요하거나, 못을 넣는 데 나사보다 다른 도구가 필요한 경우를 결정하는 데 사용될 수도 있습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_10.png)\n\n또한, 트랜스포머 아키텍처의 다중 모달성은 시연을 통한 학습에도 적합합니다. Google Deepmind와 Toyota Research Institute를 비롯한 학계와 기업 기관들이 다양한 도메인에서 대규모 이중팔 조작 작업을 기록하는 것이 증가하고 있습니다.\n\n![이미지](/assets/img/2024-05-15-TheFutureofRoboticAssembly_11.png)\n\n\n\n아래 비디오에서는 이중 팔 협력 로봇 시스템을 사용하여 속성 복제를 통한 학습의 예시가 보여집니다:\n\n그러나, 모든 AI의 발전은 여전히 연구 중이라는 점을 유념해주십시오. 예를 들어, 설거지 데모에서는 258번의 시연이 필요했으며 70%의 성공률을 보여주었지만, 아직 실용적인 사용으로는 거리가 멉니다. 그럼에도 불구하고 AI에 대한 성장과 투자는 전례없는 것으로 보이며, 우리가 관심을 갖는 조립 문제 중 많은 것들은 훨씬 제약된 환경에서 발생합니다. 동시에, 이러한 작업은 자율 주행이나 인간과 밀접하게 협업하는 인간형 어플리케이션보다 훨씬 낮은 위험을 가지는 고가치 작업을 제공합니다. 어울린 사람 형태역학에서는 엄청난 도전도 존재합니다. 협력형 조립 로봇은 아직 테이블에 고정되어 있지만, 종종 작업물을 가진 지그를 호스트하는 동일한 테이블일 수 있습니다. 그에 비해 인간형 로봇은 그러한 혜택을 누릴 수 없으며, 정밀한 감지와 제어가 필요합니다. 이 도전적인 문제에 직면한 우리는 어떻게 행동해야 하는지 고민해야 합니다.\n\n# 어플리케이션 드라이버: 전지 해체\n\n현재 하드웨어 및 소프트웨어의 한계에 충분히 견고한 대규모 시장을 지원하는 어플리케이션을 찾는 접근 방식 중 하나는, 최신 협력 로봇 도구 및 신흥 AI 모델을 결합하는 \"로봇 조립의 미래\"로 가는 길에 있을 수 있습니다. 전기 자동차(EV) 배터리 해체가 그러한 어플리케이션 중 하나일 것입니다. 전기 자동차의 가치 있는 부분은 배터리입니다. 20년이 넘은 첫 번째 세대 EV가 점점 더 나오면서, 우리는 더 많은 배터리가 재활용이 필요해질 것입니다. 이를 위해 배터리는 각 셀에 도달할 때까지 해체되어야하며, 그 후에는 셀을 분쇄하여 원자재로 분리해야 합니다. 이것은 어렵고 위험합니다. EV 셀은 최대 700V까지 충전될 수 있습니다. 또한 다양한 배터리 유형이 존재하며, 그들의 해체를 위한 맞춤형 기계를 구축하는 것은 불가능합니다. \n\n\n\n![로봇 조립의 미래](/assets/img/2024-05-15-TheFutureofRoboticAssembly_12.png)\n\n동시에 문제는 충분히 제한되어 \"레벨 5 자동화\"를 실현할 수 있다는 점입니다: 해체는 일반적으로 조립보다 쉬우며 무언가를 분해하는 추가 기회를 식별하는 데 한정됩니다. 동시에 체결물은 제한된 세트에서 가져오며 시스템은 부족한 기술을 가르칠 수 있습니다. 예를 들어 특정 보호 구역을 열어내는 것과 같은 새로운 도전이 발생할 때.\n\n# 결론\n\n맞춤 기계를 사용하면 이미 자동 조립 및 키팅이 가능함을 보았습니다. 맞춤 기계가 경제적이지 않은 고믹스, 저볼륨 문제와 손수가 가장 효율적인 옵션으로 남아 있는 문제입니다. AI 분야의 최근의 기술적 돌파는 로봇학의 중요한 진전을 이끌 수도 있습니다: 상식적인 추론과 제한된 조작 기술을 갖춘 인간형 로봇. 이 약속은 매우 크기 때문에 주요 시장 참가자들이 이미 우리가 로봇 자동화에 대해 알고 있는 것, 새로운 AI 도구가 제공하는 것, 특히 학습, 로봇과 인간 간의 새로운 상호작용 방식, 개방형 세계 추론, 그리고 새로운 수준의 이동성 통합에 동참하기 위해 준비되어 있습니다. 조립과 해체가 그런 결과물의 첫 진정한 응용일 수도 있습니다.","ogImage":{"url":"/assets/img/2024-05-15-TheFutureofRoboticAssembly_0.png"},"coverImage":"/assets/img/2024-05-15-TheFutureofRoboticAssembly_0.png","tag":["Tech"],"readingTime":9}],"page":"33","totalPageCount":99,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"33"},"buildId":"6w6Yg3qJxLtqeXNguENru","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>