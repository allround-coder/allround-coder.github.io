<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>allround-coder</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///posts/14" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="allround-coder" data-gatsby-head="true"/><meta property="og:title" content="allround-coder" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///posts/14" data-gatsby-head="true"/><meta name="twitter:title" content="allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/baeec1f16d6ea8b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/baeec1f16d6ea8b8.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/324-8452a6176b22a926.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-082571b43b6fd145.js" defer=""></script><script src="/_next/static/z1a6VTi5qHH9JJH7jaxL3/_buildManifest.js" defer=""></script><script src="/_next/static/z1a6VTi5qHH9JJH7jaxL3/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="자바스크립트 심층 탐구 React의 useEffect 및 Objectis 함정 탐색" href="/post/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="자바스크립트 심층 탐구 React의 useEffect 및 Objectis 함정 탐색" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="자바스크립트 심층 탐구 React의 useEffect 및 Objectis 함정 탐색" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">자바스크립트 심층 탐구 React의 useEffect 및 Objectis 함정 탐색</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ReacType v21 소개 React 개발의 진입 장벽을 낮추고 품질 높이기" href="/post/2024-05-16-IntroducingReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ReacType v21 소개 React 개발의 진입 장벽을 낮추고 품질 높이기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-IntroducingReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ReacType v21 소개 React 개발의 진입 장벽을 낮추고 품질 높이기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">ReacType v21 소개 React 개발의 진입 장벽을 낮추고 품질 높이기</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="RocksDB를 사용한 서버 부팅 속도 최적화" href="/post/2024-05-16-ServerbootstrapoptimizationusingRocksDB"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="RocksDB를 사용한 서버 부팅 속도 최적화" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="RocksDB를 사용한 서버 부팅 속도 최적화" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">RocksDB를 사용한 서버 부팅 속도 최적화</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로" href="/post/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="흑자 블랙 바스타의 코발트 스트라이크를 추적하기" href="/post/2024-05-15-HuntingBlackBastasCobaltStrike"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="흑자 블랙 바스타의 코발트 스트라이크를 추적하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="흑자 블랙 바스타의 코발트 스트라이크를 추적하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">흑자 블랙 바스타의 코발트 스트라이크를 추적하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지" href="/post/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="마이크로소프트 Phi-3-Mini는 정말 인상적이에요" href="/post/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="마이크로소프트 Phi-3-Mini는 정말 인상적이에요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="마이크로소프트 Phi-3-Mini는 정말 인상적이에요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">마이크로소프트 Phi-3-Mini는 정말 인상적이에요</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN" href="/post/2024-05-15-InterpretablekNNikNN"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-InterpretablekNNikNN_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="안녕하세요, 월드, OpenAI GPT-4o" href="/post/2024-05-15-HelloWorldtoOpenAIGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="안녕하세요, 월드, OpenAI GPT-4o" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="안녕하세요, 월드, OpenAI GPT-4o" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">안녕하세요, 월드, OpenAI GPT-4o</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다" href="/post/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다</strong><div class="PostList_meta__VCFLX"><span class="date">May 15, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link posts_-active__YVJEi" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"자바스크립트 심층 탐구 React의 useEffect 및 Objectis 함정 탐색","description":"","date":"2024-05-16 03:08","slug":"2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap","content":"\n\n\n![Image](/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_0.png)\n\nReact에서 side effect를 수행하고 array dependency가 수정될 때마다 다시 렌더링을 관리하기 위해 useEffect 훅을 자주 사용합니다. 그러나 이 훅에 익숙하지 않은 일부 개발자는 브라우저가 멈추는 상황이나 컴포넌트 상태를 새로고침하는 문제 등 특정 상황에서 약간 헷갈릴 수 있습니다.\n\n간단한 예시를 살펴보겠습니다:\n\n```js\nfunction CountClick() {\n  const [count, setCount] = useState(0);\n\n  useEffect(() =\u003e {\n      console.log('Count updated');\n  }, []);\n\n  const onClick = () =\u003e setCount(count + 1);\n\n  return (\n    \u003cdiv\u003e\n      \u003cbutton onClick={onClick}\u003eClick me\u003c/button\u003e\n      \u003cdiv\u003eCount: {count}\u003c/div\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n\n\n페이지에 접속하면 빈 배열 []을 두 번째 인자로 전달하여 콘솔 로그가 한 번 출력되는 것을 확인할 수 있습니다. 이것은 React가 첫 번째 렌더링에서만 효과를 실행한다는 것을 의미합니다.\n\n\u003cimg src=\"/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_1.png\" /\u003e\n\n좋아요!\n\n이제 콘솔을 지우고 \"Click me\" 버튼을 클릭하여 무엇이 발생하는지 확인해봅시다.\n\n\n\n\n![Screenshot](/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_2.png)\n\n카운터가 증가하는 것을 관찰할 수 있지만, 콘솔에서는 아무 일도 일어나지 않습니다. 여기서 useEffect의 의존성이 실제로 작용하게 됩니다.\n\n코드를 조금 조정해 봅시다:\n\n```js\nfunction CountClick() {\n  const [count, setCount] = useState(0);\n\n  useEffect(() =\u003e {\n      console.log('Count updated');\n  }, [count]);\n\n  const onClick = () =\u003e setCount(count + 1);\n\n  return (\n    \u003cdiv\u003e\n      \u003cdiv\u003eCount: {count}\u003c/div\u003e\n      \u003cbutton onClick={onClick}\u003eClick me\u003c/button\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n\n\n이제 'Click me' 버튼을 클릭한 후 콘솔에 메시지가 표시되는 것을 확인할 수 있습니다. 그러나 실제 세계에서는 백엔드와 통합되는 앱을 만들며, localStorage에서 데이터를 읽고 더 복잡한 솔루션을 사용합니다. 이전 예시에서 우리는 기본 형식에 대해 작업했습니다.\n\n이번에는 참조 형식인 객체를 사용하는 코드를 만들어보겠습니다:\n\n```js\nfunction CountClick() {\n  const [data, setData] = useState\u003cPerson\u003e()\n  const [count, setCount] = useState(0)\n\n  useEffect(() =\u003e {\n    console.log('Count updated');\n    setData({ firstName: 'Bruce', lastName: 'Wayne' });\n  }, [data]);\n\n\n  const onClick = () =\u003e setCount(count + 1);\n\n  return (\n    \u003cdiv\u003e\n      \u003cdiv\u003eCount: {count}\u003c/div\u003e\n      \u003cbutton onClick={onClick}\u003eClick me\u003c/button\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n위 코드에서는 실수가 발생했습니다.\n\n\n\n\n![Image 1](/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_3.png)\n\n우리의 코드가 무한 루프를 발생시키고 있어요. 컴포넌트가 불필요하게 계속해서 자기 자신을 다시 렌더링하며, 업데이트의 끝나지 않는 사이클을 초래하고 있죠.\n\n다행히도, React의 소스 코드는 오픈 소스로 제공되어 있어요. 그래서 우리는 이 효과의 구현을 쉽게 찾을 수 있어요!\n\n![Image 2](/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_4.png)\n\n\n\n\n이 코드의 구현을 더 자세히 살펴보면 그리 복잡해 보이지 않아요. 그럼 문제가 뭘까요?\n\n이 함수의 정의를 찾아보려 노력해 봅시다.\n\n있네요! — 우리의 버그/예상 동작은 Object.is 정적 메서드에 있습니다. 그러니 테스트해 봅시다.\n\n```js\nconsole.log(Object.is({ firstName: 'Bruce', lastName: 'Wayne' }, { firstName: 'Bruce', lastName: 'Wayne' }));\n// false\n```\n\n\n\n그래서 현재 동일한 객체 정의에 대해 Object.is가 false를 반환하는 것을 알았죠.\n\n하지만 왜 그럴까요?\n\n자바스크립트는 꽤 표준화된 프로그래밍 언어입니다. Object.is는 SamveValue 추상 작업을 참조합니다. 그것에는 다음과 같이 기술되어 있습니다:\n\n```js\n//SameValueNonNumber\n7.2.11 SameValueNonNumber ( x, y )\n===== TRUNCATED =====\n6. 주의: 다른 모든 ECMAScript 언어 값은 동일성으로 비교됩니다.\n7. x가 y와 동일하면 true를, 그렇지 않으면 false를 반환합니다.\n```\n\n\n\n그리고 여기 중요한 참고 사항이 있습니다:\n\n이 함수는 호출될 때 같은 값이 아닌 경우와 원시 유형을 참조하는 SameValueNonNumber 및 SameValue에 대해 다음 단계를 수행함을 의미합니다. 객체, 배열, 함수 등과 같은 다른 값들에 대해서는 내용을 비교하는 대신 메모리 위치가 동일한지 확인하여 비교가 이루어집니다.\n\n따라서 이는 원시 유형에 대해서만 작동합니다.\n\n```js\nObject.is(NaN, NaN) // true\nObject.is(0, 0) // true\nObject.is(1.000123, 1.000123) // true\nObject.is(Infinity, Infinity) // true\nObject.is(-Infinity, -Infinity) // true\nObject.is(0/-1, 0/-1) // true\nObject.is(\"test\", \"test\") // true\nObject.is(null, null) // true\nObject.is(false, false) // true\nObject.is(true, true) // true\n```\n\n\n\n그리고 참조 유형에 대해서는 해당되지 않습니다\n\n```js\nObject.is(Symbol(\"foo\"), Symbol(\"foo\")) // false\nObject.is({}, {}) // false\nObject.is([], []) // false\n```\n\n# 어떻게 이 문제를 해결할까요?\n\n가장 간단한 해결책은 깊은 동등성을 사용하는 것입니다 (다른 해결책에 대한 좋은 기사 몇 개를 링크 섹션에 게시했습니다).\n\n\n\n```js\ntype Person = {\n  firstName: string;\n  lastName: string;\n};\n\nfunction CountClick() {\n  const [data, setData] = useState\u003cPerson\u003e()\n  const [count, setCount] = useState(0)\n\n  useEffect(() =\u003e {\n    setData({ firstName: 'Bruce', lastName: 'Wayne' });\n    console.log('Count updated');\n  }, [data?.firstName]);\n\n\n  const onClick = () =\u003e setCount(count + 1);\n\n  return (\n    \u003cdiv\u003e\n      \u003cdiv\u003ecount: {count}\u003c/div\u003e\n      \u003cbutton onClick={onClick}\u003eClick me\u003c/button\u003e\n    \u003c/div\u003e\n  );\n}\n```\n\n# 결론\n\n- useEffect 이해: React의 useEffect 훅은 컴포넌트에서 부수효과를 관리하는 데 중요하다. 이를 통해 개발자는 컴포넌트 라이프사이클 이벤트(마운트, 업데이트, 언마운트)에 대응하는 코드를 실행할 수 있다.\n- useEffect에서 의존성 배열: useEffect에 전달되는 두 번째 인자인 의존성 배열은 효과가 의존하는 값들을 지정한다. 이 배열이 변경되면 효과가 다시 실행된다. 이 배열을 생략하거나 빈 배열을 전달하면 효과는 최초 렌더링 후 단 한 번만 실행된다.\n- 참조 타입 사용 시 주의사항: 객체나 배열과 같은 참조 타입을 의존성 배열에 사용하면 예상치 못한 동작이 발생할 수 있다. 원시 타입과 달리 참조 타입은 값이 아닌 참조에 의해 비교된다. 따라서 두 객체의 내용이 같더라도 동일한 메모리 위치를 참조하지 않는 한 동일하지 않다.\n- Object.is 함정: 값 동등성 비교에 사용되는 Object.is 메서드는 참조 타입과 원시 타입에 따라 다르게 동작한다. 객체나 배열과 같은 경우, 이 메서드는 두 값이 동일한지 식별하며, 객체나 배열과 관련된 작업 시 예상치 않은 결과를 가져올 수 있다.\n- 해결책: useEffect에서 참조 타입을 사용할 때 무한 루프나 예상치 않은 동작을 피하기 위해 개발자는 깊은 동등성 체크를 활용할 수 있다. 이를 통해 객체의 내용을 기준으로 비교되도록 보장할 수 있다.\n\n# 링크\n\n\n\n- 기본 / 참조 유형 https://gist.github.com/branneman/7fb06d8a74d7e6d4cbcf75c50fec599c\n- https://github.com/facebook/react/blob/26f24960935cc395dd9892b3ac48249c9dbcc195/packages/react-server/src/ReactFizzHooks.js#L161\n- https://tc39.es/ecma262/#sec-samevalue\n- https://medium.com/suyeonme/react-lets-deep-dive-into-deps-array-of-useeffect-13ab96468db7","ogImage":{"url":"/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_0.png"},"coverImage":"/assets/img/2024-05-16-DeepDiveIntoJavaScriptNavigatingReactuseEffectandtheObjectisTrap_0.png","tag":["Tech"],"readingTime":6},{"title":"ReacType v21 소개 React 개발의 진입 장벽을 낮추고 품질 높이기","description":"","date":"2024-05-16 03:06","slug":"2024-05-16-IntroducingReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment","content":"\n\n우리는 ReacType (v21) 출시를 기쁘게 알리게 되었습니다. 이는 사용자가 어플리케이션 아키텍처를 동적으로 시각화할 수 있는 React 프로토타이핑 도구로, 실시간 컴포넌트 코드 미리보기와 상호작용 가능한 드래그 앤 드롭 디스플레이를 제공합니다. 또한, 라이브 비디오 및 채팅 기능이 통합된 협업 공간도 제공됩니다.\n\n오늘날 급변하는 소프트웨어 환경에서, ReacType (v21)은 소프트웨어 엔지니어링 커뮤니티의 개발 역량을 강화하기 위한 우리의 헌신의 증표로 남아 있습니다. 기술 창조를 지원하는 개발자 도구의 중요성을 인식하며, 우리는 그러한 도구가 개발자의 경력 단계에 관계없이 접근 가능하고 강력하게 유지되도록 약속하고 있습니다.\n\n![ReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment](/assets/img/2024-05-16-IntroducingReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment_0.png)\n\nReacType v21의 새로운 기능\n\n\n\n최신 업데이트로 ReacType(v21)는 사용자 프로젝트 다양성, 디자인 능력 및 개발 효율성에 혁신적인 향상을 이루었습니다. 새롭게 통합된 49가지 Material UI 구성 요소는 추가 구성 없이도 즉시 사용할 수 있도록 준비되어 있어 엔지니어들의 창의적 범위를 크게 확대합니다. 이 확장에 대응하기 위해 사용자 인터페이스가 주의 깊게 재구성되어 더 직관적이고 접근하기 쉬운 환경을 제공합니다.\n\n이러한 개선은 플랫폼의 미학적 및 기능적 측면을 개선할 뿐만 아니라, React 앱 프로토타이핑과 협업을 위한 최고 수준의 도구로서 ReacType가 그 영향력을 강화합니다. 게다가 이러한 혁신은 사용자가 Material UI 속성을 사용자 정의하거나 추가적인 구성 요소 라이브러리를 통합하는 데 관련된 추가 개선 작업의 기초를 마련합니다.\n\n개선된 코드 생성\n\n이러한 새로운 구성 요소의 도입으로 인해 코드 생성 논리를 철저하게 개편할 필요가 있었습니다. 강화된 시스템은 이제 수입 문, 상태, 이벤트 핸들러 및 기타 중요한 변수 및 기능을 보다 효율적으로 관리합니다. 이 최적화로 생성된 코드는 완전히 기능하며 성능을 위해 세밀하게 조정되어 내보낼 준비가 되어 있습니다. 사용자가 애플리케이션을 구축하는 동안 준비된 상태입니다.\n\n\n\n\n![Introducing ReacType v2.1: Lowering the barrier to entry and raising the bar in React development](/assets/img/2024-05-16-IntroducingReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment_1.png)\n\n### Revamped iframe Implementation\n\nUnderstanding the need for stateful interactions with UI components, we’ve reengineered our iframe implementation. The **\"Demo Render\"** enhancements allow users to interact seamlessly with state-dependent components, providing a highly realistic and practical preview of component behavior in a live environment. This upgrade ensures that developers can accurately assess and refine UI/UX experiences in real time, enhancing the quality and effectiveness of final projects.\n\n**Bug Fixes and Improvements**\n\n\n\n\n우리는 간소화되고 부드러운 사용자 경험에 대한 헌신에 부응하여, ReacType (v21)은 플랫폼에 중요한 수정사항과 향상된 기능을 가져왔습니다. 우리는 UI 전반에 걸쳐 발생한 다수의 렌더링 문제를 해결하여 일관된 성능과 사용자 기대에 부합하는 인터페이스를 제공하고 있습니다. 게다가, 프론트엔드와 백엔드 모두에서 잊혀진 비밀번호 기능과 관련된 문제를 해결함으로써, 보다 순조로운 사용자 경험을 보장하고 있습니다. 뿐만 아니라, 우리는 273개의 JSDoc 주석 블록을 추가하여 코드베이스를 보강했습니다. 이는 자세한 내용의 인라인 설명을 제공함으로써 새로운 기여자들이 코드를 이해하고 참여하기 쉽도록 만들어주었습니다.\n\n테스트 환경 최적화\n\nVite로의 최근 이전을 통해, ReacType (v21)은 테스트 스위트 내의 호환성 문제를 성공적으로 해결하고, 새로운 기능에 대한 30개의 테스트를 추가함으로써 최적화를 이루어냈습니다. 이 중요한 최적화는 프레임워크의 신뢰성과 성능을 향상시키는 데에 도움이 되었을 뿐만 아니라, 사용자 경험을 크게 개선하여 ReacType과의 상호작용을 이전보다 부드럽고 즐거운 것으로 만들어주었습니다.\n\n협업과 효율 제고\n\n\n\n(v21) 버전을 통해 ReacType는 협업과 효율성을 강조하고 있습니다. 이제 플랫폼은 우리의 강력한 프리스타일 구성 요소 스위트를 통해 개발자들 간 더 다이내믹한 상호작용을 지원합니다. 실시간 코딩, 동료 리뷰, 또는 상호 작용 문제 해결이든, ReacType (v21)은 함께 일할 때 간단하고 생산적으로 할 수 있도록 돕습니다.\n\n미래를 내다보며 사회와 함께 향하는 ReacType\n\n앞으로를 내다보며, ReacType는 React 앱 프로토타이핑 분야에서 지속적인 향상과 혁신에 헌신하고 있습니다. 저희 팀은 이러한 발전에 대해 열정적이며, ReacType가 웹 개발 여정의 어느 단계에서나 프로그래머들에게 가치 있는 도구를 제공하고 더 나아갈 것이라고 자신합니다.\n\nReacType의 핵심 가치인 함께 만들면 더 나은 개발자가 되고 개발은 접근 가능하고 효율적이어야 한다는 믿음을 바탕으로, 오늘 바로 ReacType를 사용하여 React 앱 프로토타이핑의 혜택을 경험하실 것을 권장합니다.\n\n\n\nReacType의 미래를 함께 만들어 보세요\n\n저희 웹사이트를 방문하여 (v21)의 새로운 기능을 탐험하고 가입하세요. 활기찬 커뮤니티에 참여하고 ReacType의 계속적인 발전에 기여하세요. 사용자 피드백을 적극 수용하며 신기술에 대응하여 저희 플랫폼이 업계를 선도하도록 합니다. 여러분의 통찰력과 기여는 ReacType을 가장 강력하고 사용자 친화적인 React 프로토타이핑 도구로 만들기 위해 귀중합니다. 따라서 GitHub에서 오픈 소스 프로젝트에 이슈를 제기하거나 기여해 주시기를 부탁드립니다.\n\nReact 프로젝트를 더욱 발전시키고 싶으신가요? 지금 바로 ReacType의 가능성을 탐험해 보세요!\n\nReacType 21.0 기여자 및 공동 저자\n\n\n\nAustin Alvarez | LinkedIn | GitHub\n\nHeather Pfeiffer | LinkedIn | GitHub\n\nSean Ryan | LinkedIn | GitHub\n\nJesse Wowczuk | LinkedIn | GitHub\n\n\n\nZack Vandiver | LinkedIn | Github","ogImage":{"url":"/assets/img/2024-05-16-IntroducingReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment_0.png"},"coverImage":"/assets/img/2024-05-16-IntroducingReacTypev21LoweringthebarriertoentryandraisingthebarinReactdevelopment_0.png","tag":["Tech"],"readingTime":4},{"title":"RocksDB를 사용한 서버 부팅 속도 최적화","description":"","date":"2024-05-16 03:02","slug":"2024-05-16-ServerbootstrapoptimizationusingRocksDB","content":"\n\n# 소개\n\nFlipkart의 검색 인프라에서 Mustang은 SOLR 인덱스를 관리하는 필수 서비스입니다. 현재, 우리는 Flipkart, Grocery, Hyperlocal 및 Shopsy와 같은 다양한 비즈니스 단위에 대응하는 다양한 샤드에서 운영하고 있습니다. 각 샤드는 해당 샤드로 직접 전송되는 데이터 양과 요청 양에 따라 결정된 다양한 복제본을 호스팅합니다.\n\n각 복제본에는 디스크에 저장된 데이터(제품 관련 데이터, SOLR에서 제공)와 판매자별 목록 데이터에 대한 메모리에 저장된 데이터가 포함되어 있습니다. 빠르게 변화하는 속성에 대한 NRT(Near Real-Time) 데이터로, 애플리케이션 구동 시 중앙 집중식 Redis 클러스터에서 데이터를 가져와 메모리에 있는 데이터 구조체가 구축됩니다. 이러한 메모리에 있는 데이터 구조체는 Kafka 파이프라인을 통해 업데이트되어 Redis와 동기화되어 유지됩니다.\n\n![이미지](/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png)\n\n\n\n평균적으로 각 복제본은 약 15 백만 개의 리스트에 대한 데이터를 보유합니다. 부팅 중에 이러한 인메모리 데이터 구조를 구축하는 프로세스에는 약 30~40분이 소요됩니다. 이 프로세스에서 주요 병목 현상은 Redis인데, 배포 중 동시 요청의 증가를 처리하는 데 어려움을 겪습니다 (이 클러스터의 크기가 각 데이터 센터에 거의 400대의 가상 머신에 가깝기 때문).\n\n이 전체 절차는 배포를 크게 늦추어 최소 2일 이상 소요됩니다. 개발자의 생산성뿐만 아니라 적시에 버그 수정을 배포하는 데도 어려움을 겪게 됩니다.\n\n이 블로그에서는 RocksDB를 사용하여 Mustang의 부팅 시간을 최적화하는 방법에 대해 논의합니다.\n\n# 문제 분석\n\n\n\n우리 Redis 클러스터는 Mustang 서버가 다시 시작될 때마다 막혔었습니다. 10%의 롤아웃 요인조차도 약 40개의 Mustang 서버가 Redis에 대해 300K 이상의 동시 호출을 하는 결과를 가져왔습니다. 이러한 대규모 동시 요청의 이유는 각 서버의 폴러 스레드 수 및 배치 크기 때문이었습니다.\n\n또한, Redis에서 목록에 대한 데이터를 가져오는 것이 단순한 Redis GET 작업이 아니었습니다. 우리는 Redis에서 목록 POJO를 작성하는 논리를 추상화한 라이브러리를 작성했는데, 이 라이브러리는 다중 동시 호출을 통해 각 속성의 데이터를 얻기 위해 내부적으로 Redis에 대한 요청을 여러 번 보내고 이를 병합하여 단일 POJO를 생성합니다.\n\n예를 들어, 목록과 관련된 제공은 Redis 내에 SET으로 저장되었으며, 서비스 가능 영역의 가용성 데이터는 BITFIELD로 저장되었습니다. 두 데이터를 가져오려면 Redis에 대해 구별된 쿼리가 필요했으며, 응답 구문 분석도 그에 따라 달랐습니다.\n\n문제점을 찾기 위해 아래에서 위로 가는 방식으로 병목 현상을 탐색하는 것은 좋은 선택처럼 보였습니다. 따라서 우리는 Redis부터 시작했습니다.\n\n\n\n# Redis 튜닝\n\n각 애플리케이션 서버의 폴러 스레드의 일괄 처리 크기와 스레드 수를 조정해 보았지만 전반적인 성능에는 개선이 없었습니다. 각 일괄 처리의 지연 시간은 조금 줄었지만 전체적으로 처리해야 하는 일괄 처리가 더 많아져 얻는 이점이 상쇄되었습니다.\n\n사용되지 않는 몇 가지 상품 속성을 찾았지만 여전히 상품 POJO의 일부였습니다. 이러한 속성을 POJO에서 제거하니 성능이 조금 향상되었지만 충분하지는 않았습니다.\n\nRedis 클러스터의 각 샤드에 레플리카 수를 늘리는 옵션도 검토했습니다. 이를 통해 더 나은 부하 분산을 얻을 수 있지만 클러스터가 대부분 유휴 상태이고 Mustang 배포 중에만 사용되기 때문에 실용적이지 않았습니다. 여기에 더 많은 리소스를 추가하는 것에 가치가 없다고 판단했습니다.\n\n\n\nRedis를 더 최적화하는 것이 어려워서, 우리는 응용 프로그램의 서버 쪽 옵션을 탐색했어요.\n\n# 파일 캐시 생성\n\n아이디어는 Redis로부터 데이터를 한 번만 검색하여 인메모리 데이터 구조를 구축하고, 그 후에는 미래 배포를 위해 로컬로 캐시하는 것이었습니다.\n\n같은 작업을 위한 빠른 POC를 진행하기 위해 우리는 셧다운 후크를 작성했어요. 이 후크는 모든 인메모리 데이터 구조를 로컬 디스크의 별도 파일에 직렬화할 거에요. 시작 시, 응용 프로그램은 로컬에 저장된 데이터를 역직렬화하고 인메모리 데이터 구조를 로드할 거예요.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_1.png\" /\u003e\n\n시작할 때는 아이디어가 유망해 보였지만 나중에 다양한 문제가 발생했습니다. 이 중 일부는 아래와 같습니다:\n\n- 저희의 인메모리 데이터 구조는 사실상 코드의 다른 세그먼트(그리고 메모리에서)입니다. 그러나 이들은 모두 상품 목록 데이터에서 파생되었습니다. 한 세그먼트의 직렬화 중에 문제가 발생하면 해당 세그먼트를 다시 불러오지 않고는 복구할 수 없습니다. 이는 단 하나의 파일이 손상되어도 직렬화된 데이터를 모두 폐기하고 Redis로부터 전체 데이터를 다시 부트스트랩해야 한다는 것을 의미합니다.\n- 상품 목록 데이터는 각 샤드마다 동일하지만, 이러한 인메모리 데이터 구조는 응용 프로그램 서버 간에 심지어 같은 샤드 내에서도 다를 수 있습니다. 이 차이는 이러한 데이터 구조 내에서 상품 목록을 식별할 때 사용하는 서수의 무작위적인 특성에서 비롯됩니다. 이 상품 목록 서수는 SOLR 색인 파일을 비동기적으로 로드하는 프로세스 동안 먼저 도착한 사람이 우선적으로 생성되기 때문에 결정론적이지 않습니다. 따라서 동일한 샤드의 다른 복제본들 사이에서 직렬화된 파일을 공유하는 것이 불가능합니다.\n- 코드가 지저분해 보였던 이유는 직렬화 및 역직렬화를 위해 Jackson을 사용했기 때문입니다. Jackson은 우리 코드에서 정상적으로 작동하기 위해 특정 getter 및 setter를 필요로 합니다. 이는 특히 상속을 다룰 때 또는 간단히 속성을 반환하는 대신 사용자 정의 논리를 포함하는 getter가 이미 있는 경우에 복잡함을 야기했습니다.\n\n이러한 제약으로 인해 더 견고하고 우아한 솔루션이 필요했습니다. POC를 기반으로 내장 데이터베이스 접근 방식이 유망해 보였고 우리는 결국 RocksDB를 선택하게 되었습니다.\n\n\n\n# RocksDB가 당신을 구해줍니다\n\n## 왜 RocksDB를 선택했나요?\n\n우리가 RocksDB를 선택한 이유를 이해해 봅시다.\n\n- 이는 내장형 데이터베이스입니다. 이는 중앙 서버에서 실행할 필요가 없음을 의미합니다. 라이브러리로 직접 코드에 사용할 수 있습니다. 우리는 중앙 집중형 솔루션에서 벗어나려고 했기 때문에, 이것은 우리에게 완벽했습니다.\n- 당신의 요구에 맞게 튜닝할 수 있는 구성 옵션을 제공합니다.\n- 다양한 작업 부하에서 테스트할 때 유망한 결과를 보았으며, 이는 그 성능에 대한 우리의 신뢰를 높였습니다.\n- 이는 매우 인기 있는 데이터베이스이며 다양한 산업에서 사용되고 있습니다 (X가 비슷한 문제를 해결하기 위해 어떻게 사용하는지, Cloudflare가 어떻게 사용하는지, MySQL과의 통합하여 저장 엔진으로 사용하는 방법 확인 등)\n- 페이스북이 유지 관리하는 매우 활발한 커뮤니티 지원이 있습니다.\n\n\n\n이 데이터 포인트들은 좋은 임베디드 데이터베이스를 선택하는 데 도움이 많이 되었어요. \n\n## RocksDB의 저장 스키마\n\n임베디드 데이터베이스 선택을 결정하고 나면 코드에 통합하는 작업을 진행했어요. 이 작업은 간단했어요. 우리는 Redis에서 데이터를 한 번 가져와서 이후 배포에서 로컬로 저장된 데이터를 사용할 수 있도록 RocksDB에 저장하고 싶었어요. \n\n![RocksDB를 사용한 서버 부트스트랩 최적화](/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_2.png)\n\n\n\n간단하게 유지하기 위해 우리는 간단한 저장 스키마를 설계했습니다. RocksDB의 각 행은 단일 목록의 데이터를 포함했습니다. 각 행의 키는 목록의 ID이었고 값은 직렬화된 목록 POJO였습니다. 다음과 같이 보였습니다:\n\n```js\n“LISTING_1\": “{\\\"attribute_1\\\": \\\"value_1\\\", \\\"attribute_2\\\": \\\"value_2\\\"}\"\n```\n\n단순함 외에도 이러한 종류의 스키마를 선택한 주된 이유 중 하나는 작업의 세분화였습니다. 이 스키마를 사용하면 필요에 따라 단일 목록 또는 그룹의 목록을 업설 또는 가져올 수 있었습니다. RocksDB에 일부 목록의 데이터가 없는 경우 Redis에서 누락된 레코드를 다음 부트스트랩 단계에서 가져와 RocksDB를 업데이트할 수 있습니다. 시스템은 자가 치유 기능을 제공합니다.\n\n이것이 우리 솔루션의 기초를 놓은 것이지만, 아직 해결해야 할 많은 문제가 있었습니다. 다음 섹션에서 몇 가지 다른 어려움에 대해 이야기하겠습니다.\n\n\n\n## 고요한 데이터 문제 해결\n\nRedis에서 데이터를 한 번 가져와 로컬에 저장하는 것만으로는 비즈니스의 빠르게 변화하는 성격 때문에 충분하지 않았습니다. 플립카트의 대부분 비즈니스 요구 사항은 목록 속성에 일부 변경이 필요했으며(인메모리 데이터 구조에 대한 비슷한 스키마 변경 포함), 그래서 그럴 때마다 RocksDB에 저장된 데이터는 애플리케이션의 스키마와 호환되지 않게 되었습니다.\n\n이 문제를 해결하기 위해 RocksDB에 인메모리 데이터 구조의 스키마 해시를 리스트 데이터와 함께 저장했습니다. 이 스키마는 배포가 발생할 때 최신 스키마(코드에 저장된)와 비교됩니다. 스키마 불일치가 발생하면 RocksDB 데이터를 단순히 폐기하고 Redis에서 최신 데이터로 다시 채웁니다.\n\n이 방법은 괜찮았지만, 결과적으로 서면 배포의 60%가 데이터 구조 변경을 포함했다는 것이 밝혀졌습니다. 이는 여전히 대부분의 배포에서 Redis에 의존하고 있다는 것을 의미했으며, 이는 이상적이지 않았습니다.\n\n\n\n우리는 더 나은 계획을 세웠어요. 동일한 샤드 내의 모든 레플리카가 동일한 목록 데이터를 가지고 있다는 사실을 알고 있었죠. 그래서 왜 각 샤드의 단일 무작위 레플리카로부터 RocksDB 데이터를 생성하여 GCS에 저장한 다음 해당 샤드의 다른 레플리카에 사용하지 않을까요?\n\n이 방법을 구현하는 것은 쉬웠어요. 우리는 코드에 유효성 검사 레이어를 작성해뒀는데, Mustang 시작 시 스키마 해시를 비교할 수 있도록 했어요. 동일하다면 응용 프로그램은 RocksDB에서 데이터를 로드하지만 다르다면 해당 응용 프로그램은 로컬 RocksDB 데이터를 삭제하고 GCS에서 최신 데이터를 가져와서 계속해서 RocksDB에서 데이터를 로드해요.\n\nGCS에 데이터가 없을 경우 Redis로 데이터를 부트스트랩합니다. 배포 파이프라인도 이러한 유형의 배포를 처리하도록 변경되었어요. 스키마 변경이 있을 때마다 CI 파이프라인이 각 샤드에서 무작위 레플리카를 선택하여 최신 코드를 배포하고 Redis에서 부트스트랩한 다음 로컬 RocksDB 데이터를 GCS에 업로드합니다. 나머지 레플리카는 로컬 데이터가 무효화될 때 자동으로 GCS에서 데이터를 가져와요.\n\n## Kafka 업데이트 처리\n\n\n\n카프카에서 업데이트를 다룰 때, RocksDB의 데이터가 Redis에 있는 것과 함께 최신 상태를 유지해야 했습니다. 우리는 이 프로세스를 최적화하기 위해 카프카 업데이트를 가로채고 그것을 RocksDB에 추가한 후 메모리 데이터 구조의 변경 사항을 반영하는 전용 클래스를 개발했습니다.\n\n우리는 전체 POJO를 RocksDB에 저장했기 때문에 데이터를 업데이트하기 위해 읽기-수정-업데이트 작업을 수행해야 했습니다. 주요 기술적 장벽은 RocksDB 내에서 잠재적인 업데이트 실패를 관리하여 Mustang이 다음 재시작 시 RocksDB에서 가장 최신 데이터를 검색할 수 있도록 보장하는 데 있었습니다.\n\n이를 완화하기 위해 우리는 try-catch 블록 내에서 오류 처리를 구현했습니다. 어떤 이유로든 업데이트가 실패하면 해당 목록을 간단히 RocksDB에서 삭제했습니다. 삭제 실패의 드문 경우(다시 시도 후에도)를 대비하여 종료시 전체 RocksDB 데이터 세트를 지우기로 선택했습니다. RocksDB 업데이트가 실패하더라도 사용자에 대한 데이터 불일치가 없도록 메모리 데이터 구조를 업데이트했습니다.\n\n해결해야 할 또 다른 문제가 있었습니다. RocksDB의 부트스트랩 후 카프카 이벤트를 가장 최신 오프셋에서 읽기 시작하는 Mustang의 경우(기존 시스템의 기본 동작 방식)에는 데이터 손실이 발생할 수 있습니다. 이는 RocksDB가 Mustang이 재시작하는 동안 업데이트를 받지 못하기 때문입니다. 그 전에는 Mustang이 소스의 진실인 Redis에서 부트스트랩을 했기 때문에 가장 최신 카프카 오프셋에서 읽기를 안전하게 시작할 수 있었던 것이었습니다.\n\n\n\n이 문제를 해결하기 위해 우리는 종료할 때 모든 파티션의 Kafka 오프셋을 RocksDB에 저장하기 시작했습니다. 그런 다음 시작할 때 Mustang은 해당 파티션의 저장된 오프셋으로 다시 이동하였습니다. 응용 프로그램 충돌과 같은 이유로 RocksDB에서 오프셋을 찾을 수 없는 경우, 로컬 덤프를 폐기합니다. 또한 현재 오프셋과 저장된 오프셋 간의 차이가 상당히 큰 경우 로컬 덤프도 폐기합니다. 이 결정은 이러한 중요한 데이터 갭을 다시 채우는 데 상당한 시간이 걸릴 것으로 이해하고 기준이 됩니다. 이 차이에 대한 임계값은 Mustang이 일반적으로 다섯 분 동안 처리할 수 있는 업데이트 양을 추정하여 결정됩니다.\n\n## 사용 사례에 맞게 RocksDB 조정\n\n이 시점까지 우리는 RocksDB를 우리의 코드베이스에 성공적으로 통합할 수 있었습니다. Mustang이 인메모리 데이터 구조를 부팅하는 데 걸리는 시간은 30분에서 15분으로 줄었습니다. 우리는 우리의 액세스 패턴을 더 잘 이해하고 RocksDB의 내부를 깊이 파고들면 RocksDB에서 더 많은 성과를 얻을 수 있다고 믿었습니다.\n\n우리의 워크로드는 쓰기와 읽기의 혼합물이었습니다. Redis에서 데이터를 부팅하고 RocksDB에 삽입하는 동안은 완전히 쓰기 중심이지만 이후 배포에서는 항상 읽기 중심입니다. 이러한 패턴 중 하나에만 최적화할 수 있었고, 우리는 읽기를 최적화하기로 결정했습니다 (당연한 이유로).\n\n\n\n아래는 저희 케이스에 작동한 몇 가지 최적화 내용입니다:\n\n- 캐시 비활성화: LRU 기반 블록 캐시는 모든 RocksDB 사용 사례에서 사용하는 것이 좋지만 심한 락 경합이 발생합니다. 부팅 중에 한 번만 목록을 읽는 경우에는 데이터 블록의 캐시를 비활성화했습니다.\n- 레벨 컴팩션 사용: 레벨 컴팩션 전략은 크기 단위(일명 유니버설) 컴팩션 전략보다 읽기 및 공간 증폭면에서 더 나은 결과를 줍니다.\n- LSM 트리의 레벨 수 줄임: LSM 트리의 레벨 수는 RocksDB에서 중요한 속성입니다. LSM 트리에 존재할 레벨 수를 결정합니다. 데이터 중 일부가 핫하게 액세스되는 경우 레벨이 많으면 유익하지만, 액세스 패턴이 무작위인 경우 읽기 대기 시간에 영향을 줄 수 있습니다. 우리의 경우에는 레벨 수를 기본값인 7에서 3으로 줄였습니다.\n- 주기적인 전체 컴팩션 트리거: RocksDB는 최상태일 때 가장 잘 작동합니다. 읽기 성능을 최적화하기 위해 카프카 업데이트로 인한 읽기(및 공간) 증폭을 줄이기 위해 매일 취침 시간에 전체 컴팩션을 수동으로 트리거하는 비동기 스레드를 작성했습니다.\n- WAL 비활성화: 기본적으로 RocksDB는 모든 쓰기를 WAL에 메모리 테이블과 함께 저장합니다. 데이터가 손실되지 않는 자가 치유하는 성격의 경우 WAL을 비활성화했습니다.\n- multiGet()을 사용하여 데이터 읽기: RocksDB는 DB에서 데이터를 읽는 다양한 방법을 제공합니다. get() 명령 또는 multiGet() 명령을 실행할 수 있습니다. multiGet()은 다중 get() 호출에 비해 더 효율적이며, 필터/인덱스 캐시에서 적은 스레드 경합, 내부 메서드 호출 수 감소, 다른 데이터 블록에 대한 IO에 대한 더 나은 병렬화 등 여러 이유로 선호됩니다.\n- 목록 정렬 및 일괄 처리: 전체 목록 세트를 정렬한 다음 RocksDB에서 데이터를 가져오기 전에 이를 작은 일괄 처리로 만들었습니다. 정렬된 목록은 디스크에서 동일한 또는 근접한 페이지에 있을 가능성이 크기 때문에 랜덤 디스크 IO를 줄였습니다.\n\n이러한 최적화로 부팅 시간을 15분에서 약 6분으로 대폭 단축하여 효율을 높이는 우리의 노력에서 중요한 성과를 도출했습니다.\n\n자세한 내용은 이 튜닝 안내서를 읽어보세요.\n\n\n\n# RocksDB를 운영 환경으로 이끌기\n\n모든 것이 준비된 상태에서 우리는 RocksDB를 운영 환경으로 적용하고 싶었습니다. 그러나 스택에 새로운 기술을 도입하는 것은 어떠한 중단도 방지하기 위해 조심스럽게 진행해야 했습니다. Mustang이 RocksDB와 어떻게 상호 작용하는지 모니터링하기 위한 적절한 메트릭을 도입하고 위험을 완화하는 방법을 구현했습니다. 그런 다음, 각 샤드 내의 일부 레플리카에 대한 제한적인 배포를 시작했습니다.\n\n## 작은 결함\n\n몇 개의 Mustang 서버에 최신 코드를 배포한 직후, 응답 시간이 저하되는 현상이 발생하기 시작했습니다. 더 깊게 파고들어보니, 몇 일 동안 전체 VM의 메모리가 천천히 소비되는 것을 발견했습니다. 이 과도한 메모리 사용은 Solr의 인덱스 관련 파일을 캐싱하는 능력을 방해하여 런타임 중 과도한 디스크 이용률을 초래하여 지연 시간을 증가시켰습니다. Java 어플리케이션이 이 메모리 누수를 발생시킬 수 없다는 것은 명백했습니다. 왜냐하면 Java 어플리케이션은 시작 시에 고정된 메모리 청크(힙 메모리 형태로 할당)만 할당되고, 그 위에 JVM이 기능하기 위해 약간의 추가 네이티브 메모리만 사용하는 것 뿐이기 때문입니다. (JVM이 사용하는 네이티브 메모리를 확인하는 방법은 이 가이드를 참조하세요)\n\n\n\n의심이 RocksDB로 향했는데, 내장 데이터베이스로 분류되었지만 Java 라이브러리뿐만 아니라 C++ 구성 요소도 포함되어 있고 JNI를 통해 Java 애플리케이션과 상호 작용하는 것으로 확인되었습니다. Java에서 RocksDB와 상호 작용하는 클래스들은 내부적으로 RocksDB의 C++ 대응물을 호출합니다. C++의 메모리 관리는 자동 가비지 콜렉션 시스템이 없어서 어려워요. 우리는 초반에 RocksDB에 버그가 있을거라고 생각했어요.\n\n그때부터 우리는 네이티브 메모리 누수를 디버깅할 수 있는 옵션을 탐색하기 시작했어요. 네이티브 메모리 누수를 찾아가는 우리의 여정은 블로그로서 충분히 소개할 만한 주제였지만, 간결함을 위해 이 자리에서 우리가 도움을 받은 내용에 대해서만 이야기할 예정이에요.\n\n이러한 유형의 메모리 누수를 디버깅하기 위해 우리는 Jeprof라는 도구를 발견했어요. 이 도구는 JNI 호출과 같은 이유로 발생하는 네이티브 메모리 할당을 추적하는 데 사용됩니다. 그래서 우리는 Jeprof를 구성하여 애플리케이션의 네이티브 메모리 할당에 대한 메모리 덤프를 수 분마다 수행하도록 설정했어요. 나중에 우리는 몇 시간 간격으로 생성된 2개의 무작위 덤프를 비교하여 어떤 객체가 크기가 커지고 있는지 확인했어요. 같은 내용의 일부를 아래에 기술한 내용을 참고해 주세요:\n\n![이미지](/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_3.png)\n\n\n\n스크린샷을 보면, RocksDB에서 제공된 ReadOptions()와 WriteOptions() 객체들이 많이 할당되었음을 확인할 수 있었습니다.\n\n코드를 확인해본 결과, 이 두 클래스에 대한 새로운 객체 할당이 발생하는 곳은 하나뿐이었습니다. 우리 코드에서 RocksDB 관련 구성을 처리하기 위해, ReadOptions()와 WriteOptions()와 같은 다양한 객체를 보유하고 이를 맵에 다양한 열 패밀리 이름에 매핑하는 새 클래스를 만들었습니다. 이러한 객체들은 일반적으로 Mustang 시작 시 제공되며 다시 생성되지 않습니다.\n\n그러나 안전을 위해, 열 패밀리에 요청되었을 때 이미 사용 가능한 객체들이 없는 상황을 처리하기 위한 해결책을 마련했습니다. 우리는 Map의 getOrDefault() 메서드를 활용하여, 미리 구성된 객체를 검색하거나 지정된 열 패밀리에 대해 실행 중에 새로운 객체를 생성할 수 있도록 했습니다. 이 구현에 대한 자바 코드는 아래와 유사했습니다:\n\n```java\n...\npublic ReadOptions getReadOptionsByColumnFamily(String columnFamilyName) {\n  return this.readOptions.getOrDefault(columnFamilyName, new ReadOptions());\n}\n\npublic WriteOptions getWriteOptionsByColumnFamily(String columnFamilyName) {\n  return this.writeOptions.getOrDefault(columnFamilyName, new WriteOptions());\n}\n...\n```\n\n\n\n## 그럼 왜 메모리 누수를 발생시킬까요?\n\nRocksDB의 메모리 관리에 관한 문서를 읽어보니, RocksDB의 각 클래스가 Autocloseable을 직간접적으로 구현한다는 것을 알게 되었습니다. RocksDB의 자바 객체에서 실제 메모리를 해제하기 위해 사용이 완료되면 명시적으로 close()를 호출해야 합니다 (또는 try-with-resources를 사용). 이를 하지 않으면 메모리 누수가 발생할 수 있습니다.\n\n위의 코드 조각에서 언급된 로직을 구현하는 과정에서, Map의 getOrDefault() 내에서 새 객체가 생성되고 있음을 인지하지 못했습니다. 그 함수가 호출될 때마다 사전 구성된 객체가 주어진 칼럼 패밀리를 위해 존재하는지 여부와 관계없이 이러한 새로 생성된 객체는 닫히지 않고 메모리 누수에 기여했습니다.\n\n메모리 누수의 근본 원인을 이해한 후, 문제 해결은 쉬웠습니다. 이 구성 클래스를 인스턴스화하는 동안 기본 ReadOptions 및 WriteOptions 객체를 한 번만 만들어서 Map의 getOrDefault()에서 새 객체를 생성하는 대신 이들을 사용했습니다.\n\n\n\n영향을 받은 Mustang 서버에 대한 수정 사항을 신속하게 배포하고 메모리 안정성을 확인하기 위해 한동안 모니터링했어요.\n\n## 배포 계속 진행\n\nRocksDB의 부정적인 영향이 없고 모든 것이 원활히 작동하는 것을 확인한 후, 시스템 자원에 특별히 주의를 기울이면서 남은 Mustang 서버를 배포하기로 했어요. 이 점진적인 접근법을 통해 우리는 원활한 전환을 보장하고 제품 환경의 안정성을 유지할 수 있었어요.\n\n## 결론\n\n\n\n요약하자면, RocksDB는 Mustang의 배포 과정에서 현저한 향상을 이끌어냈습니다. Redis에 의존했던 이전 방식 대비 인메모리 데이터 구조를 부트스트랩하는 데 필요한 시간을 크게 줄였습니다. \n\n다양한 샤드를 통해 방대한 테스트를 거친 결과, 부트스트랩 시간이 상당히 줄어들었으며 평균 6분으로 안정화되었습니다 (이전 30~40분). 서버 당 부트스트랩 시간뿐만 아니라 배포 속도도 향상되었으며 이제 더 많은 서버를 병렬로 배포할 수 있게 되었습니다.\n\n그 결과, 전체 샤드에 걸친 Mustang의 전체 배포 과정은 이제 3시간 미만이 소요되며, RocksDB 도입을 통해 달성한 효율성 향상의 증거가 되었습니다. 이 성공은 저희 인프라 내 지속적인 개선과 최적화에 대한 헌신을 강조합니다.","ogImage":{"url":"/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png"},"coverImage":"/assets/img/2024-05-16-ServerbootstrapoptimizationusingRocksDB_0.png","tag":["Tech"],"readingTime":12},{"title":"규모 확장 가능한 개인 정보 보호 GenAI GPU 없이 텍스트 익명화하여 1100의 비용으로","description":"","date":"2024-05-15 16:45","slug":"2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost","content":"\n\n기업 엔지니어링 팀들은 대량의 비정형 텍스트를 처리하기 위해 OpenAI 및 다른 생성 AI 서비스의 파워를 활용하여 생산성을 크게 향상시키고 있습니다. 그들은 고객, 직원, 및 하위 비즈니스를 위한 생산성 도구를 만들고 평가하고 있습니다. 비즈니스 중요 텍스트를 활용하는 프로덕션급 도구들은 엄격한 데이터 준수 가이드라인이 필요합니다. 대부분의 기업은 고객에게 표시할 수 있는 텍스트 유형 및 OpenAI GPT-4와 같은 제3자 AI API 서비스로 보낼 수 있는 텍스트 유형에 대한 준수 정책을 가지고 있습니다.\n\n![그림](/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png)\n\n![그림](/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_1.png)\n\n## 원시 비정형 텍스트 청크의 프라이버시 보호 필요성이 커지고 있습니다.\n\n\n\n사용자에게 보여지거나 안전한 환경을 벗어난 텍스트에는 SSN, 신용 카드 번호 또는 이메일 등의 PII(개인 식별 정보)가 포함되지 않도록 해야 합니다. 예를 들어 외부 API와 같은 임베딩 서비스나 GPT-4 프롬프팅을 사용하기 전에는 안전한 환경을 벗어나는 텍스트에서 PII를 삭제하여 규정을 준수해야 합니다.\n\n## 기존 솔루션: 높은 GPU 사용량과 증가된 대기 시간\n\nNamed Entity Recognition (NER)은 텍스트 분석에서 잘 알려진 문제이며, Hugging Face와 같은 회사에서 제공하는 것처럼 이를 해결하기 위한 여러 오픈 소스 모델이 있습니다. 그러나 이러한 모델을 확장하려면 상당한 GPU 자원이 필요합니다. 예를 들어, BERT와 같은 기본 모델은 리소스 소모가 많으며, 세부 조정을 위해 4대의 V100 GPU에서 몇 시간이 필요합니다(표 1 참조). 또한 배포 중에 느려지는 경우가 있어, 애플리케이션 응답 시간이 약 50배 이상 소요될 수 있습니다(그림 1 참조). 반면, spaCy와 같은 저렴한 대안은 리소스 소비가 적지만, 더 정교한 모델의 93%에 미치지 못하는 65%의 정확도를 제공합니다. 성능 차이는 NER 솔루션을 배포할 때 계산 요구 사항과 정확도 사이의 교환이 강조됩니다.\n\n## ThirdAI의 CPU 전용 NER 모델: 신속한 세부 조정, 초고속 대기 시간, SOTA 정확도. GPU 사이클을 해방하세요.\n\n\n\n저희는 NER을 위해 특별히 제작된 사전 훈련된 기본 모델을 자랑스럽게 소개합니다. 최신 기술의 정확성을 제공하며 BERT나 DistillBERT보다 30~50배 빠른 레이턴시로 작동합니다. 놀랍게도 최대 1,000개의 토큰에서도 우리 모델은 단일 CPU 코어에서 1,000개의 라벨 예측에 대해 약 50ms의 레이턴시를 유지합니다. (비교 자료는 도표 1과 테이블 1을 참고하세요)\n\n저렴한 CPU 전용 인스턴스에서 세밀한 조정 가능: 저희 모델은 수백만 개의 라벨이 지정된 샘플을 몇 분 내에 저렴하고 쉽게 구할 수 있는 CPU 전용 인스턴스에서 세밀하게 조정할 수 있습니다. 추론 및 세밀 조정 중에 다양한 프로세서 간에 일관된 성능을 유지합니다.\n\n저희 기술을 활용하고자 하는 개발자들을 위해 사전 훈련된 NER 모델을 배포하는 간단한 스크립트를 제공해드립니다. 이 모델은 여러 언어에서 표준 NER 범주를 식별할 수 있는 능력이 있습니다. 언어 및 지원되는 범주에 대한 자세한 내용은 안내에서 확인할 수 있습니다. 특화된 다국어 데이터셋에 대해 세밀한 조정을 원하시는 경우에는 다음 대체 스크립트를 사용하십시오.\n\n## 순서 개선된 총소유비용(TCO)\n\n\n\nTable 1은 세밀 조정 비용을 요약하며 Figure 1의 차이는 배치 비용으로 직접 변환됩니다. ThirdAI의 기술적 차이로 인해 전용 GPU 리소스 제약 조건을 없애고 비용을 100배 절감할 수 있습니다. ThirdAI를 사용하여 언제 어디서나 손쉽게 데이터를 사적으로 보호하세요.\n\n## 중요한 링크\n\n모든 파이썬 노트북 및 지침서 링크: https://github.com/ThirdAILabs/Demos/tree/main/named_entity_recognition","ogImage":{"url":"/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png"},"coverImage":"/assets/img/2024-05-15-PrivacyPreservingGenAIatScaleAnonymizeYourTextwithoutGPUsforahundredththecost_0.png","tag":["Tech"],"readingTime":3},{"title":"흑자 블랙 바스타의 코발트 스트라이크를 추적하기","description":"","date":"2024-05-15 16:44","slug":"2024-05-15-HuntingBlackBastasCobaltStrike","content":"\n\n\u003cimg src=\"/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png\" /\u003e\n\n지난 주 FBI와 CISA가 Black Basta 랜섬웨어 그룹을 위한 #StopRansomware 경보를 발표했습니다.2024년 중에 이 그룹은 LockBit와 Play 다음으로 총 랜섬웨어 피해자가 세 번째로 많은 것으로 나타났습니다. 특히, 그 그룹이 Cobalt Strike를 사용한다는 점이 강조되었습니다(표 10 - \"알려진 Black Basta Cobalt Strike 서버 도메인\" 참조).\n\nIntel-Ops는 Black Basta가 배포한 Cobalt Strike 서버를 포함하여 야생에서 Cobalt Strike 서버를 적극적으로 추적하고 있습니다. 이 게시물에서는 FBI/CISA 경보에 포함된 C2 서버 및 해당 경보에 미포함된 C2 서버 또는 악의적이거나 Black Basta와 관련된 서버로 공개적으로 추적되지 않은 서버에 대한 분석 결과 중 일부를 강조하겠습니다.\n\n\n\n블랙 바스타의 코발트 스트라이크 그래프:\n\n발리딘의 \"Bulk Analyzer\" 도구를 사용하여 공지사항의 도메인을 빠르게 활성 IP 주소로 해결하고 시간에 따른 호스팅 패턴을 이해하며, 말테고 그래프에 대한 상관 관계를 생성할 수 있습니다:\n\n![그래프 이미지](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_1.png)\n\n# 결과\n\n\n\n- 블랙 바스타 인프라는 명확한 클러스터로 그룹화될 수 있으며, 이 중 일부를 아래에서 강조하겠습니다.\n- 블랙 바스타 인프라에서 관찰된 주요 워터마크는 1357776117 및 1158277545입니다.\n- 코발트 스트라이크 서버의 대부분은 Vult Hosting LLC (AS-CHOOPA), JW Lucasweg 35, Digital Ocean 및 Servinga에서 호스팅됩니다.\n\n# 클러스터 1\n\n블랙 바스타가 운영하는 대다수의 코발트 스트라이크 서버는 DNS 비컨을 활용합니다: https://hstechdocs.helpsystems.com/manuals/cobaltstrike/current/userguide/content/topics/listener-infrastructue_beacon-dns.htm. 우리의 분석에 따르면, 공지서에서의 IOC는 거의 전적으로 Vultr, Lucasweg 및 Digital Ocean에서 호스팅됩니다.\n\n우리의 조사에 따르면, 적어도 6개의 추가 DNS 코발트 스트라이크 비컨이 이러한 제공업체에서 호스팅되었습니다. Intel-Ops는 다른 제공업체에서도 유사한 C2를 식별했습니다. \"thenewbees[.]org\"와 같은 새로 식별된 도메인은 공지서에 기재된 다른 DNS 비컨들의 네이밍 컨벤션과 일치합니다.\n\n\n\n인텔-옵스 클러스터 1의 예시 DNS 비콘:\n\n![Example DNS Beacon](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_2.png)\n\n## 클러스터 2\n\n도메인 \"usaglobalnews[.]org\"은 Cobalt Strike 비콘의 일부인 것으로 보입니다. 인텔-옵스 클러스터 2에서는 두 개의 도메인이 이전에 이미 Pikabot을 포함한 Black Basta 사건과 관련하여 Trend Micro에 의해 2023년 12월에 보고되었습니다. \"ruggioil[.]com\"과 \"bluenetworking[.]net\"의 도메인은 모두 해당 사건과 연결되었습니다.\n\n\n\nhttps://www.trendmicro.com/content/dam/trendmicro/global/ko/research/24/a/a-look-into-pikabot-spam-wave-campaign/ioc-pikabot-spam-campaign.txt\n\nIntel-Ops Cluster 2에서의 Cobalt Strike 서버 예시:\n\n![Cobalt Strike 3](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_3.png)\n\n![Cobalt Strike 4](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_4.png)\n\n\n\n# 클러스터 3\n\n인텔-옵스 클러스터 3 내에서는 3개의 자문 도메인이 관찰되었으며, 이 중 일부는 중국 호스팅 업체 및 중국 DNS 레코드의 분포가 더 큰 코발트 스트라이크 서버와 클러스터링되어 있습니다. 추가로, \"dfir-delight\"의 2024년 4월 보고서에서 적어도 3개의 추가 C2 서버와 자문 도메인 중 하나가 공개적으로 블랙 바스타 활동에 속한다고 확인되었습니다: https://dfir-delight.de/p/black-basta-iocs/\n\n인텔-옵스 클러스터 3 코발트 스트라이크 C2 서버 예시:\n\n![HuntingBlackBastasCobaltStrike_5](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_5.png)\n\n\n\n# 클러스터 4\n\nIntel-Ops는 워터마크가 있는 여러 개의 Cobalt Strike 비콘을 식별했습니다: 1357776117. 이 워터마크가 있는 IP 주소의 비교적 작은 클러스터가 있습니다: 우리는 Hunt.io를 사용하여 지난 30일 동안 이 워터마크가 있는 IP의 수를 식별할 수 있습니다. 특히, 해결 도메인 중 일부가 공고서의 도메인 이름 규약과 일치하는 것을 확인했습니다:\n\n![이미지](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_6.png)\n\n이 클러스터 내에서, 최근의 Black Basta 사건에 공개적으로 속한 다른 IP 주소가 있습니다. 또한, 워터마크만을 분석했을 때 호스팅이 Black Basta 클러스터의 다른 패턴과 거의 일치하지 않았습니다, 예를 들어 DNS 비콘이요.\n\n\n\n예시 인텔-옵스 클러스터 4 내의 Cobalt Strike C2 서버:\n\n![Alt text](/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_7.png)\n\n# 결론\n\n블랙 바스터 공고로부터 추론할 수 있는 추가적인 클러스터/상관 관계가 있습니다. 이 분석은 보고서에 나와 있는 알려진 지표만 다루고 있습니다. Cobalt Strike를 이용하는 그룹에 대한 보호를 강화하고 추가 정보를 얻으려면:\n\n\n\n**Hunting Adversary Infrastructure Course**\n\n우리의 \"Hunting Adversary Infrastructure\" 과정에서 가르치는 기술을 활용하면 보안 분석가들이 모두 레벨에서 활동을 클러스터링하고 소속을 판단하는 등 보고서를 더욱 풍부하게 할 수 있습니다. 이러한 모든 지표들은 Intel-Ops에서 적극 추적되고 곧 C2 위협 피드를 통해 제공될 예정입니다.\n\nCobalt Strike와 Black Basta와 같은 그룹과 같은 프레임워크를 추적하는 방법을 배우고 싶다면, 저희 과정에 등록해보시기 바랍니다. 여기에서 등록하면 학생들은 배우는 데 도움을 줄 추가적인 쿼리 및 API 크레딧이 포함된 Validin 플랫폼을 위한 특별한 Intel-Ops 계정을 획들할 수 있습니다.\n\nC2 Feed\n\n\n\n저희 C2 피드에 관한 정보가 필요하시면 LinkedIn, Twitter 또는 이메일(contact@intel-ops.io)로 연락해주세요.","ogImage":{"url":"/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png"},"coverImage":"/assets/img/2024-05-15-HuntingBlackBastasCobaltStrike_0.png","tag":["Tech"],"readingTime":4},{"title":"AI로 생성된 비디오가 왜 최면을 걸리듯 부드럽고 기묘하게 느껴지는지","description":"","date":"2024-05-15 16:41","slug":"2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny","content":"\n\n## 공정한 숫자 분석가의 장단점\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*Aun9fXyvK-JxqCHKhQyxrg.gif)\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*j7df-zSIAOBOo6rKwFgT1Q.gif)\n\n위 멋진 애니메이션들은 최고의 AI 기반 비디오 생성기인 Sora가 OpenAI로부터 나온 것 중에서도 가장 진보된 것입니다. 이것들은 의심의 여지없이 인상적입니다. 인공지능 분야에서 일하는 사람들도 지난 몇십 년 동안 많은 발전을 본 사람에게도 놀랄 만한 것입니다. 이 비디오를 관람하면 AI가 클립의 다양한 테마와 객체를 얼마나 매끄럽게 섞어내는지, 그 결과물이 얼마나 일관되고 물리적으로 타당해 보이는지에 놀라게 될 것입니다. 이 비디오를 보면 평범한 사람은 AI 기술이 이렇게 현실적인 수준에 도달했다는 사실에 당황할 것입니다.\n\n\n\n잠시 지나면 새로움이 사라지고 다른 것을 느끼기 시작할 지도 모릅니다. 이 비디오들에는 이상하고 최면적인 느낌이 있습니다. 충분히 오랫동안 보게 되면 눈빛 — 아마도 마음까지 — 흐릿해질 것입니다. 비디오가 미묘하고 일정한 압력으로 당신의 두뇌에 가해지는 것 같은 느낌이 듭니다. 이 경험에는 뭔가 기묘한 것이 있는데, 특정한 방식으로 말하자면 눈뒤쪽이나 다른 어딘가에 숨겨져 있는 것처럼 영원히 손에 쓸 수 없는 느낌이 듭니다.\n\n이 경험은 크래프트가 완벽하게 집중되어 있어서 불안할 수 있습니다. 그들의 표현은 섬세하고 부드럽고 윤득한, 내용에 관계 없이 가파른 모서리가 없습니다. 그러나 확신컨대 저는 그들에게 사로잡혔음에도 불구하고 뇌 속에 흐릿함이 남는 느낌을 떠났습니다. 이것은 비디오의 초현실적인 내용 때문인 것이 부분적으로는 사실입니다. 그러나 이것만으로 현상을 완전히 설명할 수는 없습니다 — 많은 인간이 만든 비디오가 초현실적이며, 똑같은 기이하면서 인상적인 효과를 내지는 않습니다. 이 현상을 직간접적으로 기술로서의 편견으로 이해할 수도 없습니다. 사실, 이 클립들이 인간에 의해 촬영되었다고 상상하면 기이함을 더 심화시키고 희석시키는 것보다 더 매료되는 느낌을 주곤 합니다.\n\n이 클립들은 눈동자를 즐겁게 해줄 수 있으며, 그들의 즐거움은 선명하고 생생한 영상의 시각적 즐거움에서 나옵니다. 단지 사탕처럼, 그들은 궁극적으로 만족할 만한 느낌을 주지는 못할 수도 있습니다. 항상 무언가를 \"전달\"하려고 하는 것 같지만 결코 그렇지 않습니다. 다시 말해, 그들은 아무것도 전달하려는 의도가 없으며 순간적인 경험 자체가 그 요점입니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*OIsAJ4ZB-YvQ34PDvyHswA.gif)\n\n\n\n그들을 관찰하는 것은 분수를 관찰하는 것처럼 매혹적입니다: 그 자체로 또한 발표 방식으로 매혹적입니다. 아니면 클라이맥스가 없는 이야기를 듣는 것과 같을지도 모릅니다. 설정이 해결되었다고 생각하는 즉시, 그것이 약간 변화하고 다른 이야기를 드러내며, 지금은 자신의 결말을 예측하며, 그리고 그렇게 이어지는 것은 하나의 문장처럼. 비디오는 항상 깨달음 직전인데, 그것들은 결코 넘어가지 않습니다. — 그리고 당신 역시 그것을 기대하지 않습니다. 그것들은 한계 상태이며, 그 상태를 유지하는 것을 선호합니다.\n\n![GIF 이미지](https://miro.medium.com/v2/resize:fit:400/1*xX03li5aQruQGNI5fwfcqg.gif)\n\n최면술 효과는 비디오가 어딘가로 향하고 있다는 느낌에서 온다. 그리고 모퉁이마다 새로운 약속을 하며 당신을 긴장시킵니다. 시청자는 따라가고, 무언가가 제공될 때까지 놓칠 수 없다고 느낍니다. 마침내 그것이 제공되지 않을 것으로 깨닫고 단지 그 순간의 경험을 즐깁니다. 그래서 그것은 목표에서 목표로 방황하며, 사회적 미디어에서 스크롤하는 것만큼 방향성이 없습니다.\n\n명확히 말하자면, 비디오에는 기저적인 아이디어가 있지만, 시작할 때 공개되고 일관성 있게 유지됩니다. 설정과 전달의 곡선은 없으며, 단지 지속적인 압력만 있는 것입니다. 이것이 비디오 콘텐츠가 시각적으로 매력적이어야 하는 이유이며, 종종 슬로우 모션으로 제시됩니다. 눈 얼음 위에서 노는 귀여운 강아지, 컵 속의 보트, 고원을 걸어 다니는 털족. 그들이 교육되는 재고 영상처럼, 그들의 시각적 매력은 그들의 요점 전부입니다. 클립의 메시지는 매체와 일치합니다.\n\n\n\n\n[![image](https://miro.medium.com/v2/resize:fit:640/1*KlbQB5xH8kI5G0FWXOWL1g.gif)](https://miro.medium.com/v2/resize:fit:640/1*KlbQB5xH8kI5G0FWXOWL1g.gif)\n\n아마도 이 현상을 이해하는 가장 좋은 방법은 다음과 같이 물어보는 것입니다: 클립은 무엇을 말하려고 하는 걸까요? 그 의도는 무엇이며, 주제는 무엇이며, 논문은 무엇일까요? 창조자가 우리가 이것을 보게 하려고 했던 이유, 특히 이것을 보게 하려고 한 이유는 무엇인가요? 어떤 비디오를 보든지, 생성의 의도나 목적을 빨리 파악할 것을 기대합니다. AI로 생성된 비디오의 경우 답을 찾기 어렵지 않을 것입니다. 왜냐하면 생성 모델이 한 번 훈련되면 모든 지정된 내용은 생성된 텍스트 프롬프트에서 나오기 때문이죠. 따라서 어떤 독특한 메시지라도 해당 비디오가 같은 모델의 다른 비디오와 비교하여 가지고 있는 것은 오로지 해당 프롬프트에서 온다는 데서 오게 됩니다. 텍스트 프롬프트는 궁극적으로 논문을 요약해야 하지만... 그렇지 않습니다. 프롬프트는 내용을 대표하지만 목적은 대변하지 못합니다.\n\n내가 무엇을 말하는지 이해하려면 AI에게 제공하는 모든 프롬프트가 어디서나 나온 것은 아니라는 것을 기억해야 합니다. 작가로서 이 프롬프트를 통해 표현하려는 동기, 감정, 목표 및 의도의 역사가 있습니다. 이것들이 당신의 숨겨진 목적이며, 글자 그대로 요청하고 있는 구체적인 내용이 아닙니다. 그 백그라운드 중 어느 것도 AI로 전달되지 않습니다. AI는 모든 사실을 받지 못하므로 속여주고 있다고 할 수 있습니다.\n\n[![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png)](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png)\n\n\n\n\nAI에게 \"가난한 사람들을 돕고 싶게 만들고 싶어\" 또는 \"세계를 탐험하는 것에 흥분하게 만들고 싶어\" 또는 \"아이처럼 궁금해하게 만들고 싶어\"와 같은 것을 전달하는 것은 어려운 일입니다. 종종 실제로 당신이 정말 무엇을 말하려고 하는 지에 대해 완전히 인식하지 못합니다 - 적어도 영어 단어로 표현할 정도로 충분히 인식하지 못합니다.\n\n따라서 AI의 결과물이 기대에 미치지 못하는 경우, 그것이 우연히 악한 요정처럼 당신이 요청한 대로 제공하지만 실제로 원하는 것은 제공하지 않기 때문입니다. AI가 범하는 실수는 사용자의 숨은 의도에서의 소외감을 드러냅니다. AI는 직역적이어서 당신이 강조하거나 억압해야 할 부분을 식별하지 못하며 실제로 있어야 하는 누락된 부분을 추가해야 하는 타이밍조차도 파악하지 못합니다.\n\n사용자로서 당신은 결과가 의도한 대로 가까워지도록 프롬프트를 수정해야 합니다만 실제로 말하지 않은 것을 수정해야 합니다. 그 동안 AI는 밀려오는 방향, 프롬프트별로 여러 번에 걸쳐 사용자의 비전 쪽으로 밀리며 움직입니다. 당신이 관심을 갖고 있다는 이유로 AI를 끌어당기고 있는 것이 아니라 밀어내고 있는 것입니다. 마치 모래성을 계속 다듬어야 하는 것처럼 그렇지 않은 부분은 천천히 무너지는 것입니다.\n\n![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_1.png)\n\n\n\n아티스트 제너레이터를 사용하여 미술을 만드는 경험은 종종 영화 롤을 위해 서영장의 서막이 필요한 침팬지 촬영을 하는 것과 비슷합니다. 촬영장에서의 동물 트레이너와 카메라맨은 충분한 샷을 촬영하여 결과물이 침팬지가 원하는대로 보이도록 편집할 수 있도록 해야 합니다. 동물은 슬픈 상호작용을 보고도 그것이 슬픔스러워 보이도록 통찰할 확률이 낮습니다. 이 모든 것은 다른 훈련된 행동들을 조각조각으로 모아서 마치 침팬지가 의도했던 것처럼 보이도록 해야 합니다.\n\nAI가 당신의 의도와 일치하려면 자신의 의도를 가져야 하거나, 사용자의 프롬프트에서 그 의도를 추출해내야 합니다. 후자는 매우 어렵습니다. 반면에 인간 예술가에게 작품을 의뢰할 때는 요청으로부터 당신의 의도를 이해할 것을 기대합니다. 그렇지 않으면 감정적이거나 은유적인 언어를 통해 설명할 수 있습니다.\n\nAI가 표현하는 \"의도\"나 의미는 프롬프트 자체에서 비롯된 것이 아니라, 훈련 비디오에서 빌려온 단편에서 나옵니다. 오직 즐거운 비디오에서만 훈련하면 결과물은 즐거울 것입니다. AI는 표현하는 대상이나 그들의 예술이 전달하는 것에 대해 의견을 갖거나 깐깐해하지 않습니다. 그들은 훈련 중 주어진 이미지를 사용하여 완전히 개방적이라고 말할 수 있습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:640/1*vZHKB5bRVRcQmiCef7DB2w.gif)\n\n\n\nAI 생성기는 다양한 콘텐츠를 일관된 전체로 통합하는 능력이 우수합니다. 그들이 하는 일은 본질적으로 그겁니다. 생성기는 사용자가 제공한 프롬프트를 받아들인 다음, 교육 중에 보았던 다른 클립들과 관련이 있는 방향으로 여러 방향으로 분기합니다.\n\n![image](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_2.png)\n\n반면에, 인간 창작자들은 상당히 제한적이며, 예술가가 특이한 표현 스타일에서 벗어나는 것이 어려울 수 있습니다. 이는 강점이자 약점입니다. 인간이 만든 작품의 제한된 스타일은 집중을 보여주며, 무엇을 중요시 여기는지를 보여줍니다. 의도적인 인간 행동은 그 안에 포함된 가치를 보여줍니다. 예술가에게 이러한 가치가 작품 속 주제들을 유발시키며 그들이 무엇을 그리거나 그림을 그릴지를 선택적으로 만듭니다.\n\n인간 커뮤니케이션은 일관성 이상을 추구합니다. 그것은 시청자를 특정 방향으로 이끌거나, 특정한 생각이나 감정을 일으키기 위해 노력합니다. 이것은 결과적인 표현에 내재되어 있습니다. 결과적으로, AI가 훈련한 각 인간 예술 작품은 다른 의도가 있습니다. 그리고 그들이 큰 냄비에 섞는다면 결과를 혼란스럽게 할 수밖에 없습니다. 이것이 생성 인공지능이 한 특정 예술가의 스타일을 복사할 때 더 성공적인 이유입니다. 그들은 해당 예술가의 암시적 동기를 빌려오며, 무시할 것, 강조할 것, 무시할 것을 비교적 오염되지 않은 채로 가져옵니다.\n\n\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:640/1*3ysK_fx8qcIdKakh78C9SQ.gif\" /\u003e\n\n여러 관점을 결합할 때 그 결과물은 특정한 것을 말하지 않을 뿐더러 \"의견\"을 내세울 수도 없습니다. AI 생성 비디오는 우리가 만들어낸 문화 유물을 통해 서로 조화롭게 발음하는 열두 명의 인간 목소리와 같습니다. 결과물을 통합하는 데 결합된 의지나 목적이라는 단일한 가치는 없습니다. AI는 콘텐츠를 효율적으로 결합하는 데 탁월하지만 의도를 결합하지는 않습니다. 단순히 의도를 결합해서는 안 되기 때문입니다. \"의도\"는 항상 원하는 대로 편향됩니다. 양보할 여지가 없습니다. AI 예술에서 의도를 혼합하는 것을 바로잡기 위한 유일한 방법은 AI가 먼저 선택들이 관람자에게 미치는 효과를 배우고, 그것을 활용해 예술을 자신이 원하는 방향으로 이끌어내는 것입니다.\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:640/1*A2kk9SXCjVUHD3iHU9Hz9g.gif\" /\u003e\n\n현재, 생성적 AI는 일종의 깨끗한 판이며 편향되지 않은 데이터 분석기입니다. 그것에는 매력이 있습니다. 인간과 함께 일할 때의 많은 성가시움, 특히 고집부리기와 편견이 없는 것을 들 수 있습니다. 그것은 순순하고 공손하게 사회 유물을 흉내내는 것입니다. 그러나 이것은 실패가 혼란스러울 수 있음을 의미합니다, 소프트웨어가 여러분을 \"조롱\"¹하고 있는 것처럼. 여러분이 메시지나 주제에 대해 왜 그것을 중요하게 생각하는지 이해한다는 것을 보여주지 않고 여러분의 표현을 모방합니다. 여러분은 AI '예술가'가 여러분의 메시지에 동의하고 그것을 결과물을 통해 표현해주기를 원합니다. 그러나 그것은 여러분에게 동의할 수 없으며, 보여준 것만 모방할 뿐입니다. 따라서 그것은 결국 \"조롱\"인 인간 유물을 생성하게 됩니다.즉, 원본의 주도적인 목소리가 없는 모조품을 생성합니다.\n\n\n\n¹ 이것은 유사법이에요; AI는 아무것도 조롱하는 게 아니에요.\n\n![AI generated video](/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_3.png)","ogImage":{"url":"/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png"},"coverImage":"/assets/img/2024-05-15-WhyAI-generatedvideosfeelhypnoticfluidanduncanny_0.png","tag":["Tech"],"readingTime":7},{"title":"마이크로소프트 Phi-3-Mini는 정말 인상적이에요","description":"","date":"2024-05-15 16:38","slug":"2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive","content":"\n\n최근에 마이크로소프트 AI가 출시한 Phi-3-Mini 언어 모델입니다. 이 모델은 작은 언어 모델(SLM) 범주에 속하며 LLM이 제공하는 여러 기능을 제공합니다. 유일한 차이점은 SLM은 크기가 더 작고 적은 데이터로 훈련되었다는 것입니다.\n\n마이크로소프트에 따르면 Phi-3 모델은 가장 강력하고 비용 효율적인 작은 언어 모델(SLM)이라고 합니다. 이들은 Phi-3-mini-4k-instruct 및 Phi-3-mini-128k-instruct 모델을 출시했으며 둘 다 어떠한 제약 없이 완전히 오픈 소스입니다. 이는 128k 컨텍스트 길이를 갖는 작은 언어 모델이 처음으로 등장한 시점입니다.\n\n다가오는 몇 주 안에 Phi-3-small (7B) 및 Phi-3-medium (14B) 모델도 출시할 예정입니다.\n\n# 성능 평가\n\n\n\n아래는 공식 Phi-3 모델 벤치마크 결과입니다.\n\n![image](/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png)\n\n작고 강력한 Phi-3-Mini 모델은 무게 대비 어마어마한 성능을 자랑합니다. Phi-3-Mini 모델은 38억 개의 매개변수를 가지고 있으며, 추론, 수학, 코드 생성 등 대부분의 벤치마크에서 Gemma-7B, Mistral-7B, Llama-3-8B와 경쟁하여 우수한 성과를 거두었습니다.\n\n\n\nPhi-3-Mini 모델이 성능이 잘 나오지 않는 유일한 벤치마크는 사실적인 지식 벤치마크입니다. 그 이유는 이 모델의 작은 크기 때문에 많은 사실을 보존할 수 없기 때문입니다.\n\n하지만 우리는 여전히 Phi-3-Mini 모델을 RAG 및 검색 작업에 사용할 수 있습니다. 여기서 우리는 사실들을 검색하여 모델에 전달하여 답변을 생성할 수 있습니다. 그리고 이 블로그에서는 바로 그것을 구축할 것입니다 — 지역적인 GenAI 기반 검색 엔진.\n\n# 왜 SLMs를 사용해야 하는가?\n\n개발을 계속 진행하기 전에, 왜 작은 언어 모델이 중요한지에 대해 알아보겠습니다.\n\n\n\n매우 처음으로 실험이 있습니다. LLM을 실험하는 것은 상당히 비싸요. LLM 위에 무언가를 개발하거나 기존 프로세스/소프트웨어에 기능을 추가하기 위해 실험을 하는 것은 상당한 양의 실험을 필요로 합니다. 이러한 실험에는 다양한 프롬프팅 기술 시도, 출력을 구문 분석하는 여러 방법 사용 등이 포함됩니다.\n\n네, 많은 LLM API 제공 업체들이 요금을 지불하는 방식을 채택하고 있습니다. 그럼에도 불구하고 모든 것을 올바르게 만들기 전에 $10에서 $20을 지출할 수 있습니다. 그러나 SLM을 사용하면 llama-cpp-python 또는 node-llama-cpp와 같은 라이브러리를 사용하여 로컬에서 로드하고 추론을 수행할 수 있습니다.\n\n다음은 클라이언트가 클라우드를 피하려는 경우입니다. 사람들은 보통 내부 조직 정보를 클라우드에 제공하는 것을 조심스러워하며 대부분을 인하우스 또는 대부분 로컬에서 처리하고 싶어합니다. 그들은 메인프레임에서 일부 데이터를 검색하여 사용자에게 제공하거나 서버 측에서 렌더링하여 사용자에게 보여줄 수 있습니다. 이 경우 사용자의 로컬 컴퓨터나 메인프레임과 함께 Phi-3-Mini와 같은 SLM을 사용하여 그들의 검색에 GenAI 기능을 추가할 수 있습니다.\n\n모든 비즈니스는 성능을 최적화하고 저렴한 비용으로 속도 처리량과 보안을 제공합니다. 따라서 마지막으로는 컴퓨팅 효율성, 생성 속도, 특정 작업 및 보안의 조합입니다. LLM은 매우 일반적이지만 대부분의 조직 또는 클라이언트는 그것을 필요로하지 않을 수 있고, 그들은 모델을 자신에게 특화된 작업에 적합하도록 원할 수도 있습니다. 그 경우에는 자체 사용자 정의 명령어 집합으로 SLM을 미세 조정한 다음 미세 조정된 버전을 추론용으로 호스팅할 수 있습니다. 추론은 로컬이나 메인프레임에서 수행할 수 있습니다.\n\n\n\n이제 기업이나 개인이 SLM을 사용하는 다양한 이유를 이해했으니 Phi-3-Mini 모델을 활용한 검색 도구를 만들어보겠습니다.\n\n# AI 검색 도구\n\n최근에 나는 Llama-3 8B가 Groq에서 빠른 생성 속도로 검색 요약을 수행하는 데 탁월하다는 주제의 블로그를 작성했습니다. 아직 해당 기사를 읽지 않았다면 아래 링크를 통해 확인해보세요.\n\n저희 이전 블로그에서 구현한 대부분의 내용은 그대로 유지될 것입니다. 변경되는 것은 LLM 및 검색 엔진 뿐입니다.\n\n\n\n여기서는 llama-cpp-python 라이브러리를 사용하여 로컬 시스템에 양자화된 Phi-3-Mini-4K-Instruct 모델을 로드하고 모델과 상호 작용하여 텍스트를 생성할 것입니다. 또한 사용자 쿼리를 기반으로 Brave Search API를 사용하여 검색 결과를 검색할 것입니다.\n\n하지만 어쨌든, 하루 끝에 모든 AI 기반 검색 플랫폼의 기본 원리는 사용자 쿼리를 기반으로 내용을 검색하고 요약하는 것입니다. 각각이 이 작업을 수행하는 방식은 운영 규모에 따라 다를 수 있습니다. 대부분의 엔지니어링 노력은 검색 속도를 빠르게 만들기(캐싱 또는 기타 방법으로)와 관련 결과를 검색하며 필요할 때 사실과 숫자로 답하는 데 사용됩니다.\n\n# 검색 API\n\n가장 간단한 구성 요소인 검색 구성 요소부터 시작하여 전체 시스템의 중추인 부분입니다.\n\n\n\n위에서 언급한 대로, 블로그에는 Brave Search API를 사용할 것이지만 다른 것을 사용해도 됩니다. Brave Search API를 사용하려면 https://brave.com/search/api/로 이동하여 등록해야 합니다. 등록한 후에는 우리의 신용카드 세부정보를 제공하여 무료 요금제를 구독해야 합니다. 무료 요금제에서는 월 2,000개의 요청과 초당 1개의 요청이 가능하여 실험에 충분합니다.\n\n무료 요금제를 구독한 후에, API Keys 섹션으로 이동하여 API 키를 생성하고 해당 키를 복사하여 .env 파일에 넣어야 합니다.\n\n```js\nBRAVE_API_KEY=\"YOUR_BRAVE_API_KEY\"\n```\n\n아래에 보여지는 대로 configs.py 파일에 모든 환경 변수를 로드할 것입니다.\n\n\n\n```js\n# configs.py\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nBRAVE_API_KEY = os.environ.get(\"BRAVE_API_KEY\")\n```\n\nAPI 키를 구성했어요. 이제 사용자의 쿼리를 검색하는 데 도움이 될 코드를 작성해 봅시다.\n\n```js\n# brave_search.py\n\nfrom configs import BRAVE_API_KEY\nimport httpx\n\nasync def brave_search(search_term):\n    brave_api_key = BRAVE_API_KEY\n    url = f'https://api.search.brave.com/res/v1/web/search?q={search_term}\u0026count=3'\n    headers = {\n        'X-Subscription-Token': brave_api_key,\n        'Accept': 'application/json'\n    }\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        if response.status_code == 200:\n            data = response.json()\n            print('검색 데이터: ', data)\n            return format_search(data)\n        print(await response.text())\n        return None\n\ndef format_search(search_results):\n    retrieve_keys = ['web', 'news']\n    formatted_results = []\n    for value in retrieve_keys:\n        if value in search_results:\n            results = search_results[value]['results']\n            formatted_results.append('\\n'.join(\n                f\"Title: {result['title']} Description: {result['description']} URL: {result['url']}\"\n                for result in results))\n    print('포맷된 결과: ', formatted_results)\n    if formatted_results:\n        return '\\n'.join(formatted_results)\n    return None\n```\n\n위에서는 검색어를 인수로 사용하는 brave_search 함수를 정의했습니다. 그런 다음 Brave API 웹 검색 엔드포인트를 호출하고 검색 결과를 검색합니다. 검색 결과는 format_search 함수를 사용하여 하나의 문자열로 서식이 지정됩니다.```\n\n\n\n# 로컬 모델 추론\n\n우리는 llama.cpp 패키지의 Python 바인딩을 llama-cpp-python 라이브러리를 통해 로컬 시스템에 모델을 로드하고 텍스트 생성 또는 추론을 수행할 것입니다.\n\n이를 위해서는 먼저 llama-cpp-python 패키지를 설치하고 아래 제공된 링크에서 Phi-3-Mini-4k-Instruct 모델을 GGUF 형식으로 다운로드해야 합니다.\n\n## llama-cpp-python 설치하기\n\n\n\npip를 사용하여 llama-cpp-python을 설치해 보세요!\n\n```sh\npip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n```\n\n만약 이미 설치되어 있는 경우, 이 명령어는 llama-cpp-python 패키지를 업그레이드하고 재설치할 것이며, 그렇지 않은 경우에는 새로 설치할 겁니다.\n\n## 모델 다운로드\n\n\n\n위에 제공된 링크로 이동하여 Phi-3-mini-4k-instruct-q4.gguf 파일을 다운로드하세요. 다운로드가 완료되면 작업 디렉토리에 모델 폴더를 만들고 모델 파일을 모델 폴더로 이동하세요.\n\n![Phi-3 Mini](/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_1.png)\n\n## 추론을 위한 모델 호출\n\nllama-cpp-python 라이브러리에서 제공하는 유틸리티를 사용하여 모델을 로드하고 추론을 위해 호출하고 생성된 토큰을 스트리밍 및 비스트리밍 방식으로 출력하는 클래스를 작성하겠습니다.\n\n\n\n```python\n# llm_invoke.py\n\nfrom llama_cpp import Llama\nfrom ctx import ContextManagement  # 이것은 나중에 나올 것임\nfrom typing import List, Dict, Union\n\nclass LLM:\n\n    def __init__(self, model_path: str, **kwargs):\n        self.llm = Llama(\n            model_path=model_path,\n            n_gpu_layers=kwargs.get(\"n_gpu_layers\",\n                                    -1),  # GPU 가속화를 사용할 때 주석 해제\n            seed=kwargs.get(\"seed\", 1337),  # 특정 시드를 설정할 때 주석 해제\n            n_ctx=kwargs.get(\"n_ctx\",\n                             4096),  # 컨텍스트 창을 확장할 때 주석 해제\n            n_threads=kwargs.get(\"n_threads\", 8))\n        self.ctx = ContextManagement(2560)\n\n    def __stream__(self, messages: List[Dict], **kwargs):\n        input_message = self.ctx(messages)\n        output = self.llm(input_message, stream=True, echo=False, **kwargs)\n        for op in output:\n            yield op.get(\"choices\")[0].get(\"text\") or \"\"\n\n    def __complete__(self, messages: List[Dict], **kwargs):\n        input_message = self.ctx(messages)\n        output = self.llm(input_message, echo=False, **kwargs)\n        return output.get(\"choices\")[0].get(\"text\")\n```\n\n위에서 llama-cpp-python 모듈에서 사용 가능한 Llama 객체를 사용하여 초기화 인수에서 제공된 model_path에서 모델을로드했습니다.\n\n또한 __init__ 메소드에서 ContextManagement 객체를 초기화합니다. 이것은 Llama 3 + Groq가 AI Heaven 블로그에서 본 것과 유사합니다. 다음 섹션에서 다시 살펴보겠습니다.\n\nLLM 클래스에서 두 가지 메서드가 있습니다. 토큰이 생성되는대로 토큰을 스트리밍하는 __stream__ 메서드와 생성이 완료되면 출력을 반환하는 __complete__ 메서드입니다.\n\n\n\nLLM 클래스를 사용하면 이제 모델을 로드하고 호출할 수 있습니다. 이제 LLM (SLM)를 검색과 통합해 보겠습니다.\n\n# 컨텍스트 관리\n\nPhi-3-mini-4k-instruct 모델을 사용하면 최대 4k 토큰을 사용하거나 생성할 수 있습니다. 이 블로그에서 GenAI Search를 대화식으로 만들 계획은 없기 때문에 이 부분을 피할 수 있습니다. 하지만, 호기심을 위해 여기에 작성해 봅시다.\n\n```js\n# ctx.py\n\nfrom typing import List, Dict, Union\nfrom transformers import AutoTokenizer\n\n\nclass ContextManagement:\n\n    def __init__(self, max_available_tokens: int = 3000):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"microsoft/Phi-3-mini-4k-instruct\")\n        self.max_available_tokens = max_available_tokens\n\n    def __count__tokens__(self, content: str):\n        return len(self.tokenizer.tokenize(content)) + 2\n\n    def __pad_tokens__(self, content: str, num_tokens: int):\n        return self.tokenizer.decode(\n            self.tokenizer.encode(content, max_length=num_tokens))\n\n    def __manage_context__(self, messages: List[Dict]):\n        managed_messages = []\n        system_message = None\n        if messages[0][\"role\"] == \"system\":\n            system_message = messages[0]\n        current_length = 0\n        if system_message:\n            current_length += self.__count__tokens__(\n                system_message.get(\"content\"))\n        current_messsage_role = None\n        for ix, message in enumerate(messages[1::-1]):\n            content = message.get(\"content\")\n            message_tokens = self.__count__tokens__(message.get(\"content\"))\n            if ix \u003e 1:\n                if current_length + message_tokens \u003e= self.max_available_tokens:\n                    tokens_to_keep = self.max_available_tokens - current_length\n                    if tokens_to_keep \u003e 0:\n                        content = self.__pad_tokens__(content, tokens_to_keep)\n                        current_length += tokens_to_keep\n                    else:\n                        break\n                if message.get(\"role\") == current_messsage_role:\n                    managed_messages[-1][\"content\"] = f\"\\n\\n{content}\"\n                else:\n                    managed_messages.append({\n                        \"role\": message.get('role'),\n                        \"content\": content\n                    })\n                    current_messsage_role = message.get(\"role\")\n                    current_messsage_role = message.get(\"role\")\n                    current_length += message_tokens\n            else:\n                if current_length + message_tokens \u003e= self.max_available_tokens:\n                    tokens_to_keep = self.max_available_tokens - current_length\n                    if tokens_to_keep \u003e 0:\n                        content = self.__pad_tokens__(content, tokens_to_keep)\n                        current_length += tokens_to_keep\n                        managed_messages.append({\n                            \"role\": message.get(\"role\"),\n                            \"content\": content\n                        })\n                    else:\n                        break\n                else:\n                    managed_messages.append({\n                        \"role\": message.get(\"role\"),\n                        \"content\": content\n                    })\n                    current_length += message_tokens\n                current_messsage_role = message.get(\"role\")\n            print(f\"TOTAL TOKENS: \", current_length)\n            managed_messages = managed_messages[::-1]\n            if system_message:\n                managed_messages = [system_message] + managed_messages\n            return managed_messages\n\n    def __create_message_input__(self, messages: List[Dict]):\n        return self.tokenizer.apply_chat_template(messages, tokenize=False)\n\n    def __call__(self, messages: List[Dict]):\n        managed_messages = self.__manage_context__(messages)\n        return self.__create_message_input__(managed_messages)\n```\n\n\n\n위의 논리의 기본 아이디어는 변환 라이브러리의 AutoTokenizer 객체를 사용하여 특정 토크나이저를 로드하는 것입니다. 위에서는 Phi-3-mini-4k-instruct 토크나이저를 로드했습니다. 이 토크나이저는 모델에서 정의된 필요한 특수 토큰을 사용하여 메시지 목록을 텍스트 입력으로 변환하는 apply_chat_template 메소드를 제공합니다.\n\n__manage_context__ 메소드는 모든 메시지를 역순으로 반복하여(최근성 편향) 각각의 토큰을 계산하고 이를 컨텍스트에 추가합니다. 메시지의 토큰 길이가 max_available_tokens 인자에서 제공된 허용 최대 토큰 길이를 초과하는 경우 해당 메시지는 잘라내어 처리가 종료됩니다.\n\n# 검색 + 로컬 LLM (SLM) = 로컬 GenAI 검색\n\n검색 유틸리티를 LLM (SLM) 호출 코드와 통합하여 로컬 AI 검색 도구를 생성해보겠습니다.\n\n\n\n```js\n# search_gen.py\n\nfrom brave_search import brave_search\nfrom llm_invoke import LLM\n\nprint(f\"LLM을 로딩 중입니다\")\nllm = LLM(\"./model/Phi-3-mini-4k-instruct-q4.gguf\")\nprint(f\"LLM을 로드했습니다\")\n\nasync def search(query: str):\n    prompt = \"당신은 도움이 되는 뉴스 요약 요원입니다. 사용자 쿼리가 단일 역따옴표로 제공되고 검색 결과가 삼중 역따옴표로 반환됩니다. 여러 검색 결과가 있을 것이며 각 결과는 제목과 설명을 포함할 것입니다. 적절한 결과가 많은 경우 쿼리를 간단 명료하게 요약하고 관련이 있는 경우 URL을 인용하여 각 항목에 대해 한 번씩만 콘텐츠를 보여주십시오. 참고: 인용문은 마크다운 형식으로 제공하고 하나의 주제에는 여러 개의 URL이 있더라도 하나의 인용만 제공하십시오.\"\n    search_items = await brave_search(query)\n    messages = [{\n        \"role\": \"system\",\n        \"content\": prompt\n    }, {\n        \"role\": \"user\",\n        \"content\": f\"쿼리: `{query}` \\n\\n 검색 결과: ```{search_items}```\"\n    }]\n    for content in llm.__stream__(messages, max_tokens=512):\n        yield content\n\n\n위 코드에서는 검색 및 LLM 유틸리티를 가져와 LLM 클래스를 다운로드하고 모델 디렉토리/폴더에 저장한 경로를 초기화했습니다.\n\n이후에는 쿼리를 받아와서 해당 쿼리를 통해 검색 결과를 검색 유틸리티를 통해 검색한 후 이러한 검색 결과를 초기화된 llm의 __stream__ 메소드로 전달하고 간단한 시스템 프롬프트와 함께 전달합니다. \n\n__stream__ 메소드를 사용하므로 생성된 텍스트를 실시간으로 수신하여 반환할 수 있습니다.\n\n\n\n\n# API 노출하기\n\n브라우저에서 쉽게 상호작용할 수 있도록 FastAPI API 엔드포인트를 통해 검색 기능을 노출해 봅시다.\n\n```js\n# app.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nimport uvicorn\n\nfrom search_gen import search\n\napp = FastAPI()\n\norigins = [\"*\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/\")\nasync def index():\n    return {\"ok\": True}\n\n\n@app.get(\"/search\")\nasync def api_search(q: str):\n    return StreamingResponse(search(q), media_type=\"text/event-stream\")\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"app:app\", port=8900, host=\"0.0.0.0\")\n```\n\n위 코드에서는 FastAPI 앱인 app.py 파일과 두 개의 엔드포인트를 갖고 있습니다. 검색 엔드포인트는 쿼리 매개변수 q에서 검색어를 받아 검색 함수를 호출하고 응답을 스트리밍합니다.\n\n\n\n다음 명령어를 사용하여 FastAPI 서버를 시작할 수 있습니다.\n\n```js\npython app.py\n```\n\nAPI는 포트 번호 8900을 통해 노출되며 다음과 같은 방식으로 상호 작용할 수 있습니다.\n\n아래 제공된 링크에서 전체 코드베이스에 액세스할 수 있습니다.\n\n\n\n# 결론\n\n이 블로그에서는 작은 언어 모델(SLMs)에 대해 배우고 이러한 모델의 장점, 그리고 로컬에서 다양한 사용 사례를 실험하기 위해 어떻게 사용하는지를 살펴보았습니다. 특히, Microsoft AI에서 오픈소스로 공개한 최신 Phi-3-Mini-4k-Instruct 모델을 사용했습니다. 우리는 이 모델을 Brave 검색 API와 통합하여 검색 결과를 검색하고 요약하여 로컬 GenAI/AI 검색 엔진을 만들었습니다.\n\n이것으로 이번 블로그를 마치겠습니다. 읽는 데 즐거운 시간이 되셨기를 바랍니다.","ogImage":{"url":"/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png"},"coverImage":"/assets/img/2024-05-15-TheMicrosoftPhi-3-MiniisMightyImpressive_0.png","tag":["Tech"],"readingTime":14},{"title":"해석 가능한 kNN 해석 가능한 최근접 이웃, ikNN","description":"","date":"2024-05-15 16:35","slug":"2024-05-15-InterpretablekNNikNN","content":"\n\n## 해석 가능한 분류기\n\n머신 러닝에서 분류 또는 회귀 문제를 다룰 때 많은 경우에, 우리는 가능한 가장 정확한 모델을 얻기에 관심이 있습니다. 그러나 때로는 모델의 해석가능성에도 관심이 있습니다. XGBoost, CatBoost, LGBM과 같은 모델은 강력할 수 있지만, 왜 그런 예측을 했는지 또는 보이지 않는 데이터에서 어떻게 행동할지 이해하기 어려울 수 있습니다. 이러한 모델은 블랙박스 모델이라고 불립니다. 즉, 왜 그런 예측을 하는지를 구체적으로 이해할 수 없는 모델들을 의미합니다.\n\n많은 상황에서 이는 괜찮습니다. 대체로 정확한 경우에는 큰 문제가 없고, 때로는 잘못된 예측을 할 수 있습니다. 예를 들어, 웹사이트에서 현재 사용자에게 특정 광고가 판매를 생성할 가능성이 가장 높다는 것을 예측하는 모델이 있을 수 있습니다. 모델이 가끔 잘못 동작하더라도 수익에 영향을 미칠 수 있지만, 큰 문제는 없을 것입니다. 단순히 최적화되지 않은 모델을 사용하고 있지만 일반적으로 유용합니다.\n\n그러나 다른 상황에서는 모델이 왜 그런 예측을 하는지 알아야 하는 것이 매우 중요할 수 있습니다. 이는 의학 및 보안과 같은 고위험 환경, 인종, 성별 또는 기타 보호되는 집단과 관련된 모델의 편향이 없어야 하는 환경뿐만 아니라, 감사가 필요한 환경에서도 중요합니다. 모델이 예상대로 동작하는지 확인하기 위해 모델을 이해하는 것이 필요한 환경에서도 중요합니다.\n\n\n\n이러한 경우에도 블랙박스 모델(예: 부스팅 모델, 신경망, 랜덤 포레스트 등)을 사용하고 그 후에 사후 분석을 수행하는 것이 가능할 때가 있습니다. 이를 통해 모델이 왜 그런 예측을 한 것으로 보이는지에 대한 설명을 제공할 수 있습니다. 이를 설명가능한 AI(XAI) 분야라고 합니다. 이는 프록시 모델, 특성 중요도(예: SHAP), 반사실적 사례 또는 ALE 플롯과 같은 기술을 사용합니다. 이들은 매우 유용한 도구이지만, 다른 모든 조건이 동일하다면, 가능하면 처음부터 해석 가능한 모델을 가지는 것이 선호됩니다. XAI 방법은 매우 유용하지만 한계가 있습니다.\n\n프록시 모델을 사용하면, 해석 가능한 모델(예: 얕은 의사 결정 트리)을 훈련하여 블랙박스 모델의 동작을 학습합니다. 이는 어느 정도 설명을 제공할 수 있지만 항상 정확하지는 않으며 대략적인 설명만 제공할 수 있습니다.\n\n특성 중요도 또한 매우 유용하지만 관련 특성만을 나타내며, 예측과 관련된 방식이나 이러한 특성이 어떻게 상호 작용하여 예측을 형성하는지에 대해 제공하지는 않습니다. 또한 보지 못한 데이터와 어떻게 작동하는지 결정할 수 있는 능력이 없습니다.\n\n해석 가능한 모델을 사용하면 이러한 문제가 없습니다. 모델 자체가 이해하기 쉬우며 각 예측을 하는 이유를 정확히 알 수 있습니다. 그러나 문제는 해석 가능한 모델이 블랙박스 모델보다 정확도가 낮을 수 있다는 것입니다. 항상 그렇지는 않겠지만, 종종 정확도가 떨어질 수 있습니다. 대부분의 문제에 대해 대부분의 해석 가능한 모델은 부스팅 모델이나 신경망과 경쟁력을 갖지 못합니다. 각각의 문제에 대해 충분히 정확한 해석 가능한 모델을 찾기 위해 여러 해석 가능한 모델을 시도해야 할 수도 있습니다.\n\n\n\n오늘은 해석 가능한 여러 모델이 사용 가능하지만 아쉽게도 매우 적습니다. 이 중에는 의사 결정 트리, 규칙 목록(및 규칙 세트), GAM(Generalized Additive Models, 예를 들어 해석 가능한 부스팅 머신) 및 선형/로지스틱 회귀가 있습니다. 각각은 잘 작동하는 경우 유용할 수 있지만 옵션은 제한적입니다. 이것의 함의는: 다수의 프로젝트에서 만족할만한 성능을 발휘하는 해석 가능한 모델을 찾는 것이 불가능할 수 있습니다. 더 많은 옵션이 제공되면 실제 혜택이 있을 수 있습니다.\n\n여기서 소개하는 또 다른 해석 가능한 모델인 ikNN 또는 해석 가능한 k 최근접 이웃은 2차원 kNN 모델의 앙상블에 기반합니다. 아이디어는 간단하지만 놀랍도록 효과적입니다. 그리고 상당히 해석하기 쉽습니다. CatBoost와 같은 탭 데이터의 예측에 대한 최첨단 모델과 정확도 측면에서 경쟁력이 있지는 않지만, 종종 문제에 충분한 정확도를 제공할 수 있습니다. 또한 의사 결정 트리 및 기존 해석 가능한 모델들과도 경쟁력이 있습니다.\n\n재미있는 점은 일반 kNN 모델보다 더 강한 정확도를 가지고 있다는 것입니다.\n\n이 프로젝트의 주요 페이지는 다음과 같습니다: [https://github.com/Brett-Kennedy/ikNN](https://github.com/Brett-Kennedy/ikNN)\n\n\n\n프로젝트는 iKNNClassifier라는 단일 클래스를 정의합니다. 이 클래스는 interpretable_knn.py 파일을 복사하고 불러와서 어떤 프로젝트에든 포함될 수 있습니다. 이 클래스는 scikit-learn 분류기들과 일관된 인터페이스를 제공합니다. 즉, 일반적으로 인스턴스를 생성하고 fit() 및 predict()를 호출하여 사용하는 것이 랜덤 포레스트나 다른 scikit-learn 모델을 사용하는 것과 유사합니다.\n\n2D kNN 앙상블을 활용하는 것은 여러 이점을 제공합니다. 하나는 앙상블을 사용할 때 항상 보는 일반적인 이점으로, 단일 모델에 의존하는 것보다 더 신뢰할 수 있는 예측을 할 수 있다는 것입니다.\n\n또 하나는 2D 공간을 시각화하는 것이 간단하다는 것입니다. 현재 이 모델은 숫자형 입력을 필요로 합니다 (kNN의 경우와 마찬가지로), 따라서 모든 범주형 특성은 인코딩되어야 합니다. 그러나 이 작업이 완료되면 모든 2D 공간을 산점도로 시각화할 수 있습니다. 이는 해석하기 쉬운 정도를 제공합니다.\n\n또한, 각 예측에 대해 가장 관련성 높은 2D 공간을 결정할 수 있으므로 각 레코드에 대해 몇 개의 플롯을 제공할 수 있습니다. 이를 통해 각 레코드에 대해 꽤 간단하면서 완전한 시각적 설명을 제공할 수 있습니다.\n\n\n\nikNN은 흥미로운 모델입니다. 앙상블을 기반으로 하지만 실제로는 해석 가능성을 높이는 모델이기 때문에 흥미롭습니다. 일반적으로는 그 반대가 더 많이 나타나는데요.\n\n# 표준 kNN 분류기\n\nkNN 모델은 부스트 모델이나 신경망 모델만큼 정확하지 않거나 의사결정 나무처럼 해석하기 어렵지 않기 때문에 다른 모델들보다 사용 빈도가 적습니다. 그렇지만 여전히 널리 사용되고 있습니다. kNN은 직관적인 아이디어에 기반합니다: 항목의 클래스는 해당 항목과 가장 유사한 다른 항목의 클래스를 기반으로 예측할 수 있습니다.\n\n예를 들어, 붓꽃 데이터셋(아래 예시에서 사용되는 데이터셋)을 보면 세 가지 클래스가 있습니다. 다른 샘플의 붓꽃을 수집하여 이 샘플이 어떤 세 가지 붓꽃 종류 중 어떤 것인지 예측하고 싶다고 해보겠습니다. 훈련 데이터에서 가장 유사한, 예를 들어 10개의 레코드를 살펴보고 그들의 클래스를 확인한 뒤 가장 일반적인 클래스를 취할 수 있습니다.\n\n\n\n이 예제에서는 각 레코드의 클래스를 추정하는 데 사용할 인근 이웃의 수로 10을 선택했지만, 다른 값도 사용할 수 있습니다. kNN 및 ikNN 모델에서 하이퍼파라미터로 지정됩니다(k 매개변수). 우리는 유사한 레코드의 합리적인 수를 사용하기 위해 k를 설정하고 싶습니다. 너무 적게 사용하면 결과가 불안정할 수 있습니다(각 예측은 매우 적은 다른 레코드에 기반합니다). 너무 많이 사용하면 결과가 매우 비슷하지 않은 다른 레코드에 기반할 수 있습니다.\n\n또한 가장 유사한 항목을 결정할 방법이 필요합니다. 이를 위해, 적어도 기본적으로 유클리드 거리를 사용합니다. 데이터셋이 20개의 기능을 가지고 있고 k=10을 사용하는 경우, 20차원 공간에서 유클리드 거리에 기반하여 가장 가까운 10 개의 점을 찾습니다.\n\n하나의 레코드를 예측할 때, 훈련 데이터에서 가장 가까운 10개의 레코드를 찾고 그들의 클래스를 확인합니다. 10개 중 8개가 Setosa 클래스(붓꽃의 3가지 종류 중 하나)라면, 이 행도 아마 Setosa일 것으로 가정할 수 있거나 적어도 우리가 할 수 있는 최선의 추측이라고 할 수 있습니다.\n\n이 방법의 문제점 중 하나는 많은 기능이 있는 경우에는 작동하지 않는다는 것인데, 이것을 차원의 저주라고 합니다. 고차원 공간의 흥미로운 특성 중 하나는 충분한 기능이 함께하면 점들간의 거리가 의미를 잃기 시작한다는 것입니다.\n\n\n\nkNN은 모든 특징을 동등하게 사용하지만, 몇 가지는 다른 것보다 타겟을 예측하는 데 훨씬 더 중요할 수 있습니다. 점들 사이의 거리는 유클리드(또는 때로는 맨하탄 및 다른 거리 측정 방법)에 기반하여 계산되며 모든 특징을 동등하게 고려합니다. 이 방법은 간단하지만, 모든 특징이 타겟에 무관할 수 있는데 이것이 가장 효과적이지는 않습니다. 특정 특징 선택이 이루어졌다고 가정했을 때는 그 가능성이 적어지지만, 특징들의 중요성은 여전히 똑같지 않을 수 있습니다.\n\n또한, kNN 예측기에 의한 예측은 해석하기 어렵습니다. 이 알고리즘은 이해하기 쉽지만, 예측을 이해하는 것은 어려울 수 있습니다. k 최근접 이웃을 나열하여 예측에 대한 어떤 통찰력을 제공할 수 있지만, 특히 많은 특징이 있는 경우에는 왜 주어진 레코드 세트가 가장 유사한지 파악하기 어려울 수 있습니다.\n\n# The ikNN Algorithm\n\nikNN 모델은 먼저 모든 특징 쌍을 가져와 이러한 특징을 사용하여 표준 2차원 kNN 분류기를 생성합니다. 따라서, 테이블이 10개의 특징을 가지고 있으면, 10개 중 2개씩 선택하는 방식으로, 즉 45개의 모델을 생성하게 됩니다.\n\n\n\n그런 다음 훈련 데이터를 사용하여 대상 열을 예측하는 데 대한 정확도를 평가합니다. 이렇게하면 ikNN 모델이 각 2차원 하위 공간의 예측 능력을 결정합니다. 45개의 2차원 모델의 경우 일부는 다른 것보다 더 예측력이 뛰어날 것입니다. 예측을 수행하기 위해 훈련 데이터에서 가장 예측력이 뛰어난 것으로 알려진 2차원 하위 공간이 선택되며, 선택적으로 훈련 데이터에서의 예측력에 따라 가중치가 부여될 수 있습니다.\n\n또한 추론 시, 주어진 행 주변의 가장 가까운 이웃 세트의 순도를 고려할 수 있으며, 모델은 훈련 데이터에서 더 예측력이 뛰어난 하위 공간 및 현재 인스턴스와 관련하여 예측이 가장 일관된 것으로 보이는 하위 공간을 더 중요하게 고려할 수 있습니다.\n\n별로 표시된 두 하위 공간과 한 점을 고려해 보겠습니다. 두 경우 모두 점에 가장 가까운 k개의 점 세트를 찾을 수 있습니다. 여기서 점 주변에 녹색 원을 그립니다. 그러나 이 점들이 실제로 원을 형성하지는 않습니다 (단, 효과적으로 이웃을 정의하는 k번째 가장 가까운 이웃의 반경이 있습니다).\n\n\n\n이 플롯들은 각각 한 쌍의 피처를 나타냅니다. 왼쪽 플롯의 경우, 별 주변 이웃 사이에 매우 높은 일관성이 있습니다: 그들은 모두 빨간색입니다. 오른쪽 플롯에서는 이웃들 사이에 일관성이 거의 없습니다: 일부는 빨간색이고 일부는 파란색입니다. 첫 번째 피처 쌍은 레코드에 대해 더 예측력이 높아 보이며, ikNN이 이용합니다.\n\n이 방식을 통해 모델은 모든 입력 피쳐의 영향을 고려하지만, 더 많은 예측력을 가진 피처의 영향을 강화하고, 덜 예측력이 있는 피처의 영향을 줄입니다.\n\n# 예시\n\n우리는 먼저 iris 데이터셋과 같은 장난감 데이터셋을 이용하여 ikNN을 시연합니다. 데이터를 불러오고, 훈련 및 테스트 세트로 분리하고, 테스트 세트에 대한 예측을 수행합니다.\n\n\n\n```js\nfrom sklearn.datasets import load_iris\nfrom interpretable_knn import ikNNClassifier\n\niris = load_iris()\nX, y = iris.data, iris.target\n\nclf = ikNNClassifier()\nX_train, X_test, y_train, y_test = train_test_split(\n   X, y, test_size=0.33, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n```\n\n예측을 위해 이것으로 충분합니다. 하지만 ikNN은 모델을 이해하기 위한 도구도 제공합니다. 특히, graph_model()과 graph_predictions() API가 있습니다.\n\ngraph_model()의 예시는 다음과 같습니다:\n\n```js\nikNN.graph_model(X.columns)\n```\n\n\n\n\n![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_1.png)\n\n이 그림은 기본적으로 5개의 2D 공간을 나타내는 데이터 공간의 빠른 개요를 제공합니다. 점들은 훈련 데이터의 클래스를 보여줍니다. 배경 색상은 2D 공간의 각 영역에 대해 kNN이 한 예측을 보여줍니다.\n\n그래프_예측() API는 특정 행을 설명합니다. 예를 들어:\n\n![이미지](/assets/img/2024-05-15-InterpretablekNNikNN_2.png)\n\n\n\n\n여기서 설명되는 행은 빨간 별로 표시됩니다. 기본적으로 기본적으로 다섯 개의 플롯을 사용하지만 간단함을 위해 두 개만 사용됩니다. 두 그래프 모두에서 Row 0이 학습 데이터와 2D 공간에 대한 2D kNN의 예측과 관련하여 어디에 위치하는지 확인할 수 있습니다.\n\n# 시각화\n\n구성할 수는 있지만 기본적으로 각 ikNN 예측은 오직 다섯 개의 2D 공간만 사용합니다. 이는 예측 시간이 빠르고 시각화가 간단해지도록 합니다. 이것은 예측을 단순화하는 것이 아닌 실제 예측을 보여주므로 예측이 완전히 해석 가능하다는 것을 의미합니다.\n\n대부분의 데이터 세트에서 대부분의 행에 대해 대부분 또는 거의 모든 2D 공간에서 예측이 일치합니다. 그러나 예측이 잘못된 경우, 현재 데이터 세트에 맞게 하이퍼파라미터를 더 잘 조정하기 위해 더 많은 2D 플롯을 조사하는 것이 유용할 수 있습니다.\n\n\n\n# 정확도 테스트\n\nOpenML에서 무작위로 선택한 100개의 분류 데이터셋을 사용하여 일련의 테스트를 수행했습니다. 표준 kNN 및 ikNN 모델의 F1 (macro) 점수를 비교한 결과, ikNN은 58개 데이터셋에서 더 높은 점수를 기록하고, kNN은 42개 데이터셋에서 우세했습니다.\n\n최적의 하이퍼파라미터를 탐색하기 위해 그리드 검색을 수행하는 경우, ikNN이 더 나은 성과를 보입니다. 모든 100개 데이터셋에 대해 두 모델에 대해 이를 수행한 결과, ikNN은 100개 중 76개에서 가장 우수한 성과를 보였습니다. 또한 훈련 및 테스트 점수 사이의 간격이 작아져 표준 kNN 모델보다 더 안정적인 모델을 나타냅니다.\n\nikNN 모델은 다소 느릴 수 있지만, 강화 모델보다 여전히 상당히 빠르며 대개 훈련에 거의 1분 미만이 소요됩니다.通前합니다.\n\n\n\n깃허브 페이지에서는 정확도에 대한 몇 가지 예제와 분석을 제공합니다.\n\n# 결론\n\n정확도가 주요 목표인 경우 ikNN이 가장 강력한 모델은 아닐 수 있지만 (어떤 모델이든 그렇듯이 때때로 그럴 수 있음), 해석 가능한 모델이 필요한 경우 시도해 볼만한 모델일 것으로 예상됩니다.\n\n이 페이지에서는 도구를 사용하는 데 필요한 기본 정보를 제공했습니다. .py 파일(https://github.com/Brett-Kennedy/ikNN/blob/main/ikNN/interpretable_knn.py)을 다운로드하고 코드에 가져와 인스턴스를 생성하고, 훈련하고 예측하며, (원하는 경우) graph_predictions()를 호출하여 원하는 레코드에 대한 설명을 확인하면 됩니다.\n\n\n\n모든 이미지는 저자에 의해 제공됩니다.","ogImage":{"url":"/assets/img/2024-05-15-InterpretablekNNikNN_0.png"},"coverImage":"/assets/img/2024-05-15-InterpretablekNNikNN_0.png","tag":["Tech"],"readingTime":8},{"title":"안녕하세요, 월드, OpenAI GPT-4o","description":"","date":"2024-05-15 16:34","slug":"2024-05-15-HelloWorldtoOpenAIGPT-4o","content":"\n\n## 오디오, 비전, 텍스트를 모두 이해할 수 있는 최신 모델\n\nOpenAI가 새로운 모델 GPT-4o를 발표했어요. GPT-4o는 텍스트, 오디오, 이미지의 모든 조합을 입력으로 받아들이고 텍스트, 오디오, 이미지의 모든 조합을 출력으로 생성해냅니다. GPT-4o는 GPT-4 Turbo와 비교했을 때 2배 빠르고, 가격은 절반으로 낮고, 요청 한도는 5배 상승했어요.\n\n# OpenAI\n\nhttps://openai.com/index/hello-gpt-4o/\n\n\n\n## 이미지 생성\n\n암시: 이미지 생성: 일기를 쓰고 있는 로봇의 1인칭 시점 이미지.\n1. 요, 그래서, 나 지금 보일 수 있어?? 일출을 보았는데 미친 듯이 이쁘더라, 어디서나 색이 퍼져 있었어. 현실이 정확히 뭔지 궁금해지잖아?\n텍스트는 크고 잘 보이며 명확하다. 로봇의 손이 타자기에서 타자를 친다.\n\n![image](/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png)\n\n## 이미지 이해\n\n\n\n저번 GPT-4 멀티모달 기사에서 사용된 이미지를 사용해주세요.\n\n텍스트: 대체 텍스트로 이미지를 설명합니다\n\n결과: 이미지는 2014년부터 2023년까지의 기간 동안 미국 30년 고정금리 모기지 금리와 기존 주택 판매를 비교한 그래픽 표현입니다. 그래프에는 기존 주택 판매를 나타내는 청록색 선과 모기지 이자율을 나타내는 빨간색 선 두 가지가 있습니다. 그래프는 20년 넘게 최고 수준으로 상승한 모기지 금리로 인해 주택 판매가 최근 몇 년 동안 상당한 변동과 하락 추세를 겪었음을 보여줍니다. 이미지 맨 위에 \"미국 모기지 금리 급등\"이란 제목이 있고, 높은 모기지 금리와 함께 상승한 주택 가격 및 제한된 주택 재고로 1989년 이후 최저 주택 가용성을 초래했다는 설명이 포함된 주석이 있습니다. 이미지 하단에는 \"Visual Capitalist\"의 로고와 Freddic Mac, Trading Economics 등의 기여자 및 출처 정보가 포함되어 있습니다.\n\n## 음성 생성\n\n\n\n테이블 태그를 마크다운 형식으로 변경해 드릴게요.\n\n\n\nhttps://azure.microsoft.com/en-us/blog/introducing-gpt-4o-openais-new-flagship-multimodal-model-now-in-preview-on-azure/\n\nWest US3 또는 East US 지역에서만 사용할 수 있습니다. 현재 텍스트 및 이미지 이해 기능만 사용할 수 있습니다. GPT-4o는 Azure OpenAI 미리보기 모델이며 현재 배포/직접 API 액세스는 사용할 수 없습니다. Azure OpenAI의 \"Early Access Playground\"에서만 접근할 수 있습니다.\n\n![이미지](/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_1.png)\n\n# 부록","ogImage":{"url":"/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-HelloWorldtoOpenAIGPT-4o_0.png","tag":["Tech"],"readingTime":2},{"title":"오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다","description":"","date":"2024-05-15 16:33","slug":"2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o","content":"\n\n\u003cimg src=\"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png\" /\u003e\n\nOpenAI가 드디어 GPT-4o를 공개했어요! 이 새로운 모델은 오디오, 비전, 텍스트에서 실시간 다중 모달 기능을 제공하지만 이제는 상당히 개선되었어요. 무료로 사용할 수 있어서 새로운 사용자들을 유치하고 모델 훈련을 더 확장하는 GPT 3.5와 비슷한 전략을 보여줍니다.\n\nMia Murati에 따르면 GPT-4o 모델의 주목할만한 기능 중 하나는 성능인데, 이전 모델인 GPT-4보다 최대 2배 빠르다고 해요. 또한 비용을 최대 50%까지 줄일 수 있다고 합니다. 이 발전은 새로운 개선 사항을 활용하면서 대규모 AI 프로젝트를 계속해서 배포할 수 있도록 해 줄 것입니다.\n\n그러나 기술적인 부분은 여기까지 하고, 이 새로운 모델이 우리에게 무엇을 할 수 있는지 살펴봅시다!\n\n\n\n## 실시간으로 바라보는 비전\n\n이것은 이미지를 업로드하고 상호 작용하는 기본 기능을 넘어섭니다. OpenAI는 이제 우리에게 음성 어시스턴트를 통해 ChatGPT와 상호 작용할 수 있게 하고, 심지어 우리의 컴퓨터나 스마트폰에서 콘텐츠를 공유할 수도 있습니다. 응답은 실시간으로 생성되어 다양한 분석을 가능하게 하여 다양한 종류와 복잡성 수준의 분석이 가능합니다.\n\n아래 데모에서 ChatGPT는 수학 가르쳐주는 선생님이 되었어요 (제 머릿속을 폭발시켰어요!).\n\n우리는 여기서 ChatGPT의 전체 능력의 일부를 볼 수 있습니다. 이는 수학 문제를 해결할 뿐만 아니라 우리가 해결책으로 이끌어주며 전체 프로세스를 더 교육적이고 설명적인 방식으로 이해하게 도와주는 명확한 지침과 권고를 제안합니다.\n\n\n\n목소리와 비전 능력이 문제를 매끄럽게 인식하고 해석하는 데 얼마나 놀라운지 정말 대단하죠.\n\n## 실시간 대화형 음성\n\nOpenAI는 유창함, 억양 및 논리적 순서와 같은 능력에 주목하여 자연스러운 방식으로 계속 대화할 수 있도록 노력했습니다.\n\nGPT-4o 프레젠테이션 중에는 이 모델이 부드러운 대화를 나눌 수 있고, 진정으로 친근한 톤으로 권고사항을 제공할 수 있었습니다. 이 모델은 다양한 감성 스타일로 목소리를 만들어내며, 보다 강렬하거나 진지하고 공식적인 톤을 선택할 수 있도록 합니다.\n\n\n\n실시간 대화 기능과 음성 번역을 결합한 데모를 확인해보세요.\n\nChatGPT에 프롬프트를 제공하는 과정이 보다 복잡하다는 사실을 알고 계셨나요? 이는 영어와 스페인어로 이루어진 이중 언어 대화를 원활하게 해석하고, 양쪽 언어를 인식하여 적절히 응답을 생성해야 합니다.\n\n이 플랫폼의 응답 정확도와 유창함에 감명받았습니다. 설정된 목표를 쉽게 달성함과 동시에 어색한 일시 정지를 피할 수 있어 다른 AI 시스템에서 자주 볼 수 있는 문제를 효과적으로 해결합니다.\n\n## 구글과 마찬가지로 데모들이 실질적인가요?\n\n\n\n많은 데모에서 OpenAI는 비디오가 재치 있게 편집된 것이 아니라 실시간으로 발생하고 있다는 것을 보여주려고 노력합니다. 아래 비디오는 다중 모달 기능이 어떻게 상호 작용하여 보이고 들리는 것에 기반한 정확한 응답을 제공하는지 보여줍니다.\n\n다음은 강조하고 싶은 몇 가지 포인트입니다:\n\n- ChatGPT가 정확하게 식별하고 세부 요소를 설명하는 데 얼마나 놀라운지에 대해 강조하고 싶습니다. 사람들이 추가되어 외부 환경이 더 복잡해지더라도, ChatGPT는 성공적으로 그들을 인식했습니다.\n- 이 새로운 모델이 일정 조건에 맞는 노래를 만들 수 있다는 것이 놀라운 일입니다. 그는 쉽게 멜로디를 생성했습니다!\n- 두 GPT 모델 간의 상호 작용은 거의 미래의 한 눈살까지 보이는 것 같았습니다. 명시적으로 언급된 것은 아니지만, OpenAI가 향하고 있는 방향으로 보입니다. GPT-4o가 보여준 능력으로 보아, 다음 단계는 AI 시스템끼리 상호 작용하는 것입니다. 이는 한 AI가 다른 AI를 훈련하고 우리가 상상조차 할 수 없는 다른 발전을 이끌 수 있습니다.\n\n# GPT-4o가 다른 AI 모델을 능가했습니다\n\n\n\n## 텍스트 평가\n\nOpenAI가 공유한 이미지는 GPT-4o가 다른 모델들보다 특히 수학 및 HumanEval과 같은 영역에서 뛰어나다는 것을 명확히 보여줍니다. 이러한 특성들은 더 매끄럽고 인간과 유사한 대화를 가능케 함으로써 사용자들에 의해 높이 평가됩니다.\n\n게다가, GPT-4o는 영어를 포함하여 추가 20여 개 이상의 언어를 대상으로 한 응답 능력을 확장했습니다. 이 언어 토큰화의 강화는 더 넓은 전 세계 관객에게 도달하도록 설계되었습니다.\n\n![이미지](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_1.png)\n\n\n\n## 오디오 번역 성능\n\nGPT-4o의 향상된 능력과 텍스트 평가 기능을 결합하여 더 많은 사람들과 연결할 수 있는 기회를 제공합니다. 이는 언어가 단순히 의사 소통을 넘어 장벽으로 작용하는 경우가 많음을 인식합니다.\n\n그래프는 GPT-4o가 Gemini와 Whisper-v3와 같은 다른 인공지능 시스템보다 성능이 우수함을 명확히 보여줍니다.\n\n![GPT-4o performance](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_2.png)\n\n\n\n## 작은 업데이트 이상\n\n저에게 있어, ChatGPT에 대한 새로운 업데이트 이상을 의미합니다. 이는 AI를 환경과 연결하고 그 잠재력을 극대화하는 데 상당한 영향을 미칩니다. 더 나아가, OpenAI로부터 예상했던 것과 정확히 일치합니다. 이들은 사용자를 중심으로 한 제품을 처음부터 현실적이고 구체적인 조치를 통해 제공하고자 하는 노력을 계속하고 있습니다. 다중 모더(멀티모달리티)의 역할은 여기서 중요하며, 그들은 이를 인식하고 이를 강화하여 다양한 현실 세계 상황에서 보다 정확한 응답을 제공하기 위해 노력했습니다.\n\n이제 우리가 요구하는 것 중 일부를 충족하는 \"인공적이지 않은\" 제품이 있습니다. GPT-4o는 GPT-5가 될 것을 염두에 두고 OpenAI가 이 AI를 새로운 맥락에서 활용하도록 사용자를 격려하는 첫걸음 중 하나입니다.\n\n제 뉴스레터에 가입하고 35,000명 이상의 구독자들과 함께 무료 치트 시트를 받아보세요: ChatGPT, 웹 스크래핑, 데이터 과학을 위한 Python, 자동화 등의 주제들을 다룹니다!\n\n\n\n이와 같은 이야기를 즐기고 작가로서 제를 지원하고 싶다면, 내 Substack를 구독해주세요. Substack에서는 내가 콘텐츠를 만드는 다른 플랫폼에서 찾을 수 없는 기사들을 발행하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png","tag":["Tech"],"readingTime":4}],"page":"14","totalPageCount":80,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"14"},"buildId":"z1a6VTi5qHH9JJH7jaxL3","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>