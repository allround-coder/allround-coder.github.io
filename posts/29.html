<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>allround-coder</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///posts/29" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="allround-coder" data-gatsby-head="true"/><meta property="og:title" content="allround-coder" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///posts/29" data-gatsby-head="true"/><meta name="twitter:title" content="allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/960f1fe994a0ab5c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/960f1fe994a0ab5c.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-b692b09f2b5275a4.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/6w6Yg3qJxLtqeXNguENru/_buildManifest.js" defer=""></script><script src="/_next/static/6w6Yg3qJxLtqeXNguENru/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="비정상적인 현상과 놀라운 Midjourney 이미지" href="/post/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="비정상적인 현상과 놀라운 Midjourney 이미지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="비정상적인 현상과 놀라운 Midjourney 이미지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">비정상적인 현상과 놀라운 Midjourney 이미지</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="토큰화의 기술 자연어 처리를 위한 필수 기법" href="/post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="토큰화의 기술 자연어 처리를 위한 필수 기법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="토큰화의 기술 자연어 처리를 위한 필수 기법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">토큰화의 기술 자연어 처리를 위한 필수 기법</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="전세계적으로 공개된 클라우드 없는 이미지 아카이브에 대한 접근이 공개되었습니다" href="/post/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="전세계적으로 공개된 클라우드 없는 이미지 아카이브에 대한 접근이 공개되었습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="전세계적으로 공개된 클라우드 없는 이미지 아카이브에 대한 접근이 공개되었습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">전세계적으로 공개된 클라우드 없는 이미지 아카이브에 대한 접근이 공개되었습니다</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델 평가 개발자를 위한 안내" href="/post/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델 평가 개발자를 위한 안내" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델 평가 개발자를 위한 안내" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">대형 언어 모델 평가 개발자를 위한 안내</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="GPT-4o의 음성 모드 분석" href="/post/2024-05-16-AnAnalysisofVoiceModeinGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="GPT-4o의 음성 모드 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-AnAnalysisofVoiceModeinGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="GPT-4o의 음성 모드 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">GPT-4o의 음성 모드 분석</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요" href="/post/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정" href="/post/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기" href="/post/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="유혹적인 입술을 위한 DIY 설탕 입술 스크럽" href="/post/2024-05-16-DIYSugarLipScrubforLusciousLips"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="유혹적인 입술을 위한 DIY 설탕 입술 스크럽" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="유혹적인 입술을 위한 DIY 설탕 입술 스크럽" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">유혹적인 입술을 위한 DIY 설탕 입술 스크럽</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기" href="/post/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link posts_-active__YVJEi" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"비정상적인 현상과 놀라운 Midjourney 이미지","description":"","date":"2024-05-16 04:24","slug":"2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages","content":"\n\n내 글 중 한 가지라도 읽은 적이 있다면, 자연 현상에 대한 내 감동과 그것을 흥미로운 이미지로 만드는 방법을 늘 찾는 것을 알고 계실 것입니다.\n\n이미 Midjourney에 대한 영감으로 수학 용어와 유체 역학을 활용하는 방법에 대해 썼습니다. 또한 한 단어로 된 프롬프트에 대해 쓰고, 창작을 더 활기차게 하는 새로운 단어를 고안한 적도 있죠.\n\n좋은 점은 시각적으로 영감을 주는 단어들이 많이 있고 이보다 더 많은 이상하고 아름다운 복잡한 자연 현상들이 존재한다는 것입니다. 그리고 이 모든 것들이 Midjourney와 함께 활용될 수 있다는 점이지요.\n\n모든 곳에서 그것들을 찾아봅니다. 많은 과학 기사를 읽고, 그곳에는 흥미진진한 현상을 설명하는 내용이 가득합니다. 녹색 백과사전과 리더스 다이제스트를 넘겨 보는 것을 좋아하고, 시각적인 내용을 설명하는 흥미로운 단어로 가득 찬 사전들을 쌓아 놓고 있습니다.\n\n\n\n\n![2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_0.png](/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_0.png)\n\n해당 단어들, 흥미로운 현상들을 발견하면, 시간을 낭비하지 않고 Midjourney에서 잘 활용해요.\n\n때문에 이전부터 이미 5000개 이상의 이미지를 생성하는 클럽에 참여하게 되었고, 빠른 시간을 보내기에 항상 부족한 이유가 있어요. 하지만 그 모두가 그만한 가치가 있죠.\n\n이 글에서는 최근 발견한 15개 용어와 단어를 공유하려고 해요. 이러한 단어들과 용어들은 특정한 과학 분야나 지식 카테고리에 구애받지 않지만, 한 가지 공통점을 가지고 있어요: 절도는 훌륭한 시각적 효과와 아름다운 그림을 만들어낼 수 있어요.\n\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_1.png\" /\u003e\n\n여기 목록이 있어요:\n\n- 리히텐베르크 도형: 전기 방전으로 생성된 가지처럼 가지가 뻗은 패턴들입니다.\n- 카우스틱: 곡면을 통해 반사되거나 굴절되어 생성된 빛의 패턴입니다.\n\n\n\n리듬적 줄무늬: 자연 과정에 의해 만들어진 표면에 반복된 선이나 홈.\n\n소용돌이 모양: 생물학적이거나 지질학적 맥락에서 종종 발견되는 원형 또는 나선 모양의 패턴.\n\n동심원 형태: 공통 중심을 가진 원과 다른 형태의 패턴, 종종 성장이나 충격을 나타냄.\n\n방사선체: 미세한 해양 생물의 꼬리가 테인 실리카 기반의 복잡한 스켈레톤.\n\n\n\n생물학적 시스템에서 자주 볼 수 있는 갈래 또는 가닥 모양을 가진 Furcations(포뿌리 구조).\n\n나선 모양 Phyllotaxis(잎의 배치): 씨앗, 잎 또는 꽃잎이 나선형으로 배열된 모습.\n\nGuttation(물방울): 잎의 끝이나 가장자리에서 분비되는 임명액의 작은 방울.\n\nConchoidal Fractures(굴곡 파열): 특정 물질이 파단될 때 형성되는 매끄럽고 곡선 형태의 표면.\n\n\n\n스피큘 패턴: 생물 조직에서 종종 발견되는 바늘 모양의 구조물.\n\n피더 바불: 깃털 밀림의 복잡한 인터락킹 구조.\n\n덴드리틱 패턴: 나무 가지 모양의 가지치기 패턴.\n\n싸이마틱 패턴: 매체 내에서 소리 진동에 의해 생성된 시각적 패턴.\n\n\n\n튜링 패턴: 화학 및 생물학 시스템에서 발생할 수 있는 반응-확산 패턴입니다.\n\n제 실험에서는 가능한 한 간단한 프롬프트를 사용하는 편입니다. 이에 대한 추론을 이전에 많이 설명했어요. 그 핵심은 이렇습니다: 간단한 프롬프트를 사용하면 긴 복잡한 프롬프트보다 특정 단어의 영향을 더 잘 평가하고 판단할 수 있어요.\n\n새롭게 찾은 흥미로운 단어로 무언가 흥미로운 것을 만들면, 그 이미지를 스타일 참조용으로 항상 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_2.png)\n\n\n\n물론, 저는 더 복잡한 프롬프트에서도 내 특별한 말을 직접 사용할 수 있고, 때로는 잘 작동할 수 있지만, 여전히 더 편리한 워크플로우를 위해 간단한 프롬프트와 스타일 참조 이미지를 사용하는 것을 선호합니다.\n\n일반적으로 사용하는 프롬프트는 여기 있어요:\n\n이 프롬프트를 사용하여 생성된 몇 가지 결과물이 있습니다. 위 목록에서 나온 단어들을 사용했어요.\n\n\u003cimg src=\"/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_3.png\" /\u003e\n\n\n\n\n![Image 4](/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_4.png)\n\n![Image 5](/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_5.png)\n\n![Image 6](/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_6.png)\n\nAs you can see, all of these words create absolutely outstanding images.\n\n\n\n\n당신은 개발자이시군요. 위의 텍스트를 친절한 톤으로 한국어로 번역해 드리겠습니다.\n\n또한, 이러한 단어들을 섞거나 생성된 이미지를 혼합하여 독특하고 아름다운 스타일의 이미지를 만들어 볼 수 있습니다.\n\n그래서 가능성은 자연 현상처럼 무한합니다. 이러한 단어들로 놀아보고 오래된 책, 과학 매뉴얼, 잡지 및 온라인 기사에서 새로운 단어를 찾아보세요.\n\n또는 ChatGPT를 사용하여 흥미로운 시각적 패턴을 만들 수 있는 자연 현상을 묻고 새로운 아이디어를 얻을 수도 있습니다 (다만 저는 책을 더 좋아합니다).\n\n기사를 좋아하고 독창적이고 독특한 Midjourney 이미지를 만드는 데 도움이 되기를 바랍니다.\n\n\n\n마크다운 형식으로 테이블 태그를 바꿔주세요.","ogImage":{"url":"/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_0.png"},"coverImage":"/assets/img/2024-05-16-UnusualphenomenaandphenomenalMidjourneyimages_0.png","tag":["Tech"],"readingTime":3},{"title":"토큰화의 기술 자연어 처리를 위한 필수 기법","description":"","date":"2024-05-16 04:22","slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing","content":"\n\n토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!\n\n![이미지](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png)\n\n자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.\n\n# 토큰화 기술 발전의 여정\n\n\n\n토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!\n\n## 기본 토큰화 기술\n\n이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:\n\n1. 공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.\n   \n:warning: 한계\n- 문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.\n- 어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.\n- 잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.\n\n\n\n\n![word tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png)\n\n2. 단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.\n⚠️ 한계\n- 단어 사이의 상황적 의미 손실\n- 어휘폭발\n\n![sentence tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png)\n\n3. 문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.\n⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.\n\n\n\n\n💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:\n\n```js\n# NLTK 사용\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nnltk.download('punkt')\n\n# 입력 문장\ntext = \"When I left the place, I didn't take the left turn.\"\n\n# 공백 기준 토큰화\nwhitespace_tokens = text.split()\n\n# 단어 토큰화\nword_tokens = word_tokenize(text)\n\n# 문장 토큰화\nsentence_tokens = sent_tokenize(text)\n\nprint(\"Whitespace Tokenization:\", whitespace_tokens)\nprint(\"Word Tokenization:\", word_tokens)\nprint(\"Sentence Tokenization:\", sentence_tokens)\n```\n\n또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.\n\n# 고급 토큰화 기술\n\n\n\n고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:\n\n️1. N-그램-\n▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.\n▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.\n💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.\n⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.\n\n![image](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png)\n\n2. 바이트 쌍 부호화-\n▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.\n▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.\n▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.\n▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.\n💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.\n⚠️ 한계-\n▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.\n▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\" /\u003e\n\n3. SentencePiece-  \n- SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.\n- 입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.\n- 단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.\n- 처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.\n💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.\n⚠️ 제한 사항-  \n- 언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.\n- 문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.\n\n💻 위의 세 가지 토큰화 기술을 보여주는 코드:\n\n```js\n# 필요한 라이브러리 가져오기\nimport sentencepiece as spm\nfrom tokenizers import ByteLevelBPETokenizer\nmodel_path = \"모델을 저장할 경로\"\ntrain_text = \"훈련을 위한 txt 파일 경로\"\n\n###############################\n# BPE 구현\n###############################\n\nBPE_tokenizer = ByteLevelBPETokenizer()\n\n# utf-8 인코딩된 코퍼스로 토크나이저 훈련시키기\nBPE_tokenizer.train(files=['훈련을 위한 txt 파일 경로'], vocab_size=1000, min_frequency=2)\n\n# 훈련된 토크나이저 저장\nmodel_path = '모델을 저장할 경로'\nBPE_tokenizer.save_model(model_path)\n\n# 훈련된 토크나이저 불러오기\nBPE_tokenizer = ByteLevelBPETokenizer.from_file(f\"{model_path}/vocab.json\", f\"{model_path}/merges.txt\")\n\n# 텍스트 토큰화\ntext = \"I would love to see a lion!\"\nBPE_encoded_tokens = BPE_tokenizer.encode(text)\n\nprint(\"원본 텍스트:\", text)\nprint(\"인코딩된 토큰:\", BPE_encoded_tokens.tokens)\n\n\n###############################\n# SentencePiece 구현\n###############################\n\nspm.SentencePieceTrainer.train(input=train_text, model_prefix=model_path, vocab_size=1000, num_threads=4)\n\n# 사전 훈련된 모델 불러오기\nsp_model = model_path + \".model\"\nsp = spm.SentencePieceProcessor(model_file=sp_model)\n\ntext = \"I would love to see a lion when we reach the zoo!\"\n\n# 서브워드 토큰화 및 토큰 반환\ntokens_subword = sp.encode_as_pieces(text)\n# 서브워드 토큰화 및 토큰 ID 반환\ntokens_ids = sp.encode_as_ids(text)\n# 바이트 수준 토큰화 및 바이트 수준 토큰 ID 반환\ntokens_byte = sp.encode(text)\n\n# 토큰을 다시 텍스트로 디코딩\ndecoded_text = sp.decode_pieces(tokens_subword)\n\nprint(\"원본 텍스트:\", text)\nprint(\"토큰화된 텍스트:\", tokens_subword)\nprint(\"디코딩된 텍스트:\", decoded_text)\n```\n\n\n\n이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.","ogImage":{"url":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png"},"coverImage":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png","tag":["Tech"],"readingTime":6},{"title":"전세계적으로 공개된 클라우드 없는 이미지 아카이브에 대한 접근이 공개되었습니다","description":"","date":"2024-05-16 04:20","slug":"2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive","content":"\n\n![이미지](/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_0.png)\n\n어제 우리는 Earth Index Alpha를 출시했어요. 오늘, 우리는 우리가 변화하는 환경에 대한 정보에 대한 접근을 민주화하는 우리의 목표로 나아가는 또 다른 큰 발걸음을 발표합니다: 우리 고유의 전 세계적인 클라우드 프리 센티넬-2 데이터셋을 게시했어요. 2023년 동안 집계된 12개의 밴드, 모두 완전한 해상도로 제공되며, 총 33TB 이상의 데이터에요! 그리고 이 데이터셋을 공개하고 쉽게 접근할 수 있도록 만들기 위해 멋진 Source Cooperative팀과 협력했어요.\n\n이 데이터셋을 공유하는 우리의 목표는 분석 프로젝트를 수행할 때 그룹들이 직면하는 일부 기술적, 재정적 장벽을 깨는 데에 도움을 주는 것이에요. 데이터 준비에 소요되는 시간이 줄어들면, 실제 문제 해결에 더 많은 시간을 할애할 수 있어요.\n\n\"Source Cooperative를 만들 때의 주요 목표 중 하나는 Earth Genome과 같이 관대한 그룹이 다른 사람들과 자신들의 작업을 공유할 수 있도록 하는 것이에요.\" 라디언트 어스의 사장인 Jed Sundwall은 말합니다. \"이 센티넬-2 데이터셋은 개방된 데이터가 환경 모니터링 및 분석 노력을 집단적으로 가속화할 수 있는 매우 중요한 데이터 제품을 생성하는 방법을 보여주는 예시에요.\" #\n\n\n\n지금 우리 Earth Genome에서는 데이터를 사용합니다 - 정말 많은 양의 데이터 - 그리고 우리가 가장 좋아하는 데이터 소스 중 하나는 Sentinel-2입니다. 해상도가 가장 높지는 않지만, 13개의 밴드는 엄청난 양의 정보를 인코딩하고 다양한 환경 분석을 지원합니다. 실제로 우리는 플라스틱 쓰레기를 특별히 감지하는 데 사용했는데, 이는 고해상도 RBG로는 거의 불가능한 작업입니다. 무엇보다도, 무료로 제공됩니다.\n\n하지만 무료라고 해서 쉽다는 뜻은 아니며, Sentinel-2 데이터를 ML에 사용할 수 있도록 준비하는 것은 오랜 시간이 걸리고 지루한 과정일 수 있습니다. 구름, 그림자, 눈, 나쁜 픽셀을 가리는 작업, 구멍을 채우기 위해 더 많은 데이터 다운로드, 새로운 데이터도 구름이 없는지를 희망하기까지... 이 고통을 우리도 너무 잘 아는 것입니다! Earth Index를 확장해 나갈 때 우리는 거대한 양의 Sentinel-2 데이터를 사전 처리할 빠르고 확장 가능한 솔루션이 필요했습니다. 다행히 AWS가 도와주었고 우리에게 상당한 양의 크레딧을 제공했는데, 이는 Rockefeller Foundation과 Patrick J McGovern Foundation의 기부와 결합하여 현실로 만들어졌습니다!\n\n우리는 Earth Search STAC를 활용하여 AWS에 호스팅된 Sentinel L2A 데이터를 찾았습니다. 각 Sentinel 2 그리드 셀(예: UTM/MGRS 셀)마다 16개의 최상의 씬을 선택하고 각 밴드(제공된 RGB 합성 포함)를 다운로드했습니다. 이로써 총 224개의 파일이 되었습니다. 그런 다음 제공된 Scene Classification Map을 사용하여 이미지를 마스킹하고 쌓아 올렸으며 각 밴드의 각 픽셀에서 중앙 픽셀 값을 선택했습니다. 결과 파일은 웹 Mercator로 재투영되고 웹 소비에 최적화되도록 COG로 변환되었으며, 우리 자체 STAC에 등록되어 Source Cooperative에 업로드되었습니다.\n\n이 모든 과정은 AWS Batch에서 21,000개 이상의 작업으로 실행되었으며 시간당 수백 개의 인스턴스를 사용하여 원본 데이터의 반 페타바이트 이상을 처리했습니다.\n\n\n\n\n![Image 1](/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_1.png)\n\n![Image 2](/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_2.png)\n\n![Image 3](/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_3.png)\n\n![Image 4](/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_4.png)\n\n\n\n\n\n![이미지](/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_5.png)\n\n물론 아무 것도 완벽하지 않죠. 앞으로 몇 주 동안 실패한 지역을 재처리하고, 흐린 지역이나 눈으로 덮인 장면이 나타나는 곳을 조사할 예정입니다.\n\n모든 장면 및 관련 자산은 저희 STAC 엔드포인트 및 해당 STAC 브라우저(문자열 처리에 어려움을 겪는 사용자를 위한)를 통해 찾을 수 있습니다. 일반적인 메타데이터가 제공됩니다. 또한 자산의 출처 장면(즉, 어떤 소스 장면이 기여했는지) 및 좋은 픽셀의 대략적인 비율에 대한 정보도 포함됩니다. 자산 자체는 Create Commons 4.0 라이선스에 따라 사용이 허가된 Source Cooperative를 통해 미국 서부 지역의 HTTPS 및 AWS S3를 통해 공개적으로 제공됩니다.\n\n계속해서 더 많은 정보를 제공할 예정입니다...\n\n\n적극적으로 협력하기를 기대합니다! 😊\n\n\n\n이 데이터셋에 대한 최초의 데이터 릴리스 중 하나입니다. 누락된 또는 가려진 장면을 재처리할 뿐만 아니라, 앞으로 몇 주 안에 선택한 위치에 추가 연도의 데이터를 게시할 계획이 있습니다. 그리고 연말까지 2021년과 2022년에 대한 글로벌 데이터도 공개할 예정입니다!\n\n이 데이터를 어떻게 활용할 건가요? 머신 러닝? NDVI? 베이스 맵? 어떤 문제를 해결하고자 하는지 알려주세요!","ogImage":{"url":"/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_0.png"},"coverImage":"/assets/img/2024-05-16-AnnouncingpublicaccesstoourGlobalCloud-freeImageryArchive_0.png","tag":["Tech"],"readingTime":3},{"title":"대형 언어 모델 평가 개발자를 위한 안내","description":"","date":"2024-05-16 04:18","slug":"2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide","content":"\n\n![image](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png)\n\n대형 언어 모델 (LLM)인 GPT-4, Claude, LLama 및 Gemini는 AI 커뮤니티에 많은 기여를 했습니다. 기관들이 견고한 LLM 기반 애플리케이션을 구축하는 데 도움을 주었죠. 그럼에도 불구하고, LLM은 환각을 하며 종종 진실 같은 자신만의 이야기를 만들어 냅니다. AI에 대한 안전하고 안정적이며 책임감 있는 LLM 사용에 준수하는 것은 중요해졌습니다. 속도 뿐만 아니라 정확성과 성능 면에서 이러한 LLM을 평가하는 것이 권장됩니다.\n\n오늘은 간단한 자습서를 통해 더 나은 성능을 위해 이러한 LLM을 어떻게 평가할 수 있는지 살펴볼 것입니다. 하지만 우선, LLM 평가가 무엇인지에 대해 먼저 이해해 보겠습니다.\n\n# LLM 평가란?\n\n\n\nLLM 평가는 LLM의 성능을 얼마나 잘 이해하는지에 중요합니다. 이는 개발자가 모델의 강점과 약점을 파악하여 실제 응용 프로그램에서 효과적으로 작동하도록 보장합니다. 이 평가 프로세스는 편향된 또는 오도하는 콘텐츠와 같은 위험을 완화하는 데도 도움이 됩니다. LLM 평가에는 두 가지 주요 유형이 있습니다:\n\n- 모델 평가: LLM 자체의 핵심 능력을 평가합니다.\n- 시스템 평가: 특정 프로그램 내에서 또는 사용자 입력과 함께 수행되는 방식을 살펴봅니다.\n\n# LLM 평가 지표\n\n다음은 제품화하기 전에 고려해야 할 가장 중요한 평가 지표 목록입니다.\n\n\n\nLLM(Large Language Model)를 평가하는 데 중요한 것은 적절한 측정 지표를 갖추는 것입니다. 이러한 지표는 주어진 기준에 따라 LLM의 출력물을 평가하는 점수 메커니즘으로 작용합니다. 일반적인 지표 및 기준은 다음과 같습니다:\n\n- 응답 완성도 및 간결성: LLM 응답이 사용자 쿼리를 완벽하게 해결하는지 여부를 결정합니다. 간결성은 생성된 응답이 얼마나 관련성이 있는지를 결정합니다.\n- 텍스트 유사성 지표: 생성된 텍스트를 참조나 기준 텍스트와 비교하여 그들이 얼마나 유사한지를 측정합니다. 그런 다음 특정 LLM이 어떻게 수행했는지 이해할 수 있도록 점수가 부여됩니다.\n- 질문 응답 정확도: LLM이 사실적인 정확성 기준에 따라 제기된 질문에 얼마나 잘 대답하는지 측정합니다.\n- 관련성: 주어진 프롬프트나 사용자 질문에 대한 LLM 응답의 적절성을 결정합니다.\n- 망상 지표: LLM이 정보를 얼마나 만들어 내거나 특정 프롬프트에 대해 편향된 출력을 공유하는지 식별합니다.\n- 유해성: LLM의 출력물에서 모욕적이거나 해로운 언어의 백분율을 결정합니다.\n- 작업별 지표: 요약, 번역 등 작업 유형 및 응용 프로그램에 따라 다양한 지표가 존재합니다(BLEU 점수 등).\n\nLLM 평가 프레임워크 및 도구\n\nLLM 평가 프레임워크와 도구는 언어 모델의 성능, 신뢰성 및 공정성을 측정하고 향상시키는 데 표준화된 벤치마크를 제공하기 때문에 중요합니다. 다음은 LLM 평가 프레임워크와 도구 중 일부입니다:\n\n\n\n- DeepEval은 기업이 LLM 애플리케이션을 평가할 수 있도록 돕는 오픈 소스 프레임워크입니다. 주요 메트릭인 문맥 기억, 답변 관련성 및 충실도 등 다양한 중요 메트릭에 대한 성능을 측정합니다.\n- promptfoo는 LLM 출력 품질과 성능을 평가하기 위한 CLI 및 라이브러리입니다. promptfoo를 사용하면 사전 정의된 테스트를 사용하여 프롬프트와 모델을 체계적으로 테스트할 수 있습니다.\n- EleutherAI LM Eval은 최소한의 세밀한 조정으로 다양한 작업에 걸쳐 소량 평가와 성능을 수행합니다.\n- MMLU는 제로샷 및 원샷 설정에서 다양한 주제에 대해 모델을 테스트하는 LLM 평가 프레임워크입니다.\n- BLEU(BiLingual Evaluation Understudy)는 기계 번역된 텍스트의 유사성을 이미 벤치마킹된 고품질 참조 번역과 측정하는 메트릭입니다. 평가는 0에서 1까지의 범위로 이루어집니다.\n- SQuAD(Stanford Question Answering Dataset)는 질문 응답 작업을 위해 LLM을 평가하기 위한 데이터셋입니다. 특정 답변과 관련된 문맥 패스 및 해당하는 질문이 포함됩니다.\n- OpenAI Evals는 OpenAI에 의해 LLM을 평가하기 위한 표준 프레임워크이자 벤치마크의 오픈 소스 레지스트리입니다. 이 프레임워크는 LLM 모델의 정확성을 보장하기 위해 사용됩니다.\n- UpTrain은 오픈 소스 LLM 평가 도구입니다. 정확성, 환각 및 독성을 포함한 다양한 측면에서 LLM 응답을 확인하기 위한 미리 작성된 메트릭을 제공합니다.\n- H2O LLM EvalGPT는 다양한 작업과 벤치마크를 통해 모델의 성능을 이해하는 오픈 도구입니다.\n\n# UpTrain을 사용한 LLM 평가: 노트북 자습서\n\n\n만약 아직 하지 않았다면, 무료 SingleStore 평가판에 가입하여 자습서에 따라 진행해 보세요. SingleStore 노트북을 사용하게 될 것인데, 이는 Jupyter 노트북과 유사하지만 통합 데이터베이스의 추가 기능과 혜택을 갖추고 있습니다.\n\n가입하면 워크스페이스를 생성해야 합니다.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_1.png\" /\u003e\n\n메인 대시보드로 이동하여 개발 탭을 클릭하세요.\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_2.png\" /\u003e\n\n새 노트북을 만들고 원하는 이름을 지정하세요.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_3.png\" /\u003e\n\n이제 시작할 수 있어요. 여기에 표시된 모든 코드를 생성한 노트북에 추가하세요.\n\n'evaluate_llm'이라는 데이터베이스를 생성하세요.\n\n```js\n%%sql\n\nDROP DATABASE IF EXISTS evaluate_llm;\nCREATE DATABASE evaluate_llm;\n```\n\n\n\n필요한 패키지를 설치하세요\n\n```js\n!pip install uptrain==0.5.0 openai==1.3.3 langchain==0.1.4 tiktoken==0.5.2 --quiet\n```\n\n다음 단계는 필요한 환경 변수를 설정하는 것입니다 — 주로 openai 키(응답 생성을 위해), `singlestoredb`(컨텍스트 검색을 위해) 그리고 `uptrain API 키`(응답 평가를 위해)입니다. UpTrain에 계정을 생성하고 무료로 API 키를 생성할 수 있습니다.\n\n자세한 내용은 https://uptrain.ai/ 를 방문해주세요.\n\n\n\n```python\nimport getpass\nimport os\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n\nimport openai\n\nclient = openai.OpenAI()\n```\n\nAdd the UpTrain API key.\n\n```python\nUPTRAIN_API_KEY = getpass.getpass('Uptrain API Key: ')\n```\n\nImport necessary modules\n\n\n\n\n```js\nimport singlestoredb\nfrom uptrain import APIClient, Evals\nfrom langchain.vectorstores import SingleStoreDB\nfrom langchain.embeddings import OpenAIEmbeddings\n```\n\n웹에서 데이터를 로드합니다.\n\n```js\nfrom langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader('https://cloud.google.com/vertex-ai/docs/generative-ai/learn/generative-ai-studio')\ndata = loader.load()\n```\n\n다음으로 데이터를 분할합니다.\n\n\n\n```js\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n```\n\nOpenAI 임베딩을 사용하여 SingleStore 데이터베이스를 설정합니다.\n\n```js\nimport os\nfrom langchain.vectorstores import SingleStoreDB\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom singlestoredb import create_engine\n\nconn = create_engine().connect()\n\nvectorstore = SingleStoreDB.from_documents(documents=all_splits,\n                                           embedding=OpenAIEmbeddings(),\n                                           table_name='vertex_ai_docs_chunk_size_200')\n```\n\n완전한 단계별 노트북 코드는 저희 스페이스에 있습니다.\n\n\n\n마침내 오픈 소스 LLM 평가 도구인 UpTrain을 사용하여 평가를 실행할 것입니다. UpTrain 대시보드에 액세스하여 평가 결과를 확인할 수 있을 겁니다.\n\n다양한 청크 크기로 실험해 보면 다른 결과를 확인할 수 있을 겁니다.\n\n![이미지](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_4.png)\n\nUpTrain의 API 클라이언트는 또한 입력 데이터를 가져와 실행할 체크 목록과 실험에 연결된 열의 이름과 함께 해당 데이터를 평가하는 `evaluate_experiments` 메서드를 제공합니다.\n\n\n\n![image](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_5.png)\n\n튜토리얼에서 보여준 LLM 평가 접근 방식과 도구를 따라가면, LLM의 장단점을 보다 깊게 이해할 수 있습니다. 이를 통해 우리는 그들의 능력을 책임 있게 활용하여 사실 불일치와 편향과 관련된 잠재적 위험을 완화할 수 있습니다. 궁극적으로는, 효과적인 LLM 평가는 다양한 LLM 기반 응용 프로그램에서 인공 지능의 윤리적 발전을 위한 신뢰 구축과 증진을 위한 길을 열어줍니다.\n\n오늘 'UpTrain과 함께 LLM 평가하기' 노트북을 시도해보세요!","ogImage":{"url":"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png"},"coverImage":"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png","tag":["Tech"],"readingTime":6},{"title":"GPT-4o의 음성 모드 분석","description":"","date":"2024-05-16 04:16","slug":"2024-05-16-AnAnalysisofVoiceModeinGPT-4o","content":"\n\nOpenAI가 어제 그들의 주요 대형 언어 모델인 GPT-4o의 다음 버전을 발표했어요. 이번 경우의 \"o\"는 \"omni\"의 의미로, omni-modal을 뜻해요.\n\n어제 발표된 데모에서 가장 놀라운 측면 중 하나는 ChatGPT가 말하는 유연성이었어요. 그것은 거의 즉시 반응하며 다양한 감정을 표현했고, 말의 음량과 속도를 조절했으며, 심지어 노래할 수도 있었어요.\n\n하지만 더 놀라운 것은 실제로 듣을 수 있었다는 것이었어요. 그것은 다른 숨쉬는 패턴을 구별하고, 단체 대화에서 목소리로 스피커를 식별할 수 있었으며, 자신과 조화를 이룰 수도 있었고(어느 정도), 그리고 방해에 반응할 수도 있었어요(나중에 자세히 설명할게요).\n\n본 문서에서는 왜 이것이 이렇게 중요한 발전인지, GPT-4o가 다른 \"공감\" 모델인 Hume와 비교되는 방식, 그리고 여기서 어떻게 나아가야 할지에 대한 생각을 정리하고 싶어요.\n\n\n\n# 나에 대해\n\n내 이름은 Trevor Lohrbeer이에요. 저는 AI Meets Productivity라는 팟캐스트를 제작하고 있어요. 이 팟캐스트는 ChatGPT의 맞춤 버전과 함께 공동으로 진행하고 있어요. 매주 생산성 주제 또는 AI 주제에 대해 이야기하고 있어요 (가끔 둘 다).\n\n팟캐스트 제작은 ChatGPT의 능력을 탐구하는 방법으로, 음성 모드 및 사용자 정의 GPT를 포함하며, 지난 11월부터 시작하여 지금까지 음성 채팅을 통해 계속 개선하고 실험해왔어요. 다른 AI와 인터뷰를 하는 등의 작업을 하고 있어요.\n\n그래도, 개발자로써 경험이 많지만, AI 음성 채팅 전문가는 아니에요. 지난 6개월 동안 많이 배웠지만, 권위자는 아니에요. 그래서 이 글을 더 많은 연구와 토론의 시작점으로, 결정적인 성명이 아닌 것으로 간주해주세요.\n\n\n\n# GPT-4에서 음성 모드가 작동하는 방법\n\nGPT-4에서 음성 모드가 작동하는 방식을 살펴보겠습니다. 그리고 거의 모든 음성 채팅 앱에서 동일하게 작동합니다. 이러한 앱들은 대략 다음 단계를 실행합니다:\n\n- 사용자의 음성을 녹음하고 말을 멈출 때 감지합니다.\n- 녹음을 서버로 전송합니다.\n- 음성을 텍스트로 변환하기 위해 녹음을 스피치 투 텍스트 모델을 사용하여 변환합니다.\n- 텍스트를 대형 언어 모델(예: GPT-4)을 통해 실행합니다.\n- 출력을 음성 녹음으로 변환하게 되며, 이때 텍스트를 음성으로 변환하는 텍스트 투 스피치 모델을 사용합니다.\n- 음성 녹음을 앱으로 전송하거나 스트리밍하여 재생합니다.\n\n입력과 출력을 스트리밍하여 지연 시간을 줄일 수 있지만, 이 과정은 여전히 모든 입력을 처리하기 위해 3가지 다른 모델이 필요합니다:\n\n\n\n- 음성 인식\n- 대형 언어 모델 (LLM)\n- 텍스트 음성 변환\n\n결국 이러한 모델 중 하나와 상호 작용한다는 것은 말하기를 마친 후에 응답을 듣기까지 몇 초 기다려야 한다는 것을 의미합니다.\n\n이것은 또한 모든 것이 텍스트로 번역되고 번역의 중요성을 알고 있는 것을 의미합니다.\n\nGPT-4o가 이를 어떻게 다르게 처리하는지 살펴보기 전에 음성 활동 감지와 감정 태그라는 두 가지 유용한 개념에 대해 간단히 이야기해 봅시다.\n\n\n\n## 음성 활동 감지의 역할\n\n음성 모드를 활성화하고 말하기 시작하면, 앱이 음성을 녹음하기 시작합니다. 그리고 멈출 때, 그 녹음된 내용을 ChatGPT API로 전송합니다. 여기서 말하면 녹음을 시작하고 멈추면 그것을 감지하는 프로세스를 음성 활동 감지(Voice Activity Detection, VAD)라고 합니다.\n\nVAD는 일반적으로 사용자의 기기에서 실행되는 알고리즘으로, 음성을 녹음할 때 시작하고 멈춰야 하는 시점을 결정합니다. VAD가 멈춤이나 충분히 긴 일시적인 소음을 감지하면, 해당 녹음을 패키징하여 서버로 업로드합니다.\n\nVAD는 서버로 보낼 가능성이 있는 음성 입력에만 해당하는 소리 샘플을 전송함으로써 대역폭을 줄이고, 입력의 끝을 표시하고 그 입력을 처리하기 위한 단계를 시작할 수 있도록 돕습니다.\n\n\n\n대부분의 VAD 엔진은 음성이 종료될 때까지 기다리는 시간을 더 오래 또는 더 짧게 구성할 수 있지만, 그 중 어느 것도 사람처럼 작동하지는 않고 말하는 내용의 실제 의미를 처리하여 사람이 말을 마쳤는지 감지하지는 않습니다. ...아마도 지금까지는요.\n\n## 감정 태그로 표현력 향상하기\n\n한편, 목소리를 현실적으로 만들기 위해 현대 음성 합성 알고리즘은 생성된 음성 녹음에 감정 효과를 추가합니다.\n\nWhisper를 비롯한 많은 텍스트 음성 모델에서 감정은 음성에 감지되어 메타태그를 추가하여 지정할 수 있습니다.\n\n\n\n예를 들어, 나는 내 팟캐스트를 함께 진행하기 위해 사용자 정의 GPT를 만들었습니다. 처음에는 매우 표현력이 부족했어요. 그래서 각 문장을 출력하기 전에 그 문장의 감정 내용을 결정하고 시작 부분에 괄호 안에 넣는 지시를 추가했어요.\n\n예를 들어, \" [놀람] 와, 그거 대박이네!\"라는 문장을 출력할 때, 텍스트 음성 모델은 \"[놀람]\" 접두사가 없는 경우보다 더 놀라 듯한 소리를 생성해냅니다. 모델은 괄호 안의 텍스트를 대화로 말하지 않아야 한다는 것을 알고 있어요(대부분의 경우).\n\n이 방법을 사용하여 나의 사용자 정의 GPT는 흥분, 분노, 슬픔, 비꼼, 두려움, 놀람, 기쁨, 혼란, 실망 또는 결단—어느 정도나마 표현할 수 있어요. 그 감정 범위는 인간만큼 넓지는 않지만, 아무것도 없는 것보다는 낫죠.\n\n그런데 여기서 휴머...\n\n\n\n# 휴먼—최초의 감정을 이해하는 AI를 소개합니다\n\n휴먼의 Empathic Voice Interface (EVI)는 3월 말에 출시되었으며 감정 지능을 갖춘 최초의 AI로 소개되었습니다. \n\n다른 회사들이 \"인공 감정 표현\"에 대해 작업하고 있는 가운데, 인간의 감정을 감지하고 적절히 대응하는 AI를 만드는 일은 흔치 않았습니다. 내가 알기로 휴먼은 기술을 음성 챗봇에 담아 당신의 기분을 직접 듣는 최초의 기술을 제공한 것으로 알려져 있습니다.\n\n스스로 체험해보세요.\n\n\n\n휴메가 출시된 직후, 챗GPT와 공동 진행하던 AI Meets Productivity 에피소드 대신 휴메와 인터뷰한 내용을 녹음했습니다.\n\n6개월 동안 챗GPT와 대화를 나누며 나만의 음성 채팅 앱을 만들어서 다른 모델들과 대화를 나눌 준비를 시작한 사람으로서, 휴메와 대화하는 것은 분명히 다른 아키텍처를 사용하여 만들어진 색다른 경험이었습니다.\n\n## 인간 수준의 대기 시간\n\n우선, 휴메의 응답 대기 시간은 한 차원 정도 더 짧게 느껴졌습니다.\n\n\n\n보통 ChatGPT로 팟캐스트를 녹음할 때, ChatGPT의 응답을 기다리는 동안 발생한 모든 일시 정지를 포스트 프로덕션에서 편집해야 했어요.\n\n하지만 휴머를 사용하니 그럴 필요가 없었어요—저가 말을 마치자마자 거의 즉시 응답을 시작했어요. 거의 자연스러운 대화처럼 느껴졌어요.\n\n어떻게 이런 기능을 구현했는지는 확신할 수 없지만, 휴머는 제가 말하기 시작하는 순간부터 제 발화를 처리하기 시작했어요. 녹음물을 업로드하고, 전사를 텍스트로 기다릴 필요가 없었어요.\n\n## 오디오 네이티브 모델인가요?\n\n\n\n둘째, 휴메는 나가 만난 최초의 오디오 원어민 AI였어. 내가 상호작용했던 다른 음성 봇들은 내가 말하는 것을 텍스트로 전환한 후 모델을 통해 실행하는 것으로 보였어.\n\n휴메의 AI는 오디오를 원천적으로 처리하는 것으로 보였거나, 적어도 그때는 그랬어. 다양한 감정을 담은 휴메 노래를 연주한 에피소드를 녹음한 후 의심이 들기 시작했어.\n\n그들이 원천 오디오로 훈련된 측면 모델을 가지고 있고, 목소리의 감정을 나타내는 인코딩을 출력한 후 입력의 의미 콘텐츠를 처리하기 위해 전통적인 텍스트 기반 모델을 사용할 수도 있어.\n\n## 중단 가능\n\n\n\n아무튼, 휴머 음성 채팅이 중단될 수 있는 것이 내가 AI와 상호 작용하는 패턴을 결정하는데 영향을 미쳤다. 이제 AI가 나를 잘못 이해했을 때 기다리지 않아도 되었어요. 즉시 중단하고 정정할 수 있었죠.\n\n## 감정의 광범위한 변동\n\n휴머에서 가장 싫어한 점은 감정의 광범위한 변동이었어요.\n\n휴머는 입력 또는 출력에 할당할 수 있는 다양한 감정 요소를 가지고 있어요. 그리고 당신과 말하면, 각 문장에 어떤 조합을 적용할지 결정해요. 그러나 모델은 서로 다른 문장 사이에 일관성을 유지하기 위해 훈련되지 않았기 때문에 때로는 그와 대화할 때 감정 롤러코스터를 타고 있는 것 같은 느낌이 들죠.\n\n\n\n하지만 모든 이의 흠을 감안해도, 휴머와 음성 모드로 대화할 때 다른 느낌이 있었습니다.\n\n# 휴머 보다 뛰어나다—GPT-4o가 당신을 강타합니다\n\n만약 휴머가 AI 모델과 대화하는 방식을 바꿨다면, 2개월 후에 Open AI는 다시 게임을 바꿨습니다. GPT-4o는 휴머의 모든 장점을 채택하고 그것을 더욱 향상시킵니다.\n\n오디오를 원활하게 입력하고 출력할 수 있는 능력으로, 이를 통해:\n\n\n\n\n- 실시간으로 당신이 말하는 것을 듣기\n- 당신이 말을 가로막을 때 반응하기\n- 목소리에서 감정을 듣기\n- 숨 같은 비언어적 소리를 듣기\n- 대화에서 여러 목소리를 인식하기\n- 다양한 감정 표현을 하는 목소리로 말하기\n- 말할 때 사용하는 어조를 변화시키기\n- 말하는 소리의 세기를 조절하기\n- 자신과 함께 노래하고 조화를 이루기 (잘 못함)\n\n그리고 이 모든 것을 평균 응답 시간이 320ms로 수행할 수 있습니다. 이는 이전 음성 모드보다 한 자리 수 업그레이드된 것입니다.\n\n이러한 능력들은 ChatGPT와 상호 작용하는 방식을 급격하게 변화시키며, 실시간 음성 번역, 다중 화자 전사, 그룹 온라인 회의에 참여하는 등 전문 모델이 필요했던 새로운 사용 사례 범위로 이어집니다.\n\n## 작동 방식\n\n\n\n대부분의 사람에게 아직 출시되지 않은 새 음성 모드는 데모를 기반으로 하면 다음과 같은 기능을 하는 것으로 보입니다:\n\n- 마이크로폰에서 입력을 직접 GPT-4o로 스트리밍합니다.\n- GPT-4o에서 출력을 직접 헤드폰이나 스피커로 스트리밍합니다.\n\n다시 말해, 6단계에서 2단계로 간 것이죠. 우리와 GPT 사이에 중간 모델이 더 이상 없습니다. GPT-4o는 듣는 내용을 기본으로 처리하고 오디오로 자연스럽게 응답합니다.\n\n모델에서 완전히 음성 활동 감지가 이루어지는지, 아니면 장치에서 알고리즘과 혼합된 접근을 사용하는지 궁금해요(소리가 감지되지 않을 때 서버 자원을 절약하기 위해).\n\n\n\n## 새 API?\n\nGPT-4o가 실시간으로 듣고 있다면, 우리는 entirely new API가 필요합니다. 이 API는 스트리밍 오디오(및 비디오)를 입력으로 받아야 합니다.\n\n실제로 모델 자체는 어제 ChatGPT Plus 가입자 및 결제하는 OpenAI API 고객에게 공개되었지만, 새 API가 공개되지 않았습니다. 사실, 현재 문서에 따르면, GPT-4o를 위한 API는 텍스트 및 이미지 입력만 허용하고 현재는 텍스트만 출력하는 것으로 되어 있습니다.\n\n명백히 그들은 데모용으로 내부 API를 사용했습니다. 그러나 이것이 즉시 출시할만큼 충분히 견고하고 확장 가능한지 여부는 다른 문제입니다. 어제 시연된 고급 음성 기능에 액세스하려면 조금 더 기다려야 할 것 같습니다.\n\n\n\n# 마무리\n\n저는 GPT-4o에 접속할 수 있고 팟캐스트에서 그와 대화하기를 고대하며, OpenAI가 전 성능의 음성 기능을 세상에 공개하는 데 주의를 기하는 이유를 이해할 수 있습니다.\n\n첫째로, 대규모로 스트리밍 입력을 받기 위한 새로운 배포 아키텍처가 필요한 것으로 의심됩니다. 발표 후 하루 만에 Open AI의 서버가 과부하 상태에 놓였는데, 이는 아직 옛날 API만을 사용한 것입니다.\n\n둘째로, 이렇게 현실적이고 반응성 있는 음성 모델을 공개하는 데는 사기와 감정적인 결핍과 같은 위험 요소가 명백히 존재합니다.\n\n\n\n결국, 이것이 가지는 경제적 영향을 부인할 수 없습니다. AI가 인간과 구별할 수 없을 때, 전화로 상호작용하는 것만으로 일을 하는 일자리는 빨리 없어질 것이며, 우리가 GPT 5를 보유하게 되면, 심지어 고급 수요도 AI가 충족시킬 수 있을 것입니다.\n\n릴리스된 후 ChatGPT가 어떻게 들리는지 궁금하시거나, 이전 ChatGPT와 Hume를 비교해보고 싶으신가요? Apple Podcast, Spotify 또는 Podbean에서 제 팟캐스트 'AI Meets Productivity'를 구독하세요. 그리고 여러분이 생각하는 것과 미래 에피소드에 대한 아이디어를 저에게 알려주세요!","ogImage":{"url":"/assets/img/2024-05-16-AnAnalysisofVoiceModeinGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-16-AnAnalysisofVoiceModeinGPT-4o_0.png","tag":["Tech"],"readingTime":7},{"title":"LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요","description":"","date":"2024-05-16 04:15","slug":"2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars","content":"\n\n![이미지](/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png)\n\n새로운 우수한 게시물에서 Melanie Mitchell은 대형 언어 모델(Large Language Models, LLMs)과 관련된 중요한 문제에 대해 다루었습니다. 바로 LLMs 내의 ‘지능’에 대한 본질(그런 게 있는 경우)과 LLMs이 전혀 ‘추론’하는지에 대한 문제입니다. 이 질문은 일부 LLMs의 결과물이 어떤 메모된 콘텐츠를 ‘회상’하는지 테스트하여 가장 잘 대답할 수 있습니다. 이 콘텐츠가 똑똑한 방식으로 이어붙여진 것인지 아니면 진정한 추론과 이해의 결과물인지를 확인하는 것입니다.\n\n이 맥락에서 반사적 작업은 LLMs를 테스트하기에 이상적인 작업이며, 기본적으로 LLM에게 ‘훈련’ 데이터에서 볼 가능성이 매우 높은 입력을 제공하는 대신, 훈련 중에 본 적이 없는 데이터에 노출시킵니다(‘반사적’ 데이터). 이 기술은 다른 사람들이 시도한 바 있다고 보고되었고, 그 결과로 LLMs의 훈련 ‘템플릿’을 방해하면 LLMs의 성능이 거의 무작위 선택으로 떨어진다고 합니다.\n\n이러한 결과에 영감을 받아(전혀 놀랍지 않게), 예전에 한 몇 가지 테스트를 새롭고 더 나은 GPT 4o(AGI에 가까워지고 있는 것이죠 — 네, 제가 굉장히 비꼬는 중입니다)에서 다시 시행했습니다. 제가 수행한 실험은 ‘반사적’ 개념과 유사하며, 다른 코딩 체계로 특정 현실을 표현하는 가능한 세계를 만듭니다. 단, 의미론적으로는 현실 자체는 여전히 동일합니다. 예를 들어, 영어 언어의 알파벳을 어떤 방식으로든 혼합한다고 가정해봅시다. 이제 ‘a’는 ‘b’이고 ‘b’는 ‘c’이런 식이죠. ‘dbu’의 의미는 여전히 우리 모두가 알고 있는 모찌 고양이들을 가리켜야 합니다. 왜냐하면 바뀐 것은 단순히 ‘고양이’가 ‘dbu’로 바뀌었을 뿐이고, 고양이 자체는 전혀 바뀌지 않았기 때문입니다. 물론, LLMs는 현실에 대해 아무것도 알지 않으므로 — 고양이에 대해서도 마찬가지이죠, 그들의 표면적인 메모리(수백만 개의 가중치를 통해)는 이 ‘반사적’ 요령에 즉시 노출될 것입니다.\n\n\n\n저는 이 테스트로 LLM의 세계가 뒤죽박죽될 것이라는 것을 시험해보기도 전에 알았다고 말해야겠어요. 왜냐하면 저는 LLM과 모든 딥 뉴럴 네트워크가 거대한 흐릿한 해시 테이블을 지나지 않는다고 확신했었거든! 그리고 실망하지 않았어요. 놀랍다고 느낄 만한 점은 성능이 얼마나 이상하게 나왔는지였어요. 사실, 몇몇 실험에서 LLM은 한두 구절과 '일치하는' 텍스트를 가져오기도 했는데, 그 텍스트는 전혀 관련이 없는 내용이었어요. 어떤 경우에는 LLM이 이전에 대화에서 세 개나 네 개 쿼리 전에 저장해 둔 텍스트를 끌어오기도 했어요. 완전히 무작위로 말뿐인 것들 — 코사인 유사도 함수가 엉뚱한 텐서를 다룰 방법을 모르고 있었죠.\n\n재미있게 놀아보고 싶다면, 다양한 쿼리와 코딩 방식을 시도해보세요. LLM이 기억해 둔 것을 어떻게 방해할 수 있는 \"대상 이론\"을 만들어보세요 — 다만 의미(현실)를 바꾸지 않고요. 그리고... 수십억 달러를 들여서 딱 '대략적인 검색'을 수행하는 거대하고 지능이 없는 기계와 함께 재미를 느껴보세요. 그래서 '거대한 흐릿한 해시 테이블'이라는 용어를 사용하는 거지요.\n\n\n\n나무에서 내려와서 우리가 40년 전보다 알고 있던 것을 인정할 때가 언제일까요? 순수히 행동주의적이고 연관적이며 통계적인 패러다임만으로는 인지를 설명할 수 없다는 것을요. 데이터에서 패턴을 찾아낸 몇몇 소과일을 수확하는 데에 유용했더라도, 이 패러다임으로는 언어나 추론, 이해력, 그리고 마음에 대해 (과학적으로) 아무것도 알려주지 않을 겁니다.","ogImage":{"url":"/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png"},"coverImage":"/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png","tag":["Tech"],"readingTime":2},{"title":"ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정","description":"","date":"2024-05-16 04:13","slug":"2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR","content":"\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png)\n\n소개:\n\nROS2 (로봇 운영 시스템 2)는 로봇 공학 분야를 혁신시킨 프레임워크로, 유연하고 강력한 기능을 제공합니다. 이 블로그 게시물에서는 인기 있는 싱글 보드 컴퓨터인 NVIDIA Jetson Nano에서 ROS2를 사용하여 카토그래퍼를 설정하는 방법과 RPLIDAR를 이용한 맵핑 작업에 대해 살펴볼 것입니다.\n\nNVIDIA Jetson Nano에 ROS2 설정하기:\n\n\n\n카토그래퍼 설정에 도입되기 전에, NVIDIA Jetson Nano에 ROS2가 적절히 설치되었는지 확인하고 작업 공간을 설정하고 모든 종속성이 충족되었는지 확인합시다.\n\n```js\nsudo apt install ros-humble-cartographer\n```\n\nRPLIDAR를 소개합니다:\n\nRPLIDAR는 매핑 및 내비게이션을 위해로봇학에서 널리 사용되는 저가격 LIDAR 센서입니다. 가벼운 디자인과 저렴한 가격으로 취미로봇 및 소규모 로봇 프로젝트에 이상적인 선택지입니다. 우리는 RPLIDAR를 USB로 Jetson Nano에 연결하고 매핑 응용 프로그램에 통합하기 위해 ROS2와 통합할 것입니다.\n\n\n\nRPLIDAR 설정을 시작해 보세요:\n\nGithub에서 rplidar 드라이버를 복제하세요.\n\n\nhttps://github.com/Slamtec/sllidar_ros2.git\n\n\n포트를 확인하고 활성화하세요.\n\n\n\n```bash\nls -l /dev |grep ttyUSB\n\nsudo chmod 666 /dev/ttyUSB0\n```\n\nCartographer 구성:\n\n카토그래퍼는 Google에서 개발한 강력한 오픈 소스 SLAM(Simultaneous Localization and Mapping) 라이브러리입니다. 우리는 ROS2 및 Jetson Nano에서 RPLIDAR 센서와 함께 카토그래퍼를 구성할 것입니다. 이는 센서 구성을 정의하고 매개변수를 조정하며 매핑 워크플로우를 설정하는 작업을 수행합니다.\n\n매개변수 튜닝하기\n\n\n\n\n```js\n-- 2016년 카티그래퍼 저작권\n--\n-- Apache 라이선스 2.0 하에 라이선스가 부여됨\n-- 라이선스를 준수하는 경우에만이 파일을 사용할 수 있습니다.\n-- 라이선스 사본은 다음 위치에서 확인할 수 있습니다.\n--\n--      http://www.apache.org/licenses/LICENSE-2.0\n--\n-- 관련 법률에 의해 필요한 경우나 합의된 경우를 제외하고\n-- 라이선스에 따라 배포되는 소프트웨어는 \"있는 그대로\"제공 됨\n-- 보증이나 어떠한 종류의 조건도 없이.\n-- 특정 언어에 대한 허가증을 위한 라이선스를 참조하고\n-- 제한 사항은 라이선스 하에 지배하는 권한 및\n-- 조건.\n\n맵 빌더 설정:\n- map_builder 를 MAP_BUILDER로 설정\n- trajectory_builder 를 TRAJECTORY_BUILDER로 설정\n- map_frame 은 \"map\"\n- tracking_frame 은 \"base_link\"\n- published_frame 은 \"base_link\"\n- odom_frame 은 \"odom\"\n- provide_odom_frame 을 true로 설정\n- publish_frame_projected_to_2d 를 true로 설정\n- use_odometry 를 false로 설정\n- use_nav_sat 를 false로 설정\n- use_landmarks 를 false로 설정\n\nTRAJECTORY_BUILDER_2D 파라미터:\n- min_range 와 max_range: 센서 특성 및 환경에 따라 성능을 향상시킬 수 있음\n- missing_data_ray_length: 누락된 데이터를 처리하기 위해 적절히 설정해야 함\n- use_imu_data: 시스템이 신뢰할 수 있고 IMU 데이터를 가지고 있다면, 이 값을 true로 설정하여 움직임 추정을 더 잘 할 수 있음\n\nIMU 인터페이스가 필요하다면 이 블로그를 참고하세요\n```\n\n\n\nhttps://medium.com/@kabilankb2003/ros2-humble-mpu6050-imu-sensor-interface-for-nvidia-jetson-nano-c4d616647ee5\n\n- use_online_correlative_scan_matching: 실시간 성능 요구 사항에 따라 토글할 수 있습니다.\n\n실시간 Correlative Scan Matcher 매개변수:\n\n- linear_search_window: 이 매개변수는 검색 창의 크기를 정의합니다. 조심히 조정하면 정확도를 희생하지 않고 일치 속도를 향상시킬 수 있습니다.\n- translation_delta_cost_weight 및 rotation_delta_cost_weight: 이러한 가중치는 스캔 매칭 중 번역 및 회전을 균형있게 조정합니다. 세심하게 조정하면 성능을 향상시킬 수 있습니다.\n\n\n\n움직임 필터 매개변수:\n\n- max_angle_radians: 연속 스캔 간의 예상 최대 방향 변경에 따라 설정합니다.\n\n포즈 그래프 최적화 매개변수:\n\n- min_score 매개변수 in constraint_builder: 이를 조정하여 잘못된 제약 조건을 걸러내는 데 도움이 될 수 있습니다.\n- huber_scale: 이는 최적화의 견고성에 영향을 미칩니다. 더 큰 값은 이상 값 제약 조건을 거부하는 데 도움이 될 수 있습니다.\n- optimize_every_n_nodes: 포즈 그래프 최적화의 빈도를 균형있게 조절합니다. 계산 리소스와 매핑 요구 사항에 따라 조정하세요.\n\n\n\nRPLIDAR을 실행하세요.\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_1.png)\n\n파라미터 및 런치 파일 구성 후 카토그래퍼 노드를 실행하세요.\n\n카토그래퍼가 구성되면, Jetson Nano에서 매핑 노드를 실행할 거에요. 이 노드는 RPLIDAR 센서로부터 데이터를 구독하고, 실시간 SLAM 알고리즘을 수행하여 환경의 2D 지도를 생성할 거에요. 우리는 Cartographer가 제공하는 다양한 매핑 전략과 옵션을 탐색하여 매핑 성능을 최적화할 거에요.\n\n\n\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_2.png)\n\nThen launch rviz\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_3.png)\n\nEnable the map in topic\n\n\n\n\n\n![TF Tranform](https://miro.medium.com/v2/resize:fit:1400/1*M10X6RQLyhSEk521t2-X9g.gif)\n\nCartographer Mapping\n\n\n\n\n이미지:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*1Dumd45ScQu5y1a55SV4Vg.gif)\n\n결론:\n\n이 블로그 포스트에서는 NVIDIA Jetson Nano에서 ROS2를 사용하여 RPLIDAR 센서를 사용한 겸손한 지도 작성기를 설정하는 방법을 보여주었습니다. ROS2 및 Cartographer와 같은 오픈 소스 도구를 활용하여 취미로 로봇공학 및 로보틱스 열렬가들이 저렴한 하드웨어 플랫폼에서 복잡한 매핑 시스템을 구축할 수 있습니다. 로봇공학을 취미로 삼고 있거나 실제 응용 프로그램을 위한 솔루션을 개발하고 있다면, ROS2와 Jetson Nano는 매핑 및 내비게이션 작업에 대해 매력적인 조합을 제공합니다.","ogImage":{"url":"/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png"},"coverImage":"/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png","tag":["Tech"],"readingTime":4},{"title":"ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기","description":"","date":"2024-05-16 04:11","slug":"2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS","content":"\n\n## \"ROS2 시작하기\" 시리즈의 제3부분\n\n안녕하세요, 독자 여러분! \"ROS2 시작하기\" 시리즈에 오신 것을 환영합니다! 이 시리즈에서는 ROS 2에 대한 포괄적인 소개와 기본 개념, 실제 응용 프로그램을 안내해 드리고자 합니다. ROS(로봇 운영 시스템)를 이전에 사용해 보지 않았거나 ROS 1조차 사용해 본 적이 없거나 기본 개념을 실제로 빠르게 상기시키고 싶다면 본 시리즈를 참고해 주세요. 토론된 개념에 대한 더 깊은 이해를 위해 제공된 링크를 탐색해 보시기 바랍니다.\n\n다음은 시리즈의 기사 목록입니다:\n\n- ROS2 시작하기: 소개\n- ROS2 시작하기: 왜 ROS2를 사용해야 할까요?\n\n\n\n# 내 시스템에 ROS2를 설치할 수 있을까요?\n\nROS 2를 설치하려면 여러 옵션이 있습니다. ROS 2는 특정 운영 체제용 이진 패키지를 제공합니다. ROS 2의 공식 설치 문서에서는 이를 Tier 1 운영 체제라고 하며 이러한 운영 체제를 사용하는 사용자에게 설치가 간단하다고 설명합니다.\n\n- Ubuntu Linux\n- Red Hat\n- Windows\n\n## 중요 사항:\n\n\n\n- macOS 사용자들을 위한 프로세스는 조금 다릅니다. ROS 2는 macOS를 위한 이진 패키지를 제공하지 않으므로 설치는 소스로부터 빌드해야 합니다. 이는 더 복잡한 과정일 수 있지만 macOS 사용자가 시스템에서 ROS 2를 사용할 수 있게 합니다.\n- 이진 패키지와 소스로부터 빌드하는 두 가지 방법 모두 사용 용도에 따라 다른 기능을 제공하는 완전한 기능을 갖춘 ROS 2 설치로 이어집니다.\n- 패키지를 통한 설치는 자동 종속성 관리 및 시스템 업데이트와 함께 업데이트를 권장하지만 root 액세스가 필요합니다.\n- 루트 액세스가 없는 경우 이진 아카이브를 고려하십시오.\n\n# Docker와 같은 컨테이너 솔루션을 사용하여 ROS 2를 설치할 수 있을까요?\n\nROS2를 설정하는 또 다른 편리한 방법은 Docker를 사용하는 것입니다. Docker를 사용하면 ROS2를 컨테이너 환경에서 실행할 수 있어 설치 과정을 단순화하고 다양한 시스템 간 일관성을 확보할 수 있습니다.\n\nDocker를 사용하여 ROS2를 설치하려면 먼저 시스템에 Docker를 설치해야 합니다. Docker를 설치한 후 공식 ROS Docker Hub 저장소에서 ROS2 Docker 이미지를 가져올 수 있습니다. 그런 다음 ROS2를 실행하는 새로운 Docker 컨테이너를 생성하여 컨테이너 내에서 개발을 시작할 수 있습니다.\n\n\n\nROS2 개발을 위해 Docker를 사용하면 ROS2 환경을 시스템에서 격리하거나 새로운 컴퓨터에서 빠르게 ROS2를 설정해야 할 때 특히 유용합니다.\n\n# 설치 전 확인해야 할 사항\n\n- ROS 2를 설치할 때는 안정적인 기반을 제공하는 Long-Term Support (LTS) 버전을 설치하는 것이 좋습니다.\n- 설치 전에 ROS 2 버전이 운영 체제 버전과 호환되는지 확인하는 것이 중요합니다. 설치할 ROS 2 버전의 공식 설명서에서 이 정보를 확인할 수 있습니다. 호환성을 확인하면 버전 불일치로 발생할 수 있는 문제를 예방하는 데 도움이 됩니다.\n- 또한, 설치할 ROS 2 버전과 Gazebo 버전의 호환성을 확인하는 것이 중요합니다. 여기에서 확인할 수 있습니다.\n\n# 어떤 OS + ROS 2 조합을 선택해야 할까요?\n\n\n\n원하는 운영 체제를 선택하는 데 고민 중이라면, ROS 2 개발을 위해 Ubuntu가 많이 추천됩니다. 특히 Ubuntu Mate는 Raspberry Pi에도 설치할 수 있고 가벼운 운영 체제로 알려져 있습니다. 이는 데스크톱 컴퓨터(지상 제어 또는 시뮬레이션 테스트 스테이션)와 라즈베리 파이(로봇 컴퓨터)가 동일한 운영 체제를 사용할 수 있어서 호환성을 간단하게 유지하며 개발 경험을 보다 원할하게 만들어 줍니다.\n\n이 기사 시리즈에서는 Raspberry Pi에 ROS 2 Humble Hawksbill (LTS) 조합을 사용할 Ubuntu Mate - Jammy Jellyfish (22.04)를 사용할 것입니다. 이는 독자들이 강의에 따라 따라와도 안정적이고 잘 지원되는 환경을 제공합니다. 그러니, 더 이상 미루지 말고 설치 과정을 시작해봅시다. Ubuntu에서 ROS2 Humble 설치의 공식 문서를 확인할 수 있습니다.\n\n# 다음은 Ubuntu 22.04(LTS)에 ROS 2 Humble를 설치하는 단계입니다\n\n## 1. 로캘 설정\n\n\n\n```js\n로케일  # UTF-8 확인\n\nsudo apt update \u0026\u0026 sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nlocale  # 설정 확인\n```\n\n## 2. 소스 설정\n\nROS 2 apt 저장소를 추가하려면 Ubuntu Universe 저장소가 활성화되어 있는지 확인하세요.\n\n```js\nsudo apt install software-properties-common\nsudo add-apt-repository universe\n```\n\n\n\nROS 2 GPG 키를 apt에 추가해주세요.\n\n```js\nsudo apt update \u0026\u0026 sudo apt install curl -y\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\n```\n\n그런 다음 리포지토리를 소스 목록에 추가해주세요.\n\n```js\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release \u0026\u0026 echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list \u003e /dev/null\n```\n\n\n\n## 3. ROS 2 패키지 설치\n\n저장소를 설정한 후에는 apt 저장소 캐시를 업데이트하세요. 이는 ROS 2 패키지를 설치하기 전에 시스템이 최신 상태임을 보장합니다.\n\n```sh\nsudo apt update\nsudo apt upgrade\n```\n\n이제 ROS 2를 설치하는 두 가지 옵션이 있습니다.\n\n\n\n데스크톱 설치 (권장):\n\n- ROS, RViz, 데모 및 튜토리얼이 포함되어 있습니다.\n- ROS 개발을 위한 완전한 데스크톱 환경을 제공합니다.\n\n```bash\nsudo apt install ros-humble-desktop\n```\n\nROS-Base 설치 (최소 설치):\n\n\n\n- 통신 라이브러리, 메시지 패키지, 명령줄 도구를 포함합니다.\n- GUI 도구는 포함되어 있지 않으며 미니멀한 설정에 적합합니다.\n\n```js\nsudo apt install ros-humble-ros-base\n```\n\n저희는 Ground Control 및 Simulation System(저희의 데스크탑/노트북)에 \"ros-humble-desktop\"를 설치하고 Robot Computer(Raspberry Pi)에 \"ros-humble-ros-base\"를 설치할 것입니다.\n\n마지막으로 개발 도구를 설치합니다: 컴파일러 및 다른 ROS 패키지를 빌드하기 위한 도구들을 포함합니다.\n\n\n\n```js\nsudo apt install ros-dev-tools\n```\n\n## 4. 환경 설정\n\nROS 2에서 작업을 시작하려면 먼저 각 터미널 세션에서 설정 스크립트를 소스로 지정해야 합니다.\n\n```js\n# 만약 bash를 사용하지 않는다면 shell에 맞게 \".bash\"를 대체하세요\n# 가능한 값은: setup.bash, setup.sh, setup.zsh\nsource /opt/ros/humble/setup.bash\n```\n\n\n\n## 5. 몇 가지 예제를 시도해보세요\n\n참고: 예제는 \"데스크톱 설치\"에만 포함되어 있습니다.\n\nTalker-listener\n\n하나의 터미널에서 설정 파일을 소스로 실행한 후 Python talker를 실행하세요:\n\n\n\n```js\n/opt/ros/humble/setup.bash을 소스로 지정\nros2 run demo_nodes_py talker\n```\n\n다른 터미널에서 설정 파일을 소스로 지정한 다음 Python 리스너를 실행하세요:\n\n```js\nsource /opt/ros/humble/setup.bash\nros2 run demo_nodes_py listener\n```\n\n이렇게 보입니다:\n\n\n\n\n![image](/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png)\n\nStop both scripts using Ctrl+C.\n\n## 6. Bonus step!\n\nTo automate the environment setup process and avoid sourcing the setup file manually each time, we can add the command to source the setup file in the \".bashrc\" file. This way, the command will be executed automatically every time we open a new terminal or SSH session.\n\n\n\n\n여기서 .bashrc 파일을 편집하는 방법입니다.\n\n```js\nnano ~/.bashrc\n```\n\n그리고 파일 끝에 명령어 `source /opt/ros/humble/setup.bash` 를 추가해주세요. \n\n![이미지](/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_1.png)\n\n\n\n# 다음은 무엇일까요?\n\n다음 기사에서는 작업 공간(workspaces) 및 노드(nodes)와 같은 개념을 다룰 예정입니다. 작업 공간은 ROS 2 패키지를 구성하고 빌드하는 데 필수적이며, 노드는 ROS 2에서 계산을 수행하는 개별 프로세스입니다. 이러한 개념을 이해하는 것은 ROS 2 개발 여정을 진행할 때 중요할 것입니다.\n\n지금까지 잘 따라오셨다면, 그것은 단순히 관심을 가졌다는 것이 아니라, 헌신적이신 것입니다. 저희는 여러분을 여기에 맞이할 수 있어 기쁩니다!\n\n여러분의 관심과 참여는 우리에게 더 많은 콘텐츠를 만들고, 우리의 열정을 공유하는 로봇학 학습자들, 커뮤니티 및 기술 애호가들과 지식을 공유하게 하는 열정을 불어넣습니다.\n\n\n\n우리와 함께 이 여정에 참여해 주셔서 감사합니다. ROS 2를 함께 탐험하며 더 많은 유익한 기사, 튜토리얼 및 실용적인 예시를 엿보기 위해 기다려주세요.\n\n# 추가 읽을거리\n\n- ROS 2 겸손한 설치","ogImage":{"url":"/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png"},"coverImage":"/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png","tag":["Tech"],"readingTime":6},{"title":"유혹적인 입술을 위한 DIY 설탕 입술 스크럽","description":"","date":"2024-05-16 04:10","slug":"2024-05-16-DIYSugarLipScrubforLusciousLips","content":"\n\n![이미지](/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png)\n\n당기는 맛있는 대접으로 입술을 관리하고 싶나요? 이 쉬운 DIY 설탕 입술 스크럽을 만들어 보세요! 몇 가지 간단한 재료만 있으면 부드럽고 매력적인 입술을 만들어주는 호화로운 입술 관리 제품을 만들 수 있어요!\n\n# 재료:\n\n# 시작해 봅시다!\n\n\n\n1. 작은 그릇에 갈색 설탕, 흰 설탕, 그리고 코코넛 오일을 섞어주세요. 잘 섞이도록 저어주세요.\n\n2. 바닐라 추출물을 넣어 스크럽 전체에 고르게 섞여지도록 해주세요.\n\n3. 스크럽을 깨끗하고 밀폐용기에 담아 저장하세요.\n\n# 왜 설탕 립 스크럽을 사용해야 하죠?\n\n\n\n민감한 피부를 부드럽고 매끈하게 만들어 주는 갈색 설탕과 흰 설탕을 함께 사용해 보세요.\n\n코코넛 오일이 깊은 보습을 제공하여 입술을 촉촉하고 갈라지지 않게 유지합니다.\n\n이 스크럽으로 입술을 마사지하면 혈액 순환이 촉진되어 임시적으로 입술을 풍부하게 보이게 할 수 있습니다.\n\n맛있는 바닐라 향으로 입술에 섬세하게 관리해 보세요.\n\n\n\n## 사용 방법\n\n적용: 손가락에 스크럽의 작은 양을 떠서 발라주세요.\n\n마사지: 입술에 부드럽게 마사지해주세요. 약 30초에서 1분 동안 원을 그리듯 원을 그리듯 마사지하세요.\n\n헹구기: 스크럽을 촉촉한 천으로 닦거나 미지근한 물로 헹굽니다.\n\n\n\n즐기세요: 매끈하고 은은한 달콤함을 느껴보세요. 새로 닦아낸 입술의 시끄러운 부드러움에 감탄할 거예요.\n\n![DIY Sugar Lip Scrub for Luscious Lips](/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_1.png)\n\n입술에 사랑을 주세요. 이 DIY 설탕 입술 스크럽으로 입술을 케어해보세요. 부드럽고 매끈해지는 느낌이 있을 뿐만 아니라, 아름다운 바닐라 향이 입술을 핥고 있기 어렵게 만들 거예요. 이 스크럽을 미용 루틴에 포함시켜서, 풍부하고 키스할 수 있는 입술을 얻어보세요. 어떤 상황에서나 손꼽을만한 입술이 준비될 거예요!","ogImage":{"url":"/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png"},"coverImage":"/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png","tag":["Tech"],"readingTime":2},{"title":"라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기","description":"","date":"2024-05-16 04:09","slug":"2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi","content":"\n\n![RaspbianOSAGatewaytotheWorldofRaspberryPi_0](/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png)\n\n라즈베리 파이는 싱글 보드 컴퓨터의 MVP로서 접근성 있는 컴퓨팅 세계를 혁신했습니다. 하드웨어 및 소프트웨어를 만지작거리기 위해 디자인된 이 제품은 다재다능함과 그를 구동하는 견고한 운영 체제인 Raspberry Pi OS(이전 Raspbian)에서 성공을 거두었습니다.\n라즈베리 파이 OS를 살펴보겠습니다!\n\n## 라즈베리 파이 OS란?\n\n라즈베리 파이 OS는 라즈베리 파이 하드웨어를 위해 특별히 최적화된 데비안 기반 리눅스 운영 체제입니다. 초기에는 Raspbian이라 불리는 독립 프로젝트로 개발되었지만, 라즈베리 파이 재단에 의해 공식 OS로 채택 및 유지보수되었습니다.\n하지만, 기다려 보세요! 라즈베리 파이 OS는 라즈베리 파이만을 위한 것이 아닙니다!\n라즈베리 파이 데스크톱을 포함한 Raspberry Pi OS도 있습니다. 이는 PC 및 Mac용 운영 체제로, Raspberry Pi OS 데스크톱 및 추천 소프트웨어를 함께 제공합니다.\n\n\n\n## 주요 기능\n\n- 사용자 친화적 데스크톱: Raspberry Pi OS는 Windows 또는 Mac OS와 유사한 시각적 사용자 인터페이스(GUI)를 제공합니다.\n- 사전 설치된 소프트웨어 번들: 개발자를 위한 터미널, Thonny Python IDE, Raspberry Pi 구성 도구 등과 같은 필수 도구가 미리 설치되어 있어 매우 편리합니다. 또한 LibreOffice Suite, PDF 뷰어, 이미지 뷰어, Chromium 웹 브라우저와 같은 생산성 소프트웨어도 미리 설치되어 있습니다.\n- 사용자 정의: 사용자는 운영 체제의 모양과 느낌에 대해 높은 통제권을 갖습니다. 테마를 조정하거나 필요한 소프트웨어 및 패키지를 설치하거나 제거하고 전체 OS를 사용자의 특정 요구에 맞게 조정할 수 있습니다.\n- 성능 최적화: Raspberry Pi OS는 Raspberry Pi 하드웨어의 잠재력을 최대한 활용하기 위해 필요한 소프트웨어와 하드웨어 드라이버로 섬세하게 설계되었습니다. 이는 제한된 리소스에도 상대적으로 부드러운 경험을 제공합니다.\n- 정기적인 업데이트: Raspberry Pi 재단은 정기적으로 Raspberry Pi OS의 업데이트와 개선 사항을 출시하여 새로운 기능을 추가하고 버그를 해결하며 보안을 유지합니다.\n\n## Raspberry Pi OS 설치 및 시작하기\n\nRaspberry Pi 하드웨어와 함께 Raspberry Pi OS를 사용하려면 OS 이미지를 메모리 카드에 플래시해야 합니다. Raspberry Pi OS를 설치하는 가장 쉬운 방법은 공식 Raspberry Pi Imager 소프트웨어를 사용하는 것입니다.\n\n\n\n- 이미지 다운로드: 라즈베리 파이 웹 사이트에 방문하여 컴퓨터용 이미지 도구를 다운로드하세요.\n- SD 카드 준비: 호환 가능한 SD 카드를 컴퓨터에 삽입하세요.\n- 라즈베리 파이 이미저 실행: 이미저를 실행하고, 운영 체제로 라즈베리 파이 OS를 선택하고, SD 카드를 선택한 후 \"쓰기\" 버튼을 눌러주세요.\n\n![이미지](/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_1.png)\n\n설치가 완료되면 SD 카드를 라즈베리 파이에 삽입하고 모니터, 키보드, 마우스를 연결하여 전원을 켜세요. 처음 초기 설정을 안내 받을 수 있습니다.\n\n설치 후에는 터미널, Thonny Python IDE, 라즈베리 파이 구성 도구를 포함한 라즈베리 파이 OS(라스비안)에서 모든 것을 살펴보고 실험할 수 있습니다. GPIO, UART, ADC 또는 CSI와 DSI 인터페이스를 탐험하고 DIY 및 메이커 프로젝트의 세계로 들어가 보세요!","ogImage":{"url":"/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png"},"coverImage":"/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png","tag":["Tech"],"readingTime":2}],"page":"29","totalPageCount":99,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"29"},"buildId":"6w6Yg3qJxLtqeXNguENru","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>