<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>allround-coder</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///posts/70" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="allround-coder" data-gatsby-head="true"/><meta property="og:title" content="allround-coder" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///posts/70" data-gatsby-head="true"/><meta name="twitter:title" content="allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-ec7535a55e788b31.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/t9N7vwmpvBMQnO2PSctoH/_buildManifest.js" defer=""></script><script src="/_next/static/t9N7vwmpvBMQnO2PSctoH/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="통계적 파워와 파워 분석 개요" href="/post/2024-05-17-APrimeronStatisticalPowerandPowerAnalysis"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="통계적 파워와 파워 분석 개요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-APrimeronStatisticalPowerandPowerAnalysis_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="통계적 파워와 파워 분석 개요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">통계적 파워와 파워 분석 개요</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="새로운 langchain_huggingface 라이브러리 만들면서 배우기" href="/post/2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="새로운 langchain_huggingface 라이브러리 만들면서 배우기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="새로운 langchain_huggingface 라이브러리 만들면서 배우기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">새로운 langchain_huggingface 라이브러리 만들면서 배우기</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="CodeLlama vs CodeGemma, AI 코딩 어시스턴스에 오픈 모델 활용하기" href="/post/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="CodeLlama vs CodeGemma, AI 코딩 어시스턴스에 오픈 모델 활용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="CodeLlama vs CodeGemma, AI 코딩 어시스턴스에 오픈 모델 활용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">CodeLlama vs CodeGemma, AI 코딩 어시스턴스에 오픈 모델 활용하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="대기 시간을 통해의 신비로운 여행" href="/post/2024-05-17-AWhimsicalJourneyThroughWaitTimes"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대기 시간을 통해의 신비로운 여행" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대기 시간을 통해의 신비로운 여행" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">대기 시간을 통해의 신비로운 여행</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">20<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AWS Glue로 다수의 CSV 파일을 처리하는 ETL 단계별 팁" href="/post/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AWS Glue로 다수의 CSV 파일을 처리하는 ETL 단계별 팁" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AWS Glue로 다수의 CSV 파일을 처리하는 ETL 단계별 팁" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">AWS Glue로 다수의 CSV 파일을 처리하는 ETL 단계별 팁</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Node에서 안정적인 분산 시스템 구축하는 방법" href="/post/2024-05-17-BuildingReliableDistributedSystemsinNode"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Node에서 안정적인 분산 시스템 구축하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-BuildingReliableDistributedSystemsinNode_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Node에서 안정적인 분산 시스템 구축하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">Node에서 안정적인 분산 시스템 구축하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Nodejs에서 perf_hooks를 사용한 벤치마킹 점수" href="/post/2024-05-17-BenchmarkinginNodejswithperf_hooks"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Nodejs에서 perf_hooks를 사용한 벤치마킹 점수" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-BenchmarkinginNodejswithperf_hooks_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Nodejs에서 perf_hooks를 사용한 벤치마킹 점수" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">Nodejs에서 perf_hooks를 사용한 벤치마킹 점수</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Nest JS에서 Redis 사용하는 방법" href="/post/2024-05-17-UsingRedisinNestJS"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Nest JS에서 Redis 사용하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-UsingRedisinNestJS_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Nest JS에서 Redis 사용하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">Nest JS에서 Redis 사용하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="JavaScript에서 이벤트 기반 API를 Promises로 적용하기" href="/post/2024-05-17-BuildingaSyncBridge"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="JavaScript에서 이벤트 기반 API를 Promises로 적용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-BuildingaSyncBridge_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="JavaScript에서 이벤트 기반 API를 Promises로 적용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">JavaScript에서 이벤트 기반 API를 Promises로 적용하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Redis를 활용한 NodeJs에서 이벤트 주도 시스템 사용하기" href="/post/2024-05-17-UsingDistributedLockingwithRedisinNodeJsinanEvent-DrivenSystem"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Redis를 활용한 NodeJs에서 이벤트 주도 시스템 사용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-UsingDistributedLockingwithRedisinNodeJsinanEvent-DrivenSystem_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Redis를 활용한 NodeJs에서 이벤트 주도 시스템 사용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">Redis를 활용한 NodeJs에서 이벤트 주도 시스템 사용하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link posts_-active__YVJEi" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"통계적 파워와 파워 분석 개요","description":"","date":"2024-05-17 20:47","slug":"2024-05-17-APrimeronStatisticalPowerandPowerAnalysis","content":"\n\n만약 내 경험이 너의 것과 비슷하다면, 너도 일할 때 '통계적 파워'에 대해 다양한 사람들이 이야기하는 것을 들어본 적이 있을 거야. 대부분의 경우, 이들 사람들은 더 큰 표본 크기를 주장하면서 보통 더 많은 n이 항상 좋다는 모호한 개념을 기반으로 이야기하는 것 같아.\n\n하지만 이들 중 얼마나 많은 사람이 실제로 '통계적 파워'가 무엇인지 정의할 수 있는지 알고 있을까? 이 기사에서는 통계적 파워의 개념과 정의를 살펴보고 이것이 측정 수단으로 어디에 유용한지 확인해보려고 해.\n\n## 가설 검정\n\n‘통계적 파워’라는 용어는 가설 검정을 할 때만 의미가 있어. 아마도 네가 기억할 수 있듯이, 가설 검정은 데이터 샘플의 통계적 특성을 사용해 그 샘플이 추출된 전체 모집단에 대한 진술의 확신 수준을 결정하는 것을 포함해. 예를 들어보자. 사람들 분석 데이터 R 패키지의 세일즈인 사원들의 데이터 셋은 기술 회사의 샘플 세일즈인들의 데이터를 포함하고 있어, 이들의 연간 매출액(천 달러)과 최근에 증가하는 순서 척도의 평가 등급을 포함하고 있어. 처음 몇 행을 살펴보자.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```R\nlibrary(peopleanalyticsdata)\nsalespeople \u003c- salespeople[complete.cases(salespeople), ]\nhead(salespeople)\n\n##          promoted sales customer_rate performance\n## 1        0        594   3.94          2\n## 2        0        446   4.06          3\n## 3        1        674   3.83          4\n## 4        0        525   3.62          2\n## 5        1        657   4.40          3\n## 6        1        918   4.54          2\n```\n\n이제 이 문장을 살펴봅시다. 최상위 성과 영업 사원의 평균 매출액은 전체 인구의 최하위 성과 영업 사원들과 다를 수 있다는 것입니다. 이 문장을 검증하기 위해, 우리는 최상위 성과자와 최하위 성과자의 평균 매출액이 동일하다고 가정하고, 이를 귀무 가설이라고합니다. 그런 다음 귀무 가설이 전체 인구에서 사실일 때 샘플이 보이는 방식의 최대 확률을 설정하기 위해 테스트를 수행하고, 이를 테스트의 p값이라고합니다. 이 경우, 균등한 분산을 가진 두 샘플을 비교하기 위해 Welch의 t-테스트를 수행합니다.\n\n```R\n# 최상위 성과자의 매출\nsales4 \u003c- salespeople$sales[salespeople$performance == 4]\n\n# 최하위 성과자의 매출\nsales1 \u003c- salespeople$sales[salespeople$performance == 1]\n\n# 두 평균이 동일하다는 귀무 가설의 p값\nt.test(sales4, sales1)$p.value\n\n## 1.093244e-05\n```\n\n위 결과는 만일 우리의 귀무 가설이 전체 인구에서 사실이라면, 우리의 샘플이 보이는 방식은 매우 잘 나타나지 않을 가능성이 있다는 것을 의미합니다. 우리는 귀무 가설을 기각하기로 합의하는 확률 수준을 정의하고, 이를 알파로 알려집니다. 종종 알파 값은 0.05이지만, 때로는 더 낮을 수도 있습니다. 여기서 알파 값을 0.05로 설정하면, 귀무 가설을 편안하게 기각하고 대립 가설을 결론 내리게 됩니다 — 즉, 인구에서 낮은 성과자와 높은 성과자 간 평균 매출액에 차이가 있다는 것입니다. 알파 값을 0.05로 선택함으로써, 평균적으로 20번 중 1회 틀린 결론을 내리게 될 것이라는 것을 유의하십시오. 가설 검정은 확률에 관한 것이며, 확신에 관한 것이 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 통계적 유의성 정의\n\n우리는 가설 검정이 우리가 존재하는 모집단의 차이를 결론 내릴만큼 충분히 확신하는 수준에 대한 것임을 알 수 있습니다. 이는 우리가 그 모집단의 샘플만을 관측할 수 있다는 것을 인정하는 것입니다. 본질적으로 미 관측 모집단에 대해 100% 확실한 것은 없으며, 따라서 네 가지 경우가 발생할 수 있습니다:\n\n- 귀무 가설이 모집단에 대해 사실이고, 그것이 샘플을 기반으로 기각되지 않음\n- 귀무 가설이 모집단에 대해 사실이고, 그것이 샘플을 기반으로 기각됨 (1종 오류)\n- 귀무 가설이 모집단에 대해 부정하고, 그것이 샘플을 기반으로 기각되지 않음 (2종 오류)\n- 귀무 가설이 모집단에 대해 부정하고, 그것이 샘플을 기반으로 기각됨\n\n통계적 유의성은 4번과 관련이 있습니다 — 이는 모집단에 대해 거짓이라는 것이 주어졌을 때 샘플을 기반으로 귀무 가설이 기각될 확률입니다. 직관적으로, 이는 샘플의 크기, 실제(미관측) 모집단의 차이(적절히 정규화된), 그리고 귀무 가설을 기각하는 확신의 수준(알파)에 따라 달라집니다. 예를 들어 실제 모집단의 차이가 더 클 경우, 더 작은 샘플에서 확인할 수 있습니다. 알파가 작을 경우, 더 큰 모집단 차이나 더 높은 'n'이 필요할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방에 있는 코끼리는 물론 우리는 인구 차이를 결코 알 수 없을 것입니다. 우리는 우리 샘플의 차이만을 알고 있습니다. 따라서 보통 우리는 우리 샘플에서 관측된 통계적인 파워를 만족시키고 자신을 다스립니다. 여기서 우리의 영업사원 예시를 들어 보면, 이것이 t-검정이기 때문에 우리는 Cohen의 효과 크기 d를 정규화된 관측된 차이로 사용합니다. 이를 샘플 크기와 알파 0.05와 결합하여 우리의 가설 검정을 위한 통계적인 파워를 0.996로 계산할 수 있습니다. 우리는 귀무가설이 정확하게 기각될 것이라고 매우 확신할 수 있습니다.\n\n```js\nlibrary(effectsize)\nlibrary(WebPower)\n\n# sample sizes\nn4 \u003c- length(sales4)\nn1 \u003c- length(sales1)\n\n# cohen's effect size d\nd \u003c- cohens_d(sales4, sales1)$Cohens_d\n\n# statistical power\nwp.t(n4, n1, d = d, type = \"two.sample.2n\")\n\n\n## Unbalanced two-sample t-test\n##\n## n1 n2         d alpha    power\n## 55 60 0.8741483 0.05     0.996347\n```\n\n## 통계적인 파워를 사용하는 경우\n\n솔직히 말해서 그렇게 자주 사용되지는 않습니다. 당신이 샘플들과 데이터들을 가지고 이미 가설 테스트를 진행한 상황에서, 통계적인 파워는 실제로 단지 얼마나 잘 알파 바를 넘어섰는지를 나타내는 지표일 뿐입니다. 알파가 덜 엄격할수록 파워가 높아집니다. 한번 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```r\nlibrary(ggplot2)\n\n# 통계적 파워\ntest \u003c- WebPower::wp.t(n4, n1, d = d, type = \"two.sample.2n\", \n                       alpha = seq(0.05, 0.0001, by = -0.0001))\n\ntest_df \u003c- data.frame(\n  Alpha = test$alpha,\n  Power = test$power\n)\n\n\n# 알파에 대한 파워 플롯\nggplot(test_df, aes(x = Alpha, y = Power)) +\n  geom_point(color = \"pink\") +\n  theme_minimal()\n```\n\n\u003cimg src=\"/assets/img/2024-05-17-APrimeronStatisticalPowerandPowerAnalysis_0.png\" /\u003e\n\n샘플 데이터를 가져오지 않았거나 가설 검정을 수행하지 않았다면, 실험이나 연구를 계획 중이고 많은 작업이 필요한 경우 통계적 파워는 도움이 될 수 있습니다. 샘플 크기가 역할을 하기 때문에 이론적으로 특정 알파 기준을 달성하기 위한 최소 샘플 크기를 계산할 수 있습니다.\n\n하지만 실제로는 관측된 효과 크기를 알아야 하는데, 물론 아직 실험을 실행하지 않았기 때문에 알 수가 없습니다. 따라서 통계적 파워 계산에서 나오는 대부분의 샘플 크기 추정은 민감도 범위의 형태를 취하는 경향이 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실험은 조직하고 자원을 조달하기 어려울 수 있으며, 통계적 파워는 필요한 규모를 결정하는 데 도움이 될 수 있습니다. 또한 샘플 크기를 테스트할 때 중요한 부분에서 추가적인 n이 파워에 큰 영향을 미치지 않는 지점이 있는지 보여줄 수도 있습니다. 예를 들어, 중간 효과 크기와 알파 0.05를 가진 쌍체 t-테스트에서 여러 샘플 크기 범위를 시험하면, 추가적인 n이 파워에 큰 차이를 만들지 않는 시점을 볼 수 있습니다.\n\n```js\n# 여러 샘플 크기를 테스트\nsample_sizes \u003c- 20:100\npower \u003c- wp.t(n1 = sample_sizes, d = 0.5, type = \"paired\")\n\npower_df \u003c- data.frame(\n  n = power$n,\n  Power = power$power\n)\n\n# 샘플 크기에 따른 파워 플롯\nggplot(power_df, aes(x = n, y = Power)) +\n  geom_point(color = \"lightblue\") +\n  theme_minimal()\n```\n\n\u003cimg src=\"/assets/img/2024-05-17-APrimeronStatisticalPowerandPowerAnalysis_1.png\" /\u003e\n\n전반적으로, 통계적 파워는 총명치인 도구입니다. 이것은 주로 실험 디자인과 관련된 특정 상황에서만 유용한 가설 검정의 '볼트온'으로 생각할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 통계적 역량 및 검정 및 회귀 모델에서 사용되는 다양한 통계에 대해 더 자세히 탐구하고 싶다면, People Analytics의 Handbook of Regression Modeling의 11장을 확인해보세요.\n\n![이미지](/assets/img/2024-05-17-APrimeronStatisticalPowerandPowerAnalysis_2.png)","ogImage":{"url":"/assets/img/2024-05-17-APrimeronStatisticalPowerandPowerAnalysis_0.png"},"coverImage":"/assets/img/2024-05-17-APrimeronStatisticalPowerandPowerAnalysis_0.png","tag":["Tech"],"readingTime":6},{"title":"새로운 langchain_huggingface 라이브러리 만들면서 배우기","description":"","date":"2024-05-17 20:46","slug":"2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment","content":"\n\n\u003cimg src=\"/assets/img/2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment_0.png\" /\u003e\n\n# 배경\n\n최근에 Langchain과 HuggingFace가 함께 새로운 파트너 패키지를 발표했습니다. Langchain은 이미 커뮤니티에서 유지보수되는 HuggingFace 패키지를 보유하고 있었지만, 이 새로운 버전은 HuggingFace가 Langchain의 파트너로 공식 지원하는 것입니다! Langchain은 다양한 LLM과 상호 작용하기 위한 공통 인터페이스를 제공하며, HuggingFace는 오픈 소스 모델을 포함한 호스팅된 LLM에 대한 추론 엔드포인트를 제공합니다.\n\n이 블로그에서는 HuggingFace의 오픈 소스 모델의 추론을 이 새로운 Langchain 라이브러리로 사용하는 내 경험을 공유하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# TL;DR\n\n직접 시도해 보고 싶다면, 아래 저장소를 클론해보세요:\n\n# 실험\n\n## HuggingFace를 통한 추론 옵션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nHuggingFace에서 추론을 수행하는 세 가지 방법이 제공됩니다:\n\n- UI를 통해 직접: 각 모델에 대한 채팅 위젯이 제공됩니다. Meta의 LLAMA 모델과 같은 목록에서 모델을 선택할 수 있습니다.\n- (무료) 추론 API (서버리스): 이 옵션은 최소한의 테스트에 적합합니다. HuggingFace의 공유 인프라를 사용하므로 요율 제한이 적용됩니다. API 키로 계정 설정에서 액세스 토큰을 사용합니다. 이 옵션을 사용하여 Langchain 라이브러리를 시도해 볼 것입니다.\n- (유료) 추론 엔드포인트 (전용 API): 제품 사용에 적합하지만, 이번 실험에서는 배포하고 이 옵션을 사용하지 않을 것입니다.\n\n## Langchain_HuggingFace 라이브러리\n\n이 라이브러리는 HuggingFace LLMs와 상호 작용하기 위해 두 가지 클래스를 노출합니다: HuggingFacePipeline 및 HuggingFaceEndpoint.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 원격 추론을 가능하게 하는 HuggingFaceEndpoint를 사용하는 것에 관심이 있습니다. 이 클래스의 내부에서는 InferenceClient를 사용합니다. 특정 모델의 경우, 해당 모델의 HuggingFace 페이지(예: Meta의 LLAMA)에서 해당 모델의 약관에 동의해야 사용할 수 있습니다. HuggingFacePipeline은 모델을 로컬로 다운로드해야 하기 때문에 특정 이유가 없는 이상 이상적이지 않습니다.\n\nHuggingFaceEndpoint 클래스를 인스턴스화한 후, 몇 가지 langchain.schema 메시지를 정의합니다. 이 라이브러리에서 또 한 가지 중요한 클래스는 ChatHuggingFace 클래스인데, 이는 특정 모델에 따라 특별 토큰으로 프롬프트를 향상시킵니다. 또한 사용된 토큰과 같은 모델 메타데이터를 응답에 추가하여 Langchain이 약속한 응답의 일관성을 보장합니다.\n\n이 실험을 위해 작성한 코드를 확인해보세요!\n\n## 전반적인 인상\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n라이브러리는 작업 중인 것 같아서 전반적인 경험은 원활하지 않았어요. 여기 몇 가지 구체적인 문제가 있었어요:\n\n- 오래된 독스트링: IDE의 클래스 독스트링이 최신으로 업데이트되지 않았어요.\n\n![2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment_1.png](/assets/img/2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment_1.png)\n\n2. 불완전한 문서화: Langchain의 문서가 최신으로 업데이트되지 않아서 아마도 Langchain v0.2에서 업데이트 예정일 것 같아요. 그들의 공지를 따라서 사용하면 분명히 작동하지 않을 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 비기능적 매개변수: 몇 가지 매개변수는 모델 응답에 영향을 미치지 않거나 오류를 발생시킵니다.\n\n```js\nllm = HuggingFaceEndpoint(\n    repo_id=LLAMA_INSTRUCT,  # endpoint_url을 사용하는 경우 model_id도 제공해야 함. ChatHuggingFace에서 model_id는 repo_id만큼 영향을 미침\n    task=\"text-generation\",\n    streaming=True,\n    max_new_tokens=1024,  # 출력 길이에 영향을 주는 것 같지 않음\n    model=\"\",  # 이 필드는 필수이지만 출력에는 영향을 미치지 않음, repo_id만 영향 있음\n    client=None,\n    async_client=None,\n    return_full_text=True,\n    repetition_penalty=1.1,\n    cache=False,\n    do_sample=False,\n)\n```\n\n# 결론 및 가능한 향후 작업\n\n그들의 발표는 다음과 같이 마무리되었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 실험이 가치 있는 피드백으로 작용하기를 희망하며, Langchain 저장소에 이슈를 만들 계획입니다. 이 실험을 통해 HuggingFace의 무료 OSS LLM 추론 및 그 Langchain 통합 라이브러리 상태에 대해 더 나은 이해를 얻을 수 있었습니다.\n\n향후 실험에서는 이 예시를 따라 에이전트를 만들고자 합니다. 제 다음 블로그 포스트를 기대해 주세요!","ogImage":{"url":"/assets/img/2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment_0.png"},"coverImage":"/assets/img/2024-05-17-ExploringtheNewlangchain_huggingfacelibraryAHands-OnExperiment_0.png","tag":["Tech"],"readingTime":3},{"title":"CodeLlama vs CodeGemma, AI 코딩 어시스턴스에 오픈 모델 활용하기","description":"","date":"2024-05-17 20:44","slug":"2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance","content":"\n\n\u003cimg src=\"/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_0.png\" /\u003e\n\nAI 코딩 도구 시장은 수십억 달러의 산업입니다. 2030년까지 172억 달러에 이를 것으로 예상되며, 현재에도 VS Code 또는 JetBrains IDE용 AI 플러그인은 수백만 번 다운로드되었습니다. 하지만 무료 코딩 도우미로 로컬 모델을 실행할 수 있을까요? 그리고 그 성능은 어떨까요? 이 기사에서는 두 개의 오픈 모델, Code Gemma와 Code Llama를 테스트해 보겠습니다. 제 PC에 설치하고, 그들이 어떻게 작동하는지 확인할 것입니다.\n\n더 이상의 말이 필요 없으니, 시작해 봅시다!\n\n## 1. 모델들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n본문 작성 시점에서 코딩 목적으로 두 가지 주요 오픈 모델이 무료로 다운로드할 수 있으며 사용할 수 있습니다:\n\n- CodeLlama. 이 모델은 2023년 Meta에서 출시되었으며, 7B, 13B, 34B, 70B 크기로 제공됩니다. \"Base\", \"Instruct\", \"Python\" 모델을 사용할 수 있습니다. 4가지 크기이지만, 로컬에서 실제로 사용할 수 있는 것은 7B 및 13B 모델뿐입니다; 다른 크기는 너무 \"무겁습니다.\"\n- CodeGemma. 이 모델은 2024년 Google에서 출시되었으며, 2B 및 7B 크기로 제공됩니다. 2B 모델은 코드 완성을 위해 훈련되었으며, 7B 모델은 코드 채움 및 자연어 프롬프트를 위해 훈련되었습니다.\n\n본문에서는 HuggingFace에서 제공되며 GGUF 형식으로 다운로드할 수 있는 7B 및 13B 모델을 테스트할 것이며, 이를 사용하여 다양한 앱에서 이 모델들을 사용할 수 있도록 OpenAI 호환 로컬 서버를 실행할 것입니다. 그러나 이를 수행하기 전에 단순히 모델을 Python으로 실행하여 무엇을 할 수 있는지 살펴보겠습니다. 실제 사용으로 넘어가고 싶은 독자분들은 이 부분을 건너뛸 수 있습니다.\n\n두 모델을 테스트하기 위해 Google Colab 인스턴스를 무료로 사용할 것입니다. 먼저, 모델과 토크나이저를 로드해보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport transformers\nimport torch\n\n\nmodel_id = \"...\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=False,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"cuda\",\n    torch_dtype=torch.bfloat16,\n)\n```\n\nHuggingFace의 Transformers 라이브러리는 모델 파일을 자동으로 다운로드해줍니다. 7B 모델은 약 16.2 GB의 GPU RAM을 필요로 하지만, bits and bytes 라이브러리를 활용하여 4비트 해상도로 모델을 실행하면 필요한 메모리 용량은 약 5GB 정도로 줄어듭니다.\n\n이제 모델을 테스트하기 위한 코드 조각을 만들어 봅시다. 예를 들어, 문자열 목록을 파일에 작성하는 Python 메서드를 작성해보겠습니다:\n\n```python\npython_code = \"\"\"\nclass Writer:\n   def write_file(self, filename: str, data: List[str]):\n        \\\"\\\"\\\" Write list of strings to a text file \\\"\\\"\\\"\n        with open(filename, 'w') as f_out:\n            for line in data:\n                f_out.write(f\"{line}\\n\")\n\"\"\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델의 코딩 능력을 테스트하기 위해, 두 모델에게 \"pytest\"를 만들도록 요청해보겠습니다:\n\n```js\nchat = [{\n    \"role\": \"user\",\n    \"content\": f\"이 파이썬 메소드에 대한 pytest를 작성해주세요:\\n{python_code}. \"\\\n               f\"테스트가 끝나면 생성된 파일을 삭제하세요.\"\n    }]\n\n\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=1024)\nresult = tokenizer.decode(outputs[0])\n```\n\n결과적으로, CodeLlama 7B가 이 코드를 생성했고, 이 과정은 19초가 걸렸습니다:\n\n```js\nimport pytest\n\n\nclass TestWriter:\n    def test_write_file(self):\n        writer = Writer()\n        filename = 'test.txt'\n        data = ['line1', 'line2', 'line3']\n        writer.write_file(filename, data)\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n            assert lines == data\n        os.remove(filename)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCodeGemma이 이 코드를 생성했고, 프로세스에는 16초가 걸렸어요:\n\n```js\nimport pytest\n\n\ndef test_write_file():\n    \"\"\" write_file 메소드를 테스트함 \"\"\"\n    filename = \"test.txt\"\n    data = [\"This is a test\", \"line 2\", \"line 3\"]\n    Writer().write_file(filename, data)\n\n    with open(filename, \"r\") as f:\n        assert f.read() == \"This is a test\\nline 2\\nline 3\\n\"\n\n    import os\n    os.remove(filename)\n```\n\n개인적으로, 저는 두 번째 버전을 선호해요. 첫째, CodeGemma가 메소드의 설명을 나타내는 docstring을 제공했고, 이는 현대적인 \"linter\" 도구의 요구 사항이에요. 둘째, Writer().write_file(...) 코드는 writer 변수를 선언하고 나중에 사용하는 것보다 더 간결하고 가독성이 좋아 보여요. 셋째, CodeGemma는 \"os\" 파이썬 모듈을 가져왔는데, CodeLlama는 이를 \"잊어버렸어요\".\n\n첫눈에는 두 코드 조각이 모두 올바르게 보여요. pytest -v file.py 명령을 실행하여 코드를 실행해 보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_1.png\" /\u003e\n\n실제로 두 테스트의 정확성에 대해 잘못 이야기 했었고, 첫 번째 테스트에 버그가 있습니다. 재미있게도, 두 번째 테스트는 뿐만 아니라 더 나은 모습을 하고 있으며, 작동하기도 합니다. 그 반면 첫 번째는 작동하지 않습니다. 스크린샷에서 오류는 명백합니다. 독자들은 자신의 힘으로 어떻게 수정할지 찾아보세요.\n\n처음에는 CodeGemma 2B \"코드 완성\" 모델을 테스트할 계획이 없었지만, 독자들을 위한 추가 혜택으로 해보자구요! 모델을 로드하는 방법은 동일합니다. 오직 모델 ID만 바꾸면 됩니다:\n\n```js\nmodel_id = \"google/codegemma-2b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, ...)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 코드 완성을 위해 훈련된 모델입니다. 영어 설명이 없어도 되며, 소스 코드만 제공하면 됩니다:\n\n```js\n# Prompt\npython_code = \"\"\"\nclass Writer:\n   def write_file(self, filename: str, data: List[str]):\n      ...\n\nimport pytest\n\ndef test_write_file():\n    \\\"\\\"\\\"\\ Test the write_file method \\\"\\\"\\\"\n\"\"\"\n\nprompt = f\"\"\"\n\u003c|fim_prefix|\u003e{python_code}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 볼 수 있듯이 해당 코드는 \"그대로 사용\"되지 않을 것입니다. 하지만 논리는 올바른 것으로 보입니다. 필요한 수정은 assert 라인을 올바르게 포맷하는 것입니다:\n\n```js\nassert lines == [\"Hello\\n\", \"World\\n\"]\n```\n\n이후에 \"pytest\"가 통과되었습니다. 모델은 테스트 이후 파일을 제거하지 않았지만, 나는 프롬프트에서 그것을 요청하지 않았습니다. 마지막으로, 소형 모델의 실행 시간은 단지 3.3초로, 더 큰 모델과 비교했을 때 약 5배 빠릅니다.\n\n## 2. 람마 서버 실행\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 파이썬에서 모델을 테스트했고, 이제 로컬 OpenAI 호환 서버를 실행해볼 차례입니다. 이를 위해 Llama-cpp-python을 사용할 거예요. 이 프로젝트는 멋지고 가벼워요. 한 줄의 명령어로 우리가 원하는 어떤 모델이든 실행할 수 있어요:\n\n```js\n# 코드 Gemma\npython3 -m llama_cpp.server --model codegemma-7b-it-Q4_K_M.gguf --n_ctx 8192 --n_gpu_layers -1 --host 0.0.0.0 --port 8000\n\n# 코드 Llama 7B\npython3 -m llama_cpp.server --model codellama-7b-instruct.Q4_K_M.gguf --n_ctx 8192 --n_gpu_layers -1 --host 0.0.0.0 --port 8000\n\n# 코드 Llama 13B\npython3 -m llama_cpp.server --model codellama-13b-instruct.Q4_K_M.gguf --n_ctx 8192 --n_gpu_layers -1 --host 0.0.0.0 --port 8000\n```\n\n모델을 로드할 GPU RAM이 충분하지 않으면, n_gpu_layers 매개변수를 변경하여 GPU에 일부 레이어만 로드할 수 있어요. 또한 Apple Silicon이나 심지어 CPU에서 모델을 실행할 수도 있지만 물론 느릴 거예요.\n\n## 3. 앱들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 로컬 OpenAI 호환 서버가 있으며 몇 가지 앱을 테스트할 준비가 되어 있습니다!\n\n### 3.1 AI Shell\n\nAI Shell은 자연어 프롬프트를 콘솔 명령어로 변환할 수 있는 오픈 소스 앱입니다. 이 앱은 꽤 인기가 있으며 작성 당시 프로젝트는 GitHub에서 3.6K개의 스타를 받았습니다. AI Shell은 TypeScript로 작성되었으며 npm 패키지 관리자를 통해 이 앱을 설치할 수 있습니다 (저는 여기서 Node JS 20.13.0도 설치했습니다):\n\n```js\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\nnvm install v20.13.0\nnpm install -g @builder.io/ai-shell\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앱을 실행하기 전에 API 엔드포인트를 구성해야 합니다:\n\n```js\nai config set OPENAI_KEY=12345678\nai config set OPENAI_API_ENDPOINT=http://127.0.0.1:8000/v1\n```\n\n이제 콘솔에서 \"ai chat\" 명령을 입력하여 언제든지 모델과 대화를 시작할 수 있습니다:\n\n![대화 모델](https://miro.medium.com/v2/resize:fit:1400/1*9zJpuyFx_-HW4AZ4b9ZH8A.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로그램을 사용하는 또 다른 방법은 실행하려는 명령어를 입력하는 것입니다. 예를 들어, \"현재 폴더에 있는 파일 표시\"와 같은 내용을 입력할 수 있어요:\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*4PElpWscaef11mHRzCdZ5Q.gif)\n\n안타깝게도 무료 7B 모델로는 작동하지 않았고, 모델이 올바른 쉘 명령어를 생성하지 못했어요. 또한 프롬프트 안에 있는 \"스크립트\"라는 단어가 모델을 혼란스럽게 만들었고, 영화 대본과 관련된 텍스트를 생성했어요.\n\n이 문제는 아마도 프롬프트를 조정하여 해결할 수 있겠죠. 그러나 이 텍스트를 작성할 때에는 프롬프트가 TypeScript 소스에 하드코딩되어 있어 쉽게 구성할 수 없었어요. 아직까지 GitHub에서 제 기능 제안에 응답한 사람이 없지만, 향후 개선될 것을 희망해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3.2 ShellGPT\n\nShellGPT는 이 텍스트를 작성하는 시점에서 GitHub에서 8.3K개의 스타를 가진 또 다른 흥미로운 오픈소스 프로젝트입니다. 우리는 다음과 같이 pip를 사용하여 쉽게 응용 프로그램을 설치할 수 있습니다:\n\n```js\npip3 install shell-gpt\n```\n\n로컬 모델과 함께 ShellGPT를 사용하려면 ~/.config/shell_gpt/.sgptrc 파일에서 API 엔드포인트를 변경해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nAPI_BASE_URL=http://127.0.0.1:8000/v1\nOPENAI_API_KEY=12345678\n```\n\n그럼 이제 우리는 이전 앱과 거의 같은 방식으로 터미널 쉘에 직접 요청을 입력할 수 있어요:\n\n```js\nsgpt \"로컬 파일을 표시하는 명령어를 작성해주세요\"\n```\n\n안타깝게도, CodeGemma 모델은 ShellGPT에서 작동하지 않았고, LlamaCpp 서버는 Server 500 오류를 반환했어요: '시스템 역할이 지원되지 않음'. 처음에는 LlamaCpp 문제인 줄 알았지만 로그를 확인한 후에는 모델 메타데이터에 이런 라인이 있는 것을 보았어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n{ if messages[0]['role'] == 'system' }\n  { raise_exception('시스템 역할은 지원되지 않습니다')\n```\n\n코드젬마가 \"시스템\" 역할을 지원하지 않는 것은 안타깝습니다. 왜냐하면 OpenAI API에서 널리 사용되기 때문입니다. 따라서 OpenAI 호환 앱은 코드젬마를 사용할 수 없습니다. 이전에 보았던 것처럼, 코드젬마가 생성한 코드는 꽤 좋았기 때문에 아쉽습니다.\n\n코드람마에 대한 셸GPT는 잘 작동합니다:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*N6gwsFM7ZNt7OW2sZcaNZg.gif)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n터미널 셸에서 '—shell' 접두어를 지정하여 명령을 직접 실행하는 기능이 편리합니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*xTTNTI0Ykh8NGuqkIpwLVg.gif)\n\n더 개선할 공간이 있습니다. 예를 들어, \"문서 폴더의 크기 표시하기\" 프롬프트에 대한 du -sh ~/Documents 응답이 반환됩니다. 이것은 올바른 bash 명령어입니다. 그러나 ShellGPT는  문자열에서 해당 명령을 가져오지 못했고 \"명령을 찾을 수 없음\" 오류만 받았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nbash 명령어를 사용하는 것도 유용하지만, 실제 코딩 지원은 어떨까요? 오픈소스 CodeGPT 플러그인을 통해 이를 할 수 있어요. 먼저, PyCharm IDE에 플러그인을 설치하고 LlamaCpp와 함께 사용할 수 있도록 설정했어요:\n\n![CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance](/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_2.png)\n\n예를 들어, 다음과 같은 Python 클래스를 고려해봅시다:\n\n```js\nclass ServerConnection:\n    \"\"\" Server connection handling \"\"\"\n\n    def __init__(self):\n        self.is_connected = False\n        self.connection_time = -1\n        self.uploads_total = 0\n        self.reconnects_total = 0\n        self.reconnect_threshold_sec = 64\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 모델에게 변수를 따로 Python 데이터 클래스로 리팩터링하도록 요청할 것입니다.\n\n결과적으로 CodeGemma는 이를 수행하지 못했으며 \"시스템 역할을 지원하지 않음\"이라는 오류가 발생했습니다. CodeLlama 7B는 작업을 완료할 수 없었고 대신에 데이터 클래스 대신 표준 클래스를 생성했습니다. 반면에 CodeLlama 13B는 잘 수행했습니다:\n\n![이미지](/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_3.png)\n\n다음 단계로, 더 복잡한 내용을 요청하고 텍스트 필드와 버튼 프롬프트가 있는 UI Python 애플리케이션을 만들어보았습니다. Llama 13B 모델이 이 코드를 생성했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport tkinter as tk\n\n# 메인 창 생성\nroot = tk.Tk()\nroot.title(\"Hello World\")\nroot.geometry(\"320x200\")\n\n# 텍스트 필드 생성\ntext_field = tk.Entry(root)\ntext_field.pack()\n\n# 버튼 생성\nbutton = tk.Button(root, text=\"Click Me!\", command=lambda: print(\"You clicked the button!\"))\nbutton.pack()\n\n# 메인 루프 시작\nroot.mainloop()\n``` \n\n코드는 올바르지만, 애플리케이션 창이 보이지 않았습니다. 크기가 지정되지 않았습니다. 나는 모델에게 제목을 \"Hello World\"로 변경하고 창 크기를 320x200으로 설정하도록 요청했습니다: \n\n\u003cimg src=\"/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_4.png\" /\u003e\n\n결과가 적절하게 나와 요청한 애플리케이션이 예상대로 작동했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_5](/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_5.png)\n\n저는 13B 모델이 완벽하지 않다는 것을 인정해야합니다. 이론적으로는 큰 컨텍스트 창과 이전 채팅 결과를 사용해야 하지만, 제가 모델에게 생성된 코드를 클래스로 이동하도록 요청했을 때 창 크기나 제목을 설정하지 않은 새로운 코드를 생성했습니다:\n\n```js\nimport tkinter as tk\n\nclass HelloWorld(tk.Frame):\n    def __init__(self, master=None):\n        super().__init__(master)\n        self.pack()\n\n        # 텍스트 필드 생성\n        self.text_field = tk.Entry(self)\n        self.text_field.pack()\n\n        # 버튼 생성\n        self.button = tk.Button(self, text=\"Click Me!\", command=lambda: print(\"Button clicked!\"))\n        self.button.pack()\n\n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    app = HelloWorld(root)\n    root.mainloop()\n```\n\n하지만 일반적으로 말하자면, 모델이 정확한 클래스를 생성했으며 조금의 복사 붙여넣기로 작업을 완료하는 것이 쉬웠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4. 단점\n\n지금까지 모든 예시를 통해 모델이 작동하는 것을 확인할 수 있습니다; 코드와 bash 명령을 모두 생성할 수 있습니다. 그러나 몇 가지 단점과 문제점도 있습니다:\n\n- 로컬 LLM 인스턴스를 사용하려면 좋은 그래픽 카드가 필요합니다. 저는 2.5년 전에 구매한 8GB GPU RAM을 갖춘 GeForce RTX 3060 카드를 사용하고 있습니다. Colab 테스트에서는 8 GB가 7B 모델을 실행하는 데 충분하다는 것을 확인했지만, 실제 데스크탑에서는 그 용량이 부족했습니다. OS 자체도 일부 GPU를 필요로 하기 때문입니다. 실제로 13B 모델을 실행하려면 적어도 16 GB의 GPU RAM이 필요하며, 미래 개선을 위한 여유 공간으로 24 GB가 필요합니다. 현실적으로 고려할만 한가요? 현재 GPU 가격을 고려할 때, 1000-1500달러에는 AI 구독을 여러 년간 할 수 있습니다.\n- 오픈 소스 앱은 완벽하지 않습니다. 제 테스트에서 LlamaCpp 서버는 때로 \"segmentation fault\"와 함께 충돌하고, CodeGPT 앱은 때로는 모델에 요청을 전송하지 않았고, PyCharm을 재시작해야 했고 등등 발생했습니다. 이것은 오픈 소스이며 어떤 종류의 보장도 없으므로 불평할 것이 아니지만, 이러한 AI 도구들에 대해서는 아직 \"초기 채택\" 단계에 있다는 것을 인정해야 합니다.\n- 또한 대형 로컬 언어 모델 실행은 에너지를 많이 소비하는 작업입니다. 마지막 테스트로 내 데스크톱 PC에 전력계를 연결했습니다. 평상시에는 약 80 와트를 소비하는 것으로 나타났습니다. 하지만 LLM 요청이 실행될 때는 에너지 소비량이 거의 3배 증가합니다: \n\n![이미지](/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 결론\n\n본 기사에서는 오픈 언어 모델이 코딩 어시스턴트로서 작동하는 능력을 테스트하였고, 결과는 흥미로웠습니다:\n\n- 작은 7B 및 13B 모델조차도 리팩토링, 단위 테스트 생성 또는 작은 코드 템플릿 작성과 같은 일부 코딩 작업을 수행할 수 있습니다. 물론, 이러한 모델들은 175B ChatGPT 3.5와 같이 큰 모델에 비해 능력이 떨어지지만, 로컬 모델을 사용하는 것은 구독 비용이 필요하지 않을 뿐만 아니라, 개인 정보 관점에서 빠르고 효율적일 수도 있습니다.\n- 반면에 로컬 모델을 실행하려면 고사양의 하드웨어가 필요하며, 이는 비용 부담뿐만 아니라 에너지 소모도 초래할 수 있습니다. 본 기사 작성 시, 고사양 GPU는 최대 $1500에 이를 수 있으며, 이는 로컬 LLMs만 실행하기에는 현실적이지 않습니다 — 해당 비용으로 클라우드 서비스 구독을 매우 오랜 기간 동안 이용할 수 있습니다.\n- AI 도구를 사용하는 도전 과제는 하드웨어뿐만 아니라 소프트웨어에도 있습니다. 최소한 본 게시물 작성 시점에는 AI 소프트웨어의 오픈 소스 생태계가 아직 미성숙한 것으로 나타났습니다. HuggingFace에서 39,769개의 오픈 7B 모델을 발견했으나 GitHub에서의 오픈 소스 AI 앱 수는 미미합니다. 이 기사에서 설명한 3가지가 거의 제가 찾아낸 전부였습니다 (만약 놓친 것이 있다면, 아래 댓글에 쓰거나, 추가 리뷰를 진행할지도 모릅니다).\n\n일반적으로 일상적인 코딩 작업에 로컬 LLM을 사용하는 것은 가능하지만, 소프트웨어와 하드웨어 모두에서 여전히 많은 도전 과제가 있음을 알 수 있습니다. 더 나은 AI 칩 및 효율적인 모델을 위해 노력하고 있는 다른 기업들이 있음도 알고 있습니다. Microsoft의 Phi-3와 같은 새로운 모델은 이제 모바일 하드웨어에서도 작동할 수 있습니다. 그것이 AI 산업을 어떻게 바꿀지 어떻게 알 수 있을까요? 다음 세대의 통합 그래픽 카드는 저렴하고 조용하며 CUDA 호환될 것인가요? 아직 모릅니다. 분명히 새로운 AI 관련 하드웨어가 발표될 것이며 (M4가 이미 첫 번째였습니다), 적어도 오픈 사용을 위한 드라이버 없이 독점적인 새 하드웨어가 되지 않기를 희망합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어 주셔서 감사합니다. 이야기가 마음에 드셨다면 Medium에 구독해보세요. 그러면 새 기사가 발행될 때 알림을 받을 수 있을 뿐만 아니라 수천 편의 다른 작가들의 이야기에도 완전한 접속 권한을 얻을 수 있습니다. 또한 LinkedIn을 통해 연락하실 수도 있습니다. 거기에서는 전체 기사로 충분치 않은 작은 포스트를 주기적으로 발행하고 있습니다. 이번 포스트와 다른 포스트의 전체 소스 코드를 원하신다면 Patreon 페이지를 방문해보세요.\n\n자연어 처리와 언어 모델을 사용하는 것에 관심이 있는 분들은 다른 논문들도 읽어보세요:\n\n- GPT 모델: 어떻게 작동합니까?\n- 16, 8 및 4비트 부동 소수점 형식 - 어떻게 작동합니까?\n- 대규모 언어 모델로 판다 데이터프레임 처리하기\n- 주말 AI 프로젝트 (제1부): 라즈베리 파이에서 음성 인식 및 LLaMA-2 GPT 실행\n- 주말 AI 프로젝트 (제2부): 음성 인식, PTT 및 라지 액션 모델을 라즈베리 파이에서 사용하기\n- 주말 AI 프로젝트 (제3부): 시각 장애인을 위한 시각 보조 도구 만들기","ogImage":{"url":"/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_0.png"},"coverImage":"/assets/img/2024-05-17-CodeLlamavsCodeGemmaUsingOpenModelsforAICodingAssistance_0.png","tag":["Tech"],"readingTime":14},{"title":"대기 시간을 통해의 신비로운 여행","description":"","date":"2024-05-17 20:39","slug":"2024-05-17-AWhimsicalJourneyThroughWaitTimes","content":"\n\n## 파이썬을 사용하여 전자레인지 카운트다운부터 끝나지 않는 전화 대기 시간까지\n\n![image](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_0.png)\n\n전자레인지 오븐의 카운트다운이 빠르게 0으로 수렴하는 것을 본 적이 있나요? 반면 전화 대기시간은 영원처럼 늘어날까요?\n\n한가지 생각해 보세요. 포플콘을 전자레인지에 넣어 가열한 지 겨우 1분 지난 때에는 그릇을 준비하고 서빙할 준비를 합니다. 하지만 전화 대기 중에 1분이 지난다면? 다시 사람과 대화를 나눌 수 있을지 의문이 들 정도입니다. 10분 후, 포플콘을 즐기는 중이겠죠. 하지만 전화는? 대기 음악이 끝도 없는 연옥의 배경음악이 되고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 팝콘을 기다리는 사이와 전화 대기를 이어가는 서사 속을 맴도는 … 주간 복권. 승리를 기다립니다. 매주 새로운 티켓은 이전 주의 실망과는 거리가 먼 신선한 약속을 간직하고 있습니다.\n\n요약하자면, 세 가지 다른 종류의 대기가 나타납니다:\n\n- “대기 전화”형 — 기다린 시간이 오래 될수록 더 오랫동안 기다릴 것으로 기대합니다.\n- “팝콘”형 — 기다린 시간이 길어질수록 더 짧게 기다릴 것으로 기대합니다.\n- “복권 당첨”형 — 지금까지 기다린 것과 관계없이 예상 대기 시간은 변하지 않습니다.\n\n이 대기 시간의 차이는 실제로 존재하는 것일까요, 아니면 마음의 장난일까요? 이 질문에 대한 대답은 두 부분으로 나누어 알아보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 부분 1 — 데이터 분석\n- 부분 2 — 데이터 모델링\n\n각 부분에서 대기 시간 유형을 각각 살펴보겠습니다. 자세한 Python 코드와 설명이 번갈아 나옵니다. Python에 관심이 있다면 코드 부분을 읽어보세요. 대기 시간에 대해 배우고 싶다면 코드를 건너뛰어도 됩니다.\n\n# \"대기 중\" 유형 대기 시간 — 기다린 시간이 길수록 더 오래 기다리게 됩니다.\n\n데이터로 시작하고 싶지만 \"대기 중\" 시간에 대한 데이터가 없습니다. 대신 컴퓨터 파일의 편집 사이의 시간에 대해서 어떠세요? 그런 편집 시간을 보는 곳 한 곳이 바로 위키피디아입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위키피디아 페이지에서 마지막 편집 이후의 시간을 보고 다음 편집까지 얼마나 남았는지 예측할 수 있을까요?\n\n위키피디아 페이지 편집에 대한 다음 편집까지의 시간을 어떻게 예측할 수 있을까요? 다음 편집이 언제 발생할지 정확히 예측해 보세요: \"저는 이 페이지가 정확히 5일 3시간 20분 후에 편집될 것으로 예측합니다.\" 하지만 그렇게 구체적으로 예측하는 것은 너무 정확성이 떨어질 것입니다.\n\n시간 범위를 예측할 수도 있습니다: \"저는 이 페이지가 다음 100년 이내에 언제든지 편집될 것으로 예측합니다.\" 이렇게 하면 거의 항상 맞을 수 있겠지만, 너무 모호하고 흥미롭지 않습니다.\n\n더 실용적인 예측은 \"중위 다음 편집 시간\"의 형태입니다. 이렇게 말할 수 있습니다: \"저는 이 페이지가 다음 5일 3시간 20분 이내에 50% 확률로 편집될 것으로 예측합니다.\" 저, 당신의 적,는 \"이전\" 또는 \"이후\"를 선택할 것입니다. 만약 실제 중위 다음 편집 시간이 3일이라고 가정하면, \"이전\"을 선택할 것입니다. 그럼 우리는 최대 5일 3시간 20분까지 기다립니다. 그 동안 누군가(다시 말해서, 우리 둘을 제외한 누군가) 페이지를 편집하면 상대방이 점수를 획들하고, 그렇지 않으면 당신이 점수를 획득합니다. 이러한 점수 체계를 통해, 만약 제가 당신보다 더 좋은 예측자라면 더 많은 점수를 획득해야 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬에 대해 알아보고 이러한 예측을 어떻게 할 수 있는지 살펴봅시다:\n\n## “대기 중” 유형의 대기 시간 — Python\n\n아티스트 Marie Cochran에 관한 위키피디아 문서를 살펴보겠습니다. 문서의 개정 내역을 살펴볼 수 있습니다:\n\n![image](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 위키피디아 문서에서 데이터를 수집하기 위해 작은 파이썬 스크립트를 작성했어요. 다음과 같은 작업을 합니다:\n\n- https://en.wikipedia.org/wiki/Special:Random을 통해 랜덤한 영어 위키백과 페이지를 선택합니다.\n- 해당 페이지의 편집 이력으로 이동합니다. 예를 들어, https://en.wikipedia.org/w/index.php?title=Marie_Cochran\u0026action=history.\n- (최대) 최근 50회 편집의 날짜와 시간을 추출합니다. 시간은 분 단위로 표시됩니다.\n- 문서 제목, 수정 시간, 스크립트 실행 시간으로 구성된 줄을 생성합니다. 모든 시간은 UTC 시간대를 사용합니다. 탭으로 열을 구분합니다.\n- 줄을 파일에 추가합니다.\n\n편집 시간 데이터 일부를 보여드리겠습니다:\n\n```js\nMarie_Cochran 01:20, 8 January 2024 01:16, 08 February 2024\nMarie_Cochran 01:10, 27 September 2023 01:16, 08 February 2024\nMarie_Cochran 00:59, 12 September 2023 01:16, 08 February 2024\nMarie_Cochran 11:43, 2 November 2022 01:16, 08 February 2024\n...\nMarie_Cochran 19:20, 10 March 2018 01:16, 08 February 2024\nPeter_Tennant 15:03, 29 July 2023 01:16, 08 February 2024\nPeter_Tennant 21:39, 15 April 2022 01:16, 08 February 2024\n...\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport pandas as pd\n\n# 데이터 읽기\nwiki_df = pd.read_csv(\"edit_history.txt\", sep='\\t', header=None, names=[\"Title\", \"Edit DateTime\", \"Probe DateTime\"], usecols=[\"Title\", \"Edit DateTime\"])\nwiki_df['Edit DateTime'] = pd.to_datetime(wiki_df['Edit DateTime']) # 텍스트를 날짜 및 시간으로 변환\n\n# 'Title' 및 'Edit DateTime'을 기준으로 DataFrame 정렬하여 시간 간격이 올바르게 계산되도록 함\nwiki_df.sort_values(by=['Title', 'Edit DateTime'], inplace=True)\n\n# 동일한 제목 내에서 연속해서 편집한 경우의 시간 간격 계산\nwiki_df['Time Delta'] = wiki_df.groupby('Title')['Edit DateTime'].diff()\nwiki_df.head()\n```\n\n결과로 나온 Pandas 데이터프레임은 샘플된 기사 중 알파벳상으로 가장 빠른 기사(제목 기준)로 시작합니다. 이 기사는 몽골 출신인 매우 키가 큰 사람 인 Öndör Gongor에 대해 독자들에게 알려줍니다:\n\n![image](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_2.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 기사의 마지막 50개의 편집 중 첫 번째 편집은 2008년 1월 27일 오후 3시 13분 (UTC)에 이루어졌습니다. 다음 편집은 16분 후에 이루어졌습니다. 그 다음 편집은 데이터의 해상도 한계로 인해 1분 내로 발생하여 0일 00:00:00으로 표시됩니다.\n\n계속 처리하면, 각 기사 맨 처음에 나타나는 NaT (not-a-time) 행을 제거해 보겠습니다. 또한 대기 시간에 따라 정렬하고 판다의 인덱스를 재설정할 것입니다:\n\n```js\n# 'Time Delta' 열에서 NaT(시간이 아님) 값이 포함된 행 제거\nwiki_df.dropna(subset=['Time Delta'], inplace=True)\n# 시간 간격으로 정렬 및 인덱스 재설정\nwiki_df.sort_values(by='Time Delta', inplace=True)\nwiki_df.reset_index(drop=True, inplace=True)\ndisplay(wiki_df)\nwiki_df['Time Delta'].describe()\n```\n\n이를 통해 다음과 같이 시작하고 끝나는 데이터프레임이 생성됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 통계 요약입니다.\n\n```js\ncount                          36320\nmean      92 days 13:46:11.116189427\nstd      195 days 11:36:52.016155110\nmin                  0 days 00:00:00\n25%                  0 days 00:27:00\n50%                 15 days 05:41:00\n75%                100 days 21:45:45\nmax               4810 days 17:39:00\n```\n\n조사 결과, 샘플링된 대기 시간은 0일 00:00:00(즉, 1분 미만)부터 13년 이상까지 다양합니다. (13년 편집 대기는 버지니아 대학교의 건물에 관한 기사였습니다.) 편집의 1/4은 이전 편집 후 27분 이내에 발생합니다. 편집 간 중위값은 약 15일을 조금 넘습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n조금 더 발전하기 전에, 웨이팅 시간을 향상시키고 싶은데요. 다음과 같이 작은 함수를 사용해서 웨이팅 시간을 표시할 수 있습니다:\n\n```js\ndef seconds_to_text(seconds):\n    seconds = round(seconds)\n    result = []\n    for unit_name, unit_seconds in [('y', 86400 * 365.25),('d', 86400),('h', 3600),('m', 60),('s', 1)]:\n        if seconds \u003e= unit_seconds:\n            unit_value, seconds = divmod(seconds, unit_seconds)\n            result.append(f\"{int(unit_value)}{unit_name}\")\n    return ' '.join(result) if result else \"\u003c1s\"\n\nseconds_to_text(100)\n```\n\n위의 `seconds_to_text` 함수는 100초를 `1m 40s`로 표시합니다.\n\n이제 위키피디아 데이터를 위한 \"웨이팅 테이블\"을 만들 수 있습니다. 기존에 기사의 다음 편집을 기다린 시간을 주면, 이 테이블은 중간 추가로 기다려야 할 시간을 알려줍니다. (\"중간값\"은 이 시간보다 덜 기다릴 확률이 50%이고, 시간이 더 걸리는 확률이 50%라는 것을 의미합니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport numpy as np\n\ndef wait_wait_table(df, wait_ticks):\n    sorted_time_deltas_seconds = df['Time Delta'].dt.total_seconds()\n    results = []\n    for wait_tick in wait_ticks:\n        greater_or_equal_values = sorted_time_deltas_seconds[sorted_time_deltas_seconds \u003e= wait_tick]\n        median_wait = np.median(greater_or_equal_values)\n        additional_wait = median_wait - wait_tick\n        results.append({\"Wait So Far\": seconds_to_text(wait_tick), \"Median Additional Wait\": seconds_to_text(additional_wait)})\n    return pd.DataFrame(results)\n\nwiki_wait_ticks = [0, 60, 60*5, 60*15, 3600, 3600*4, 86400, 86400 * 7,86400 * 30, 86400 * 100, 86400 * 365.25, 86400 * 365.25 * 5, 86400 * 365.25 * 10]\nwiki_wait_tick_labels = [seconds_to_text(wait_tick) for wait_tick in wiki_wait_ticks]\nwait_wait_table(wiki_df, wiki_wait_ticks).style.hide(axis=\"index\")\n```\n\n이제 이 표의 출력에 대해 알아보겠습니다.\n\n## \"대기 중\" 유형의 대기 - 토론\n\n앞의 파이썬 코드는 이 표를 생성합니다. 이것을 \"대기-대기\" 표라고 부르죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_4.png)\n\n만약 아무도 기다리지 않았다면(다시 말해, 누군가가 페이지를 편집했다) 다음 편집은 15일이 넘게 기다릴 것으로 예상됩니다. 그러나 1분 후에도 누군가 기사를 편집하지 않았다면, 19일을 기다려야 할 것으로 예상됩니다. 따라서 1분 기다리면 예상 추가 대기 시간이 거의 4일 더 늘어납니다. 한 시간 후에도 누구도 기사를 편집하지 않았다면, 예상 추가 대기 시간은 47일로 두 배 넘게 늘어납니다.\n\n이 현상을 생각하는 한 가지 방법은 다음 편집을 기다리기 시작할 때 우리가 어떤 종류의 페이지에 있는지 모르는 것입니다. 이것이 테일러 스위프트와 같은 핫 팝컬쳐 주제의 기사인가요? 아니면 5000명 대학의 건물인 '로턴다(The Rotunda)'와 같은 니치하고 느린 주제인가요? 수정이 일어나지 않는 매 분이 지날수록, 확률은 이것이 테일러 스위프트와 같은 기사에서 '로턴다(The Rotunda)'와 같은 기사로 이동합니다.\n\n마찬가지로, 고객 서비스에 전화하고 대기시간이 발생할 때 - 처음에는 어떤 종류의 고객 서비스를 기다리고 있는지 모릅니다. 그러나 매 분이 지날 때마다, 우리는 서서히 나쁜, 느린 고객 서비스를 기다리고 있다는 것을 알게 됩니다. 따라서 예상 추가 대기 시간은 늘어납니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지는 데이터를 직접 사용했습니다. 데이터를 확률 분포로 모델링해 볼 수도 있습니다. 그러나 모델링으로 넘어가기 전에 다른 두 예제인 마이크로파 팝콘 요리와 복권 당첨을 살펴보겠습니다.\n\n# \"팝콘\"형 기다림 - 기다릴수록 덜 기다리는 것을 기대합니다.\n\n위키피디아 편집을 기다리는 기법을 마이크로파 팝콘 조리를 기다리는 것에 적용해 봅시다. (매력적일지도 모르는) 실제 데이터를 수집하는 대신 모의 데이터를 시뮬레이션하는 것으로 만족합니다. 난수 생성기를 사용할 것입니다. 요리 시간은 센서를 기반으로 하는 것이라 가정하며, 5분에서 15초 차이가 날 수 있다고 가정합니다.\n\n## \"팝콘\"형 기다림 - 파이썬\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬에서 특히:\n\n```python\nseed = 0\nrng = np.random.default_rng(seed)\nsorted_popcorn_time_deltas = np.sort(rng.normal(5*60, 15, 30_000))\npopcorn_df = pd.DataFrame(pd.to_timedelta(sorted_popcorn_time_deltas, unit=\"s\"), columns=[\"Time Delta\"])\nprint(popcorn_df.describe())\n```\n\n이 코드는 다음과 같은 통계 요약이 포함된 판다 데이터프레임을 생성합니다:\n\n\n                      Time Delta\ncount                      30000\nmean   0 days 00:05:00.060355606\nstd    0 days 00:00:14.956424467\nmin    0 days 00:03:52.588244397\n25%    0 days 00:04:50.011437922\n50%    0 days 00:04:59.971380399\n75%    0 days 00:05:10.239357827\nmax    0 days 00:05:59.183245298\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예상대로, 이 정규 분포에서 데이터를 생성할 때 평균은 5분이고 표준 편차는 약 15초입니다. 우리가 시뮬레이션한 대기 시간은 3분 52초에서 6분까지 범위에 있습니다.\n\n이제 \"대기-대기\" 테이블을 생성할 수 있습니다:\n\n```js\nwait_wait_table(popcorn_df, [0, 10, 30, 60, 2*60, 3*60, 4*60, 5*60]).style.hide(axis=\"index\")\n```\n\n## \"팝콘\" 형태의 대기 시간 — 토론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 \"기다려-기다려\" 소프트웨어는 팝콘 테이블을 아래와 같이 보여줍니다:\n\n![팝콘 대기 시간](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_5.png)\n\n우리의 테이블에 따르면, 처음에는 5분 기다림을 예상합니다. 그리고 10초를 기다린 후에는 추가로 기대되는 대기 시간이 정확히 10초 줄어듭니다 (4분 50초로). 1분을 기다린 후에는 추가 대기 시간이 4분으로 줄어들고, 그러한 식으로 이어집니다. 5분에 이르러서도 추가 대기 시간은 계속해서 줄어들지만 0으로는 안 줄어듭니다.\n\n나중에 데이터 모델링 하는 방법을 보게 될 것입니다. 지금은 복권 당첨을 기다리는 것에 대해 다음으로 살펴봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# “로또 당첨” 스타일 대기 시간 — 지금까지 기다린 시간과는 무관하게, 예상 대기 시간은 동일합니다.\n\n로또 데이터에 대해서는 다시 시뮬레이션된 데이터를 생성하는 것이 편합니다. 워싱턴 주의 로또는 당첨 확률을 1 대 27.1로 제공합니다. (가장 흔한 당첨은 $1 베팅에 $3를 지불합니다.) 100만 주 (약 1만 9천 년) 동안 로또를 플레이하고 당첨 사이의 대기 시간에 대한 데이터를 수집해 봅시다.\n\n## “로또 당첨” 스타일 대기 시간 — 파이썬\n\n우리는 100만 주 동안의 로또 플레이를 시뮬레이션합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n시드 = 0\nrng = np.random.default_rng(시드)\n지난주_당첨 = None\n로또_대기 = []\nfor 주차 in range(1_000_000):\n    if rng.uniform(high=27.1) \u003c 1.0:\n        if 지난주_당첨 is not None:\n            로또_대기.append(주차 - 지난주_당첨)\n        지난주_당첨 = 주차\n정렬된_로또_시간_간격 = np.sort(np.array(로또_대기) * 7 * 24 * 60 * 60)\nlotto_df = pd.DataFrame(pd.to_timedelta(정렬된_로또_시간_간격, unit=\"s\"), columns=[\"시간 간격\"])\nprint(lotto_df.describe())\n```\n\n```js\n                        시간 간격\ncount                        36773\nmean   190 days 08:21:00.141951976\nstd    185 days 22:42:41.462765808\nmin                7 days 00:00:00\n25%               56 days 00:00:00\n50%              133 days 00:00:00\n75%              259 days 00:00:00\nmax             2429 days 00:00:00\n```\n\n우리의 최단 가능한 당첨 간격은 7일입니다. 가장 긴 시뮬레이션된 건조 기간은 6년 이상입니다. 중앙값 대기 시간은 133일입니다.\n\n우리는 \"대기-대기\" 테이블을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nlotto_days = [0, 7, 7.00001,  2*7, 4*7, 183, 365.25, 2*365.25, 5*365.25]\nlotto_waits = [day * 24 * 60 * 60 for day in lotto_days]\nwait_wait_table(lotto_df, lotto_waits).style.hide(axis=\"index\")\n```\n\n## \"로또 당첨\" 스타일 대기 시간 — 토론\n\n여기 \"대기-대기\" 테이블이 있습니다:\n\n\u003cimg src=\"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_6.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 Markdown 형식으로 작성된 텍스트입니다.\n\n테이블에 따르면 복권은 우리가 이기기까지 얼마나 기다렸는지에 신경을 쓰지 않습니다. 우리가 방금 이겼던지 (지금까지 기다린 시간 ` 1초) 아니면 1년 동안 이기지 못했던지, 우리가 다음 승리까지 기다려야 하는 예상 추가 기다림은 대부분 항상 126일부터 133일 사이입니다.\n\n표의 세 항목은 이상할 수 있습니다. 7일과 7일 1초에서 무슨 일이 일어나는지 생각해보세요. 추가 기다림이 126일에서 거의 즉시 133일 정도로 급격히 증가하는 이유는 무엇일까요? 답은 매주 추첨하는 시점에서 승리까지의 최소 기다림이 0일에서 7일로 변경되기 때문입니다. 그리고 5년은 어떻게 되는 걸까요? 5년을 기다린다면 보통 133일이 걸리는 대신 단지 50일만에 승리를 기대할 수 있는 것일까요? 안타깝게도 아닙니다. 오히려 이는 우리 데이터의 한계를 보여줍니다. 데이터에서는 5년을 기다리는 경우를 세 번만 볼 수 있습니다:\n\n```js\nlotto_df[lotto_df[\"Time Delta\"] \u003e pd.to_timedelta(24*60*60 * 365.25 * 5, unit=\"s\")]\n```\n\n\u003cimg src=\"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_7.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n삼 가지 값은 중위수의 노이즈 추정치로 이어집니다.\n\n지금까지 실제 및 모의 데이터에서 본 것을 요약해보면:\n\n- 위키피디아 편집 — 기다릴수록 기대하는 대기 시간이 길어집니다.\n- 팝콘 — 기다릴수록 기대하는 대기 시간이 줄어듭니다.\n- 복권 당첨 — 지금까지의 대기 시간과 관계없이 기대 대기 시간은 동일합니다.\n\n다음 섹션에서는 모델링의 방법과 그 이유에 대해 살펴보겠습니다. 미국 로또 데이터부터 시작하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 부분에서는 대기 시간 예측을 위한 간단한 표현을 찾아보겠습니다. 예측에는 이러한 간소화가 필요하지 않습니다. 우리가 지금까지 만든 것은 경험적 분포라고 불리며 잘 작동합니다. 그러나 더 간단한 표현은 더 편리할 수 있습니다. 또한 다른 종류의 대기를 이해하기 쉽게 비교할 수 있게 해줄 수도 있습니다.\n\n우리는 세 가지 예제를 살펴보면서 진행할 것입니다. 가장 간단한 것부터 시작하여 (복권 당첨) 가장 복잡한 것(Wikipedia 편집)으로 넘어갈 것입니다. 이전과 마찬가지로 Python 코드(건너뛸 수 있는)와 토론 사이를 오가겠습니다.\n\n먼저 대기 시간 데이터프레임에 누적 분포 열을 추가하는 것부터 시작하겠습니다. 이전에 데이터프레임을 시간 딜타로 정렬했음을 기억해주세요.\n\n```python\nwiki_df['CDF'] = wiki_df['Time Delta'].rank(pct=True)\npopcorn_df['CDF'] = popcorn_df['Time Delta'].rank(pct=True)\nlotto_df['CDF'] = lotto_df['Time Delta'].rank(pct=True)\nwiki_df\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCDF 컬럼은 누적 분포 함수(Cumulative Distribution Function)를 나타내며, 가장 짧은 대기 시간에는 0.0에 가까운 값이 있고, 가장 긴 대기 시간에는 1.0이 있습니다. 다시 말해, 각 행의 순위가 분수로 나타난 것입니다. 위키피디아 데이터프레임은 이제 다음과 같습니다:\n\n\n| Time Delta  |  CDF  |\n|-------------|-------|\n| 0 days 00:00:10 | 0.1 |\n| 0 days 00:00:30 | 0.3 |\n| 0 days 00:01:00 | 0.5 |\n| 0 days 00:02:00 | 0.7 |\n| 0 days 00:05:00 | 0.9 |\n| 0 days 00:10:00 | 1.0 |\n\n\n이제 CDF(누적 분포 함수)를 대기 시간 Time Delta(x-축)에 대해 그릴 수 있습니다. 파이썬에서 다음과 같은 플로팅 코드를 사용할 수 있습니다:\n\n```python\nimport matplotlib.pyplot as plt\n\ndef wait_cdf(title, sorted_df, wait_ticks, dist=None, dist_label=None, left=None, right=None, xscale='linear'):\n    wait_seconds = sorted_df['Time Delta'].dt.total_seconds() # x values\n    cdf = sorted_df['CDF'] # y values\n\n    left = left or wait_seconds.min()\n    right = right or wait_seconds.max()\n\n    plt.figure(figsize=(10, 6))\n    plt.title(title + ' 누적 분포 함수(CDF)')\n    plt.plot(wait_seconds, cdf, marker='.', linestyle=\" \", label='경험적인 CDF')\n\n    if dist is not None:\n        dist_x = np.logspace(np.log10(left), np.log10(right), 100) if xscale == 'log' else np.linspace(left, right, 100)\n        dist_y = dist.cdf(dist_x)\n        plt.plot(dist_x, dist_y, label = dist_label)\n\n    plt.xlabel('대기 시간')\n    plt.ylabel('CDF')\n    plt.xscale(xscale)\n    plt.xticks(wait_ticks, [seconds_to_text(wait_tick) for wait_tick in wait_ticks], rotation=45)\n    plt.xlim(left=left, right=right)\n    plt.grid(True, which=\"both\", ls=\"--\")\n    plt.legend(loc='upper left')\n    plt.show()\n\nwait_cdf(\"로또 당첨\", lotto_df, wiki_wait_ticks, xscale='log')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로또 당첨과 대기 시간의 CDF 플롯을 로그 스케일로 표시하였습니다:\n\n![Lottery Wins CDF Plot](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_9.png)\n\n곡선이 간단해 보이니 이에 간단한 곡선을 적합해보려고 합니다. 가장 적합한 곡선은 지수 분포입니다. 이는 대기 시간과 관련된 가장 간단한 일반 함수입니다.\n\nPython의 scipy.stats 패키지를 사용하면 데이터에 지수 곡선을 맞추고 해당 결과 곡선을 Python 객체로 표현하는 것이 쉽습니다. 여기서는 lotto_expon_dist라는 이름으로 이를 표현했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom scipy.stats import expon\n\n_, lotto_e_scale = expon.fit(lotto_df['Time Delta'].dt.total_seconds(), floc=0)\nlotto_expon_dist = expon(scale=lotto_e_scale)\nprint(f\"복권 당첨 지수 중앙값은 {seconds_to_text(lotto_expon_dist.median())} 입니다. 스케일 매개변수는 {seconds_to_text(lotto_e_scale)} 입니다.\")\n```\n\n이 코드는 출력합니다:\n\n복권 당첨 지수 중앙값은 131일 22시간 32분 20초 입니다. 스케일 매개변수는 190일 8시간 21분 입니다.\n\n적합된 곡선의 중앙값은 약 132일로, 경험적인 중앙값인 133일과 근접합니다. 지수곡선을 관행적으로 스케일이라는 단일 숫자로 매개변수화하는데, 이것은 분포의 평균에 해당하지만 평균에서 중앙값을 쉽게 계산하거나 그 반대로 할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로또 당첨금에 대한 경험적 누적 분포(EMCDF) 및 적합 누적 분포(FCDF) 플롯입니다:\n\n```js\nlotto_expon_label = f'ExponentialDistribution(scale={seconds_to_text(lotto_e_scale)})'\nwait_cdf(\"당첨금\", lotto_df, wiki_wait_ticks, dist=lotto_expon_dist, dist_label=lotto_expon_label, xscale='log')\n```\n\n\u003cimg src=\"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_10.png\" /\u003e\n\n둘이 꽤 근접합니다. 왼쪽의 약간의 불일치는 복권 추첨시 모멘트의 즉시 7일 점프에 의해 발생합니다. 이 글에서는 이 작은 불일치를 무시하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리 (모의) 복권 당첨 데이터에 지수 함수가 잘 작동합니다. Popcorn과 Wikipedia 데이터에도 어떻게 작동하는지 살펴봅시다. 다음은 이러한 데이터프레임에 지수 분포를 맞추는 코드입니다.\n\n```js\n_, popcorn_e_scale = expon.fit(popcorn_df['Time Delta'].dt.total_seconds(), floc=0)\npopcorn_expon_dist = expon(scale=popcorn_e_scale)\nprint(f\"Popcorn exponential median is {seconds_to_text(popcorn_expon_dist.median())}\")\npopcorn_expon_label = f'ExponentialDistribution(scale={seconds_to_text(popcorn_e_scale)})'\nwait_cdf(\"Popcorn\", popcorn_df, popcorn_ticks, dist=popcorn_expon_dist, dist_label=popcorn_expon_label, left=10, right=6*60, xscale='linear' )\n\n_, wiki_e_scale = expon.fit(wiki_df['Time Delta'].dt.total_seconds(), floc=0)\nwiki_expon_dist = expon(scale=wiki_e_scale)\nprint(f\"Wiki exponential median is {seconds_to_text(wiki_expon_dist.median())}\")\nwiki_expon_label = f'ExponentialDistribution(scale={seconds_to_text(wiki_e_scale)})'\nwait_cdf(\"Wiki Edits\", wiki_df, wiki_wait_ticks, dist=wiki_expon_dist, dist_label=wiki_expon_label, xscale='log', left=60)\n```\n\n그리고 여기가 그림들입니다:\n\n\u003cimg src=\"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_11.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_12.png)\n\n이런, 이 곡선 맞추기 결과는 정말 최악이네요! 문제는 지수 분포가 \"복권 당첨\"과 유사한 데이터만 모델링한다는 것입니다. 구체적으로 말하면, 대기 시간이 이전 대기 시간에 관계없이 기대 대기 시간이 동일한 경우에 해당합니다. 이전 대기 시간을 무시하는 대기 시간에 대해 좌우되는 경우, 이것이 메모리리스(exponential)이라고 불립니다. 또한 연속 분포 중에서 지수 분포는 유일한 메모리리스 분포입니다.\n\n그렇다면 분포에 메모리가 필요하다면 어떨까요? 다음으로 시도할 수 있는 가장 간단한 분포는 와이블(Weibull) 분포입니다.\n\n와이블 분포는 형태(shape)와 척도(scale) 두 매개변수로 매개화됩니다. 복권 데이터로 시작해 보죠:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nfrom scipy.stats import weibull_min\n\nlotto_shape, _, lotto_w_scale = weibull_min.fit(lotto_df['Time Delta'].dt.total_seconds(), floc=0)\nlotto_weibull_dist = weibull_min(c=lotto_shape,scale=lotto_w_scale)\n\nprint(f\"복권 당첨 위블 중앙값은 {seconds_to_text(lotto_weibull_dist.median())}\")\nlotto_weibull_label = f'WeibullDistribution(shape={lotto_shape:.3},scale={seconds_to_text(lotto_w_scale)})'\nwait_cdf(\"복권 당첨\", lotto_df, wiki_wait_ticks, dist=lotto_weibull_dist, dist_label=lotto_weibull_label, xscale='log')\n\n\n이는 지수함수와 유사한 장착 곡선을 생성합니다. 실제로 형태가 1일때 위블 분포는 지수 분포입니다. 여기서 형태는 1.06입니다.\n\n\u003cimg src=\"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_13.png\" /\u003e\n\n팝콘 데이터에 위블을 적합하려고 하면 무엇이 발생하나요?\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\npopcorn_shape, _, popcorn_w_scale = weibull_min.fit(popcorn_df['Time Delta'].dt.total_seconds(), floc=0)\npopcorn_weibull_dist = weibull_min(c=popcorn_shape, scale=popcorn_w_scale)\nprint(f\"Popcorn Weibull median is {seconds_to_text(popcorn_weibull_dist.median())}\")\npopcorn_df_weibull_label = f'Weibull(shape={popcorn_shape:.3}, scale={seconds_to_text(popcorn_w_scale)})'\nwait_cdf(\"Popcorn\", popcorn_df, popcorn_ticks, dist=popcorn_weibull_dist, dist_label=popcorn_df_weibull_label, left=3*60, right=7*60, xscale='linear')\n```\n\n![Image](/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_14.png)\n\n안전하진 않지만, 이 적합은 지수 함수의 적합보다 훨씬 낫습니다. 모양 모수의 값이 20임을 주목하세요. Weibull의 모양 모수가 1보다 큰 경우 \"대기 시간이 길수록 대기 시간을 기대하는 것이 줄어든다\"를 나타냅니다.\n\n마지막으로, 위키피디아 데이터에 Weibull을 시도해보겠습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nwiki_shape, _, wiki_w_scale = weibull_min.fit(wiki_df['Time Delta'].dt.total_seconds(), floc=0)\nwiki_weibull_dist = weibull_min(c=wiki_shape, scale=wiki_w_scale)\nprint(f\"위키 위불 중앙값은 {seconds_to_text(wiki_weibull_dist.median())}\")\nwiki_df_weibull_label = f'위불(모양={wiki_shape:.3},스케일={seconds_to_text(wiki_w_scale)})'\nwait_cdf(\"위키 편집\", wiki_df, wiki_wait_ticks, dist=wiki_weibull_dist, dist_label=wiki_df_weibull_label, xscale='log', left=60)\n```\n\n\u003cimg src=\"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_15.png\" /\u003e\n\n이 곡선 맞춤은 완벽하지 않지만, 지수함수의 맞춤보다 훨씬 좋습니다. 모양 모수값인 0.292에 주목해보세요. 위불의 모양 모수가 1보다 작을 때는 \"기다린 시간이 길수록 더 기다려야 한다\"는 것을 나타냅니다. 그러나 위불만이 이 특성을 갖고 있는 것은 아닙니다. 이 특성을 갖는 무수히 많은 분포들도 있습니다. 실제로 위키피디아 분포는 이 특성을 갖지만 위불 분포가 아닙니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결론적으로, 당신과 나는 미친 것이 아닙니다(필요에 따라).\n\n우리는 정말 기다린 시간이 길수록 더 기다려야 할 상황이 있는 것을 보았습니다. 위키피디아 편집 사이의 시간 간격에서 경험적으로 확인할 수 있습니다. 또한 Weibull 분포에서 형태 매개변수가 1보다 작은 경우에도 확인할 수 있습니다.\n\n똑같이, 다른 몇 가지 대기 시간에는 \"기다린 시간이 길수록 더 적게 기다리게 된다\"는 규칙이 적용됩니다. 팝콘에서 이 현상을 확인할 수도 있습니다. 또한 Weibull 분포에서 형태 매개변수가 1보다 큰 경우에도 이를 확인할 수 있습니다.\n\n마지막으로, 세 번째 종류의 대기 시간인 \"메모리리스\"도 존재합니다. 이 경우, 지금까지 기다린 시간에 상관없이 기대 대기 시간은 동일합니다. 복권 당첨 간의 시간에서 이를 확인했습니다. 이는 형태 매개변수가 1인 Weibull 분포(지수 분포와 동일)와 관련이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터를 분석할 때 기다릴 데이터가 있는 경우, Weibull 분포를 시도하는 것을 권장합니다. Python을 사용하면 이러한 곡선을 fitting하는 것이 쉽습니다. 그러나 데이터가 Weibull 분포와 잘 맞지 않는 경우에는 Weibull을 사용하지 않는 것이 좋습니다. 대신, 자료 분포를 직접 사용하여 데이터가 스스로 말하도록하십시오.\n\n기다림 시간에 대한 이 여정에 참여해 주셔서 감사합니다. 이제 기다림 시간과 그 분석에 대해 더 잘 이해하게 되었으면 좋겠습니다.\n\n칼을 Medium에서 팔로우해 주세요. 저는 Rust 및 Python에서의 과학적 프로그래밍, 머신러닝 및 통계에 대해 씁니다. 월 한 번 정도 기사를 씁니다.","ogImage":{"url":"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_0.png"},"coverImage":"/assets/img/2024-05-17-AWhimsicalJourneyThroughWaitTimes_0.png","tag":["Tech"],"readingTime":20},{"title":"AWS Glue로 다수의 CSV 파일을 처리하는 ETL 단계별 팁","description":"","date":"2024-05-17 20:37","slug":"2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3","content":"\n\n이건 훌륭한 이미지입니다! 이 디지털 시대에 데이터는 기업에게 귀중한 자산이 되었습니다. 데이터를 효과적으로 처리하고 분석하는 것이 유용한 통찰력을 얻고 스마트한 의사결정을 하는 데 중요합니다. AWS Glue는 데이터를 쉽고 효율적으로 관리하고 분석하는 데 도움이 되는 포괄적인 솔루션이 됩니다.\n\n이 세션에서는 AWS Glue, 데이터 카탈로그, 및 크롤러가 하나의 버킷에 있는 여러 CSV 파일을 단일 데이터 세트로 읽는 방법에 대해 논의할 것입니다. 아래는 아키텍처 요약입니다:\n\n![아키텍처 이미지](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# S3 버킷 준비하기\n\n처리하려는 모든 CSV 파일이 아마존 S3의 단일 버킷에 저장되어 있는지 확인하세요.\n\n![이미지](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_2.png)\n\n# AWS Glue 데이터 카탈로그에서 데이터베이스 생성하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAWS Management Console에서 AWS Glue를 열고, \"데이터베이스\" 섹션으로 이동하여 메타데이터를 저장할 새 데이터베이스를 생성하세요.\n\n![이미지](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_3.png)\n\n# 크롤러 생성\n\n이전에 생성한 데이터베이스를 선택하여 크롤러에 의해 생성된 테이블을 저장하세요. 크롤러를 사용하여 테이블 추가를 선택하세요. 크롤러에 이름을 지정하고 CSV 파일을 포함하는 S3 버킷 위치를 선택하여 데이터 원본을 지정하세요. S3의 데이터 원본에 액세스할 수 있는 IAM 역할을 지정하고 Glue 데이터 카탈로그에 항목을 생성할 수 있는 권한이 있는 IAM 역할을 지정하세요. 필요에 따라 크롤러 옵션을 구성하세요. 크롤러를 주기적으로 실행하려면 빈도를 설정하세요. 구성을 완료한 후 크롤러를 실행하세요. 크롤러는 지정된 버킷의 모든 CSV 파일을 읽고 Glue 데이터 카탈로그에 하나 이상의 테이블을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_4.png)\n\n![Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_5.png)\n\nOnce the crawler status is complete you can preview the table data that has been created using Athena\n\n![Step-by-Step ETL Tips with AWS Glue: Handling Multiple CSV Files from S3](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# AWS Glue에서 ETL 작업 만들기\n\n![ETL image](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_7.png)\n\nAWS Glue은 데이터 변환의 핵심 프로세스인 ETL(추출, 변환, 로드)을 수행하는 다양한 방법을 제공합니다. 시각적 ETL, 주피터, 또는 스크립팅을 통해 가장 적합한 방법을 선택할 수 있습니다.\n\n## 시각적 ETL\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_8.png\" /\u003e\n\n기술적 배경이 없는 분들에게 시각적 ETL은 이상적인 선택지입니다. 직관적인 드래그 앤 드롭 인터페이스를 통해 코드 작성 없이도 ETL 워크플로를 구축할 수 있습니다. 다양한 데이터 원본을 쉽게 연결하고 데이터 변환을 적용하며 처리된 데이터를 원하는 대상에로 로드할 수 있습니다.\n\n여기 AWS Glue로 Data Catalog에서 S3로 시각적 ETL을 구축하는 단계별 안내서가 있습니다.\n\n- AWS Glue Studio에 액세스\n- 새 워크플로 생성\n- 데이터 원본 선택\n- 변환 추가\n- 데이터 대상 선택\n- 작업 구성\n- 작업 검토 및 실행\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Jupyter Notebook\n\n![Image](/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_9.png)\n\n보다 경험 많은 데이터 전문가들에게 Jupyter는 더 많은 유연성과 파워를 제공합니다. Jupyter 노트북을 사용하면 Python 코드와 텍스트, 시각화를 결합하여 복잡한 데이터 분석을 수행할 수 있습니다.\n\n다음은 AWS Glue를 사용하여 Jupyter Notebook을 사용하는 단계입니다. Data Catalog에서 S3로 콘솔에서 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- AWS Glue Studio를 열어주세요.\n- 새로운 주피터 노트북을 생성해주세요.\n- 주피터 노트북에 파이썬 코드를 작성해주세요.\n- 작성한 파이썬 코드를 실행해주세요.\n- 주피터 노트북을 저장하고 공유해주세요.\n\n## 스크립팅\n\nETL 프로세스를 완전히 제어하고 싶은 경우, AWS Glue를 사용하여 Python 및 Scala와 같은 다양한 프로그래밍 언어로 스크립트를 작성할 수 있습니다. 이러한 스크립트는 귀하의 특정 요구에 맞게 설계된 복잡한 데이터 변환을 수행하는 데 사용될 수 있습니다.\n\n아래는 데이터 카탈로그부터 S3까지 콘솔에서 AWS Glue를 스크립팅과 함께 사용하는 단계입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- AWS Glue 콘솔을 열어주세요.\n- 좌측 탐색 패널에서 Glue를 선택합니다.\n- 메인 패널 상단에서 Jobs를 선택합니다.\n- 'Create job'을 클릭합니다.\n- 작업의 이름을 입력해주세요. 예를 들어 \"TransferDataFromCatalogToS3\"와 같이 지정합니다.\n- Script location 섹션에서 Glue 스크립트를 선택합니다.\n- Glue 스크립트 상자에 다음과 같은 Python 스크립트를 입력하세요. 이는 예시입니다.\n\n```js\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# Read\ndyf = glueContext.create_dynamic_frame.from_catalog(database='db-s3-glue ', \n                                                    table_name='1_source'\n                                                   )\n\n# Store\noutput_dyf = glueContext.write_dynamic_frame.from_options(frame=dyf, \n                                                          connection_type=\"s3\", \n                                                          format=\"glueparquet\", \n                                                          connection_options={\"path\": \"s3://s3-glue/2-target/\", \"partitionKeys\": []}, \n                                                          format_options={\"compression\": \"uncompressed\"}\n                                                         )\n\njob.commit()\n```\n\n# 다음은 무엇이 있을까요?\n\n- MySQL, SQL Server, Aurora와 같은 RDBMS 소스 탐색하기.\n- Redshift와 같은 데이터 웨어하우스로의 대상 데이터 탐색하기.\n- Workflows(오케스트레이션)를 사용하여 작업 자동화하기.\n- 스트림 처리.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 최선의 인사\n\n린탕 길랑","ogImage":{"url":"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_0.png"},"coverImage":"/assets/img/2024-05-17-Step-by-StepETLTipswithAWSGlueHandlingMultipleCSVFilesfromS3_0.png","tag":["Tech"],"readingTime":5},{"title":"Node에서 안정적인 분산 시스템 구축하는 방법","description":"","date":"2024-05-17 20:34","slug":"2024-05-17-BuildingReliableDistributedSystemsinNode","content":"\n\n이 게시물은 Stripe, Netflix, Coinbase, Snap 및 기타 많은 회사들이 분산 시스템에서 다양한 문제를 해결하기 위해 사용하는 durable execution 개념을 소개합니다. 그리고 Temporal의 TypeScript/JavaScript SDK를 사용하여 durable 코드를 작성하는 것이 얼마나 간단한지 보여줍니다.\n\n# 분산 시스템\n\n트랜잭션을 지원하는 단일 데이터베이스로 뒷받침된 요청-응답 단일체를 구축할 때, 우리는 분산 시스템에 대한 많은 고려 사항이 없습니다. 단순한 실패 모드를 가질 수 있으며 쉽게 정확한 상태를 유지할 수 있습니다:\n\n- 클라이언트가 서버에 도달할 수 없는 경우 클라이언트가 다시 시도합니다.\n- 클라이언트가 서버에 도달하지만 서버가 데이터베이스에 도달하지 못하는 경우 서버는 오류로 응답하고 클라이언트가 다시 시도합니다.\n- 서버가 데이터베이스에 도달하지만 트랜잭션이 실패하는 경우 서버는 오류로 응답하고 클라이언트가 다시 시도합니다.\n- 트랜잭션이 성공하지만 서버가 클라이언트에 응답하기 전에 종료된 경우 클라이언트가 서버가 다시 켜질 때까지 다시 시도하고, 트랜잭션은 두 번째로 실패합니다(트랜잭션이 이미 적용되었는지를 알려주는 idempotency token과 같은 확인이 있음을 가정), 그리고 서버는 클라이언트에게 작업이 이미 수행되었음을 보고합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번에 두 번째 위치를 추가하면 데이터베이스를 사용하는 서비스나 외부 API와 같은 일은 장애를 처리하고 일관성을 유지하는 것(모든 데이터 저장소 간의 정확성)이 훨씬 더 복잡해집니다. 예를 들어, 서버가 신용 카드를 청구하고 데이터베이스를 업데이트해야 하는 경우처럼, 단순한 코드를 더 이상 작성할 수 없게 됩니다.\n\n```js\nfunction handleRequest() {\n  paymentAPI.chargeCard()\n  database.insertOrder()\n  return 200\n}\n```\n\n카드 청구(첫 번째 단계)는 성공했지만 데이터베이스에 주문 추가(두 번째 단계)가 실패할 경우 시스템은 일관성 없는 상태에 놓일 수 있습니다. 이 일관성을 유지하기 위해 두 번째 단계를 데이터베이스에 도달할 때까지 다시 시도하도록 할 수 있습니다. 그러나 코드를 실행하는 프로세스가 실패할 수도 있으며, 이 경우 우리는 첫 번째 단계가 발생한 사실을 전혀 알 수 없게 됩니다. 이 문제를 해결하려면 세 가지를 수행해야 합니다:\n\n- 주문 세부 정보를 유지\n- 완료한 프로그램 단계를 유지\n- 데이터베이스에서 미완료 주문을 확인하고 다음 단계로 계속 진행하는 워커 프로세스 실행\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그, 그리고 재시도 상태를 유지하고 각 단계에 시간 제한을 추가하는 것만으로도 많은 코드를 작성해야 하고, 특정 가장자리 경우나 실패 모드를 놓칠 수 있습니다. 만약 전체적이고 확장 가능한 아키텍처를 보려면 클릭하세요. 우리가 모든 그 코드를 작성하고 디버그할 필요 없이 좀 더 빠르고 신뢰할 수 있는 것들을 구축할 수 있었으면 좋겠다. 그걸 할 필요 없다는 건 우리가 내구성 실행을 사용할 수 있기 때문입니다.\n\n# 내구성 실행\n\n내구성 실행 시스템은 우리의 코드를 각 단계를 지속시키는 방식으로 실행합니다. 코드를 실행하는 프로세스나 컨테이너가 종료되어도 코드는 호출 스택과 로컬 변수를 포함한 모든 상태를 유지한 채 다른 프로세스에서 자동으로 계속 실행됩니다.\n\n내구성 실행은 하드웨어가 얼마나 신뢰할지나 하류 서비스가 얼마나 오랫동안 오프라인인지에 상관없이 코드가 완료되도록 보장합니다. 재시도와 타임아웃은 자동으로 수행되며, 코드가 아무것도 하지 않을 때(예를 들어 sleep('1 month') 문을 기다리는 동안) 자원이 해제됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내구성 있는 실행은 이벤트 주도 아키텍처, 작업 대기열, 사가, 회로 차단기 및 트랜잭션 아웃박스와 같은 분산 시스템 패턴을 구현하는 것이 중요하지 않거나 불필요하게 만듭니다. 이것은 더 높은 추상화 수준에서 프로그래밍하는 것으로, 서버 충돌이나 네트워크 문제와 같은 일시적인 실패에 대해 걱정할 필요가 없는 곳입니다. 이것은 다음과 같은 새로운 가능성을 엽니다:\n\n- 로컬 변수에 상태를 저장하는 것이 데이터베이스보다 낫습니다. 로컬 변수는 자동으로 저장되기 때문입니다.\n- 한 달 동안 잠자는 코드를 작성할 수 있습니다. 다음 달에도 잠자던 프로세스가 여전히 존재할 필요가 없고, 리소스가 지속되는 동안 사용되지 않아도 됩니다.\n- 영원히 실행할 수 있는 함수, 그리고 이러한 함수와 상호작용할 수 있는 (명령을 보내거나 데이터를 쿼리하는) 기능들.\n\n내구성 있는 실행 시스템의 몇 가지 예는 Azure 내구성 함수, Amazon SWF, Uber Cadence, Infinitic, 그리고 Temporal(내가 일하는 곳)입니다. 완벽히 객관적이지 못할 리스크를 감수하더라도, 나는 Temporal이 이러한 옵션 중에서 최고라고 생각합니다 😊.\n\n# 내구성 있는 JavaScript\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 분산 시스템에서의 일관성과 내구 실행이 무엇인지 살펴보았으니, 실제 예시를 살펴보겠습니다. 내가 만든 이 음식 주문 앱은 내구성 있는 코드가 어떻게 생겼고 어떤 문제를 해결하는지 보여줍니다:\n\ntemporal.menu\n\n![Building Reliable Distributed Systems in Node](/assets/img/2024-05-17-BuildingReliableDistributedSystemsinNode_0.png)\n\n이 앱은 네 가지 주요 기능을 갖고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 주문 생성 및 고객에게 청구\n- 주문 상태 가져오기\n- 주문 수령 처리\n- 주문 배달 처리\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*5Ivi74IEaDxDo182J91nSA.gif)\n\n메뉴에서 품목을 주문하면 배송 기사 사이트(drive.temporal.menu)에 나타나며, 운전자는 주문을 수령 처리하고 배달된 것으로 표시할 수 있습니다.\n\n모든 이 기능은 내구성이 있는 JavaScript 또는 TypeScript의 단일 기능에서 구현할 수 있습니다. 저희는 TypeScript를 사용하고 있습니다 - TypeScript를 권장하며 라이브러리의 이름은 TypeScript SDK입니다. 그러나 npm에는 JavaScript 형식으로 게시되어 있으며 모든 Node.js 프로젝트에서 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 주문 생성하기\n\n이 앱의 코드를 살펴봅시다. 몇 가지 API 경로를 살펴보겠지만 대부분은 order라는 단일 내구성 함수의 각 부분을 검토할 겁니다. 앱을 실행하거나 코드를 보려면 프로젝트를 다운로드하고 설정하려면 다음을 실행하세요:\n\n```js\nnpx @temporalio/create@latest --sample food-delivery\n```\n\n사용자가 주문 버튼을 클릭하면 React 프론트엔드가 tRPC 백엔드에서 정의된 createOrder 뮤테이션을 호출합니다. createOrder API 경로 핸들러는 내구성 주문 함수를 시작하여 주문을 생성합니다. 내구성 함수인 Workflows은 @temporalio/client의 Client 인스턴스를 사용하여 시작된다. 이는 tRPC 컨텍스트에 ctx.temporal로 추가되었으며, 경로 핸들러는 유효성이 검증된 입력(제품 ID 번호 및 주문 ID 문자열을 포함한 객체)을 받아들이며, ctx.temporal.workflow.start를 호출하여 주문 Workflows을 시작합니다. 입력.productId를 인수로 제공합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```typescript\napps/menu/pages/api/[trpc].ts\n\nimport { initTRPC } from '@trpc/server'\nimport { z } from 'zod'\nimport { taskQueue } from 'common'\nimport { Context } from 'common/trpc-context'\nimport { order } from 'workflows'\n\nconst t = initTRPC.context\u003cContext\u003e().create()\n\nexport const appRouter = t.router({\n  createOrder: t.procedure\n    .input(z.object({ productId: z.number(), orderId: z.string() }))\n    .mutation(async ({ input, ctx }) =\u003e {\n      await ctx.temporal.workflow.start(order, {\n        workflowId: input.orderId,\n        args: [input.productId],\n        taskQueue,\n      })\n      return 'Order received and persisted!'\n    }),\n```\n\nThe order function starts out validating the input, setting up the initial state, and charging the customer:\n\npackages/workflows/order.ts\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntype OrderState = 'Charging card' | 'Paid' | 'Picked up' | 'Delivered' | 'Refunding'\n\nexport async function order(productId: number): Promise\u003cvoid\u003e {\n  const product = getProductById(productId)\n  if (!product) {\n    throw ApplicationFailure.create({ message: `Product ${productId} not found` })\n  }\n  let state: OrderState = 'Charging card'\n  let deliveredAt: Date\n  try {\n    await chargeCustomer(product)\n  } catch (err) {\n    const message = `Failed to charge customer for ${product.name}. Error: ${errorMessage(err)}`\n    await sendPushNotification(message)\n    throw ApplicationFailure.create({ message })\n  }\n  state = 'Paid'\n```\n\n어떤 기능이 실패할 수있는 함수는 자동으로 재시도됩니다. 이 경우 chargeCustomer 및 sendPushNotification은 두 서비스에 액세스하며, 현재 가동 중이거나 \"일시적으로 사용할 수 없음\"과 같은 일시적 오류 메시지를 반환할 수 있습니다. Temporal은 이러한 함수를 실행하는 것을 자동으로 재시도합니다 (기본적으로 제곱 백오프 방식으로 무제한으로, 그러나 이것은 구성 가능합니다). 함수는 \"카드 거절\"과 같은 재시도할 수 없는 오류도 throw할 수 있습니다. 이 경우에는 재시도되지 않습니다. 대신, 에러가 chargeCustomer(product)에서 throw되고 catch 블록에서 캐치됩니다. 고객은 결제 방법이 실패했다는 알림을 받으며, 우리는 주문 Workflow를 실패시키기 위해 ApplicationFailure를 throw합니다.\n\n# 주문 상태 확인\n\n다음 코드 부분은 약간의 백그라운드 지식이 필요합니다. 일반 함수는 오래 실행할 수 없으므로, 일이 발생할 때까지 대기하는 동안 리소스를 차지하고, 언젠가는 새 코드를 배포하고 이전 컨테이너가 종료되면 종료될 것입니다. 내구성 함수는 두 가지 이유로 임의의 길이로 실행할 수 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 어떤 것을 기다리고 있을 때에는 리소스를 차지하지 않습니다.\n- 그들을 실행하는 프로세스가 종료되어도 문제가 되지 않습니다. 다른 프로세스가 실행을 계속할 것이기 때문입니다.\n\n따라서 일부 영구 함수는 돈을 이체하는 함수처럼 짧은 시간 운영되지만, 어떤 것은 주문이 완료될 때 끝나는 주문 함수와 고객의 평생을 지속하는 고객 함수와 같이 길게 운영됩니다.\n\n긴 시간 동안 실행되는 함수와 상호 작용할 수 있는 것은 유용합니다. Temporal은 함수로 데이터를 보내는 신호(Signals)와 함수에서 데이터를 가져오는 쿼리(Queries)를 제공합니다. 드라이버 사이트는 이 API 경로를 통해 주문 함수에 쿼리를 보내어 각 주문의 상태를 보여줍니다:\n\napps/menu/pages/api/[trpc].ts\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ngetOrderStatus: t.procedure\n  .input(z.string())\n  .query(({ input: orderId, ctx }) =\u003e ctx.temporal.workflow.getHandle(orderId).query(getStatusQuery)),\n```\n\n특정 주문 함수(Workflow Execution이라고도 함)의 핸들을 가져와 getStatusQuery를 보내고 결과를 반환합니다. getStatusQuery는 주문 파일에 정의되어 있으며 주문 함수에서 처리됩니다:\n\npackages/workflows/order.ts\n\n```js\nimport { defineQuery, setHandler } from '@temporalio/workflow'\n\nexport const getStatusQuery = defineQuery\u003cOrderStatus\u003e('getStatus')\n\nexport async function order(productId: number): Promise\u003cvoid\u003e {\n  let state: OrderState = 'Charging card'\n  let deliveredAt: Date\n  //…\n  setHandler(getStatusQuery, () =\u003e {\n    return { state, deliveredAt, productId }\n  })\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\ngetStatusQuery를 전달하는 order 함수가 호출되면, setHandler에 전달된 함수가 호출되어 로컬 변수의 값을 반환합니다. chargeCustomer 호출이 성공하면 상태가 '지불 완료'로 변경되고, getStatusQuery를 계속 폴링하던 드라이버 사이트가 업데이트된 상태를 받습니다. 그리고 'Pick up' 버튼을 표시합니다.\n\n# 주문 픽업하기\n\n드라이버가 주문을 픽업으로 표시하기 위해 버튼을 탭하면, 사이트는 API 서버에 pickUp 변경을 보내고, 이는 order 함수에 pickedUpSignal을 보냅니다:\n\napps/driver/pages/api/[trpc].ts\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\npickUp: t.procedure\n  .input(z.string())\n  .mutation(async ({ input: orderId, ctx }) =\u003e \n    ctx.temporal.workflow.getHandle(orderId).signal(pickedUpSignal)\n  ),\n```\n\n신청 함수는 상태를 업데이트하여 시그널을 처리합니다:\n\npackages/workflows/order.ts\n\n```js\nexport const pickedUpSignal = defineSignal('pickedUp')\n\nexport async function order(productId: number): Promise\u003cvoid\u003e {\n  // …\n  setHandler(pickedUpSignal, () =\u003e {\n    if (state === 'Paid') {\n      state = 'Picked up'\n    }\n  })\n```  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n함수의 하단에서는 고객에 청구된 후에 픽업이 발생할 때까지 기다려왔다:\n\npackages/workflows/order.ts\n\n```js\nimport { condition } from '@temporalio/workflow'\n\nexport async function order(productId: number): Promise\u003cvoid\u003e {\n  // ...\n  try {\n    await chargeCustomer(product)\n  } catch (err) {\n    // ...\n  }\n  state = 'Paid'\n  const notPickedUpInTime = !(await condition(() =\u003e state === 'Picked up', '1 min'))\n  if (notPickedUpInTime) {\n    state = 'Refunding'\n    await refundAndNotify(\n      product,\n      '⚠️ No drivers were available to pick up your order. Your payment has been refunded.'\n    )\n    throw ApplicationFailure.create({ message: 'Not picked up in time' })\n  }\n```\n\n`await condition(() =\u003e state === 'Picked up', '1 min')` 함수는 상태 변화를 'Picked up'으로 변경할 때까지 1분 동안 대기합니다. 1분이 지나도 상태가 변경되지 않으면 false를 반환하고 고객에게 환불을 합니다. (우리는 요리사와 배송 기사의 속도에 엄격한 기준을 가지고 있거나, 데모 앱의 사용자가 모든 실패 모드를 볼 수 있기를 원하는 것 같아요 😄.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 배송\n\n마찬가지로 \"배송\" 버튼으로 전송된 deliveredSignal이 있습니다. 픽업 후 1분 이내에 운전자가 배송을 완료하지 않으면 고객에게 환불이 이뤄집니다.\n\npackages/workflows/order.ts\n\n```js\nexport const deliveredSignal = defineSignal('delivered')\n\nexport async function order(productId: number): Promise\u003cvoid\u003e {\n  setHandler(deliveredSignal, () =\u003e {\n    if (state === 'Picked up') {\n      state = 'Delivered'\n      deliveredAt = new Date()\n    }\n  })\n  // …\n  await sendPushNotification('🚗 주문 픽업됨')\n  const notDeliveredInTime = !(await condition(() =\u003e state === 'Delivered', '1 min'))\n  if (notDeliveredInTime) {\n    state = 'Refunding'\n    await refundAndNotify(product, '⚠️ 운전자가 주문을 배달하지 못했습니다. 결제가 환불되었습니다.')\n    throw ApplicationFailure.create({ message: '제시간 배송되지 않음' })\n  }\n  await sendPushNotification('✅ 주문이 배달되었습니다!')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n배송이 성공적으로 완료되면 고객이 식사를 하는 데 한 분을 기다리고, 그들에게 경험을 평가하도록 요청합니다.\n\n```js\n  await sleep('1 min') // 이것은 몇 시간 또는 심지어 몇 달이 될 수도 있습니다\n  await sendPushNotification(`✍️ 식사를 평가해주세요. ${product.name.toLowerCase()}는 어떠셨나요?`)\n}\n```\n\n최종 푸시 알림 이후, 주문 함수 실행이 종료되고 Workflow Execution이 성공적으로 완료됩니다. 함수가 완료되었더라도 Temporal이 함수의 최종 상태를 저장하고 있기 때문에 여전히 쿼리를 보낼 수 있습니다. 주문이 배달된 후 1분 뒤 페이지를 새로 고치면 여전히 getStatusQuery가 작동하고 \"배송됨\"이 상태로 표시됩니다:\n\n![이미지](/assets/img/2024-05-17-BuildingReliableDistributedSystemsinNode_1.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 요약\n\n단일 내구성 함수를 사용하여 다단계 주문 흐름을 구현할 수 있는 방법을 살펴보았습니다. 이 함수는 네트워크, 데이터 저장소 또는 하위 서비스와 관련된 일시적 문제뿐만 아니라 다음과 같은 문제가 발생할 경우에도 완료가 보장됩니다:\n\n- 네트워크, 데이터 저장소 또는 하위 서비스와 관련된 일시적 문제\n- 함수 실행 중 문제 발생\n- 기반이 되는 Temporal 서비스 또는 데이터베이스 다운\n\n이를 통해 분산 시스템에 대한 여러 문제점을 해결할 수 있었으며:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 로컬 변수를 사용하여 상태를 데이터베이스에 저장하는 대신에 활용할 수 있습니다.\n- 주문이 너무 오래 걸려 주문을 취소하거나 chargeCustomer와 같은 일시적 함수를 재시도하고 시간 초과하는 내장 기능을 위해 데이터베이스에 타이머를 설정할 필요가 없었습니다.\n- 다음 단계로 진행하기 위해 워커가 조사하는 작업 대기열을 설정할 필요가 없었으며, 실패한 프로세스에 의해 중단된 미완료 작업을 진행하거나 선택하기 위해서도 필요하지 않았습니다.\n\n다음 글에서는 배송 앱의 코드를 더 살펴보고 Temporal이 우리에게 내구성 실행을 제공하는 방법을 배우게 될 것입니다. 새로운 글이 올라오면 알림을 받으려면 Twitter 또는 LinkedIn에서 팔로우해주세요.\n\n질문이 있으면 언제든지 도와드릴게요! Temporal의 미션이 개발자를 돕는 데에 있고, 개인적으로 그것에서 기쁨을 느낍니다 🤗. 트위터에서는 @lorendsr로, temporal-typescript 태그가 달린 StackOverflow 질문에는 답변(그리고 좋아요 😄)을 달며, 커뮤니티 Slack에는 @Loren으로 활동하고 있습니다 💃.\n\n# 더 배우기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 알아보려면 다음 자료를 추천합니다:\n\n- 영상: Temporal 소개 및 TypeScript SDK 사용하기\n- 몇 가지 일반적인 사용 사례\n- TypeScript SDK 문서: t.mp/ts\n- TypeScript API 참조: t.mp/ts-api\n- TypeScript 튜토리얼\n\n더 많은 TypeScript SDK에 관한 블로그 포스트:\n\n- Node.js 작업 큐로서 Temporal 사용하기\n- 장기 워크플로를 통해 API 요청 캐싱하기\n- 워크플로에 대한 REST API를 생성하는 Express 미들웨어\n- TS SDK 1.0.0 릴리스\n- Workflow 결정론을 강제하기 위해 V8 독립체 사용하는 방법\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n💬 하커 뉴스, 레딧, 트위터, 또는 링크드인에서 토론해보세요. \n\n이 게시물 초안을 읽어준 제시카 웨스트, 브라이언 호건, 애밀리아 망고, 그리고 짐 워커에게 감사드립니다.","ogImage":{"url":"/assets/img/2024-05-17-BuildingReliableDistributedSystemsinNode_0.png"},"coverImage":"/assets/img/2024-05-17-BuildingReliableDistributedSystemsinNode_0.png","tag":["Tech"],"readingTime":12},{"title":"Nodejs에서 perf_hooks를 사용한 벤치마킹 점수","description":"","date":"2024-05-17 20:33","slug":"2024-05-17-BenchmarkinginNodejswithperf_hooks","content":"\n\n성능은 소프트웨어 개발에서 중요한 측면입니다, 특히 확장 가능하고 효율적인 애플리케이션을 구축할 때에는 더욱 그렇습니다. 비동기 및 이벤트 기반 아키텍처로 유명한 Node.js는 코드를 측정하고 최적화하는 데 도움이 되는 내장 도구를 제공합니다. 이 목적을 위해 가장 강력한 도구 중 하나는 `perf_hooks` 모듈입니다. 이 블로그에서는 Node.js의 벤치마킹을 위한 `perf_hooks` 사용 방법과 개발 툴킷에서 꼭 필요한 이유를 살펴보겠습니다.\n\n![이미지](/assets/img/2024-05-17-BenchmarkinginNodejswithperf_hooks_0.png)\n\n# `perf_hooks`란 무엇인가요?\n\n`perf_hooks` 모듈은 Node.js의 코어 라이브러리 중 하나로, 성능을 측정하기 위한 API를 제공합니다. 이 모듈은 브라우저에서 사용 가능한 Performance Timing API를 활용하여 높은 해상도의 타임스탬프를 캡처하고 애플리케이션 내 작업의 지속 시간을 측정할 수 있도록 해줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 벤치마킹의 중요성\n\n벤치마킹은 응용 프로그램의 여러 측면의 성능을 측정하여 병목 현상과 개선이 필요한 부분을 식별하는 실천입니다. 효과적인 벤치마킹은 다음을 이끌어낼 수 있습니다:\n\n1. 최적화된 코드: 코드의 느린 부분을 식별하고 성능을 개선하기 위해 최적화합니다.\n2. 자원 관리: 응용 프로그램이 자원을 효율적으로 활용하도록 하여 비용을 줄이고 사용자 경험을 개선합니다.\n3. 확장성: 성능 저하 없이 증가하는 부하를 처리할 수 있도록 응용 프로그램을 준비합니다.\n\n# `perf_hooks` 시작하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`perf_hooks`를 사용하려면 Node.js 애플리케이션에 가져와야 합니다:\n\n```js\nconst { performance, PerformanceObserver } = require('perf_hooks');\n```\n\n# 실행 시간 측정\n\n`perf_hooks`를 가장 기본적인 방법은 함수 또는 코드 블록의 실행 시간을 측정하는 것입니다. 다음은 예시입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nconst { performance } = require('perf_hooks');\n\nfunction someFunction() {\n const start = performance.now();\n // 측정하려는 코드\n for (let i = 0; i \u003c 1e6; i++) {}\n const end = performance.now();\n console.log(`실행 시간: ${end - start} 밀리초`);\n}\n\nsomeFunction();\n```\n\n# 성능 마크 및 측정 사용하기\n\n보다 복잡한 응용 프로그램에서는 코드의 여러 부분을 측정하고 싶을 수 있습니다. `perf_hooks`를 사용하면 사용자 정의 성능 마크와 측정을 만들 수 있습니다:\n\n```js\nperformance.mark('A');\n// 코드 블록 A\nfor (let i = 0; i \u003c 1e6; i++) {}\nperformance.mark('B');\n\nperformance.mark('C');\n// 코드 블록 B\nfor (let i = 0; i \u003c 1e6; i++) {}\nperformance.mark('D');\n\nperformance.measure('A to B', 'A', 'B');\nperformance.measure('C to D', 'C', 'D');\n\nconst measures = performance.getEntriesByType('measure');\nmeasures.forEach((measure) =\u003e {\n  console.log(`${measure.name}: ${measure.duration} 밀리초`);\n});\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 성능 항목 관찰\n\n더 발전된 사용법을 위해, `PerformanceObserver`를 사용하여 실시간으로 성능 항목들을 관찰할 수 있습니다:\n\n```js\nconst observer = new PerformanceObserver((list) =\u003e {\n  const entries = list.getEntries();\n  entries.forEach((entry) =\u003e {\n    console.log(`${entry.name}: ${entry.duration} milliseconds`);\n  });\n});\n\nobserver.observe({ entryTypes: ['measure'] });\n\nperformance.mark('start');\n// 벤치마크할 코드\nsetTimeout(() =\u003e {\n  performance.mark('end');\n  performance.measure('시작부터 끝까지', 'start', 'end');\n}, 1000);\n```\n\n# 실제 응용 프로그램\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실제 상황에서는 `perf_hooks`를 사용하여 데이터베이스 쿼리, API 요청 또는 무거운 계산과 같은 애플리케이션의 중요한 부분의 성능을 측정할 수 있습니다. 이러한 지표를 지속적으로 모니터링하여 애플리케이션이 발전함에 따라 성능을 유지할 수 있습니다.\n\n# 결론\n\n벤치마킹은 어떤 진지한 개발자에게 필수적인 실천 방법이며, Node.js의 `perf_hooks` 모듈은 코드를 측정하고 최적화하는 강력하고 유연한 방법을 제공합니다. 이 도구를 이해하고 활용함으로써 애플리케이션의 성능을 향상시킬 수 있어 자원 관리를 개선하고 사용자 경험을 향상시킬 수 있습니다.\n\n기억하세요, 소프트웨어 개발 세계에서 측정되는 대로 개선됩니다. 그래서 오늘부터 `perf_hooks`로 Node.js 애플리케이션의 벤치마킹을 시작하고 성능 최적화를 더욱 높여보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 마크다운 형식으로 변경해주세요.","ogImage":{"url":"/assets/img/2024-05-17-BenchmarkinginNodejswithperf_hooks_0.png"},"coverImage":"/assets/img/2024-05-17-BenchmarkinginNodejswithperf_hooks_0.png","tag":["Tech"],"readingTime":3},{"title":"Nest JS에서 Redis 사용하는 방법","description":"","date":"2024-05-17 20:30","slug":"2024-05-17-UsingRedisinNestJS","content":"\n\n\u003cimg src=\"/assets/img/2024-05-17-UsingRedisinNestJS_0.png\" /\u003e\n\n# 소개\n\n확장 가능하고 고성능 응용 프로그램을 구축하는 기술은 모든 개발자가 추구하는 기술입니다.\n\n이 글에서는 Redis를 활용하여 응용 프로그램의 속도와 보안을 향상시키는 방법을 알아보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNestJS와 함께 사용할 것입니다. 네스트는 Kamil Mysliwiec가 만든 간단하고 가벼운 프레임워크로, 강건함과 개발자 친화적인 아키텍처로 유명합니다.\n\n응용 프로그램이 복잡성이 증가하고 데이터 처리량이 증가함에 따라, 효율적인 데이터 캐싱은 응답 시간을 향상시키고 데이터베이스에 가해지는 부하를 줄이는 데 필수적입니다. 여기에서 Redis가 등장합니다. Redis는 강력한 인메모리 데이터 구조 저장소로, NestJS에서 캐싱 솔루션으로 사용될 수 있습니다.\n\n# NestJS와 Redis 설치\n\n가정: -\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- NodeJS가 설치되어 있습니다.\n- VS Code 또는 원하시는 편집기가 설치되어 있습니다.\n- NestJS의 기본 지식\n\n시작하려면 명령줄을 실행하고 선택한 폴더로 이동하세요 (해당 폴더에 코드를 작성할 것입니다). 그리고 다음 명령을 실행하세요.\n\n```js\nnpm i -g @nestjs/cli \nnest new using-redis-in-nestjs #원하는 프로젝트 이름으로 \"using-redis-in-nestjs\"를 대체할 수 있습니다\n```\n\n해당 폴더로 이동하면 다음처럼 간단한 시작 코드가 있는 기본 NestJS 프로젝트가 표시됩니다…\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-UsingRedisinNestJS_1.png\" /\u003e\n\n새로 만든 NestJS 프로젝트를 위해 프로젝트 시작\n\n이 문서의 주요 관심사가 캐싱이므로 이 파일들이 무엇을 의미하고 하는지는 다루지 않고 캐싱과 Redis 사용에 초점을 맞출 것입니다.\n\n첫 번째 할 일은 NestJS의 캐시 매니저와 패키지 cache-manager 자체를 설치하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nnpm install @nestjs/cache-manager cache-manager\n```\n\n@nestjs/cache-manager은 원래 @nestjs/common 패키지의 일부였으며 NestJS 자체를 사용하여 캐시와 상호작용할 수 있게 해주는 패키지입니다. 캐시 패키지를 직접 사용하는 대신 NestJS의 통합 API를 사용하여 코드를 변경하지 않고도 더 편리하게 사용할 수 있습니다. 이는 우리가 나중에 캐시 공급업체를 변경하기로 결정해도 코드를 변경할 필요가 없기 때문에 우리에게 유리합니다.\n\n이제 캐시 모듈을 설정하려면 \"app.module.ts\" 파일로 이동하여 @nestjs/cache-manager에서 캐시 모듈을 import하고 매개변수 없이 register 메서드를 사용하면 됩니다.\n\n```js\nimport { CacheModule } from '@nestjs/cache-manager';\nimport { Module } from '@nestjs/common';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n@Module({\n  imports: [CacheModule.register()],\n  controllers: [AppController],\n  providers: [AppService],\n})\nexport class AppModule {}\n```\n\n이 레지스트리를 통해 \"app.service.ts\" 파일에서 캐시 매니저를 사용하여 데이터를 저장하고 검색할 수 있게 되었습니다. 사용하려면 \"app.service.ts\"의 생성자에 주입해야 합니다.\n\n```javascript\nimport { CACHE_MANAGER } from '@nestjs/cache-manager';\nimport { Cache } from 'cache-manager'; // ! 이 import를 빠뜨리지 마세요\nimport { Inject, Injectable } from '@nestjs/common';\n```\n\n```javascript\n@Injectable()\nexport class AppService {\n  constructor(@Inject(CACHE_MANAGER) private cacheManager: Cache) {}\n  getHello(): string {\n    return 'Hello World!';\n  }\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n캐시 모듈은 NestJS의 다른 모듈과 마찬가지로 작동합니다. 따라서 사용 중인 모듈에서 가져오거나 전역으로 사용할 수 있도록 설정해야 합니다.\n\n```js\nCacheModule.register({isGlobal: true})\n```\n\n이제 사용 방법을 알아보겠습니다. 시작하려면 세 가지 메서드만 알아야 합니다.\n\n```js\nawait this.cacheManager.set('키', '값'); // 캐시에 데이터 설정\nconst value = await this.cacheManager.get\u003cstring\u003e('키'); // 캐시에서 데이터 가져오기\nawait this.cacheManager.del('키'); // 캐시에서 데이터 삭제\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 세 가지 메소드는 기본적으로 캐싱과 관련된 거의 모든 작업을 수행할 수 있도록 해줍니다. 이를 실제로 보기 위해 세 가지 메소드를 테스트할 수 있는 세 가지 라우트를 준비했는데요. 이를 따라오시면서 저의 GitHub을 방문하셔서 코드를 확인해보실 수 있어요.\n\n아래는 컨트롤러에서의 라우트들입니다.\n\n```js\nimport { Controller, Delete, Get, Post } from '@nestjs/common';\nimport { Body } from '@nestjs/common/decorators';\nimport { CreateDataDto } from 'dtos/create-data.dto';\nimport { AppService } from './app.service';\n```\n\n```js\n@Controller()\nexport class AppController {\n  constructor(private readonly appService: AppService) {}\n  @Get()\n  async getData() {\n    try {\n      return await this.appService.getData();\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n  @Post()\n  async postData(@Body() createDataDto: CreateDataDto) {\n    try {\n      return await this.appService.postData(createDataDto);\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n  @Delete()\n  async deleteData() {\n    try {\n      return await this.appService.deleteData();\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 이것이 서비스입니다.\n\n```js\nimport { CACHE_MANAGER } from '@nestjs/cache-manager';\nimport { Cache } from 'cache-manager'; // ! 이 임포트를 빠트리지 마세요\nimport { Inject, Injectable } from '@nestjs/common';\nimport { CreateDataDto } from 'dtos/create-data.dto';\n```\n\n```js\n@Injectable()\nexport class AppService {\n  constructor(@Inject(CACHE_MANAGER) private cacheManager: Cache) {}\n  async getData(): Promise\u003cstring | undefined\u003e {\n    const value = await this.cacheManager.get\u003cstring\u003e('key'); // ? 캐시에서 데이터를 가져옵니다\n    return value;\n  }\n  async postData(createDataDto: CreateDataDto) {\n    const { value } = createDataDto;\n    await this.cacheManager.set('key', value); // ? 캐시에 데이터를 설정합니다\n  }\n  async deleteData() {\n    await this.cacheManager.del('key'); // ? 캐시에서 데이터를 삭제합니다\n  }\n}\n```\n\nNestJS의 캐시 모듈을 사용하는 것은 매우 간단하고 쉬우며, 꽤 간단한 애플리케이션에서 필요한 모든 것일 수 있음을 확인할 수 있습니다. 물론, 여기에 그치지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Redis\n\n네스트JS가 사용하는 기본 인메모리 스토어는 우리가 찾고 있는 체계적인 해결책이 아닐 수 있습니다. 스토어를 변경하려면 캐시 모듈 등록 방식을 변경해야 합니다.\n\n먼저 네스트JS 캐시 매니저 옵션을 살펴봅시다.\n\n```js\nexport interface CacheManagerOptions {\n    store?: string | CacheStoreFactory | CacheStore;\n    ttl?: number;\n    max?: number;\n    isCacheableValue?: (value: any) =\u003e boolean;\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옵션에 'store' 속성이 있는 것을 확인할 수 있습니다. 여기에는 Redis Store의 초기화가 필요합니다.\n\n먼저 캐시 매니저 Redis Store를 설치해야 합니다.\n\n```js\nnpm i --save cache-manager-redis-store\n```\n\n이제 레지스터 메서드를 변경해야 합니다. 코드를 깔끔하게 유지하기 위해 'configs'라는 폴더를 만들고 그 안에 'app-options.constants.ts'라는 파일을 생성하여 레지스터 메서드를 작성했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport { CacheModuleAsyncOptions } from \"@nestjs/cache-manager\";\nimport { ConfigModule, ConfigService } from \"@nestjs/config\";\nimport { redisStore } from \"cache-manager-redis-store\";\n```\n\n```js\nexport const RedisOptions: CacheModuleAsyncOptions = {\n  isGlobal: true,\n  imports: [ConfigModule],\n  useFactory: async (configService: ConfigService) =\u003e {\n    const store = await redisStore({\n      socket: {\n        host: configService.get\u003cstring\u003e('REDIS_HOST'),\n        port: parseInt(configService.get\u003cstring\u003e('REDIS_PORT')!),\n      },\n    });\n    return {\n      store: () =\u003e store,\n    };\n  },\n  inject: [ConfigService],\n};\n```\n\n그리고 앱 모듈에서 간단히 사용하세요.\n\n```js\nimport { CacheModule } from '@nestjs/cache-manager';\nimport { Module } from '@nestjs/common';\nimport { ConfigModule } from '@nestjs/config';\nimport { RedisOptions } from 'configs/app-options.constants';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```ts\n@Module({\n  imports: [\n    ConfigModule.forRoot({ isGlobal: true }),\n    CacheModule.registerAsync(RedisOptions),\n  ],\n  controllers: [AppController],\n  providers: [AppService],\n})\nexport class AppModule {}\n```\n\n# 프로덕션 앱에서 Redis 활용하기\n\n이제 이 모듈을 활용하기 시작해봅시다. 우리가 프로덕션 앱에서 이 모듈을 어떻게 사용해야 하는지 살펴봅시다.\n\n- 응답 캐싱 (속도)\n- JWT 토큰 유효성 검사 (보안)\n- 채팅을 위한 소켓 ID 저장 (속도)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 하나씩 깊이 파고들어 봅시다.\n\n# 1. 응답 캐싱\n\n응답 캐싱은 API에 터보 부스트를 제공하는 것과 같습니다. 특정 위치(엔드포인트)에서 무언가를 요청하는 첫 번째 시간에, 우리는 받은 것을 캐시에 저장합니다. 그러니까, 짧은 시간 내에 같은 것을 다시 요청하면, 우리는 메인 저장소(데이터베이스)로 돌아가는 대신 저장된 버전을 그대로 전달합니다.\n\nNestJS로 이를 실현하려면, GET 요청에서 반환된 데이터를 추적하고 저장할 Interceptor를 사용해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론 NestJS에서 인터셉터를 사용하는 것은 매우 간단합니다. 원하는 컨트롤러에 내장 인터셉터를 바인딩하기만 하면 됩니다.\n\n```js\nimport { CacheInterceptor } from '@nestjs/cache-manager';\nimport { Controller, Delete, Get, Post } from '@nestjs/common';\nimport { Body, UseInterceptors } from '@nestjs/common/decorators';\nimport { CreateDataDto } from 'dtos/create-data.dto';\nimport { AppService } from './app.service';\n```\n\n```js\n@Controller()\n@UseInterceptors(CacheInterceptor) // 여기에 추가하세요\nexport class AppController {\n  constructor(private readonly appService: AppService) {}\n  @Get()\n  async getData() {\n    try {\n      return await this.appService.getData();\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n  @Post()\n  async postData(@Body() createDataDto: CreateDataDto) {\n    try {\n      return await this.appService.postData(createDataDto);\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n  @Delete()\n  async deleteData() {\n    try {\n      return await this.appService.deleteData();\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n}\n```\n\n이렇게 캐싱 인터셉터를 추가하면 기본 TTL에 따라 모든 GET 라우트 핸들러 응답이 캐시됩니다. 물론 이는 register 메서드에서 수정할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n{\n  ttl: 5, // 초\n  max: 10, // 캐시에 저장될 최대 항목 수\n}\n```\n\n우리 앱 전체의 모든 GET 요청을 캐시하려면 앱 모듈에서 전역으로 바인딩해야 합니다.\n\n```js\nimport { CacheInterceptor, CacheModule } from '@nestjs/cache-manager';\nimport { Module } from '@nestjs/common';\nimport { ConfigModule } from '@nestjs/config';\nimport { APP_INTERCEPTOR } from '@nestjs/core';\nimport { RedisOptions } from 'configs/app-options.constants';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\n\n\n\n@Module({\n  imports: [\n    ConfigModule.forRoot({ isGlobal: true }),\n    CacheModule.registerAsync(RedisOptions),\n  ],\n  controllers: [AppController],\n  providers: [\n    AppService,\n    {\n      provide: APP_INTERCEPTOR, // 인터셉터를 전역으로 바인딩\n      useClass: CacheInterceptor,\n    },\n  ],\n})\nexport class AppModule {}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n캐시의 지속 시간이나 라우트에 저장된 키를 사용자 정의하는 경우 NestJS의 데코레이터를 사용할 수 있습니다.\n\n- 기본 킷 값은 엔드포인트의 이름이며, 이를 변경하면 쿼리 매개변수와 함께 요청에 영향을 주므로 코드가 손상되지 않도록 주의하는 것이 중요합니다.\n\n```js\nimport { CacheInterceptor, CacheKey, CacheTTL } from '@nestjs/cache-manager';\nimport { Controller, Delete, Get, Post } from '@nestjs/common';\nimport { Body, UseInterceptors } from '@nestjs/common/decorators';\nimport { CreateDataDto } from 'dtos/create-data.dto';\nimport { AppService } from './app.service';\n```\n\n```js\n@Controller()\n@UseInterceptors(CacheInterceptor)\nexport class AppController {\n  constructor(private readonly appService: AppService) {}\n  @Get()\n  @CacheKey('custom_key') // 키 제어\n  @CacheTTL(20) // 지속시간 제어\n  async getData() {\n    try {\n      return await this.appService.getData();\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n  @Post()\n  async postData(@Body() createDataDto: CreateDataDto) {\n    try {\n      return await this.appService.postData(createDataDto);\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n  @Delete()\n  async deleteData() {\n    try {\n      return await this.appService.deleteData();\n    } catch (error) {\n      console.log(error);\n      return error;\n    }\n  }\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. JWT 토큰 유효성 검사\n\n이 섹션을 읽기 전에 사용자 인증하는 방법을 알고 있는 것이 명백하게 선행되어야 합니다.\n\n현재 JWT 기반 인증을 사용하는 사람들의 주요 문제점은 JWT가 쉽게 가져가지고, 디코딩하며, 복사할 수 있다는 것을 잊는다는 것이며, 모든 사람이 민감한 데이터를 내부에 저장한다는 문제점도 있습니다.\n\n이외에도, 대다수의 자습서들은 사용자가 로그인할 때 JWT가 특정 기간 동안 생성되지만 해당 사용자가 로그아웃했을 때에도 일정 기간이 지나지 않았더라도 토큰을 폐기해야 하는 것에 대해 언급하지 않는 것 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그럼 이 경우에는 무엇을 해야 할까요? Redis를 세션 관리자로 사용할 수 있어요!\n\n사용자의 ID를 키로 저장하고 값으로 JavaScript 오브젝트를 만들면, 그 안에 \"accessToken\": \"...\"를 저장할 수 있어요!\n\n이게 무슨 말인지 알려 드릴게요.\n\n```js\n{\n  \"64c900a9d01c8c1a4351040c\": {\n    \"accessToken\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsIndoeSBhcmUgeW91IjoibG9va2luZyBoZXJlPyJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.l_2-T20JUdnz5rrOORgH6zfI6nrEzmIHMH5JlU76IIE\"\n  }\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물론 \"HEST\" 명령어를 사용하여 순수 Redis로 구현할 수도 있고, Redis 명령어를 NestJS 캐시 매니저를 사용하여 추상화하여 통일된 API 아이디어를 유지할 수도 있어요.\n\n아래의 코드를 원하는 곳에 자유롭게 넣어보세요. 단, 캐시 모듈과 함께 연결되어 있는지 확인해주세요.\n\n```js\nasync hset(key: string, field: string, value: string) {\n    const stringObject = await this.cache.get\u003cstring\u003e(key);\n    const object = checkNullability(stringObject)\n      ? JSON.parse(stringObject!)\n      : {};\n    object[field] = value;\n```\n\n```js\n    await this.cache.set(key, JSON.stringify(object));\n  }\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 순수한 Redis를 사용하고 싶다면, underline client를 가져와서 'HSET' 메서드를 호출할 수 있어요.\n\n```js\nprivate readonly redisStore!: RedisStore;\nconstructor(@Inject(CACHE_MANAGER) private readonly cache: Cache) {\n    this.redisStore = cache.store as unknown as RedisStore;\n}\n```\n\n그리고 함수 내부에서 client를 호출하고 'HSET' 명령을 실행할 수 있어요.\n\n```js\nconst client = this.redisStore.getClient();\nawait client.HSET('KEY', 'FIELD', 'VALUE')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 서버 캐시에 액세스 토큰을 저장할 수 있으므로 사용자 로그아웃시 이를 제거하고 이 아이디어를 기반으로 논리를 구축할 수 있습니다.\n\n# 3. 채팅을 위한 소켓 ID 저장\n\n이전에 채팅 애플리케이션을 구축한 사람은 두 사람 간의 채팅을 위한 소켓 ID를 어디에 저장해야 할지에 대한 질문에 직면했습니다. 인터넷에서 본 바로는 대부분의 사람들이 이 정보를 DB에 저장하거나 인메모리 변수 내에 저장하는 것으로 보여집니다. 둘 다 실행 가능한 방법이지만 최적은 아닙니다.\n\n일반적인 DB에 저장하는 것의 문제점은 당연히 속도 입니다. 우리는 이전에 얼마나 느릴 수 있는지에 대해 논의했던 것을 기억할 겁니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 반면, 메모리 변수에 저장하는 것은 많은 사용자 수에 대응할 수 없는 해결책입니다.\n\n이제 두 가지 옵션이 남았습니다. 소켓.io 내장 기능을 활용하거나 이전 섹션에서 논의한 캐시된 사용자 객체에 저장하는 방법입니다.\n\n```js\n{\n  \"64c900a9d01c8c1a4351040c\": {\n    \"accessToken\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsIndoeSBhcmUgeW91IjoibG9va2luZyBoZXJlPyJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.l_2-T20JUdnz5rrOORgH6zfI6nrEzmIHMH5JlU76IIE\",\n    \"socketID\": \"소켓 ID\"\n  }\n}\n```\n\n이제 상담 중인 상대방으로부터 소켓 ID를 받아와 두 당사자 간 쉽게 메시지를 전송할 수 있게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 이 글이 제 첫 글이니, 내가 실수를 한 경우 언제든지 연락주세요\n\n# 결론\n\nRedis를 NestJS 애플리케이션에 사용하면 성능을 향상시키고 보안을 강화하며 실시간 기능을 활성화하는 다면적인 접근 방법을 제공합니다.\nRedis를 캐싱 솔루션으로 원활하게 통합하여 응답 시간을 크게 개선하고 데이터베이스 부하를 줄이면서 전반적인 사용자 경험을 향상시킬 수 있습니다.\n또한 Redis는 JWT 토큰 유효성 검사를 위한 신뢰할 수 있는 세션 관리자로 작용하며, 실시간 애플리케이션의 소켓 연결을 관리하는 전략적 방법을 제공하여 애플리케이션 보안을 보장하는 데 귀중한 역할을 합니다.\n본 문서를 통해 개발자들은 Redis의 전체 잠재력을 깨달을 수 있어서, NestJS 프로젝트를 확장 가능성, 효율성 및 견고성을 향상시키는 데 도움이 될 것입니다.\n\n🚀 전문 풀스택 개발자로 프로젝트를 강화하세요 🚀\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n안녕하세요! 저는 요르단 암만을 기반으로 활동하는 숙련된 시니어 풀스택 웹 개발자, 무틀락 알사따입니다.\n원활한 디지털 경험을 만들어내는 데 열정을 가지고 문제 해결에 능숙한 저는 아이디어를 기능적이고 매력적인 웹 솔루션으로 변환하는 데 전문화되어 있습니다.\n\n👨‍💻 기술 전문성: 전체 웹 개발 스펙트럼에 걸쳐 전문 지식을 확보하고 있습니다. Angular와 같은 프런트엔드 기술부터 NestJS와 같은 백엔드 프레임워크까지 다룹니다.\n기능과 디자인을 완벽하게 조화시키는 응용 프로그램 설계에서 힘을 발휘합니다.\n\n🔗 엔드 투 엔드 솔루션: 사용자 중심 인터페이스 구축, 백엔드 성능 최적화, 서드파티 서비스 통합 등 다양한 프로젝트 요구 사항을 충족하는 종합적인 솔루션을 만드는 데 능숙합니다.\n\n💡 혁신적인 문제 해결자: 복잡한 도전에 직면하는 것을 즐깁니다.\n캐싱 전략을 통한 성능 최적화 경험 및 안전한 사용자 인증 솔루션 구현을 통해 혁신에 대한 저의 헌신을 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n🛠️ 맞춤형 개발: 모든 프로젝트는 같지 않습니다. 저는 각 클라이언트의 고유한 요구 사항에 맞게 접근 방식을 맞춤화하는 데 자부심을 갖고 있습니다. 전자 상거래 플랫폼부터 동적 웹 앱까지, 영향을 주는 맞춤 솔루션을 전달하는 데 헌신하고 있습니다.\n\n🌐 글로벌 시각: 글로벌 기술 분야의 통찰력을 활용하여 다양한 시각을 제공합니다. 협동적인 성향과 다양한 산업 트렌드에 적응하는 능력을 바탕으로 프로젝트가 최첨단 기술을 유지하도록 보장합니다.\n\n🌱 지속적인 학습: 끊임없이 변화하는 기술 세계에서 앞서 나가는 것이 저에게 중요합니다. 새로운 도구, 프레임워크 및 모베스트 프랙티스를 지속적으로 탐색하여 제공하는 솔루션이 기술의 선두에 있도록 합니다.\n\n📊 결과 중심: 제 관심사는 코드 작성뿐만 아니라 측정 가능한 결과를 제공하는 데 있습니다. 사용자 참여도 개선, 사이트 속도 향상 또는 전환율 최적화 등 명확한 결과를 달성하기 위해 헌신하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n🤝 고객 중심 접근: 효과적인 커뮤니케이션과 이해는 제 프리랜스 실무의 핵심입니다. 저는 고객과 긴밀히 협력하여 그들의 비전이 기능적 현실로 옮겨지고 기대를 뛰어넘는 결과물이 되도록 합니다.\n\n🌟 함께 협업해요: 디지털 포부를 실현해 줄 전문 풀스택 개발자를 찾고 계시다면, 여러분의 프로젝트에 참여할 수 있는 기회를 소개해드릴게요. MutlaqAlsadeed@gmail.com 으로 연락 주시거나, 어떻게 제가 여러분의 성공에 기여할 수 있는지 알아보세요.\n\nLinkedIn에서 저와 연결하세요: Mutlaq Alsadeed\nGitHub에서 제작물을 탐험하세요: Mut1aq","ogImage":{"url":"/assets/img/2024-05-17-UsingRedisinNestJS_0.png"},"coverImage":"/assets/img/2024-05-17-UsingRedisinNestJS_0.png","tag":["Tech"],"readingTime":15},{"title":"JavaScript에서 이벤트 기반 API를 Promises로 적용하기","description":"","date":"2024-05-17 20:29","slug":"2024-05-17-BuildingaSyncBridge","content":"\n\n## JavaScript에서 이벤트 기반 API를 Promises로 적응하기\n\n![이미지](/assets/img/2024-05-17-BuildingaSyncBridge_0.png)\n\n이벤트 기반 아키텍처(EDA)는 느슨하게 결합되고 성능이 우수하며 확장 가능한 웹 앱을 구축하는 강력한 방법입니다. 이것은 푸시 알림, 공동 편집 및 멀티플레이어와 같은 풍부한 경험을 제공하며 실시간 상호작용 및 모듈화를 장려합니다.\n\n하지만 때로는 모델이 개발자로서 우리가 해야 할 일과 일치하지 않을 수 있습니다. 두 응용 프로그램 계층이 비동기 메시지 전달을 통해만 통신할 수 있는 경우, 코드를 서투르게 구조화해야 할 수도 있습니다. 요청 코드를 수신 코드와 함께 동일한 위치에 두지 못하고, 청취기 또는 구독을 직접 관리해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사에서는 이벤트 기반 API를 편리한 Promise 기반 API로 적응하는 일반적인 솔루션을 소개하고 있습니다. 이를 통해 메시지 전달의 복잡성과 보일러플레이트를 숨기고 응용 프로그램 경계를 가로지르는 선형 코드를 작성할 수 있습니다.\n\n## 요청/응답 대 이벤트 기반 아키텍처\n\n웹의 전통적인 응용 프로그램은 REST, GraphQL, RPC 및 기타 요청/응답 모델을 따르는 사양을 통해 HTTP를 통해 경계를 가로지를 통신을 처리합니다. 이 모델은 요청자가 메시지를 보낸 다음 응답자가 메시지를 받아들이고 처리하고 응답하기를 기다리는 방식으로 특징 지어집니다. 이는 자바스크립트의 비동기 함수 안에서 발생할 수 있지만, 이를 \"동기식\" 또는 \"인밴드\"로 일반적으로 참조할 수 있습니다. 요청에 대한 응답이 즉시 기대되고 요청을 실행하는 컨텍스트가 해당 응답을 기다리게 된다는 점에서입니다.\n\n![Building a Sync Bridge - 1](/assets/img/2024-05-17-BuildingaSyncBridge_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nEDA(이벤트 기반 아키텍처) 또는 발행/구독 모델이라고도 불리는 것은 데이터 요청 및 수신 프로세스가 분리되어 논블로킹 및 비동기적으로 수행된다는 특징이 있어요. 일반적으로 클라이언트는 서버로부터 메시지를 구독하고, 서버는 클라이언트로부터 메시지를 받습니다. 클라이언트가 데이터를 요청할 때, 단순히 메시지를 보내고 실행을 계속합니다. 서버는 이 메시지를 받아 처리하고 어느 시점에서 다시 클라이언트로 다른 메시지를 보낼 것이에요. 클라이언트는 구독자로서 이 메시지를 \"원래 요청으로부터 외줄로\" 받아 유용한 대로 처리할 수 있습니다. 중요한 점은 이것이 다른 시간에 이루어지거나 다른 네트워크 요청 또는 다른 프로토콜을 사용해도 된다는 것이에요.\n\n![이미지](/assets/img/2024-05-17-BuildingaSyncBridge_2.png)\n\n이벤트 기반 모델의 능력에는 몇 가지 주요 장점이 있어요. 먼저, EDA는 클라이언트에게 요청하지 않아도 서버에서 이벤트를 알릴 수 있습니다. 이는 비싼 폴링을 제거하고, 다른 곳에서 발생한 알림 및 이벤트에 대한 \"푸시\" 동작을 가능하게 합니다. 둘째, 이는 메시지 처리를 메시지 전송과 분리할 수 있어서 덜 결합된 코드를 유도할 수 있습니다. 셋째, 이는 개발자에게 병렬 처리를 할 수 있게 하고, 견고하고 이해하기 쉬운 시스템을 구축할 수 있게 합니다. 넷째, 오직 인식된 메시지만이 구독자에 의해 처리되므로 시스템이 기본적으로 내결함적입니다.\n\n웹 기술인 웹훅(Webhooks), 웹소켓(WebSockets), 서버-전송 이벤트(Server-Sent Events)와 MQTT, AMQP와 같은 프로토콜, 이러한 것 위에 구축된 다양한 도구를 사용하여 강력한 이벤트 기반 응용 프로그램을 구현할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## EDA(이벤트 주도 아키텍처)가 방해할 때\n\n복잡한 앱에서 이벤트 주도 아키텍처는 많은 장점을 제공할 수 있습니다. 하지만 때로는 데이터를 특정 실행 컨텍스트에서 즉시 필요로 하는 경우도 있습니다. 때로는 원격 리소스나 절차를 마치 로컬인 것처럼 다루고 싶은 경우도 있습니다.\n\n다소 인위적인 예로, 사용자 입력에 대해 비용이 많이 드는 계산을 수행해야 하는 애플리케이션이 있다고 가정해 봅시다. 이 계산을 웹 워커에서 수행하는 것이 가장 좋다고 판단하여, 이 작업을 메인 UI 스레드에서 사이클을 소비하지 않고 작업을 수행하는 별도의 스레드를 사용하는 방법으로 설정했습니다. 앱에서 워커와 통신하기 위해 몇 가지 간단한 로직을 설정했습니다:\n\n우리의 Worker 모듈은 메인 스레드로부터 메시지를 수신하고, 비용이 많이 드는 계산을 수행한 후 결과를 메인 스레드로 응답하는 리스너 역할을 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 기대하는 대로 작동합니다. 고객이 계산을 요청한 다음 나중에 결과를 받아 doSomethingWithResult를 수행할 수 있습니다. 그러나 이 솔루션은 expensiveComputation을 수행할 위치에 제한을 가합니다. 우리는 요청을 하고 응답을 동일한 위치에서 사용할 수 없습니다. 이것은 외부 라이브러리 코드나 비동기 함수의 중간과 같은 우리가 제어력이 부족한 컨텍스트에서 해당 기능을 사용하려고 할 때 도전이 될 수 있습니다. \"이 데이터가 필요하고 여기서 기다리겠다\"라고 말할 수 있다면 좋을텐데요.\n\nSync Bridge가 나타났습니다.\n\n## Sync Bridge\n\n이벤트 스트림을 \"동기적\" 방식으로 사용하려면 인터페이스를 Promise를 사용하는 방식으로 변환해야 합니다. 즉, Sync Bridge가 필요합니다. 이벤트 기반 API를 Promise 기반 API로 변환하는 과정은 몇 가지 단계로 나눌 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 클라이언트에서 메시지를 보내기 전에 요청자를 고유하게 식별할 수 있는 ID나 방법을 메시지에 첨부하십시오. 이것이 우리의 \"회신 주소\"입니다.\n- \"빈\" Promise를 만들고 이 Promise와 연결된 해결 및 거부 콜백에 메시지의 ID를 연결하여 보류 중인 데이터 구조에 저장하십시오. Map을 사용하는 것이 좋습니다.\n- Promise를 요청자에게 반환하고 메시지를 보냅니다.\n- 호스트에서 클라이언트 메시지를 구독하십시오. ID를 가진 메시지를 받으면 보통대로 처리하되, 응답 메시지에 동일한 ID를 포함하십시오.\n- 클라이언트에서 호스트 메시지를 구독하십시오. ID를 가진 메시지가 수신되면 해당 ID에 대해 보류 중인 Promise가 있는지 확인하십시오. 해당 Promise를 데이터 구조에서 빼내어 호스트 메시지의 내용에 따라 적절히 해결하거나 거부하십시오.\n\n여기서 \"클라이언트\"와 \"호스트\"는 메시지 전달에 참여하는 모든 엔티티가 될 수 있습니다. 때로는 클라이언트가 호스트로 작용하거나 그 반대로 작용할 수 있으므로, 사용되는 문맥에 따라 이러한 엔티티를 \"요청자\"와 \"응답자\"로도 참조할 수 있습니다.\n\n우리는 클라이언트와 호스트 간의 계약을 체결하여 EDA의 제한을 극복할 수 있습니다. 요청과 해당 응답 메시지에 공통적이고 고유한 ID를 표시하는 것에 동의함으로써, 응답을 올바른 요청자에게 \"라우팅\"할 수 있습니다. 우리는 요청 측에 약속을 열어 놓아 메시지를 받았을 때 기다리고 있는 메시지에 실제 데이터를 채워넣습니다.\n\n우리가 이를 이전에 다룬 웹 워커 예제에 적용해 보기 위해, 위에 나열된 프로세스를 추상화하는 도우미 클래스를 작성해 보겠습니다. 메시지에 ID를 할당하고 보류 중인 요청을 추적하고 응답을 청취하는 클라이언트 추상화가 필요할 것입니다. 이를 WorkerClient라고 부르겠습니다 :\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n호스트 쪽에서는, 우리가 관심 없는 메시지를 걸러내고 어떤 작업을 수행하며, 요청자의 \"반송 주소\"로 메시지를 다시 보내는 컨트롤러가 필요할 것입니다. 일종의 프록시 메시지 핸들러, 그렇게 해주는 것이 WorkerHost입니다.\n\n이러한 도우미들을 사용하여 새로운 Promise 기반 API를 사용하도록 애플리케이션 코드를 다시 작성할 것입니다. 이제 클릭 핸들러에서 데이터를 직접 기다릴 수 있다는 점에 유의해 주세요.\n\n우리의 워커도 비슷해 보이지만, 핸들러가 메시지를 게시하는 대신 값만 반환한다는 점이 다릅니다 (메시지 전달은 이미 처리되었습니다).\n\n자, 상당량의 코드를 작성했습니다. 정확히 어떤 이득을 얻었을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n신크 브릿지 어댑터는 실제로 미래에 어떤 메시지를 받을 것을 기대하는 것을 약속으로 변환합니다. 이를 통해 원격 컨텍스트에서 데이터와 코드를 로컬처럼 처리할 수 있습니다. 무엇보다도, 동일한 위치에서 원격 데이터를 요청하고 사용할 수 있게 해줍니다. 데이터베이스 트랜잭션 가운데서 비싼 계산을 해야 하거나 임의 이벤트 핸들러에서, 심지어 다른 이벤트 스트림의 메시지 핸들러에서도 그냥 전화를 해서 처리할 수 있습니다.\n\n또한 이제 다른 유형의 메시지를 이산 채널에 제한하여 메시지 처리를 구체적이고 빠르게 유지하고 코드를 필요한 곳에만 로컬라이징할 수 있습니다. 원한다면 여러 WorkerClient 객체가 동일한 채널을 공유할 수도 있습니다.\n\n이 패턴은 대부분의 이벤트 주도 시스템으로 쉽게 일반화될 수 있습니다. 예제의 helpers를 수정해서 자체 Worker를 구성하는 대신 어떤 EventTarget을 가져오도록 만들어 임의의 메시지 스트림에 대한 일반적이고 동기적인 인터페이스를 제공할 수 있습니다. 또는 figments와 같이 특정 인터페이스에 대한 래퍼 라이브러리를 작성할 수도 있습니다. 최근에 Figma 플러그인 API의 이벤트 주도 부분을 \"브릿지\"한 래퍼 세트인 figments와 같이 말입니다.\n\n## 실제 세계에서\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이벤트 주도 시스템에서 작업할 때 \"이벤트로 생각하는 것\"이 가장 좋지만, 때로는 탈출구가 필요할 수 있습니다. Sync Bridge는 여기 유용한 도구가 될 수 있지만, 구현하기 전에 이 접근 방식이 필요한지 고려해보세요. 대부분의 경우에는 이벤트 처리가 그냥 작동합니다.\n\n오늘 하루 즐겁고 유익한 일들 가득하길 바라요!\n\n```js\n추가 정보 \u0026 자료\n---------------------------\n\n이 기사의 예시 코드\n• rektdeckard/promisize\n\nEDA에 대한 더 많은 정보\n• 이벤트 주도 API – 원리 이해하기\n• 이벤트 주도 아키텍처가 뭐죠?\n\nasync/await 및 Promises에 대한 깊은 논의\n• 이벤트 루프는 대체 뭐죠?\n• Loupe\n• sindresorhus/promise-fun\n```\n\n```js\n나와 내가 하는 일\n-----------------\n\n• 트위터\n• GitHub\n• Phosphor 아이콘\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n표 태그를 Markdown 포맷으로 변경해주세요.","ogImage":{"url":"/assets/img/2024-05-17-BuildingaSyncBridge_0.png"},"coverImage":"/assets/img/2024-05-17-BuildingaSyncBridge_0.png","tag":["Tech"],"readingTime":6},{"title":"Redis를 활용한 NodeJs에서 이벤트 주도 시스템 사용하기","description":"","date":"2024-05-17 20:27","slug":"2024-05-17-UsingDistributedLockingwithRedisinNodeJsinanEvent-DrivenSystem","content":"\n\n\u003cimg src=\"/assets/img/2024-05-17-UsingDistributedLockingwithRedisinNodeJsinanEvent-DrivenSystem_0.png\" /\u003e\n\n공유 리소스를 작업할 때 분산 시스템을 다루다보면 어려움이 있을 수 있어요. 분산 락킹 개념은 일반적으로 데이터베이스, 공유 파일 시스템 및 분산 컴퓨팅 환경에서 사용됩니다.\n\n최근에 저는 직장에서 현재 인보이스 및 결제 시스템에서 작업 중인 시스템에서 경쟁 조건 문제를 마주쳤어요. 이는 실제 결제 인보이스를 덮어쓰고 막대한 고통이 되었습니다. 결제는 실시간으로 이루어지며, 때문에 추적 가능성과 책임 추적이 중요합니다.\n\n맥락을 이해하기 위해 프로세스 및 시스템 아키텍처에 대해 간단히 설명하겠어요. 서비스는 NodeJS로 작성되었으며 Kubernetes 클러스터에 배포되어 있으며 언제든 동일한 서비스의 여러 인스턴스가 실행됩니다. 인보이스 시스템은 단계별로 분할되어 각 이벤트에 의해 순차적으로 실행되는 유한 상태로 구성되어 있습니다. 결제 프로세스는 언제든지 어느 상태에 있을 수 있습니다. 먼저 결제해야 할 최종 금액이 계산됩니다. 그런 다음 인보이스가 생성되며, 마지막으로 해당 인보이스에 대한 결제가 즉시 이루어집니다. 어떤 결함이 있을 경우 여러 인보이스가 생성되는 것을 피하기 위해 여러 체크 및 균형이 유지되고 있지만 여전히 무언가가 잘못되고 있었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제의 근본 원인은 여러 팟에서 송장 생성 상태를 동시에 실행했기 때문에, 이를 추적하는 것이 조금 까다로웠습니다. RabbitMQ는 메시지 패킷이 지정된 기간 내에 확인되지 않으면 재큐됩니다. 이 조건은 멀티 팟이 활성화되고 API 호출이 느릴 가능성이 높을 때 시스템에서만 발생했습니다. 상태 실행이 예상보다 오래 걸려 timeout이 발생하고 실행이 진행 중일 때 이벤트가 재큐됩니다. 부하 상태에서 API 호출이 실패하면 상태가 예외 상태로 전환되고, 상태를 다시 시도하는 새 이벤트가 대기열에 추가됩니다. 이는 대기열에 두 개 이상의 중복 이벤트가 결과로 생기게 되며, 다른 팟이 소비하면 동시 실행이 발생하여 양쪽 실행이 모두 확인을 통과하고 마지막 실행이 이전 송장을 덮어쓰지만 결제가 이미 완료된 상태에 대해 발생합니다. 다행히도, 우리는 결제 중에 결제가 성사된 경우에 대한 추가 확인 사항이 있었습니다. 휴!\n\n나는 이 문제에 대한 해결책을 찾기 위해 연구를 시작했고, 내 첫 번째 생각은 상태에 잠금을 구현하는 것이었습니다. 그 후에 Redis를 사용한 분산 잠금 개념을 알게 되었는데, 이미 산업계에서 다양한 응용 분야에 널리 사용되고 있습니다. 그래서 표준 구현을 사용하고 우리의 필요에 맞게 적용하기로 결정했습니다. 이것이 간단하고 영리한 Redis 사용이 얼마나 강력할 수 있는지 흥미롭습니다. 기본 개념은 리소스 실행 시작 시 Redis에 키가 설정되며, 그 키는 실행이 끝날 때 지워질 수 있습니다. 키는 리소스에 고유해야 합니다. 실행 전마다 Redis에서 해당 키가 있는지 확인하여 이미 잠그인 리소스가 있으면 실행을 계속하지 말아야 합니다. 데드락을 피하려면 잠금에 만료 시간을 설정해야 하는 몇 가지 유의사항이 있습니다. 또한, 잠금 키에 대해 매번 고유한 값을 설정하고 이를 해제 시 검사하여 잠금된 리소스를 시도하는 것을 보장해야 합니다. 이러한 기능들은 모두 쉽게 구현할 수 있습니다. 더불어 분산 Redis 클러스터에 구현하는 것은 훨씬 더 까다로울 것입니다. 서로 다른 프레임워크용 많은 라이브러리가 이미 제공되고 있어 같은 기능을 제공합니다. NodeJS의 경우, 공식 권장 라이브러리는 RedLock입니다.\n\nRedis는 다양한 용도로 확장할 수 있는 다재다능한 도구입니다.","ogImage":{"url":"/assets/img/2024-05-17-UsingDistributedLockingwithRedisinNodeJsinanEvent-DrivenSystem_0.png"},"coverImage":"/assets/img/2024-05-17-UsingDistributedLockingwithRedisinNodeJsinanEvent-DrivenSystem_0.png","tag":["Tech"],"readingTime":2}],"page":"70","totalPageCount":151,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"70"},"buildId":"t9N7vwmpvBMQnO2PSctoH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>