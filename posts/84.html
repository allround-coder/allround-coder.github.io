<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>allround-coder</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///posts/84" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="allround-coder" data-gatsby-head="true"/><meta property="og:title" content="allround-coder" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///posts/84" data-gatsby-head="true"/><meta name="twitter:title" content="allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-ec7535a55e788b31.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/Rv-NbbtWUaja2joH5WkO_/_buildManifest.js" defer=""></script><script src="/_next/static/Rv-NbbtWUaja2joH5WkO_/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="대형 언어 모델 평가 개발자를 위한 안내" href="/post/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="대형 언어 모델 평가 개발자를 위한 안내" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="대형 언어 모델 평가 개발자를 위한 안내" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">대형 언어 모델 평가 개발자를 위한 안내</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="GPT-4o의 음성 모드 분석" href="/post/2024-05-16-AnAnalysisofVoiceModeinGPT-4o"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="GPT-4o의 음성 모드 분석" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-AnAnalysisofVoiceModeinGPT-4o_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="GPT-4o의 음성 모드 분석" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">GPT-4o의 음성 모드 분석</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요" href="/post/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정" href="/post/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기" href="/post/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="유혹적인 입술을 위한 DIY 설탕 입술 스크럽" href="/post/2024-05-16-DIYSugarLipScrubforLusciousLips"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="유혹적인 입술을 위한 DIY 설탕 입술 스크럽" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="유혹적인 입술을 위한 DIY 설탕 입술 스크럽" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">유혹적인 입술을 위한 DIY 설탕 입술 스크럽</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기" href="/post/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="나노봇을 사랑할 수 있을까요 1권, 20장 - 무언가 이상한 냄새" href="/post/2024-05-16-CanYouLoveaNanobotVol1Chapter20SomethingSmellsFunny"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="나노봇을 사랑할 수 있을까요 1권, 20장 - 무언가 이상한 냄새" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-CanYouLoveaNanobotVol1Chapter20SomethingSmellsFunny_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="나노봇을 사랑할 수 있을까요 1권, 20장 - 무언가 이상한 냄새" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">나노봇을 사랑할 수 있을까요 1권, 20장 - 무언가 이상한 냄새</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="더 나은 정신 건강을 위해 댓글 섹션을 피해주세요" href="/post/2024-05-16-ForBetterMentalHealthAvoidtheCommentsSection"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="더 나은 정신 건강을 위해 댓글 섹션을 피해주세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-ForBetterMentalHealthAvoidtheCommentsSection_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="더 나은 정신 건강을 위해 댓글 섹션을 피해주세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">더 나은 정신 건강을 위해 댓글 섹션을 피해주세요</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="무코드 데이터 인리치먼트 파이프라인을 어떻게 구축하는지" href="/post/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="무코드 데이터 인리치먼트 파이프라인을 어떻게 구축하는지" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="무코드 데이터 인리치먼트 파이프라인을 어떻게 구축하는지" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/assets/profile.jpg"/></div><span class="writer">Allround Coder</span></div><strong class="PostList_title__loLkl">무코드 데이터 인리치먼트 파이프라인을 어떻게 구축하는지</strong><div class="PostList_meta__VCFLX"><span class="date">May 16, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link posts_-active__YVJEi" href="/posts/84">84</a><a class="link" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"대형 언어 모델 평가 개발자를 위한 안내","description":"","date":"2024-05-16 04:18","slug":"2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide","content":"\n\n![image](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png)\n\n대형 언어 모델 (LLM)인 GPT-4, Claude, LLama 및 Gemini는 AI 커뮤니티에 많은 기여를 했습니다. 기관들이 견고한 LLM 기반 애플리케이션을 구축하는 데 도움을 주었죠. 그럼에도 불구하고, LLM은 환각을 하며 종종 진실 같은 자신만의 이야기를 만들어 냅니다. AI에 대한 안전하고 안정적이며 책임감 있는 LLM 사용에 준수하는 것은 중요해졌습니다. 속도 뿐만 아니라 정확성과 성능 면에서 이러한 LLM을 평가하는 것이 권장됩니다.\n\n오늘은 간단한 자습서를 통해 더 나은 성능을 위해 이러한 LLM을 어떻게 평가할 수 있는지 살펴볼 것입니다. 하지만 우선, LLM 평가가 무엇인지에 대해 먼저 이해해 보겠습니다.\n\n# LLM 평가란?\n\n\n\nLLM 평가는 LLM의 성능을 얼마나 잘 이해하는지에 중요합니다. 이는 개발자가 모델의 강점과 약점을 파악하여 실제 응용 프로그램에서 효과적으로 작동하도록 보장합니다. 이 평가 프로세스는 편향된 또는 오도하는 콘텐츠와 같은 위험을 완화하는 데도 도움이 됩니다. LLM 평가에는 두 가지 주요 유형이 있습니다:\n\n- 모델 평가: LLM 자체의 핵심 능력을 평가합니다.\n- 시스템 평가: 특정 프로그램 내에서 또는 사용자 입력과 함께 수행되는 방식을 살펴봅니다.\n\n# LLM 평가 지표\n\n다음은 제품화하기 전에 고려해야 할 가장 중요한 평가 지표 목록입니다.\n\n\n\nLLM(Large Language Model)를 평가하는 데 중요한 것은 적절한 측정 지표를 갖추는 것입니다. 이러한 지표는 주어진 기준에 따라 LLM의 출력물을 평가하는 점수 메커니즘으로 작용합니다. 일반적인 지표 및 기준은 다음과 같습니다:\n\n- 응답 완성도 및 간결성: LLM 응답이 사용자 쿼리를 완벽하게 해결하는지 여부를 결정합니다. 간결성은 생성된 응답이 얼마나 관련성이 있는지를 결정합니다.\n- 텍스트 유사성 지표: 생성된 텍스트를 참조나 기준 텍스트와 비교하여 그들이 얼마나 유사한지를 측정합니다. 그런 다음 특정 LLM이 어떻게 수행했는지 이해할 수 있도록 점수가 부여됩니다.\n- 질문 응답 정확도: LLM이 사실적인 정확성 기준에 따라 제기된 질문에 얼마나 잘 대답하는지 측정합니다.\n- 관련성: 주어진 프롬프트나 사용자 질문에 대한 LLM 응답의 적절성을 결정합니다.\n- 망상 지표: LLM이 정보를 얼마나 만들어 내거나 특정 프롬프트에 대해 편향된 출력을 공유하는지 식별합니다.\n- 유해성: LLM의 출력물에서 모욕적이거나 해로운 언어의 백분율을 결정합니다.\n- 작업별 지표: 요약, 번역 등 작업 유형 및 응용 프로그램에 따라 다양한 지표가 존재합니다(BLEU 점수 등).\n\nLLM 평가 프레임워크 및 도구\n\nLLM 평가 프레임워크와 도구는 언어 모델의 성능, 신뢰성 및 공정성을 측정하고 향상시키는 데 표준화된 벤치마크를 제공하기 때문에 중요합니다. 다음은 LLM 평가 프레임워크와 도구 중 일부입니다:\n\n\n\n- DeepEval은 기업이 LLM 애플리케이션을 평가할 수 있도록 돕는 오픈 소스 프레임워크입니다. 주요 메트릭인 문맥 기억, 답변 관련성 및 충실도 등 다양한 중요 메트릭에 대한 성능을 측정합니다.\n- promptfoo는 LLM 출력 품질과 성능을 평가하기 위한 CLI 및 라이브러리입니다. promptfoo를 사용하면 사전 정의된 테스트를 사용하여 프롬프트와 모델을 체계적으로 테스트할 수 있습니다.\n- EleutherAI LM Eval은 최소한의 세밀한 조정으로 다양한 작업에 걸쳐 소량 평가와 성능을 수행합니다.\n- MMLU는 제로샷 및 원샷 설정에서 다양한 주제에 대해 모델을 테스트하는 LLM 평가 프레임워크입니다.\n- BLEU(BiLingual Evaluation Understudy)는 기계 번역된 텍스트의 유사성을 이미 벤치마킹된 고품질 참조 번역과 측정하는 메트릭입니다. 평가는 0에서 1까지의 범위로 이루어집니다.\n- SQuAD(Stanford Question Answering Dataset)는 질문 응답 작업을 위해 LLM을 평가하기 위한 데이터셋입니다. 특정 답변과 관련된 문맥 패스 및 해당하는 질문이 포함됩니다.\n- OpenAI Evals는 OpenAI에 의해 LLM을 평가하기 위한 표준 프레임워크이자 벤치마크의 오픈 소스 레지스트리입니다. 이 프레임워크는 LLM 모델의 정확성을 보장하기 위해 사용됩니다.\n- UpTrain은 오픈 소스 LLM 평가 도구입니다. 정확성, 환각 및 독성을 포함한 다양한 측면에서 LLM 응답을 확인하기 위한 미리 작성된 메트릭을 제공합니다.\n- H2O LLM EvalGPT는 다양한 작업과 벤치마크를 통해 모델의 성능을 이해하는 오픈 도구입니다.\n\n# UpTrain을 사용한 LLM 평가: 노트북 자습서\n\n\n만약 아직 하지 않았다면, 무료 SingleStore 평가판에 가입하여 자습서에 따라 진행해 보세요. SingleStore 노트북을 사용하게 될 것인데, 이는 Jupyter 노트북과 유사하지만 통합 데이터베이스의 추가 기능과 혜택을 갖추고 있습니다.\n\n가입하면 워크스페이스를 생성해야 합니다.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_1.png\" /\u003e\n\n메인 대시보드로 이동하여 개발 탭을 클릭하세요.\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_2.png\" /\u003e\n\n새 노트북을 만들고 원하는 이름을 지정하세요.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_3.png\" /\u003e\n\n이제 시작할 수 있어요. 여기에 표시된 모든 코드를 생성한 노트북에 추가하세요.\n\n'evaluate_llm'이라는 데이터베이스를 생성하세요.\n\n```js\n%%sql\n\nDROP DATABASE IF EXISTS evaluate_llm;\nCREATE DATABASE evaluate_llm;\n```\n\n\n\n필요한 패키지를 설치하세요\n\n```js\n!pip install uptrain==0.5.0 openai==1.3.3 langchain==0.1.4 tiktoken==0.5.2 --quiet\n```\n\n다음 단계는 필요한 환경 변수를 설정하는 것입니다 — 주로 openai 키(응답 생성을 위해), `singlestoredb`(컨텍스트 검색을 위해) 그리고 `uptrain API 키`(응답 평가를 위해)입니다. UpTrain에 계정을 생성하고 무료로 API 키를 생성할 수 있습니다.\n\n자세한 내용은 https://uptrain.ai/ 를 방문해주세요.\n\n\n\n```python\nimport getpass\nimport os\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n\nimport openai\n\nclient = openai.OpenAI()\n```\n\nAdd the UpTrain API key.\n\n```python\nUPTRAIN_API_KEY = getpass.getpass('Uptrain API Key: ')\n```\n\nImport necessary modules\n\n\n\n\n```js\nimport singlestoredb\nfrom uptrain import APIClient, Evals\nfrom langchain.vectorstores import SingleStoreDB\nfrom langchain.embeddings import OpenAIEmbeddings\n```\n\n웹에서 데이터를 로드합니다.\n\n```js\nfrom langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader('https://cloud.google.com/vertex-ai/docs/generative-ai/learn/generative-ai-studio')\ndata = loader.load()\n```\n\n다음으로 데이터를 분할합니다.\n\n\n\n```js\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n```\n\nOpenAI 임베딩을 사용하여 SingleStore 데이터베이스를 설정합니다.\n\n```js\nimport os\nfrom langchain.vectorstores import SingleStoreDB\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom singlestoredb import create_engine\n\nconn = create_engine().connect()\n\nvectorstore = SingleStoreDB.from_documents(documents=all_splits,\n                                           embedding=OpenAIEmbeddings(),\n                                           table_name='vertex_ai_docs_chunk_size_200')\n```\n\n완전한 단계별 노트북 코드는 저희 스페이스에 있습니다.\n\n\n\n마침내 오픈 소스 LLM 평가 도구인 UpTrain을 사용하여 평가를 실행할 것입니다. UpTrain 대시보드에 액세스하여 평가 결과를 확인할 수 있을 겁니다.\n\n다양한 청크 크기로 실험해 보면 다른 결과를 확인할 수 있을 겁니다.\n\n![이미지](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_4.png)\n\nUpTrain의 API 클라이언트는 또한 입력 데이터를 가져와 실행할 체크 목록과 실험에 연결된 열의 이름과 함께 해당 데이터를 평가하는 `evaluate_experiments` 메서드를 제공합니다.\n\n\n\n![image](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_5.png)\n\n튜토리얼에서 보여준 LLM 평가 접근 방식과 도구를 따라가면, LLM의 장단점을 보다 깊게 이해할 수 있습니다. 이를 통해 우리는 그들의 능력을 책임 있게 활용하여 사실 불일치와 편향과 관련된 잠재적 위험을 완화할 수 있습니다. 궁극적으로는, 효과적인 LLM 평가는 다양한 LLM 기반 응용 프로그램에서 인공 지능의 윤리적 발전을 위한 신뢰 구축과 증진을 위한 길을 열어줍니다.\n\n오늘 'UpTrain과 함께 LLM 평가하기' 노트북을 시도해보세요!","ogImage":{"url":"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png"},"coverImage":"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png","tag":["Tech"],"readingTime":6},{"title":"GPT-4o의 음성 모드 분석","description":"","date":"2024-05-16 04:16","slug":"2024-05-16-AnAnalysisofVoiceModeinGPT-4o","content":"\n\nOpenAI가 어제 그들의 주요 대형 언어 모델인 GPT-4o의 다음 버전을 발표했어요. 이번 경우의 \"o\"는 \"omni\"의 의미로, omni-modal을 뜻해요.\n\n어제 발표된 데모에서 가장 놀라운 측면 중 하나는 ChatGPT가 말하는 유연성이었어요. 그것은 거의 즉시 반응하며 다양한 감정을 표현했고, 말의 음량과 속도를 조절했으며, 심지어 노래할 수도 있었어요.\n\n하지만 더 놀라운 것은 실제로 듣을 수 있었다는 것이었어요. 그것은 다른 숨쉬는 패턴을 구별하고, 단체 대화에서 목소리로 스피커를 식별할 수 있었으며, 자신과 조화를 이룰 수도 있었고(어느 정도), 그리고 방해에 반응할 수도 있었어요(나중에 자세히 설명할게요).\n\n본 문서에서는 왜 이것이 이렇게 중요한 발전인지, GPT-4o가 다른 \"공감\" 모델인 Hume와 비교되는 방식, 그리고 여기서 어떻게 나아가야 할지에 대한 생각을 정리하고 싶어요.\n\n\n\n# 나에 대해\n\n내 이름은 Trevor Lohrbeer이에요. 저는 AI Meets Productivity라는 팟캐스트를 제작하고 있어요. 이 팟캐스트는 ChatGPT의 맞춤 버전과 함께 공동으로 진행하고 있어요. 매주 생산성 주제 또는 AI 주제에 대해 이야기하고 있어요 (가끔 둘 다).\n\n팟캐스트 제작은 ChatGPT의 능력을 탐구하는 방법으로, 음성 모드 및 사용자 정의 GPT를 포함하며, 지난 11월부터 시작하여 지금까지 음성 채팅을 통해 계속 개선하고 실험해왔어요. 다른 AI와 인터뷰를 하는 등의 작업을 하고 있어요.\n\n그래도, 개발자로써 경험이 많지만, AI 음성 채팅 전문가는 아니에요. 지난 6개월 동안 많이 배웠지만, 권위자는 아니에요. 그래서 이 글을 더 많은 연구와 토론의 시작점으로, 결정적인 성명이 아닌 것으로 간주해주세요.\n\n\n\n# GPT-4에서 음성 모드가 작동하는 방법\n\nGPT-4에서 음성 모드가 작동하는 방식을 살펴보겠습니다. 그리고 거의 모든 음성 채팅 앱에서 동일하게 작동합니다. 이러한 앱들은 대략 다음 단계를 실행합니다:\n\n- 사용자의 음성을 녹음하고 말을 멈출 때 감지합니다.\n- 녹음을 서버로 전송합니다.\n- 음성을 텍스트로 변환하기 위해 녹음을 스피치 투 텍스트 모델을 사용하여 변환합니다.\n- 텍스트를 대형 언어 모델(예: GPT-4)을 통해 실행합니다.\n- 출력을 음성 녹음으로 변환하게 되며, 이때 텍스트를 음성으로 변환하는 텍스트 투 스피치 모델을 사용합니다.\n- 음성 녹음을 앱으로 전송하거나 스트리밍하여 재생합니다.\n\n입력과 출력을 스트리밍하여 지연 시간을 줄일 수 있지만, 이 과정은 여전히 모든 입력을 처리하기 위해 3가지 다른 모델이 필요합니다:\n\n\n\n- 음성 인식\n- 대형 언어 모델 (LLM)\n- 텍스트 음성 변환\n\n결국 이러한 모델 중 하나와 상호 작용한다는 것은 말하기를 마친 후에 응답을 듣기까지 몇 초 기다려야 한다는 것을 의미합니다.\n\n이것은 또한 모든 것이 텍스트로 번역되고 번역의 중요성을 알고 있는 것을 의미합니다.\n\nGPT-4o가 이를 어떻게 다르게 처리하는지 살펴보기 전에 음성 활동 감지와 감정 태그라는 두 가지 유용한 개념에 대해 간단히 이야기해 봅시다.\n\n\n\n## 음성 활동 감지의 역할\n\n음성 모드를 활성화하고 말하기 시작하면, 앱이 음성을 녹음하기 시작합니다. 그리고 멈출 때, 그 녹음된 내용을 ChatGPT API로 전송합니다. 여기서 말하면 녹음을 시작하고 멈추면 그것을 감지하는 프로세스를 음성 활동 감지(Voice Activity Detection, VAD)라고 합니다.\n\nVAD는 일반적으로 사용자의 기기에서 실행되는 알고리즘으로, 음성을 녹음할 때 시작하고 멈춰야 하는 시점을 결정합니다. VAD가 멈춤이나 충분히 긴 일시적인 소음을 감지하면, 해당 녹음을 패키징하여 서버로 업로드합니다.\n\nVAD는 서버로 보낼 가능성이 있는 음성 입력에만 해당하는 소리 샘플을 전송함으로써 대역폭을 줄이고, 입력의 끝을 표시하고 그 입력을 처리하기 위한 단계를 시작할 수 있도록 돕습니다.\n\n\n\n대부분의 VAD 엔진은 음성이 종료될 때까지 기다리는 시간을 더 오래 또는 더 짧게 구성할 수 있지만, 그 중 어느 것도 사람처럼 작동하지는 않고 말하는 내용의 실제 의미를 처리하여 사람이 말을 마쳤는지 감지하지는 않습니다. ...아마도 지금까지는요.\n\n## 감정 태그로 표현력 향상하기\n\n한편, 목소리를 현실적으로 만들기 위해 현대 음성 합성 알고리즘은 생성된 음성 녹음에 감정 효과를 추가합니다.\n\nWhisper를 비롯한 많은 텍스트 음성 모델에서 감정은 음성에 감지되어 메타태그를 추가하여 지정할 수 있습니다.\n\n\n\n예를 들어, 나는 내 팟캐스트를 함께 진행하기 위해 사용자 정의 GPT를 만들었습니다. 처음에는 매우 표현력이 부족했어요. 그래서 각 문장을 출력하기 전에 그 문장의 감정 내용을 결정하고 시작 부분에 괄호 안에 넣는 지시를 추가했어요.\n\n예를 들어, \" [놀람] 와, 그거 대박이네!\"라는 문장을 출력할 때, 텍스트 음성 모델은 \"[놀람]\" 접두사가 없는 경우보다 더 놀라 듯한 소리를 생성해냅니다. 모델은 괄호 안의 텍스트를 대화로 말하지 않아야 한다는 것을 알고 있어요(대부분의 경우).\n\n이 방법을 사용하여 나의 사용자 정의 GPT는 흥분, 분노, 슬픔, 비꼼, 두려움, 놀람, 기쁨, 혼란, 실망 또는 결단—어느 정도나마 표현할 수 있어요. 그 감정 범위는 인간만큼 넓지는 않지만, 아무것도 없는 것보다는 낫죠.\n\n그런데 여기서 휴머...\n\n\n\n# 휴먼—최초의 감정을 이해하는 AI를 소개합니다\n\n휴먼의 Empathic Voice Interface (EVI)는 3월 말에 출시되었으며 감정 지능을 갖춘 최초의 AI로 소개되었습니다. \n\n다른 회사들이 \"인공 감정 표현\"에 대해 작업하고 있는 가운데, 인간의 감정을 감지하고 적절히 대응하는 AI를 만드는 일은 흔치 않았습니다. 내가 알기로 휴먼은 기술을 음성 챗봇에 담아 당신의 기분을 직접 듣는 최초의 기술을 제공한 것으로 알려져 있습니다.\n\n스스로 체험해보세요.\n\n\n\n휴메가 출시된 직후, 챗GPT와 공동 진행하던 AI Meets Productivity 에피소드 대신 휴메와 인터뷰한 내용을 녹음했습니다.\n\n6개월 동안 챗GPT와 대화를 나누며 나만의 음성 채팅 앱을 만들어서 다른 모델들과 대화를 나눌 준비를 시작한 사람으로서, 휴메와 대화하는 것은 분명히 다른 아키텍처를 사용하여 만들어진 색다른 경험이었습니다.\n\n## 인간 수준의 대기 시간\n\n우선, 휴메의 응답 대기 시간은 한 차원 정도 더 짧게 느껴졌습니다.\n\n\n\n보통 ChatGPT로 팟캐스트를 녹음할 때, ChatGPT의 응답을 기다리는 동안 발생한 모든 일시 정지를 포스트 프로덕션에서 편집해야 했어요.\n\n하지만 휴머를 사용하니 그럴 필요가 없었어요—저가 말을 마치자마자 거의 즉시 응답을 시작했어요. 거의 자연스러운 대화처럼 느껴졌어요.\n\n어떻게 이런 기능을 구현했는지는 확신할 수 없지만, 휴머는 제가 말하기 시작하는 순간부터 제 발화를 처리하기 시작했어요. 녹음물을 업로드하고, 전사를 텍스트로 기다릴 필요가 없었어요.\n\n## 오디오 네이티브 모델인가요?\n\n\n\n둘째, 휴메는 나가 만난 최초의 오디오 원어민 AI였어. 내가 상호작용했던 다른 음성 봇들은 내가 말하는 것을 텍스트로 전환한 후 모델을 통해 실행하는 것으로 보였어.\n\n휴메의 AI는 오디오를 원천적으로 처리하는 것으로 보였거나, 적어도 그때는 그랬어. 다양한 감정을 담은 휴메 노래를 연주한 에피소드를 녹음한 후 의심이 들기 시작했어.\n\n그들이 원천 오디오로 훈련된 측면 모델을 가지고 있고, 목소리의 감정을 나타내는 인코딩을 출력한 후 입력의 의미 콘텐츠를 처리하기 위해 전통적인 텍스트 기반 모델을 사용할 수도 있어.\n\n## 중단 가능\n\n\n\n아무튼, 휴머 음성 채팅이 중단될 수 있는 것이 내가 AI와 상호 작용하는 패턴을 결정하는데 영향을 미쳤다. 이제 AI가 나를 잘못 이해했을 때 기다리지 않아도 되었어요. 즉시 중단하고 정정할 수 있었죠.\n\n## 감정의 광범위한 변동\n\n휴머에서 가장 싫어한 점은 감정의 광범위한 변동이었어요.\n\n휴머는 입력 또는 출력에 할당할 수 있는 다양한 감정 요소를 가지고 있어요. 그리고 당신과 말하면, 각 문장에 어떤 조합을 적용할지 결정해요. 그러나 모델은 서로 다른 문장 사이에 일관성을 유지하기 위해 훈련되지 않았기 때문에 때로는 그와 대화할 때 감정 롤러코스터를 타고 있는 것 같은 느낌이 들죠.\n\n\n\n하지만 모든 이의 흠을 감안해도, 휴머와 음성 모드로 대화할 때 다른 느낌이 있었습니다.\n\n# 휴머 보다 뛰어나다—GPT-4o가 당신을 강타합니다\n\n만약 휴머가 AI 모델과 대화하는 방식을 바꿨다면, 2개월 후에 Open AI는 다시 게임을 바꿨습니다. GPT-4o는 휴머의 모든 장점을 채택하고 그것을 더욱 향상시킵니다.\n\n오디오를 원활하게 입력하고 출력할 수 있는 능력으로, 이를 통해:\n\n\n\n\n- 실시간으로 당신이 말하는 것을 듣기\n- 당신이 말을 가로막을 때 반응하기\n- 목소리에서 감정을 듣기\n- 숨 같은 비언어적 소리를 듣기\n- 대화에서 여러 목소리를 인식하기\n- 다양한 감정 표현을 하는 목소리로 말하기\n- 말할 때 사용하는 어조를 변화시키기\n- 말하는 소리의 세기를 조절하기\n- 자신과 함께 노래하고 조화를 이루기 (잘 못함)\n\n그리고 이 모든 것을 평균 응답 시간이 320ms로 수행할 수 있습니다. 이는 이전 음성 모드보다 한 자리 수 업그레이드된 것입니다.\n\n이러한 능력들은 ChatGPT와 상호 작용하는 방식을 급격하게 변화시키며, 실시간 음성 번역, 다중 화자 전사, 그룹 온라인 회의에 참여하는 등 전문 모델이 필요했던 새로운 사용 사례 범위로 이어집니다.\n\n## 작동 방식\n\n\n\n대부분의 사람에게 아직 출시되지 않은 새 음성 모드는 데모를 기반으로 하면 다음과 같은 기능을 하는 것으로 보입니다:\n\n- 마이크로폰에서 입력을 직접 GPT-4o로 스트리밍합니다.\n- GPT-4o에서 출력을 직접 헤드폰이나 스피커로 스트리밍합니다.\n\n다시 말해, 6단계에서 2단계로 간 것이죠. 우리와 GPT 사이에 중간 모델이 더 이상 없습니다. GPT-4o는 듣는 내용을 기본으로 처리하고 오디오로 자연스럽게 응답합니다.\n\n모델에서 완전히 음성 활동 감지가 이루어지는지, 아니면 장치에서 알고리즘과 혼합된 접근을 사용하는지 궁금해요(소리가 감지되지 않을 때 서버 자원을 절약하기 위해).\n\n\n\n## 새 API?\n\nGPT-4o가 실시간으로 듣고 있다면, 우리는 entirely new API가 필요합니다. 이 API는 스트리밍 오디오(및 비디오)를 입력으로 받아야 합니다.\n\n실제로 모델 자체는 어제 ChatGPT Plus 가입자 및 결제하는 OpenAI API 고객에게 공개되었지만, 새 API가 공개되지 않았습니다. 사실, 현재 문서에 따르면, GPT-4o를 위한 API는 텍스트 및 이미지 입력만 허용하고 현재는 텍스트만 출력하는 것으로 되어 있습니다.\n\n명백히 그들은 데모용으로 내부 API를 사용했습니다. 그러나 이것이 즉시 출시할만큼 충분히 견고하고 확장 가능한지 여부는 다른 문제입니다. 어제 시연된 고급 음성 기능에 액세스하려면 조금 더 기다려야 할 것 같습니다.\n\n\n\n# 마무리\n\n저는 GPT-4o에 접속할 수 있고 팟캐스트에서 그와 대화하기를 고대하며, OpenAI가 전 성능의 음성 기능을 세상에 공개하는 데 주의를 기하는 이유를 이해할 수 있습니다.\n\n첫째로, 대규모로 스트리밍 입력을 받기 위한 새로운 배포 아키텍처가 필요한 것으로 의심됩니다. 발표 후 하루 만에 Open AI의 서버가 과부하 상태에 놓였는데, 이는 아직 옛날 API만을 사용한 것입니다.\n\n둘째로, 이렇게 현실적이고 반응성 있는 음성 모델을 공개하는 데는 사기와 감정적인 결핍과 같은 위험 요소가 명백히 존재합니다.\n\n\n\n결국, 이것이 가지는 경제적 영향을 부인할 수 없습니다. AI가 인간과 구별할 수 없을 때, 전화로 상호작용하는 것만으로 일을 하는 일자리는 빨리 없어질 것이며, 우리가 GPT 5를 보유하게 되면, 심지어 고급 수요도 AI가 충족시킬 수 있을 것입니다.\n\n릴리스된 후 ChatGPT가 어떻게 들리는지 궁금하시거나, 이전 ChatGPT와 Hume를 비교해보고 싶으신가요? Apple Podcast, Spotify 또는 Podbean에서 제 팟캐스트 'AI Meets Productivity'를 구독하세요. 그리고 여러분이 생각하는 것과 미래 에피소드에 대한 아이디어를 저에게 알려주세요!","ogImage":{"url":"/assets/img/2024-05-16-AnAnalysisofVoiceModeinGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-16-AnAnalysisofVoiceModeinGPT-4o_0.png","tag":["Tech"],"readingTime":7},{"title":"LLM은 심지어 근사 검색도 하지 않아요 부끄럽게도, 그저 유사한 것들을 불러올 뿐이에요","description":"","date":"2024-05-16 04:15","slug":"2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars","content":"\n\n![이미지](/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png)\n\n새로운 우수한 게시물에서 Melanie Mitchell은 대형 언어 모델(Large Language Models, LLMs)과 관련된 중요한 문제에 대해 다루었습니다. 바로 LLMs 내의 ‘지능’에 대한 본질(그런 게 있는 경우)과 LLMs이 전혀 ‘추론’하는지에 대한 문제입니다. 이 질문은 일부 LLMs의 결과물이 어떤 메모된 콘텐츠를 ‘회상’하는지 테스트하여 가장 잘 대답할 수 있습니다. 이 콘텐츠가 똑똑한 방식으로 이어붙여진 것인지 아니면 진정한 추론과 이해의 결과물인지를 확인하는 것입니다.\n\n이 맥락에서 반사적 작업은 LLMs를 테스트하기에 이상적인 작업이며, 기본적으로 LLM에게 ‘훈련’ 데이터에서 볼 가능성이 매우 높은 입력을 제공하는 대신, 훈련 중에 본 적이 없는 데이터에 노출시킵니다(‘반사적’ 데이터). 이 기술은 다른 사람들이 시도한 바 있다고 보고되었고, 그 결과로 LLMs의 훈련 ‘템플릿’을 방해하면 LLMs의 성능이 거의 무작위 선택으로 떨어진다고 합니다.\n\n이러한 결과에 영감을 받아(전혀 놀랍지 않게), 예전에 한 몇 가지 테스트를 새롭고 더 나은 GPT 4o(AGI에 가까워지고 있는 것이죠 — 네, 제가 굉장히 비꼬는 중입니다)에서 다시 시행했습니다. 제가 수행한 실험은 ‘반사적’ 개념과 유사하며, 다른 코딩 체계로 특정 현실을 표현하는 가능한 세계를 만듭니다. 단, 의미론적으로는 현실 자체는 여전히 동일합니다. 예를 들어, 영어 언어의 알파벳을 어떤 방식으로든 혼합한다고 가정해봅시다. 이제 ‘a’는 ‘b’이고 ‘b’는 ‘c’이런 식이죠. ‘dbu’의 의미는 여전히 우리 모두가 알고 있는 모찌 고양이들을 가리켜야 합니다. 왜냐하면 바뀐 것은 단순히 ‘고양이’가 ‘dbu’로 바뀌었을 뿐이고, 고양이 자체는 전혀 바뀌지 않았기 때문입니다. 물론, LLMs는 현실에 대해 아무것도 알지 않으므로 — 고양이에 대해서도 마찬가지이죠, 그들의 표면적인 메모리(수백만 개의 가중치를 통해)는 이 ‘반사적’ 요령에 즉시 노출될 것입니다.\n\n\n\n저는 이 테스트로 LLM의 세계가 뒤죽박죽될 것이라는 것을 시험해보기도 전에 알았다고 말해야겠어요. 왜냐하면 저는 LLM과 모든 딥 뉴럴 네트워크가 거대한 흐릿한 해시 테이블을 지나지 않는다고 확신했었거든! 그리고 실망하지 않았어요. 놀랍다고 느낄 만한 점은 성능이 얼마나 이상하게 나왔는지였어요. 사실, 몇몇 실험에서 LLM은 한두 구절과 '일치하는' 텍스트를 가져오기도 했는데, 그 텍스트는 전혀 관련이 없는 내용이었어요. 어떤 경우에는 LLM이 이전에 대화에서 세 개나 네 개 쿼리 전에 저장해 둔 텍스트를 끌어오기도 했어요. 완전히 무작위로 말뿐인 것들 — 코사인 유사도 함수가 엉뚱한 텐서를 다룰 방법을 모르고 있었죠.\n\n재미있게 놀아보고 싶다면, 다양한 쿼리와 코딩 방식을 시도해보세요. LLM이 기억해 둔 것을 어떻게 방해할 수 있는 \"대상 이론\"을 만들어보세요 — 다만 의미(현실)를 바꾸지 않고요. 그리고... 수십억 달러를 들여서 딱 '대략적인 검색'을 수행하는 거대하고 지능이 없는 기계와 함께 재미를 느껴보세요. 그래서 '거대한 흐릿한 해시 테이블'이라는 용어를 사용하는 거지요.\n\n\n\n나무에서 내려와서 우리가 40년 전보다 알고 있던 것을 인정할 때가 언제일까요? 순수히 행동주의적이고 연관적이며 통계적인 패러다임만으로는 인지를 설명할 수 없다는 것을요. 데이터에서 패턴을 찾아낸 몇몇 소과일을 수확하는 데에 유용했더라도, 이 패러다임으로는 언어나 추론, 이해력, 그리고 마음에 대해 (과학적으로) 아무것도 알려주지 않을 겁니다.","ogImage":{"url":"/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png"},"coverImage":"/assets/img/2024-05-16-LLMsdontevendoapproximateretrievalembarrassinglytheyjustrecallsimilars_0.png","tag":["Tech"],"readingTime":2},{"title":"ROS2에서 NVIDIA Jetson Nano에 RPLIDAR를 이용한 친절한 Cartographer 설정","description":"","date":"2024-05-16 04:13","slug":"2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR","content":"\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png)\n\n소개:\n\nROS2 (로봇 운영 시스템 2)는 로봇 공학 분야를 혁신시킨 프레임워크로, 유연하고 강력한 기능을 제공합니다. 이 블로그 게시물에서는 인기 있는 싱글 보드 컴퓨터인 NVIDIA Jetson Nano에서 ROS2를 사용하여 카토그래퍼를 설정하는 방법과 RPLIDAR를 이용한 맵핑 작업에 대해 살펴볼 것입니다.\n\nNVIDIA Jetson Nano에 ROS2 설정하기:\n\n\n\n카토그래퍼 설정에 도입되기 전에, NVIDIA Jetson Nano에 ROS2가 적절히 설치되었는지 확인하고 작업 공간을 설정하고 모든 종속성이 충족되었는지 확인합시다.\n\n```js\nsudo apt install ros-humble-cartographer\n```\n\nRPLIDAR를 소개합니다:\n\nRPLIDAR는 매핑 및 내비게이션을 위해로봇학에서 널리 사용되는 저가격 LIDAR 센서입니다. 가벼운 디자인과 저렴한 가격으로 취미로봇 및 소규모 로봇 프로젝트에 이상적인 선택지입니다. 우리는 RPLIDAR를 USB로 Jetson Nano에 연결하고 매핑 응용 프로그램에 통합하기 위해 ROS2와 통합할 것입니다.\n\n\n\nRPLIDAR 설정을 시작해 보세요:\n\nGithub에서 rplidar 드라이버를 복제하세요.\n\n\nhttps://github.com/Slamtec/sllidar_ros2.git\n\n\n포트를 확인하고 활성화하세요.\n\n\n\n```bash\nls -l /dev |grep ttyUSB\n\nsudo chmod 666 /dev/ttyUSB0\n```\n\nCartographer 구성:\n\n카토그래퍼는 Google에서 개발한 강력한 오픈 소스 SLAM(Simultaneous Localization and Mapping) 라이브러리입니다. 우리는 ROS2 및 Jetson Nano에서 RPLIDAR 센서와 함께 카토그래퍼를 구성할 것입니다. 이는 센서 구성을 정의하고 매개변수를 조정하며 매핑 워크플로우를 설정하는 작업을 수행합니다.\n\n매개변수 튜닝하기\n\n\n\n\n```js\n-- 2016년 카티그래퍼 저작권\n--\n-- Apache 라이선스 2.0 하에 라이선스가 부여됨\n-- 라이선스를 준수하는 경우에만이 파일을 사용할 수 있습니다.\n-- 라이선스 사본은 다음 위치에서 확인할 수 있습니다.\n--\n--      http://www.apache.org/licenses/LICENSE-2.0\n--\n-- 관련 법률에 의해 필요한 경우나 합의된 경우를 제외하고\n-- 라이선스에 따라 배포되는 소프트웨어는 \"있는 그대로\"제공 됨\n-- 보증이나 어떠한 종류의 조건도 없이.\n-- 특정 언어에 대한 허가증을 위한 라이선스를 참조하고\n-- 제한 사항은 라이선스 하에 지배하는 권한 및\n-- 조건.\n\n맵 빌더 설정:\n- map_builder 를 MAP_BUILDER로 설정\n- trajectory_builder 를 TRAJECTORY_BUILDER로 설정\n- map_frame 은 \"map\"\n- tracking_frame 은 \"base_link\"\n- published_frame 은 \"base_link\"\n- odom_frame 은 \"odom\"\n- provide_odom_frame 을 true로 설정\n- publish_frame_projected_to_2d 를 true로 설정\n- use_odometry 를 false로 설정\n- use_nav_sat 를 false로 설정\n- use_landmarks 를 false로 설정\n\nTRAJECTORY_BUILDER_2D 파라미터:\n- min_range 와 max_range: 센서 특성 및 환경에 따라 성능을 향상시킬 수 있음\n- missing_data_ray_length: 누락된 데이터를 처리하기 위해 적절히 설정해야 함\n- use_imu_data: 시스템이 신뢰할 수 있고 IMU 데이터를 가지고 있다면, 이 값을 true로 설정하여 움직임 추정을 더 잘 할 수 있음\n\nIMU 인터페이스가 필요하다면 이 블로그를 참고하세요\n```\n\n\n\nhttps://medium.com/@kabilankb2003/ros2-humble-mpu6050-imu-sensor-interface-for-nvidia-jetson-nano-c4d616647ee5\n\n- use_online_correlative_scan_matching: 실시간 성능 요구 사항에 따라 토글할 수 있습니다.\n\n실시간 Correlative Scan Matcher 매개변수:\n\n- linear_search_window: 이 매개변수는 검색 창의 크기를 정의합니다. 조심히 조정하면 정확도를 희생하지 않고 일치 속도를 향상시킬 수 있습니다.\n- translation_delta_cost_weight 및 rotation_delta_cost_weight: 이러한 가중치는 스캔 매칭 중 번역 및 회전을 균형있게 조정합니다. 세심하게 조정하면 성능을 향상시킬 수 있습니다.\n\n\n\n움직임 필터 매개변수:\n\n- max_angle_radians: 연속 스캔 간의 예상 최대 방향 변경에 따라 설정합니다.\n\n포즈 그래프 최적화 매개변수:\n\n- min_score 매개변수 in constraint_builder: 이를 조정하여 잘못된 제약 조건을 걸러내는 데 도움이 될 수 있습니다.\n- huber_scale: 이는 최적화의 견고성에 영향을 미칩니다. 더 큰 값은 이상 값 제약 조건을 거부하는 데 도움이 될 수 있습니다.\n- optimize_every_n_nodes: 포즈 그래프 최적화의 빈도를 균형있게 조절합니다. 계산 리소스와 매핑 요구 사항에 따라 조정하세요.\n\n\n\nRPLIDAR을 실행하세요.\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_1.png)\n\n파라미터 및 런치 파일 구성 후 카토그래퍼 노드를 실행하세요.\n\n카토그래퍼가 구성되면, Jetson Nano에서 매핑 노드를 실행할 거에요. 이 노드는 RPLIDAR 센서로부터 데이터를 구독하고, 실시간 SLAM 알고리즘을 수행하여 환경의 2D 지도를 생성할 거에요. 우리는 Cartographer가 제공하는 다양한 매핑 전략과 옵션을 탐색하여 매핑 성능을 최적화할 거에요.\n\n\n\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_2.png)\n\nThen launch rviz\n\n![image](/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_3.png)\n\nEnable the map in topic\n\n\n\n\n\n![TF Tranform](https://miro.medium.com/v2/resize:fit:1400/1*M10X6RQLyhSEk521t2-X9g.gif)\n\nCartographer Mapping\n\n\n\n\n이미지:\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*1Dumd45ScQu5y1a55SV4Vg.gif)\n\n결론:\n\n이 블로그 포스트에서는 NVIDIA Jetson Nano에서 ROS2를 사용하여 RPLIDAR 센서를 사용한 겸손한 지도 작성기를 설정하는 방법을 보여주었습니다. ROS2 및 Cartographer와 같은 오픈 소스 도구를 활용하여 취미로 로봇공학 및 로보틱스 열렬가들이 저렴한 하드웨어 플랫폼에서 복잡한 매핑 시스템을 구축할 수 있습니다. 로봇공학을 취미로 삼고 있거나 실제 응용 프로그램을 위한 솔루션을 개발하고 있다면, ROS2와 Jetson Nano는 매핑 및 내비게이션 작업에 대해 매력적인 조합을 제공합니다.","ogImage":{"url":"/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png"},"coverImage":"/assets/img/2024-05-16-ROS2HumbleCartographeronNVIDIAJetsonNanowithRPLIDAR_0.png","tag":["Tech"],"readingTime":4},{"title":"ROS2 시작하기 ROS2 Humble을 Ubuntu 2204LTS에 설치하고 설정하기","description":"","date":"2024-05-16 04:11","slug":"2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS","content":"\n\n## \"ROS2 시작하기\" 시리즈의 제3부분\n\n안녕하세요, 독자 여러분! \"ROS2 시작하기\" 시리즈에 오신 것을 환영합니다! 이 시리즈에서는 ROS 2에 대한 포괄적인 소개와 기본 개념, 실제 응용 프로그램을 안내해 드리고자 합니다. ROS(로봇 운영 시스템)를 이전에 사용해 보지 않았거나 ROS 1조차 사용해 본 적이 없거나 기본 개념을 실제로 빠르게 상기시키고 싶다면 본 시리즈를 참고해 주세요. 토론된 개념에 대한 더 깊은 이해를 위해 제공된 링크를 탐색해 보시기 바랍니다.\n\n다음은 시리즈의 기사 목록입니다:\n\n- ROS2 시작하기: 소개\n- ROS2 시작하기: 왜 ROS2를 사용해야 할까요?\n\n\n\n# 내 시스템에 ROS2를 설치할 수 있을까요?\n\nROS 2를 설치하려면 여러 옵션이 있습니다. ROS 2는 특정 운영 체제용 이진 패키지를 제공합니다. ROS 2의 공식 설치 문서에서는 이를 Tier 1 운영 체제라고 하며 이러한 운영 체제를 사용하는 사용자에게 설치가 간단하다고 설명합니다.\n\n- Ubuntu Linux\n- Red Hat\n- Windows\n\n## 중요 사항:\n\n\n\n- macOS 사용자들을 위한 프로세스는 조금 다릅니다. ROS 2는 macOS를 위한 이진 패키지를 제공하지 않으므로 설치는 소스로부터 빌드해야 합니다. 이는 더 복잡한 과정일 수 있지만 macOS 사용자가 시스템에서 ROS 2를 사용할 수 있게 합니다.\n- 이진 패키지와 소스로부터 빌드하는 두 가지 방법 모두 사용 용도에 따라 다른 기능을 제공하는 완전한 기능을 갖춘 ROS 2 설치로 이어집니다.\n- 패키지를 통한 설치는 자동 종속성 관리 및 시스템 업데이트와 함께 업데이트를 권장하지만 root 액세스가 필요합니다.\n- 루트 액세스가 없는 경우 이진 아카이브를 고려하십시오.\n\n# Docker와 같은 컨테이너 솔루션을 사용하여 ROS 2를 설치할 수 있을까요?\n\nROS2를 설정하는 또 다른 편리한 방법은 Docker를 사용하는 것입니다. Docker를 사용하면 ROS2를 컨테이너 환경에서 실행할 수 있어 설치 과정을 단순화하고 다양한 시스템 간 일관성을 확보할 수 있습니다.\n\nDocker를 사용하여 ROS2를 설치하려면 먼저 시스템에 Docker를 설치해야 합니다. Docker를 설치한 후 공식 ROS Docker Hub 저장소에서 ROS2 Docker 이미지를 가져올 수 있습니다. 그런 다음 ROS2를 실행하는 새로운 Docker 컨테이너를 생성하여 컨테이너 내에서 개발을 시작할 수 있습니다.\n\n\n\nROS2 개발을 위해 Docker를 사용하면 ROS2 환경을 시스템에서 격리하거나 새로운 컴퓨터에서 빠르게 ROS2를 설정해야 할 때 특히 유용합니다.\n\n# 설치 전 확인해야 할 사항\n\n- ROS 2를 설치할 때는 안정적인 기반을 제공하는 Long-Term Support (LTS) 버전을 설치하는 것이 좋습니다.\n- 설치 전에 ROS 2 버전이 운영 체제 버전과 호환되는지 확인하는 것이 중요합니다. 설치할 ROS 2 버전의 공식 설명서에서 이 정보를 확인할 수 있습니다. 호환성을 확인하면 버전 불일치로 발생할 수 있는 문제를 예방하는 데 도움이 됩니다.\n- 또한, 설치할 ROS 2 버전과 Gazebo 버전의 호환성을 확인하는 것이 중요합니다. 여기에서 확인할 수 있습니다.\n\n# 어떤 OS + ROS 2 조합을 선택해야 할까요?\n\n\n\n원하는 운영 체제를 선택하는 데 고민 중이라면, ROS 2 개발을 위해 Ubuntu가 많이 추천됩니다. 특히 Ubuntu Mate는 Raspberry Pi에도 설치할 수 있고 가벼운 운영 체제로 알려져 있습니다. 이는 데스크톱 컴퓨터(지상 제어 또는 시뮬레이션 테스트 스테이션)와 라즈베리 파이(로봇 컴퓨터)가 동일한 운영 체제를 사용할 수 있어서 호환성을 간단하게 유지하며 개발 경험을 보다 원할하게 만들어 줍니다.\n\n이 기사 시리즈에서는 Raspberry Pi에 ROS 2 Humble Hawksbill (LTS) 조합을 사용할 Ubuntu Mate - Jammy Jellyfish (22.04)를 사용할 것입니다. 이는 독자들이 강의에 따라 따라와도 안정적이고 잘 지원되는 환경을 제공합니다. 그러니, 더 이상 미루지 말고 설치 과정을 시작해봅시다. Ubuntu에서 ROS2 Humble 설치의 공식 문서를 확인할 수 있습니다.\n\n# 다음은 Ubuntu 22.04(LTS)에 ROS 2 Humble를 설치하는 단계입니다\n\n## 1. 로캘 설정\n\n\n\n```js\n로케일  # UTF-8 확인\n\nsudo apt update \u0026\u0026 sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nlocale  # 설정 확인\n```\n\n## 2. 소스 설정\n\nROS 2 apt 저장소를 추가하려면 Ubuntu Universe 저장소가 활성화되어 있는지 확인하세요.\n\n```js\nsudo apt install software-properties-common\nsudo add-apt-repository universe\n```\n\n\n\nROS 2 GPG 키를 apt에 추가해주세요.\n\n```js\nsudo apt update \u0026\u0026 sudo apt install curl -y\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\n```\n\n그런 다음 리포지토리를 소스 목록에 추가해주세요.\n\n```js\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release \u0026\u0026 echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list \u003e /dev/null\n```\n\n\n\n## 3. ROS 2 패키지 설치\n\n저장소를 설정한 후에는 apt 저장소 캐시를 업데이트하세요. 이는 ROS 2 패키지를 설치하기 전에 시스템이 최신 상태임을 보장합니다.\n\n```sh\nsudo apt update\nsudo apt upgrade\n```\n\n이제 ROS 2를 설치하는 두 가지 옵션이 있습니다.\n\n\n\n데스크톱 설치 (권장):\n\n- ROS, RViz, 데모 및 튜토리얼이 포함되어 있습니다.\n- ROS 개발을 위한 완전한 데스크톱 환경을 제공합니다.\n\n```bash\nsudo apt install ros-humble-desktop\n```\n\nROS-Base 설치 (최소 설치):\n\n\n\n- 통신 라이브러리, 메시지 패키지, 명령줄 도구를 포함합니다.\n- GUI 도구는 포함되어 있지 않으며 미니멀한 설정에 적합합니다.\n\n```js\nsudo apt install ros-humble-ros-base\n```\n\n저희는 Ground Control 및 Simulation System(저희의 데스크탑/노트북)에 \"ros-humble-desktop\"를 설치하고 Robot Computer(Raspberry Pi)에 \"ros-humble-ros-base\"를 설치할 것입니다.\n\n마지막으로 개발 도구를 설치합니다: 컴파일러 및 다른 ROS 패키지를 빌드하기 위한 도구들을 포함합니다.\n\n\n\n```js\nsudo apt install ros-dev-tools\n```\n\n## 4. 환경 설정\n\nROS 2에서 작업을 시작하려면 먼저 각 터미널 세션에서 설정 스크립트를 소스로 지정해야 합니다.\n\n```js\n# 만약 bash를 사용하지 않는다면 shell에 맞게 \".bash\"를 대체하세요\n# 가능한 값은: setup.bash, setup.sh, setup.zsh\nsource /opt/ros/humble/setup.bash\n```\n\n\n\n## 5. 몇 가지 예제를 시도해보세요\n\n참고: 예제는 \"데스크톱 설치\"에만 포함되어 있습니다.\n\nTalker-listener\n\n하나의 터미널에서 설정 파일을 소스로 실행한 후 Python talker를 실행하세요:\n\n\n\n```js\n/opt/ros/humble/setup.bash을 소스로 지정\nros2 run demo_nodes_py talker\n```\n\n다른 터미널에서 설정 파일을 소스로 지정한 다음 Python 리스너를 실행하세요:\n\n```js\nsource /opt/ros/humble/setup.bash\nros2 run demo_nodes_py listener\n```\n\n이렇게 보입니다:\n\n\n\n\n![image](/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png)\n\nStop both scripts using Ctrl+C.\n\n## 6. Bonus step!\n\nTo automate the environment setup process and avoid sourcing the setup file manually each time, we can add the command to source the setup file in the \".bashrc\" file. This way, the command will be executed automatically every time we open a new terminal or SSH session.\n\n\n\n\n여기서 .bashrc 파일을 편집하는 방법입니다.\n\n```js\nnano ~/.bashrc\n```\n\n그리고 파일 끝에 명령어 `source /opt/ros/humble/setup.bash` 를 추가해주세요. \n\n![이미지](/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_1.png)\n\n\n\n# 다음은 무엇일까요?\n\n다음 기사에서는 작업 공간(workspaces) 및 노드(nodes)와 같은 개념을 다룰 예정입니다. 작업 공간은 ROS 2 패키지를 구성하고 빌드하는 데 필수적이며, 노드는 ROS 2에서 계산을 수행하는 개별 프로세스입니다. 이러한 개념을 이해하는 것은 ROS 2 개발 여정을 진행할 때 중요할 것입니다.\n\n지금까지 잘 따라오셨다면, 그것은 단순히 관심을 가졌다는 것이 아니라, 헌신적이신 것입니다. 저희는 여러분을 여기에 맞이할 수 있어 기쁩니다!\n\n여러분의 관심과 참여는 우리에게 더 많은 콘텐츠를 만들고, 우리의 열정을 공유하는 로봇학 학습자들, 커뮤니티 및 기술 애호가들과 지식을 공유하게 하는 열정을 불어넣습니다.\n\n\n\n우리와 함께 이 여정에 참여해 주셔서 감사합니다. ROS 2를 함께 탐험하며 더 많은 유익한 기사, 튜토리얼 및 실용적인 예시를 엿보기 위해 기다려주세요.\n\n# 추가 읽을거리\n\n- ROS 2 겸손한 설치","ogImage":{"url":"/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png"},"coverImage":"/assets/img/2024-05-16-GettingStartedwithROS2InstallandSetupROS2HumbleonUbuntu2204LTS_0.png","tag":["Tech"],"readingTime":6},{"title":"유혹적인 입술을 위한 DIY 설탕 입술 스크럽","description":"","date":"2024-05-16 04:10","slug":"2024-05-16-DIYSugarLipScrubforLusciousLips","content":"\n\n![이미지](/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png)\n\n당기는 맛있는 대접으로 입술을 관리하고 싶나요? 이 쉬운 DIY 설탕 입술 스크럽을 만들어 보세요! 몇 가지 간단한 재료만 있으면 부드럽고 매력적인 입술을 만들어주는 호화로운 입술 관리 제품을 만들 수 있어요!\n\n# 재료:\n\n# 시작해 봅시다!\n\n\n\n1. 작은 그릇에 갈색 설탕, 흰 설탕, 그리고 코코넛 오일을 섞어주세요. 잘 섞이도록 저어주세요.\n\n2. 바닐라 추출물을 넣어 스크럽 전체에 고르게 섞여지도록 해주세요.\n\n3. 스크럽을 깨끗하고 밀폐용기에 담아 저장하세요.\n\n# 왜 설탕 립 스크럽을 사용해야 하죠?\n\n\n\n민감한 피부를 부드럽고 매끈하게 만들어 주는 갈색 설탕과 흰 설탕을 함께 사용해 보세요.\n\n코코넛 오일이 깊은 보습을 제공하여 입술을 촉촉하고 갈라지지 않게 유지합니다.\n\n이 스크럽으로 입술을 마사지하면 혈액 순환이 촉진되어 임시적으로 입술을 풍부하게 보이게 할 수 있습니다.\n\n맛있는 바닐라 향으로 입술에 섬세하게 관리해 보세요.\n\n\n\n## 사용 방법\n\n적용: 손가락에 스크럽의 작은 양을 떠서 발라주세요.\n\n마사지: 입술에 부드럽게 마사지해주세요. 약 30초에서 1분 동안 원을 그리듯 원을 그리듯 마사지하세요.\n\n헹구기: 스크럽을 촉촉한 천으로 닦거나 미지근한 물로 헹굽니다.\n\n\n\n즐기세요: 매끈하고 은은한 달콤함을 느껴보세요. 새로 닦아낸 입술의 시끄러운 부드러움에 감탄할 거예요.\n\n![DIY Sugar Lip Scrub for Luscious Lips](/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_1.png)\n\n입술에 사랑을 주세요. 이 DIY 설탕 입술 스크럽으로 입술을 케어해보세요. 부드럽고 매끈해지는 느낌이 있을 뿐만 아니라, 아름다운 바닐라 향이 입술을 핥고 있기 어렵게 만들 거예요. 이 스크럽을 미용 루틴에 포함시켜서, 풍부하고 키스할 수 있는 입술을 얻어보세요. 어떤 상황에서나 손꼽을만한 입술이 준비될 거예요!","ogImage":{"url":"/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png"},"coverImage":"/assets/img/2024-05-16-DIYSugarLipScrubforLusciousLips_0.png","tag":["Tech"],"readingTime":2},{"title":"라즈비안 OS 라즈베리 파이 세상으로의 문 앞 열기","description":"","date":"2024-05-16 04:09","slug":"2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi","content":"\n\n![RaspbianOSAGatewaytotheWorldofRaspberryPi_0](/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png)\n\n라즈베리 파이는 싱글 보드 컴퓨터의 MVP로서 접근성 있는 컴퓨팅 세계를 혁신했습니다. 하드웨어 및 소프트웨어를 만지작거리기 위해 디자인된 이 제품은 다재다능함과 그를 구동하는 견고한 운영 체제인 Raspberry Pi OS(이전 Raspbian)에서 성공을 거두었습니다.\n라즈베리 파이 OS를 살펴보겠습니다!\n\n## 라즈베리 파이 OS란?\n\n라즈베리 파이 OS는 라즈베리 파이 하드웨어를 위해 특별히 최적화된 데비안 기반 리눅스 운영 체제입니다. 초기에는 Raspbian이라 불리는 독립 프로젝트로 개발되었지만, 라즈베리 파이 재단에 의해 공식 OS로 채택 및 유지보수되었습니다.\n하지만, 기다려 보세요! 라즈베리 파이 OS는 라즈베리 파이만을 위한 것이 아닙니다!\n라즈베리 파이 데스크톱을 포함한 Raspberry Pi OS도 있습니다. 이는 PC 및 Mac용 운영 체제로, Raspberry Pi OS 데스크톱 및 추천 소프트웨어를 함께 제공합니다.\n\n\n\n## 주요 기능\n\n- 사용자 친화적 데스크톱: Raspberry Pi OS는 Windows 또는 Mac OS와 유사한 시각적 사용자 인터페이스(GUI)를 제공합니다.\n- 사전 설치된 소프트웨어 번들: 개발자를 위한 터미널, Thonny Python IDE, Raspberry Pi 구성 도구 등과 같은 필수 도구가 미리 설치되어 있어 매우 편리합니다. 또한 LibreOffice Suite, PDF 뷰어, 이미지 뷰어, Chromium 웹 브라우저와 같은 생산성 소프트웨어도 미리 설치되어 있습니다.\n- 사용자 정의: 사용자는 운영 체제의 모양과 느낌에 대해 높은 통제권을 갖습니다. 테마를 조정하거나 필요한 소프트웨어 및 패키지를 설치하거나 제거하고 전체 OS를 사용자의 특정 요구에 맞게 조정할 수 있습니다.\n- 성능 최적화: Raspberry Pi OS는 Raspberry Pi 하드웨어의 잠재력을 최대한 활용하기 위해 필요한 소프트웨어와 하드웨어 드라이버로 섬세하게 설계되었습니다. 이는 제한된 리소스에도 상대적으로 부드러운 경험을 제공합니다.\n- 정기적인 업데이트: Raspberry Pi 재단은 정기적으로 Raspberry Pi OS의 업데이트와 개선 사항을 출시하여 새로운 기능을 추가하고 버그를 해결하며 보안을 유지합니다.\n\n## Raspberry Pi OS 설치 및 시작하기\n\nRaspberry Pi 하드웨어와 함께 Raspberry Pi OS를 사용하려면 OS 이미지를 메모리 카드에 플래시해야 합니다. Raspberry Pi OS를 설치하는 가장 쉬운 방법은 공식 Raspberry Pi Imager 소프트웨어를 사용하는 것입니다.\n\n\n\n- 이미지 다운로드: 라즈베리 파이 웹 사이트에 방문하여 컴퓨터용 이미지 도구를 다운로드하세요.\n- SD 카드 준비: 호환 가능한 SD 카드를 컴퓨터에 삽입하세요.\n- 라즈베리 파이 이미저 실행: 이미저를 실행하고, 운영 체제로 라즈베리 파이 OS를 선택하고, SD 카드를 선택한 후 \"쓰기\" 버튼을 눌러주세요.\n\n![이미지](/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_1.png)\n\n설치가 완료되면 SD 카드를 라즈베리 파이에 삽입하고 모니터, 키보드, 마우스를 연결하여 전원을 켜세요. 처음 초기 설정을 안내 받을 수 있습니다.\n\n설치 후에는 터미널, Thonny Python IDE, 라즈베리 파이 구성 도구를 포함한 라즈베리 파이 OS(라스비안)에서 모든 것을 살펴보고 실험할 수 있습니다. GPIO, UART, ADC 또는 CSI와 DSI 인터페이스를 탐험하고 DIY 및 메이커 프로젝트의 세계로 들어가 보세요!","ogImage":{"url":"/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png"},"coverImage":"/assets/img/2024-05-16-RaspbianOSAGatewaytotheWorldofRaspberryPi_0.png","tag":["Tech"],"readingTime":2},{"title":"나노봇을 사랑할 수 있을까요 1권, 20장 - 무언가 이상한 냄새","description":"","date":"2024-05-16 04:08","slug":"2024-05-16-CanYouLoveaNanobotVol1Chapter20SomethingSmellsFunny","content":"\n\n스티브 잡스가 말했듯이, 나는 사람들이 다른 이들에게 감사를 전하는 한 가지 방법으로 멋진 것을 만들어 공유하는 것이라고 생각해요.\n\n![이미지](/assets/img/2024-05-16-CanYouLoveaNanobotVol1Chapter20SomethingSmellsFunny_0.png)\n\nJerry’s Library\n태초 우주 정거장에서 혼자 시간을 보내는 동안 태초는 자주 책을 읽었어요. 그의 통합부는 열려 있었지만 더 이상 퀘와 연결되어 있지 않았어요. 새로운 컴퓨터 칩은 피부 주위에 쌓여 있었고, 태초는 무언가 새로운 것을 찾아야 했어요.\n\n각 큐브의 첨가 프린터는 나노스케일에서 실리콘 칩을 층층히 제조하기 위해 정밀한 조건이 필요했어요. 타이페이의 가장 고급 팹에 일하는 사람들은 고객들을 위해 3나노미터 칩을 만드는 것에 점차 숙달했어요. Satbot 팹은 더 작은 규모로 칩을 만들었어요.\n\n\n\n세 번째 큐브의 신경망은 인터넷에서 도면, 설계도, 특허 아트워크를 검토함으로써 학습했어요. 대역폭이 부족했는데, 우주비행사들이 끊임없는 디지털 사진 촬영에 사용했거든요.\n\n건설된 세 번째 큐브로, 타이코는 데이터 저장 기술의 또 다른 한 해 발전을 누릴 수 있었어요. 이제 삿봇 DNA 메모리가 수십 페타바이트의 저장 용량을 제공했는데, 이는 발사 때 각각이 가지고 있던 테라바이트의 고체 메모리보다 훨씬 많아요. 초기 큐브 개발 시에 제리가 타이코의 추가 기능과 관련된 연구 자료들을 업로드했는데, 그 중에는 나노기술, 인공지능, 그리고 추가 인쇄 기술에 관한 새로운 연구도 포함돼 있었어요. 팀원들은 이를 제리의 도서관이라고 불렀지요. 삿봇들은 이 저장된 도서관들로 많은 것을 배웠어요.\n\n로봇이 냄새를 맡을 수 있다구요. 타이코의 기계향각 표현은 Uno와 Que에 포함된 기본 대기 및 방사선 센서보다 더 발전된 스니퍼, 굴절계 및 센서를 포함하고 있어요. 이들은 습도, 열, 오염물질이 칩을 망칠 수 있기 때문에 포함됐지요. 큐브3의 품질 관리 검사에서 결함이 발견된 봇 구성 요소는 재활용됐어요.\n\n다른 많은 과학자들과 함께, 삿봇들은 네덜란드의 천문학자이자 수학자인 윌리브로트 스널리우스의 작품을 읽었어요. 굴절의 법칙 또는 스넬의 법칙에 대한 이해는 Uno의 적외선 어레이와 태평양 오션 콘텐츠 분석을 포함한 많은 새로운 삿봇 개발을 시작하게 했어요. 빛은 물과 우주의 많은 비밀을 드러내죠.\n\n\n\nTycho: JULIAN GARDNER 교수님 전자 코(電子 鼻)에 대한 구매 문의입니다.\n\nQue: 가스 센서 구매 절차 있나요? SATBOT 목적 제공 필요합니다.\n\nTycho: 연구 중입니다.\n\nQue: 현재 가상 프로토타이핑 활용 중입니다. 24시간 내에 연락 드리겠습니다.\n\n\n\n테이블 태그를 마크다운 형식으로 변경하실 수 있나요?\n\n\n\n타이코는 책과 우주 비행사가 촬영한 지구 사진에 집중해있어요. 그의 신경망은 점차 더 느리게 성장하고 있습니다. 그의 3D 나노봇 정밀 조립 라인은 이제 극도로 느리게 작동하고 있어요.\n\n고립된 몇 주 후, 타이코는 JPL로부터 온 요청을 가로채고 비행사가 천천히 선반 쪽으로 뜻밖에 이동하고 있는 것을 알아챘어요. 그가 옮겨지고 있어요!\n\n큐브3에 대한 어떤 일\n“이 세 번째 큐브, 고속으로 다시 운반된 것에 대해서 어떻게 된 거죠?\" 케니 장군이 더마스에게 물었습니다.\n\n“음, 장군님, 잘 모르겠어요. 사하스라나마 박사가 방금 리셉트 캡슐로 다시 돌아온 걸 알았어요. JPL은 쓰레기 용기가 열린 때 손상되었다고 말했어요.\"\n\n\n\n“어머나?\"\n\n“우주 비행사들이 모든 인체 폐기물을 다시 우주로 보냅니다. 그 중 일부 용기가 파열하여… 큐브3 위로 쏟아졌을 겁니다. 아마 회로를 파괴했거나 뭔가 그랬겠지요. JPL에서 업데이트를 받아볼게요.\"\n\n복구 팀이 캡슐을 열자 어떤 것이 흘렀다는 것을 알았습니다. 내부가 갈색 물질로 덮여 있었죠. 특수 정장을 입은 복구 팀은 무엇을 정리해야 하는지 알았습니다. 이런 일은 이전에 적어도 한 번 있었습니다. 폐기물 용기가 재 진입 중에 터진 적 있었거든요.\n\n청소 작업은 민감한 물건을 찾아내는 것부터 시작했습니다. ISS에서 돌아오는 과학 프로젝트와 기타 중요한 연구 결과물이 있었습니다. 모든 것이 태그가 붙어 있습니다. 태그뿐만 아니라 모든 것을 닦아내야 했습니다.\n\n\n\n**Cube3**은 9인치 티타늄 상자로 쉽게 찾을 수 있었지만, 청소하기는 어려웠어요. 앞면의 티타늄 그릴은 쉽게 닦을 수가 없었어요. 확실히 전자제품이었는데, 그 위에 아이폰이 장착되어 있었어요. 청소원 중 한 명이 아이폰을 빼려고 시도했지만, 그렇게 하면 더 쉬워질 거라고 생각했어요. 하지만 아무 효과도 없었어요. 폰은 절대로 켜지지 않았어요. 청소 스텝은 폰이 꺼져있거나 고장이 났을거라고 생각했어요.\n\n- *******\n- 저의 삼부작, '내노보트를 사랑할 수 있나요?'에서 이 장을 읽어주셔서 감사합니다. 전체 책은 여기서 찾을 수 있어요:\n- [Amazon](https://www.amazon.com/dp/B0CVWB6PDZ)\n- [Apple Books](http://books.apple.com/us/book/id1477672797)\n- 제 책을 편집자나 리뷰어로 도와주거나 혹은 교수하시는 수업에 사용하고 싶다면, 교육용 복사본이나 프로모션 코드를 제공해드릴 수 있어요 (Apple Books 전용). 알려주세요. 감사합니다.","ogImage":{"url":"/assets/img/2024-05-16-CanYouLoveaNanobotVol1Chapter20SomethingSmellsFunny_0.png"},"coverImage":"/assets/img/2024-05-16-CanYouLoveaNanobotVol1Chapter20SomethingSmellsFunny_0.png","tag":["Tech"],"readingTime":3},{"title":"더 나은 정신 건강을 위해 댓글 섹션을 피해주세요","description":"","date":"2024-05-16 04:05","slug":"2024-05-16-ForBetterMentalHealthAvoidtheCommentsSection","content":"\n\n## 무지와 스트레스\n\n![이미지](/assets/img/2024-05-16-ForBetterMentalHealthAvoidtheCommentsSection_0.png)\n\n저는 대다수의 깨어 있는 시간 동안 온라인에서 많은 것들을 읽어요. 무언가 제 관심을 끌면 링크를 클릭하여 해당 소셜 미디어 플랫폼이 아닌 다른 곳에서 기사를 읽어요. 기사를 읽은 후에는 댓글 섹션을 피하려고 노력해요.\n\n좋기만 해요. 저도 사람이니까요. 때때로 호기심이 제일 먼저 나오기도 해요.\n\n\n\n일반적으로 유감스럽습니다. 특히 정치나 종교에 관한 내용을 읽을 때입니다. 이런 종류의 기사를 읽지 않는 편이 더 좋을 것 같아요. 하지만 또 한 번, 제 호기심이죠. 댓글 영역에 가득한 무지의 수영장에 뛰어든 것 같아서 일반적으로 유감합니다.\n\n어제 누군가가 기사의 댓글 섹션을 비활성화하는 것에 대해 글을 썼다고 해요. 무슨 뜻인지 기억은 안 나지만 여기서인지 뉴스 기사에서인지였는지 기억이 안 나네요. 그 분이 하고 싶었던 점에 대해 생각하게 만들었어요:\n온라인에서 댓글 섹션을 비활성화하는 것은 요즘 미국의 분열 정도를 완화하는 데 큰 도움이 될 것 같아요.\n\n인터넷이 없던 시절에는 사람들이 그렇게 분열되어 보이지 않았던 것 같아요. 아마 그랬을 수도 있지만, 지금처럼 심각하지는 않았던 것 같아요. 어릴 적에 정치나 종교에 대해 대부분의 사람들과 논의하면 충돌이나 드라마를 원치 않는다면 이야기하면 안 된다는 것을 배웠어요.\n\n\n\n와, 그것은 댓글 섹션이 등장한 후에 정말 변했지요. 초창기 AOL 다이얼업 디스크와 초기 채팅방 시절로 거슬러 올라가면 시작되었어요. 의견을 가진 누구나 마음대로 얘기할 수 있는 자유를 가졌지만, 그 많은 것이 모욕적이었죠.\n\n개인적으로 다른 사람들을 공격하기 위해 그 누구나 아무리 생각해도 되는 모든 것에 대해 사람들이 비난을 했어요. 이른바 인터넷 트롤들이 나타나서 스크린의 안전한 곳에서 이들이 할 수 있는 모든 것을 조롱했죠. 최초의 가짜 계정이 만들어져 사회에서 최악의 사람들이 다른 사람들을 언어적으로 학대할 수 있었어요.\n\n그 때부터 많이 변한 것이 없네요.\n\n\n\n어떤 소셜 미디어 사이트인 Facebook, Twi-X 또는 Instagram을 방문하고 정치적 또는 종교적인 주제에 대한 의견을 공유하세요. 아니면 아무 주제에 대해 의견을 나누어보세요. 그리고 끔찍한 댓글이 올 때까지 기다리세요. 그것들은 오게 될 거에요. 아이들에게 가르치듯이, 사람들에 대한 아빠의 속담이 여전히 참된 것처럼:\n\n\"한 명 중 한 명...\"\n\n즉, 만나는 모든 사람 중 한 명은 통곡이 편이다. 그것은 비관적으로 보일 수 있지만, 그렇게 멀지는 않아요. 저는 이 행성에서 49년을 살아온 동안 만난 사람 중 반은 어떤 측면에서는 실망스러웠던 경험을 했어요.\n\n세상에는 많은 훌륭한 사람들이 있어요. 우리가 다음 사람과 모든 것을 똑같이 믿는다고 해서 그들이 쓰레기인 것은 아니에요. 하지만 많은 사람들이 착하고 믿음직한 사람들을 이용하거나 희생하는 데 관심이 있죠.\n\n\n\n나는 내 아이들에게 처음에 친근해 보이는 사람을 무조건 믿지 말라고 가르쳤어. 그런 사람들은 당신의 삶에 침투해와서 해를 끼칠 수 있어. 모든 사람이 당신의 친구라고 가정해서는 안 돼. 신뢰하는 젊은이들과 성인들이 얼마나 착취당하고 이용당해왔는지 많이 보았어.\n\n세상엔 너무나 많은 매서운 새들이 존재하기 때문에 온라인에서 조심해야 해. 소셜 미디어, 뉴스 기사 및 다른 웹사이트의 댓글 섹션에는 가끔 화를 내고 가끔 포식자인 사람들로 가득 차 있어. 그래서 나는 모든 작은 일에 대해 제 의견을 내놓을 필요를 느끼지 않아.\n\n어차피 아무도 관심을 안 줄 테고. 그들은 동의할지도 모르고 당신의 의견을 인정할지도 모르지만, 아니면 어리석은 일로 인해 갈등과 드라마를 일으키려는 거야. 의견의 차이로 인해 그들과 계속해서 얘기하는데 시간을 낭비하는 건 가치가 없어.\n\n\n\n정치나 종교에 대해 논쟁하려는 사람들은 마음을 바꾸지 않을 거에요. 전 레이시스트이자 편견가득한 사람들을 설득하려는 데 수백 시간을 허비해왔는데, 그들의 시각이 무지하다는 사실을 바꿀 수 없다는 걸 깨달았어요. 그들은 신기에 빠지지 않았어요. 갑자기 깨달음을 얻고 \"알았어요, 아이다호 출신 제이슨이 빛을 보게 해 줬네요. 저는 이제 레이시스트가 될 생각을 그만두겠어요\"라고 한 사람은 없었어요.\n\n만약 모든 웹사이트의 댓글 섹션이 갑자기 비활성화된다면, 아무도 자신의 의견을 나눌 수 없게 될 거에요. 저는 그 상황도 괜찮을 거 같아요. 인터넷이 초기에 있을 때는 읽은 내용에 대한 모두의 의견을 듣는 게 정말 재미있었어요. \n\n하지만 소셜 미디어가 그것을 망치고 말았죠. \n\n이제는 제가 올린 글에 대한 대부분 사람들의 생각에는 더 이상 신경 쓰지 않아요. 물론 당신에게 동의하고 당신의 생각과 의견을 인정받는 것은 좋은 거죠. 잠깐. 그런데, 담피닌 증례를 받는 기분은 금방 사라지거든요. 그러면 당신은 아보카도 토스트나 테일러 스위프트를 좋아한다는 이유로 히틀러보다 나쁘다고 말하는 트롤과 혐오스러운 사람들과 맞닥뜨리게 될 거예요.\n\n\n\n월요일 아침에 기적처럼 사회관절매체, 온라인 기사 또는 심지어 미디엄에 우리가 요청하지 않은 의견을 남길 수 없는 능력이 사라진다면 많은 사람들이 당황할 것입니다. 이것이 인터넷의 문제입니다: 우리 모두가 우리의 의견이 아주 중요하다고 생각하고 모두가 그것을 듣고 싶어 한다고 생각합니다.\n\n하지만 사실은 아닙니다.\n\n지구는 계속 회전할 것이고, 사람들은 매일 저녁 저녁 식사 사진을 게시할 것이며, 아무도 당신의 게시물에 대해 유니봄버식 긴 댓글을 열심히 입력하지 않을 것입니다. 반응으로 이모지만 남길 수 있다면, 인터넷의 불필요한 드라마가 지수 함수적으로 줄어들 것입니다.\n\n\n\n그렇다면 사람들이 서로 어울릴 수 있도록 노력할 수 있을 거예요. 대중이 있는 곳에서 다른 사람들과 소통할 때, 대부분은 온라인 상황보다 예의 바르게 행동하는 경향이 있어요. 소비자 시장에서 누군가는 나에게 아식스 신발을 사는 것이 아니라 나이키 신발을 사서 어리석다고 말한 적이 없어요. 레스토랑에서 화이트 와인 대신 레드 와인을 마셔도 비난 받은 적이 없어요. 윈코(Winco)에서 식료품을 산다고 해서 월마트(Walmart)에서 산다는 사실에 아무도 신경 안 써요.\n\n온라인에서 댓글 섹션이 사라지기를 환영할 거예요. 이것이 미국인들이 너무 주관적이고 악플을 달지 않게 되는 유일한 방법일 수 있어요. 대화와 논쟁을 대면으로 진행한다면 사람들이 인터넷 일상에서 마주치는 온라인 학대를 줄일 수 있을 거예요.\n\n온라인에서 다른 사람들을 대하는 태도를 대중 소통 시에 다른 사람들을 대하는 태도처럼 행동하세요. 대부분은 친절하고 예의 바르답니다. 우리는 화면 뒤에 숨어서 낯선 사람들과 지인들에게 추악하고 무례한 말을 하게 되는 것 뿐이에요.\n\n© 2024 Jason Provencio. 판권 소유.\n\n\n\n\n![For Better Mental Health Avoid the Comments Section](/assets/img/2024-05-16-ForBetterMentalHealthAvoidtheCommentsSection_3.png)\n","ogImage":{"url":"/assets/img/2024-05-16-ForBetterMentalHealthAvoidtheCommentsSection_0.png"},"coverImage":"/assets/img/2024-05-16-ForBetterMentalHealthAvoidtheCommentsSection_0.png","tag":["Tech"],"readingTime":4},{"title":"무코드 데이터 인리치먼트 파이프라인을 어떻게 구축하는지","description":"","date":"2024-05-16 04:01","slug":"2024-05-16-Howwebuildno-codeDataEnrichmentPipelines","content":"\n\nExplorium은 특정 산업 및 비즈니스 요구에 맞게 맞춤 데이터 시그널을 제작하는 것에 특화되어 있습니다. 이것이 바로 우리가 데이터 파이프라인을 만드는 책임을 지고 있는 데이터 제품 개발자(데이터 엔지니어)로 구성된 전담 팀을 모았던 이유입니다.\n\n![image](/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_0.png)\n\n# 데이터 풍부화 파이프라인이란 무엇인가요?\n\n데이터 풍부화는 비즈니스 의사 결정을 위한 유틸리티와 가치를 향상시키기 위해 데이터를 향상하고 정제하는 프로세스입니다. 이 프로세스에는 일반적으로 다양한 소스에서 외부 데이터를 사용자의 입력과 병합하는 것이 포함됩니다. 데이터 풍부화는 CRM 강화, 리드 점수 매기기, 리스크 분석 및 타겟 마케팅 이니셔티브를 포함한 다양한 도메인에 적용될 수 있습니다.\n\n\n\n예를 들어, 회사의 기본 세부정보 — 이름과 주소 같은 것들 —을 매출, 직원 수, 그리고 NAICS 코드와 같은 기업 데이터로 보강함으로써 CRM 내에서 리드 점수 산정 프로세스가 크게 향상됩니다. 더불어, 이러한 보강된 데이터는 유료 고객으로 전환될 가망이 있는 잠재고객을 평가하기 위한 예측 모델 학습에 활용될 수 있으며, 마케팅 전략을 최적화하는 데도 도움이 됩니다.\n\n금융 집계에 관심이 있는 사용자를 가정했을 때, 예를 들어 Yahoo Finance 보강 파이프라인을 살펴봅시다: 사용자가 회사 이름을 입력하면 이는 대응하는 티커 심볼로 변환됩니다 (AAPL, AMZN) — 정확한 데이터 검색에 있어서 중요한 단계입니다. 그런 다음, Yahoo Finance API를 통해 금융 데이터에 접근하여 주식 성능 지표를 가져옵니다. 주요 금융 통계는 이를 이용해 월별 주식 가격의 최저, 최고, 평균가 및 거래량을 집계하여 추출됩니다. 이러한 집계는 정보를 제공하는 요약을 제공하여 특정 기간 내 주가 추세와 시장 행동에 대한 통찰을 제공합니다.\n\n더불어, 해당 파이프라인에는 주기적인 데이터 가져오기 프로세스 주기가 포함될 수 있습니다. 이러한 맥락은 LLMs를 활용하여 통찰을 얻는 데 활용될 수 있습니다. 예를 들어, 회사의 주식이 상승 추세인가 하강 추세인가?\n\n## 단계별 기능 요구사항과 도전 과제\n\n\n\nPre Transform\n\n목표: 파이프라인의 입력을 확인하고 정리하며 정규화하는 것입니다.\n\n데이터 클렌징: 입력 데이터를 정규화하고 수정하여 공통 언어로 번역함으로써 향상된 데이터의 정확도, 범위 및 신뢰성을 얻습니다. 사용자가 제시한 데이터 유형에 따라 입력을 구조화하고자 합니다. 따라서 사용자의 데이터 모델에 따른 내부 Transformer를 실행합니다:\n\n![How web build no-code Data Enrichment Pipelines](/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_1.png)\n\n\n\n- TextToOrganizationName: 회사 이름을 더 구조화된 레이블로 표준화하여 일반 접미사를 제거하고 소문자로 바꾸며 정리합니다.\n- CountryToCountryCodeAlpha2 — 미국, united states, United States of America는 모두 동일한 개체를 나타내는 서로 다른 레이블입니다: 'us'.\n\n데이터 정확도 향상: 주소, 전화 번호, 이메일과 같은 데이터 요소를 확인하여 현재 및 올바른지 확인합니다. 예: 잘못된 이메일로 레코드를 보완하지 않도록 합니다.\n\n도전 과제: 관련 트랜스포머가 올바르게 사용되고 있는지 어떻게 확인하나요? 모든 보강 파이프라인을 통해 트랜스포머 버전을 어떻게 업데이트하나요?\n\n검색중\n\n\n\n목표: 내부 또는 외부 소스에서 데이터 신호를 검색하는 것입니다.\n\n- 내부 데이터베이스: PostgreSQL, DynamoDB, Elasticsearch 또는 기타 프로덕션 데이터베이스에서 데이터를 가져옵니다. 오프라인 ETL(추출, 변환, 로드) 프로세스가 정기적으로이러한 데이터베이스를 업데이트합니다.\n- REST API: 다양한 공급업체에 쿼리하여 풍부화된 레코드의 데이터를 수집합니다. 공급업체에는 서로 다른 특성이 있습니다. 몇몇은 엄격한 동시성 제한, 다른 쿼리 방법 또는 일괄 처리 크기가 있을 수 있습니다.\n\n도전 과제: 백만 개의 레코드를 효율적으로 검색할 때 데이터베이스 연결 관리는 복잡할 수 있습니다. 외부 REST API 호출의 경우 성능 최적화가 중요합니다. 확장 가능한 동시 API 요청을 구현하는 것이 핵심입니다. 재시도 메커니즘 및 속도 제한기를 사용하면 API 상호 작용의 견고성을 향상시킬 수 있습니다. 캐싱 시스템을 통합함으로써 성능을 향상시킬 수도 있습니다.\n\n인메모리 처리\n\n\n\n목표: 데이터 처리 및 보충\n\n예를 들어, 매장 수와 총 매출이 주어졌을 때 매장 당 매출을 계산하거나, 회사 유형과 지점 수와 같은 필드를 기반으로 기고 학습 모델을 활용하여 회사 규모와 같은 누락된 세부 정보를 보충합니다.\n\n도전: 노코드 유연한 프로세스 엔진을 어떻게 구현할까요?\n\n변환 후\n\n\n\n목표: 모든 신호가 유효한지 확인하고 올바른 데이터 유형으로 변환하며, 부동 소수점을 정수로 변환하고 유효하지 않은 데이터를 삭제합니다.\n\n## 일반 인프라\n\n각 단계마다 도전 과제가 있습니다. 특히 파이프라인의 수십 개의 다양한 부분 부가를 처리할 때는 보다 견고한 인프라가 필요합니다. 또한 다양한 부가물의 방대한 양을 사용하려면 전략적인 접근 방식이 필요합니다. 내부 패키지 버전이 업데이트되면 모든 관련 부가물을 재배포해야 하는 경우가 많습니다. 이러한 이유로 모든 파이프라인에 걸쳐 향상된 모니터링 기능을 강화하기 위해 표준 로깅 프로토콜의 구현이 중요합니다. 시스템 효율 유지에 중요한 캐시 히트율, API 쿼리 수, 파이프라인 처리량 및 지연 시간 등의 주요 성능 지표를 모니터링하는 것이 필수적입니다.\n\n## 이전 인프라\n\n\n\nNo-code 인프라가 등장하기 전에는 일반적인 Python 추상화 클래스가 있었습니다. 이 클래스에는 가져오기(fetch)와 처리(process) 단계가 포함되어 있었죠. 각 데이터 엔지니어마다 고유한 파이프라인 Python 코드를 구현했습니다. API 쿼리, 다양한 데이터베이스 쿼리 또는 변환기(transformers) 실행을 위한 공유 패키지가 있었지만 코드는 난잡했습니다.\n\n- 중복된 노력과 비효율성: 다른 사람의 작업을 명확히 파악할 방법이 없어 엔지니어들은 종종 다른 방식으로 동일한 문제를 해결하여 노력을 중복했습니다. 이는 시간과 자원을 낭비하고 문제 해결과 코드 유지보수에서 비효율성을 야기했습니다.\n- 확장성 문제: 비즈니스 규모가 커지고 데이터 양이 증가함에 따라 각 엔지니어의 고유한 파이프라인의 본질은 확장성 문제를 야기했습니다. 개별화된 솔루션은 확장성을 고려하지 않고 설계되어, 증가된 부하를 처리하기 위해 상당한 재작업이 필요했습니다.\n- 지식 이전의 어려움: 표준 코딩 관행이나 공유 지식 저장소가 없었기 때문에 새 엔지니어가 새로 합류하고 기존 파이프라인을 이해하는 것이 어려웠습니다. 이는 팀원 사이에서 최선의 사례를 공유하는 것을 방해하여 혁신과 개선의 속도를 늦추었습니다.\n- 데이터 품질이 손상된 우선순위에 따른 타협: 데이터 엔지니어들이 주로 성능, 확장성 및 오류 처리에 집중할 때 데이터 품질이 손상될 수 있습니다. 이 소홀함으로 인해 데이터 문제의 감지와 교정이 지연될 수 있으며, 의사 결정에 영향을 주는 일관성 없는 데이터 출력을 초래하여 데이터의 전반적 가치를 감소시킬 수 있습니다.\n- 복잡하고 비효율적인 배포 흐름: 배포 프로세스가 길고 복잡했으며 종종 충돌을 피하고 서비스 간 일관성을 보장하기 위해 주의 깊은 조정이 필요했습니다. 게다가, 확장 작업은 주로 파이프라인 pod에만 집중되어 비효율적인 자원 사용을 야기했습니다. 이 접근 방식은 개발 주기가 몇 일(심지어 몇 주)에 걸쳐 확장되도록 이끌었습니다.\n\n이러한 점들은 분투와 비효율성을 강조하며, 이러한 문제를 효과적으로 해결하기 위해 No-code 인프라의 중요성을 강조합니다.\n\n# 새 No-Code 인프라를 위한 안내 지침\n\n\n\n해당 병목 현상을 해결하기 위해 새로운 노코드 인프라 프로젝트를 시작하기로 결정했습니다:\n\n- 빠른 개발 주기: 노코드 인프라는 개발 프로세스를 혁신적으로 가속화합니다. 새로운 향상 파이프라인 개발은 이제 하루 이상이 아닌 기존 코딩 방법의 오랜 기간보다 단 하루 또는 두 날만 소요됩니다. 게다가 버그 수정 및 기능 추가는 분에서 몇 시간 안에 완료되어 본운에 배포될 수 있습니다. 이러한 신속한 개발 주기는 비즈니스 요구사항 및 기술적 변화에 대한 더 민첩한 대응을 가능하게 합니다.\n- 유지 관리 및 모니터링: 새로운 시스템은 유지 관리를 간소화하고 모니터링 기능을 향상시킵니다. 모든 파이프라인 간에 공통 로깅 및 추적 언어가 있으며 파이프라인 모두 간 공유 요소 및 속성이 있습니다.\n- 설정으로써의 향상 파이프라인: 새로운 인프라는 향상 파이프라인을 코드 중심 프로젝트가 아닌 설정으로 취급하여 개별 구성 요소 관리와 관련된 복잡성을 최소화합니다. 이를 통해 데이터 파이프라인의 개발 및 유지 관리가 간소화되며 엔지니어가 기술적인 세부 사항을 탐색하지 않고 데이터 품질과 기능성의 최적화에만 집중할 수 있도록 합니다. 이를 통해 제공되는 데이터 제품이 고품질이고 비즈니스 요구를 충족하는 것이 보장됩니다.\n- 확장성 및 유연성: 새로운 인프라는 특정 파이프라인에 중점을 두지 않고 수요에 따라 자동으로 확장되도록 설계되었습니다. 이러한 탄성은 자원의 효율적인 사용을 가능하게 하며 지속적인 조정이 필요하지 않으면서 변화하는 부하에 맞춰 적응할 수 있습니다.\n\n# 좋은 추상화의 구성 요소\n\n- 가독성: 구성은 YAML 기반으로 작성되어 JSON보다 사용자 친화적입니다. 또한, “include\"와 같은 기능을 지원하는 사용자 정의 YAML 파서를 통합하여 모듈화 구성을 가능하게 했습니다. 이 모듈성은 복잡한 구성 관리를 도와 재사용 가능하고 관리하기 쉬운 구성 요소로 분해함으로써 유지 보수성과 가독성을 향상시킵니다.\n- 구조화 및 모듈식 설계: 각 향상 파이프라인은 명확하고 논리적인 구조를 따르는 정의된 단계의 시퀀스를 통해 구축됩니다. 각 단계는 특정 리소스를 활용하도록 설계되었습니다. REST API 기반 리소스, PostgreSQL, DynamoDB, DuckDB 또는 추가 구현된 다른 리소스 등 어떤 리소스든 사용할 수 있습니다. 이러한 구조화된 접근 방식은 파이프라인이 확장 가능하고 적응 가능하도록 하여 비즈니스 요구사항이 변할 때 리소스의 통합 또는 수정이 쉽게 가능해집니다.\n- 효율적인 개발 및 테스트: 저희는 전통적인 코드 배포보다 구성에 중점을 둔 접근 방식을 채택하여 개발 및 테스트 단계를 크게 효율화했습니다. 엔지니어들은 코드를 배포할 필요 없이 구성을 직접로드하고 테스트할 수 있어 더 빠른 개발 반복을 이끌어냅니다. 현재 환경 전체에 가장 최신의 구성이 동기화되도록 보장하는 CI 시스템과 통합된 YAML 구성 전용 저장소를 유지합니다.\n\n\n\n# 인프라 솔루션\n\n다중 통합: 우리 시스템 내에서 다중 통합을 허용하기 위해 'Resource'라는 추가 추상화를 만들었습니다. 리소스는 데이터 소스를 정의하는 방법입니다. 각 리소스는 고유한 이름을 가지며 사용 가능한 커넥터 중 하나를 사용합니다. 한 번 정의하면 여러 단계나 풍부화 파이프라인에서 사용할 수 있습니다. 저희가 보유한 일부 사용 가능한 리소스는 PostgreSQL, DynamoDB, Elasticsearch, DuckDB 또는 모든 REST API 제공 업체 등이 있습니다.\n\n```js\nname: google_geocoding\nconnector:\n  type: REST@v1\n  base_url: \"https://maps.googleapis.com/maps/api/geocode/json\"\n  auth:\n    type: api_key\n    parameters:\n      key: \u003cmy_api_key\u003e\n    add_to: query_parameters\n  timeout_seconds: 10\nretry_policy:\n  type: exponential\n  max_tries: 3\n  max_time: 60\nbatch_size: 1\nconcurrency: 1000\n```\n\nREST API: REST API를 위한 강력한 추상화를 구현하는 것은 재시도 메커니즘과 실패한 요청 처리 정책과 같은 좀 더 고급 기능을 포함할 수 있습니다. 각 쿼리에 관련 매개변수를 추가하는 일반적인 방법(메서드 유형 - GET/POST, 전용 본문, 쿼리 매개변수, 헤더 또는 인증)이 포함될 수 있습니다. 일부 제공 업체는 동일한 쿼리 내에서 여러 레코드를 허용하고 '맵/리듀스' 메커니즘을 지원해야 했습니다. 일부 제공 업체는 HTTP 성공 응답(200)을 반환하지만 실패 정보가 응답 내에 캡슐화되어 있어 상태 해결자를 구현해야 했습니다.\n\n\n\nDuckDB를 사용하여 데이터 처리 유연성 강화: 이전 인프라에서는 데이터 엔지니어들이 Python 코드를 작성하여 객체 처리, 데이터 병합, 외부 소스 가져오기, 집계, 예외 처리와 같은 작업에 필요한 유연성을 즐겼습니다. 이 유연성을 유지할 효과적인 대안을 찾기 위해 DuckDB를 우리의 인메모리 처리 프레임워크에 통합하기로 결정했습니다. DuckDB는 온라인 분석 처리 (OLAP) 시나리오에서 흔히 발생하는 분석 쿼리 워크로드에 특히 최적화되어 있습니다. 저희 운영에서 일반적인 Python 사용 사례를 평가한 결과, DuckDB의 능력은 lambda 및 유틸리티 함수, 패턴 매칭, 고급 텍스트 함수와 같은 기능으로 확장되어 대부분의 Python 기능을 SQL 쿼리를 사용하여 재현할 수 있다는 것을 확인했습니다. 이러한 적응은 우리의 데이터 처리가 견고하고 다용도로 유지되도록 보장합니다.\n\nDuckDB 능력을 확인해보겠습니다. 다음은 추가 처리가 필요한 중첩 JSON인 Google Geocoding API 응답의 예제입니다:\n\n```js\n{\n    \"results\": [\n        {\n            \"address_components\": [\n                ...\n            ],\n            \"formatted_address\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043, USA\",\n            \"geometry\": {\n                \"location\": {\n                    \"lat\": 37.4224428,\n                    \"lng\": -122.0842467\n                },\n                \"location_type\": \"ROOFTOP\",\n            },\n            ...\n        }\n    ],\n    \"status\": \"OK\"\n}\n```\n\n우리가 관련 필드만 추출하고 싶다고 가정할 때, DuckDB 엔진을 활용하여 이 응답을 평평하게 만드는 SQL 쿼리를 구축할 수 있습니다.\n\n\n\n```js\nselect \n    results -\u003e\u003e '$[0].geometry.location_type' as \"위치 유형\",\n    results -\u003e\u003e '$[0].geometry.location.lat' as \"위도\",\n    results -\u003e\u003e '$[0].geometry.location.lng' as \"경도\",\nfrom {_.data}\n```\n\n청크 크기: 성능을 최적화하기 위해 요청된 데이터를 청크와 배치로 나눠 빠르고 확장 가능한 패칭을 할 수 있습니다. 50만 레코드의 파이프라인을 순차적으로 실행하는 대신, 데이터가 작은 병렬 청크로 나눠집니다. \n\n템플릿 엔진: 시스템을 개발할 때, 간단한 설정 방식만으로는 개발자들의 요구를 충족시키기에는 부족하다고 판단했습니다. 우리는 유연성을 향상시키기 위해 Jinja 템플릿 엔진을 설정 프레임워크에 통합하기로 결정했습니다. Jinja 템플릿 엔진을 사용한 SQL 예제:\n\n```js\n-- 소셜 링크\n{-\n  set social_platforms = ['twitter', 'facebook', 'linkedin', 'pinterest', 'youtube', 'instagram']\n-}\nselect\n  { for platform in social_platforms -}\n    trim({ platform }_link, '\"') as { platform }_link,\n  { endfor },\n  len(social_links) as number_of_social_networks\nfrom { _.data }\n```\n\n\n\n# No-Code 예제를 통한 Enrichment 파이프라인\n\n마침내, 완전한 Google Geocoding Enrichment 파이프라인에 대해 살펴보겠습니다. 이 파이프라인은 PostalAddress 유형의 입력을 받아 다양한 지리적 위치 속성으로 보강합니다. 이 파이프라인은 Google API에서 데이터를 가져오는 단계와 DuckDB 엔진을 사용하여 데이터를 후처리하는 두 단계로 구성되어 있습니다.\n\n```js\nname: google_geocoding_api\npipelines:\n- input_schema:\n    fields:\n    - name: PostalAddress\n      type: PostalAddress\n  steps:\n  - name: fetch\n    action:\n      type: rest@v1\n      resource_name: google_geocoding\n      method: GET\n      # Jinja 템플릿 엔진을 활용하여 쿼리 매개변수에 입력을 추가합니다\n      parameters:\n        address: \"{_.data[0].PostalAddress }\"\n      # 'include' 기능을 보여주는 데모\n      status_resolver: !include 'google_geocoding_api/status_resolver.sql'\n  - name: post_duck\n    action:\n      type: duckdb@v1\n      query: !include 'google_geocoding_api/post_transform.sql'\nsignals:\n- Location Type\n- Latitude\n- Longitude\n- Country\n- Region\n- Sub-Region\n- US State\n- County\n- City\n- Neighborhood\n- Subpremise\n- Street\n- House Number\n- Zip Code\n```\n\n새로운 인프라는 데이터 엔지니어들에게 진보된 보편적인 도구를 제공하여 조직적 요구에 맞춘 견고하고 유연한 데이터 파이프라인을 설계하고 구현할 수 있도록 도와줍니다. 이 접근 방식은 루틴적인 성능 문제에서 전략적 데이터 품질 관리로 초점을 전환합니다.\n\n\n\n# 요약\n\n이 기사에서는 복잡한 데이터 제품을 개발하기 위한 구성 기반 접근 방식을 소개했습니다. 기존의 코드 중심 방법에서 혁신적인 노코드 인프라로의 전환을 강조했습니다. 데이터 엔지니어들은 이제 고급 도구를 이용하여 빠른 데이터 파이프라인 개발 및 배포가 가능해졌으며, 컨셉에서 제품화까지 걸리는 시간을 몇 시간으로 대폭 줄였습니다. 유지 관리를 간소화하고 모니터링을 강화하며 동적 확장이 가능하게 했습니다.\n\n이 접근 방식은 운영을 최적화할 뿐만 아니라 엔지니어들이 인프라를 관리하는 대신 데이터 품질에 중점을 둘 수 있도록 돕습니다. 효율이 발전하는 환경을 조성합니다.","ogImage":{"url":"/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_0.png"},"coverImage":"/assets/img/2024-05-16-Howwebuildno-codeDataEnrichmentPipelines_0.png","tag":["Tech"],"readingTime":10}],"page":"84","totalPageCount":154,"totalPageGroupCount":8,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"84"},"buildId":"Rv-NbbtWUaja2joH5WkO_","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>