<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>YOLOv10 Custom Object Detection | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-27-YOLOv10CustomObjectDetection" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="YOLOv10 Custom Object Detection | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="YOLOv10 Custom Object Detection | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-27-YOLOv10CustomObjectDetection" data-gatsby-head="true"/><meta name="twitter:title" content="YOLOv10 Custom Object Detection | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-27 18:31" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/t9N7vwmpvBMQnO2PSctoH/_buildManifest.js" defer=""></script><script src="/_next/static/t9N7vwmpvBMQnO2PSctoH/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">YOLOv10 Custom Object Detection</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="YOLOv10 Custom Object Detection" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 27, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-27-YOLOv10CustomObjectDetection&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>YOLOv10 및 사용자 지정 데이터로 모델 학습 개요</p>
<p><img src="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png" alt="YOLOv10CustomObjectDetection_0.png"></p>
<h2>개요</h2>
<p>Ultralytics Python 패키지를 사용하여 개발된 YOLOv10은 실시간 객체 검출을 위한 새로운 접근 방식을 제공합니다. Qinghua 대학 연구원들이 개발한 이 모델은 모델 아키텍처 개선과 non-maximum suppression (NMS) 제거를 통해 성능을 향상시켰습니다. 이러한 최적화로 인해 더 낮은 계산 요구사항으로 최신 기술 성능을 제공합니다. YOLOv10은 다양한 모델 규모에 대해 우수한 정확도-대기간 교환을 제공하는 것으로 실험 결과 보여졌습니다.</p>
<p>내 이전 기사를 읽은 사람들은 알겠지만, YOLO 모델을 사용한 다양한 프로젝트를 공유해 왔습니다. 사전 학습된 모델 중에서 성능과 효율성 면에서 두드러지는 YOLO 모델입니다. 그러나 실시간 객체 감지는 비최대 억제 (NMS)와 구조적 비효율성에 의해 도전을 겪어왔습니다. YOLOv10은 이러한 문제를 해결하기 위해 NMS를 제거하고 효율성과 정확도 양쪽을 모두 고려한 설계 전략을 채택했습니다.</p>
<h2>구조</h2>
<p><img src="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_1.png" alt="YOLOv10CustomObjectDetection_1"></p>
<ul>
<li>백본: 특징 추출을 담당하는 백본은 CSPNet (Cross Stage Partial Network)의 향상된 버전을 사용하여 기울기 흐름을 개선하고 계산 중복성을 줄였습니다.</li>
<li>넥: 다양한 스케일의 특징을 집계하고 헤드로 전달하는 넥은 효과적인 다중 스케일 특징 퓨전을 위해 PAN (Path Aggregation Network) 레이어를 포함하고 있습니다.</li>
<li>One-to-Many 헤드: 훈련 중 하나의 객체에 대해 여러 예측을 생성하여 풍부한 지도 신호를 제공하고 학습 정확도를 향상시킵니다.</li>
<li>One-to-One 헤드: 추론 중 하나의 객체에 대해 최상의 예측을 생성하여 NMS의 필요성을 제거하고 지연 시간을 줄이며 효율성을 향상시킵니다.</li>
</ul>
<h2>모델 변형과 성능</h2>
<p>YOLOv10은 여섯 가지 모델로 제공됩니다:</p>
<ul>
<li>YOLOv10-N: 매우 자원이 제한된 환경을 위한 나노 버전.</li>
<li>YOLOv10-S: 속도와 정확도를 균형있게 유지한 작은 버전.</li>
<li>YOLOv10-M: 일반적인 용도를 위한 중간 버전.</li>
<li>YOLOv10-B: 정확도를 높이기 위해 넓이를 증가시킨 균형잡힌 버전.</li>
<li>YOLOv10-L: 컴퓨팅 자원을 늘리는 대가로 더 높은 정확도를 가진 대형 버전.</li>
<li>YOLOv10-X: 최대 정확도와 성능을 위한 초대형 버전</li>
</ul>
<p><img src="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_2.png" alt="이미지"></p>
<h2>비교</h2>
<p>서로 다른 모델 간의 지연 시간과 정확도에 대한 비교를 살펴봅시다. 이는 COCO와 같은 표준 벤치마크에서 테스트되었습니다.</p>
<p><img src="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_3.png" alt="Image 1"></p>
<p><img src="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_4.png" alt="Image 2"></p>
<p>YOLOv10은 실시간 객체 검출 애플리케이션에 대한 첨단 기술로, 더 적은 매개변수로 더 높은 정확도와 속도 성능을 제공합니다.</p>
<h2>사용자 정의 객체 검출을 위한 YOLOv10 훈련</h2>
<p>먼저, 공식 YOLOv10 GitHub 저장소를 복제하여 필요한 yolov10n 모델을 다운로드하세요.</p>
<pre><code class="hljs language-js">!pip install -q git+<span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/THU-MIG/yolov10.git</span>

!wget -P -q <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt</span>
</code></pre>
<p>로보플로 유니버스에서 원하는 사용자 정의 프로젝트를 실험하고, 직접 데이터셋을 생성하며, 인텔이 후원하는 RF100 데이터셋을 사용할 수 있어요. 이 게시물에서는 X-레이 이미지에서 위험한 항목을 감지하기 위해 준비된 데이터셋을 사용할 거에요.</p>
<p>로보플로 API를 사용하여 YOLOv8 형식으로 모델을 다운로드하세요.</p>
<pre><code class="hljs language-python">!pip install -q roboflow
<span class="hljs-keyword">from</span> roboflow <span class="hljs-keyword">import</span> Roboflow
rf = Roboflow(api_key=<span class="hljs-string">"your-api-key"</span>)
project = rf.workspace(<span class="hljs-string">"vladutc"</span>).project(<span class="hljs-string">"x-ray-baggage"</span>)
version = project.version(<span class="hljs-number">3</span>)
dataset = version.download(<span class="hljs-string">"yolov8"</span>)
</code></pre>
<p>매개변수와 파일 경로를 지정한 다음, 모델 훈련을 시작하세요.</p>
<p>!yolo task=detect mode=train epochs=25 batch=32 plots=True <br>
model='/content/-q/yolov10n.pt' <br>
data='/content/X-Ray-Baggage-3/data.yaml'</p>
<p>예시 data.yaml 파일</p>
<p>names:</p>
<ul>
<li>Gun</li>
<li>Knife</li>
<li>Pliers</li>
<li>Scissors</li>
<li>Wrench</li>
</ul>
<p>nc: 5</p>
<p>roboflow:
license: CC BY 4.0
project: x-ray-baggage
url: <a href="https://universe.roboflow.com/vladutc/x-ray-baggage/dataset/3" rel="nofollow" target="_blank">https://universe.roboflow.com/vladutc/x-ray-baggage/dataset/3</a>
version: 3
workspace: vladutc</p>
<p>test: /content/X-Ray-Baggage-3/test/images
train: /content/X-Ray-Baggage-3/train/images
val: /content/X-Ray-Baggage-3/valid/images</p>
<p>결과를 살펴봅시다.</p>
<p>md
<img src="/content/runs/detect/train/results.png" alt="Training results">{width=1000}</p>
<p><img src="/assets/img/2024-05-27-YOLOv10CustomObjectDetection_5.png" alt="Prediction results"></p>
<p>테스트 데이터를 예측하고 결과를 5x2 그리드로 표시합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> ultralytics <span class="hljs-keyword">import</span> YOLOv10

model_path = <span class="hljs-string">'/content/runs/detect/train/weights/best.pt'</span>
model = YOLOv10(model_path)
results = model(source=<span class="hljs-string">'/content/X-Ray-Baggage-3/test/images'</span>, conf=<span class="hljs-number">0.25</span>, save=<span class="hljs-literal">True</span>)
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> glob
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">image</span> <span class="hljs-keyword">as</span> mpimg

images = glob.<span class="hljs-title function_">glob</span>(<span class="hljs-string">'/content/runs/detect/predict/*.jpg'</span>)

images_to_display = images[:<span class="hljs-number">10</span>]

fig, axes = plt.<span class="hljs-title function_">subplots</span>(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>))

<span class="hljs-keyword">for</span> i, ax <span class="hljs-keyword">in</span> <span class="hljs-title function_">enumerate</span>(axes.<span class="hljs-property">flat</span>):
    <span class="hljs-keyword">if</span> i &#x3C; <span class="hljs-title function_">len</span>(images_to_display):
        img = mpimg.<span class="hljs-title function_">imread</span>(images_to_display[i])
        ax.<span class="hljs-title function_">imshow</span>(img)
        ax.<span class="hljs-title function_">axis</span>(<span class="hljs-string">'off'</span>)  
    <span class="hljs-attr">else</span>:
        ax.<span class="hljs-title function_">axis</span>(<span class="hljs-string">'off'</span>)  

plt.<span class="hljs-title function_">tight_layout</span>()
plt.<span class="hljs-title function_">show</span>()
</code></pre>
<h2>결론 및 권장 사항</h2>
<ul>
<li>이 글을 작성하는 동안 여러 데이터셋에서 YOLOv10n 모델을 학습하여 Colab의 15GB 무료 T4 GPU 한도를 고갈시켰습니다. Colab 환경에서 모델을 학습할 때 한도를 초과하면 T4 GPU에 제한이 있습니다. 이 문제를 해결하기 위해 다른 구글 계정으로 로그인할 수 있습니다.</li>
<li>기술이 빠르게 발전함에 따라 컴퓨터 비전과 대형 언어 모델 양쪽에서 단일 기술에 갇히지 않고 주요 개념을 배우는 것이 유익하다고 생각됩니다. 이를 적응하기 위해 이러한 기술의 개발자들로부터 배우는 것이 도움이 됩니다. Ultralytics와 Roboflow의 콘텐츠는 이 분야에서 매우 가치 있으며, 그들을 팔로우하는 것이 바람직합니다.</li>
</ul>
<h2>참고 자료</h2>
<ul>
<li>공식 레포: <a href="https://github.com/THU-MIG/yolov10" rel="nofollow" target="_blank">https://github.com/THU-MIG/yolov10</a></li>
<li>울트라리틱스 (Ultralytics)</li>
<li>로보플로우 (Roboflow)</li>
</ul>
<pre><code class="hljs language-js">@article{<span class="hljs-variable constant_">THU</span>-<span class="hljs-title class_">MIGyolov10</span>,
  title={<span class="hljs-title class_">YOLOv10</span>: 실시간 엔드 투 엔드 객체 검출},
  author={<span class="hljs-title class_">Ao</span> <span class="hljs-title class_">Wang</span>, <span class="hljs-title class_">Hui</span> <span class="hljs-title class_">Chen</span>, <span class="hljs-title class_">Lihao</span> <span class="hljs-title class_">Liu</span> 등},
  journal={arXiv 사전 인쇄 <span class="hljs-attr">arXiv</span>:<span class="hljs-number">2405.14458</span>},
  year={<span class="hljs-number">2024</span>},
  institution={<span class="hljs-title class_">Tsinghua</span> <span class="hljs-title class_">University</span>},
  license={<span class="hljs-variable constant_">AGPL</span>-<span class="hljs-number">3.0</span>}
}
</code></pre>
<pre><code class="hljs language-js">@misc{
x-ray-baggage_dataset,
title={X-레이 수하물 데이터셋},
type={오픈 소스 데이터셋},
author={vladutc},
howpublished={\url{ <span class="hljs-attr">https</span>:<span class="hljs-comment">//universe.roboflow.com/vladutc/x-ray-baggage }},</span>
url={<span class="hljs-attr">https</span>:<span class="hljs-comment">//universe.roboflow.com/vladutc/x-ray-baggage},</span>
journal={<span class="hljs-title class_">Roboflow</span> <span class="hljs-title class_">Universe</span>},
publisher={<span class="hljs-title class_">Roboflow</span>},
year={<span class="hljs-number">2024</span>},
month={<span class="hljs-number">5</span>},
note={방문일: <span class="hljs-number">2024</span>년 <span class="hljs-number">5</span>월 <span class="hljs-number">26</span>일},
}
</code></pre>
<p>저는 청화 대학교의 연구원들, Ultralytics와 Roboflow 팀, 그리고 오픈 소스 커뮤니티의 모든 기여자들에게 감사드립니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"YOLOv10 Custom Object Detection","description":"","date":"2024-05-27 18:31","slug":"2024-05-27-YOLOv10CustomObjectDetection","content":"\n\nYOLOv10 및 사용자 지정 데이터로 모델 학습 개요\n\n![YOLOv10CustomObjectDetection_0.png](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png)\n\n## 개요\n\nUltralytics Python 패키지를 사용하여 개발된 YOLOv10은 실시간 객체 검출을 위한 새로운 접근 방식을 제공합니다. Qinghua 대학 연구원들이 개발한 이 모델은 모델 아키텍처 개선과 non-maximum suppression (NMS) 제거를 통해 성능을 향상시켰습니다. 이러한 최적화로 인해 더 낮은 계산 요구사항으로 최신 기술 성능을 제공합니다. YOLOv10은 다양한 모델 규모에 대해 우수한 정확도-대기간 교환을 제공하는 것으로 실험 결과 보여졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n내 이전 기사를 읽은 사람들은 알겠지만, YOLO 모델을 사용한 다양한 프로젝트를 공유해 왔습니다. 사전 학습된 모델 중에서 성능과 효율성 면에서 두드러지는 YOLO 모델입니다. 그러나 실시간 객체 감지는 비최대 억제 (NMS)와 구조적 비효율성에 의해 도전을 겪어왔습니다. YOLOv10은 이러한 문제를 해결하기 위해 NMS를 제거하고 효율성과 정확도 양쪽을 모두 고려한 설계 전략을 채택했습니다.\n\n## 구조\n\n![YOLOv10CustomObjectDetection_1](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_1.png)\n\n- 백본: 특징 추출을 담당하는 백본은 CSPNet (Cross Stage Partial Network)의 향상된 버전을 사용하여 기울기 흐름을 개선하고 계산 중복성을 줄였습니다.\n- 넥: 다양한 스케일의 특징을 집계하고 헤드로 전달하는 넥은 효과적인 다중 스케일 특징 퓨전을 위해 PAN (Path Aggregation Network) 레이어를 포함하고 있습니다.\n- One-to-Many 헤드: 훈련 중 하나의 객체에 대해 여러 예측을 생성하여 풍부한 지도 신호를 제공하고 학습 정확도를 향상시킵니다.\n- One-to-One 헤드: 추론 중 하나의 객체에 대해 최상의 예측을 생성하여 NMS의 필요성을 제거하고 지연 시간을 줄이며 효율성을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모델 변형과 성능\n\nYOLOv10은 여섯 가지 모델로 제공됩니다:\n\n- YOLOv10-N: 매우 자원이 제한된 환경을 위한 나노 버전.\n- YOLOv10-S: 속도와 정확도를 균형있게 유지한 작은 버전.\n- YOLOv10-M: 일반적인 용도를 위한 중간 버전.\n- YOLOv10-B: 정확도를 높이기 위해 넓이를 증가시킨 균형잡힌 버전.\n- YOLOv10-L: 컴퓨팅 자원을 늘리는 대가로 더 높은 정확도를 가진 대형 버전.\n- YOLOv10-X: 최대 정확도와 성능을 위한 초대형 버전\n\n![이미지](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 비교\n\n서로 다른 모델 간의 지연 시간과 정확도에 대한 비교를 살펴봅시다. 이는 COCO와 같은 표준 벤치마크에서 테스트되었습니다.\n\n![Image 1](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_3.png)\n\n![Image 2](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nYOLOv10은 실시간 객체 검출 애플리케이션에 대한 첨단 기술로, 더 적은 매개변수로 더 높은 정확도와 속도 성능을 제공합니다.\n\n## 사용자 정의 객체 검출을 위한 YOLOv10 훈련\n\n먼저, 공식 YOLOv10 GitHub 저장소를 복제하여 필요한 yolov10n 모델을 다운로드하세요.\n\n```js\n!pip install -q git+https://github.com/THU-MIG/yolov10.git\n\n!wget -P -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n로보플로 유니버스에서 원하는 사용자 정의 프로젝트를 실험하고, 직접 데이터셋을 생성하며, 인텔이 후원하는 RF100 데이터셋을 사용할 수 있어요. 이 게시물에서는 X-레이 이미지에서 위험한 항목을 감지하기 위해 준비된 데이터셋을 사용할 거에요.\n\n로보플로 API를 사용하여 YOLOv8 형식으로 모델을 다운로드하세요.\n\n```python\n!pip install -q roboflow\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"your-api-key\")\nproject = rf.workspace(\"vladutc\").project(\"x-ray-baggage\")\nversion = project.version(3)\ndataset = version.download(\"yolov8\")\n```\n\n매개변수와 파일 경로를 지정한 다음, 모델 훈련을 시작하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n!yolo task=detect mode=train epochs=25 batch=32 plots=True \\\nmodel='/content/-q/yolov10n.pt' \\\ndata='/content/X-Ray-Baggage-3/data.yaml'\n\n\n예시 data.yaml 파일\n\n\nnames:\n- Gun\n- Knife\n- Pliers\n- Scissors\n- Wrench\n\nnc: 5\n\nroboflow:\n  license: CC BY 4.0\n  project: x-ray-baggage\n  url: https://universe.roboflow.com/vladutc/x-ray-baggage/dataset/3\n  version: 3\n  workspace: vladutc\n\ntest: /content/X-Ray-Baggage-3/test/images\ntrain: /content/X-Ray-Baggage-3/train/images\nval: /content/X-Ray-Baggage-3/valid/images\n\n\n결과를 살펴봅시다.\n \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nmd\n![Training results](/content/runs/detect/train/results.png){width=1000}\n\n![Prediction results](/assets/img/2024-05-27-YOLOv10CustomObjectDetection_5.png)\n\n테스트 데이터를 예측하고 결과를 5x2 그리드로 표시합니다.\n\n```python\nfrom ultralytics import YOLOv10\n\nmodel_path = '/content/runs/detect/train/weights/best.pt'\nmodel = YOLOv10(model_path)\nresults = model(source='/content/X-Ray-Baggage-3/test/images', conf=0.25, save=True)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimages = glob.glob('/content/runs/detect/predict/*.jpg')\n\nimages_to_display = images[:10]\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 10))\n\nfor i, ax in enumerate(axes.flat):\n    if i \u003c len(images_to_display):\n        img = mpimg.imread(images_to_display[i])\n        ax.imshow(img)\n        ax.axis('off')  \n    else:\n        ax.axis('off')  \n\nplt.tight_layout()\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_6.png\" /\u003e\n\n## 결론 및 권장 사항\n\n- 이 글을 작성하는 동안 여러 데이터셋에서 YOLOv10n 모델을 학습하여 Colab의 15GB 무료 T4 GPU 한도를 고갈시켰습니다. Colab 환경에서 모델을 학습할 때 한도를 초과하면 T4 GPU에 제한이 있습니다. 이 문제를 해결하기 위해 다른 구글 계정으로 로그인할 수 있습니다.\n- 기술이 빠르게 발전함에 따라 컴퓨터 비전과 대형 언어 모델 양쪽에서 단일 기술에 갇히지 않고 주요 개념을 배우는 것이 유익하다고 생각됩니다. 이를 적응하기 위해 이러한 기술의 개발자들로부터 배우는 것이 도움이 됩니다. Ultralytics와 Roboflow의 콘텐츠는 이 분야에서 매우 가치 있으며, 그들을 팔로우하는 것이 바람직합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 참고 자료\n\n- 공식 레포: https://github.com/THU-MIG/yolov10\n- 울트라리틱스 (Ultralytics)\n- 로보플로우 (Roboflow)\n\n```js\n@article{THU-MIGyolov10,\n  title={YOLOv10: 실시간 엔드 투 엔드 객체 검출},\n  author={Ao Wang, Hui Chen, Lihao Liu 등},\n  journal={arXiv 사전 인쇄 arXiv:2405.14458},\n  year={2024},\n  institution={Tsinghua University},\n  license={AGPL-3.0}\n}\n```\n\n```js\n@misc{\nx-ray-baggage_dataset,\ntitle={X-레이 수하물 데이터셋},\ntype={오픈 소스 데이터셋},\nauthor={vladutc},\nhowpublished={\\url{ https://universe.roboflow.com/vladutc/x-ray-baggage }},\nurl={https://universe.roboflow.com/vladutc/x-ray-baggage},\njournal={Roboflow Universe},\npublisher={Roboflow},\nyear={2024},\nmonth={5},\nnote={방문일: 2024년 5월 26일},\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 청화 대학교의 연구원들, Ultralytics와 Roboflow 팀, 그리고 오픈 소스 커뮤니티의 모든 기여자들에게 감사드립니다.","ogImage":{"url":"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png"},"coverImage":"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eYOLOv10 및 사용자 지정 데이터로 모델 학습 개요\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_0.png\" alt=\"YOLOv10CustomObjectDetection_0.png\"\u003e\u003c/p\u003e\n\u003ch2\u003e개요\u003c/h2\u003e\n\u003cp\u003eUltralytics Python 패키지를 사용하여 개발된 YOLOv10은 실시간 객체 검출을 위한 새로운 접근 방식을 제공합니다. Qinghua 대학 연구원들이 개발한 이 모델은 모델 아키텍처 개선과 non-maximum suppression (NMS) 제거를 통해 성능을 향상시켰습니다. 이러한 최적화로 인해 더 낮은 계산 요구사항으로 최신 기술 성능을 제공합니다. YOLOv10은 다양한 모델 규모에 대해 우수한 정확도-대기간 교환을 제공하는 것으로 실험 결과 보여졌습니다.\u003c/p\u003e\n\u003cp\u003e내 이전 기사를 읽은 사람들은 알겠지만, YOLO 모델을 사용한 다양한 프로젝트를 공유해 왔습니다. 사전 학습된 모델 중에서 성능과 효율성 면에서 두드러지는 YOLO 모델입니다. 그러나 실시간 객체 감지는 비최대 억제 (NMS)와 구조적 비효율성에 의해 도전을 겪어왔습니다. YOLOv10은 이러한 문제를 해결하기 위해 NMS를 제거하고 효율성과 정확도 양쪽을 모두 고려한 설계 전략을 채택했습니다.\u003c/p\u003e\n\u003ch2\u003e구조\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_1.png\" alt=\"YOLOv10CustomObjectDetection_1\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e백본: 특징 추출을 담당하는 백본은 CSPNet (Cross Stage Partial Network)의 향상된 버전을 사용하여 기울기 흐름을 개선하고 계산 중복성을 줄였습니다.\u003c/li\u003e\n\u003cli\u003e넥: 다양한 스케일의 특징을 집계하고 헤드로 전달하는 넥은 효과적인 다중 스케일 특징 퓨전을 위해 PAN (Path Aggregation Network) 레이어를 포함하고 있습니다.\u003c/li\u003e\n\u003cli\u003eOne-to-Many 헤드: 훈련 중 하나의 객체에 대해 여러 예측을 생성하여 풍부한 지도 신호를 제공하고 학습 정확도를 향상시킵니다.\u003c/li\u003e\n\u003cli\u003eOne-to-One 헤드: 추론 중 하나의 객체에 대해 최상의 예측을 생성하여 NMS의 필요성을 제거하고 지연 시간을 줄이며 효율성을 향상시킵니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e모델 변형과 성능\u003c/h2\u003e\n\u003cp\u003eYOLOv10은 여섯 가지 모델로 제공됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYOLOv10-N: 매우 자원이 제한된 환경을 위한 나노 버전.\u003c/li\u003e\n\u003cli\u003eYOLOv10-S: 속도와 정확도를 균형있게 유지한 작은 버전.\u003c/li\u003e\n\u003cli\u003eYOLOv10-M: 일반적인 용도를 위한 중간 버전.\u003c/li\u003e\n\u003cli\u003eYOLOv10-B: 정확도를 높이기 위해 넓이를 증가시킨 균형잡힌 버전.\u003c/li\u003e\n\u003cli\u003eYOLOv10-L: 컴퓨팅 자원을 늘리는 대가로 더 높은 정확도를 가진 대형 버전.\u003c/li\u003e\n\u003cli\u003eYOLOv10-X: 최대 정확도와 성능을 위한 초대형 버전\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e비교\u003c/h2\u003e\n\u003cp\u003e서로 다른 모델 간의 지연 시간과 정확도에 대한 비교를 살펴봅시다. 이는 COCO와 같은 표준 벤치마크에서 테스트되었습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_3.png\" alt=\"Image 1\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_4.png\" alt=\"Image 2\"\u003e\u003c/p\u003e\n\u003cp\u003eYOLOv10은 실시간 객체 검출 애플리케이션에 대한 첨단 기술로, 더 적은 매개변수로 더 높은 정확도와 속도 성능을 제공합니다.\u003c/p\u003e\n\u003ch2\u003e사용자 정의 객체 검출을 위한 YOLOv10 훈련\u003c/h2\u003e\n\u003cp\u003e먼저, 공식 YOLOv10 GitHub 저장소를 복제하여 필요한 yolov10n 모델을 다운로드하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!pip install -q git+\u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/THU-MIG/yolov10.git\u003c/span\u003e\n\n!wget -P -q \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e로보플로 유니버스에서 원하는 사용자 정의 프로젝트를 실험하고, 직접 데이터셋을 생성하며, 인텔이 후원하는 RF100 데이터셋을 사용할 수 있어요. 이 게시물에서는 X-레이 이미지에서 위험한 항목을 감지하기 위해 준비된 데이터셋을 사용할 거에요.\u003c/p\u003e\n\u003cp\u003e로보플로 API를 사용하여 YOLOv8 형식으로 모델을 다운로드하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e!pip install -q roboflow\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e roboflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Roboflow\nrf = Roboflow(api_key=\u003cspan class=\"hljs-string\"\u003e\"your-api-key\"\u003c/span\u003e)\nproject = rf.workspace(\u003cspan class=\"hljs-string\"\u003e\"vladutc\"\u003c/span\u003e).project(\u003cspan class=\"hljs-string\"\u003e\"x-ray-baggage\"\u003c/span\u003e)\nversion = project.version(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\ndataset = version.download(\u003cspan class=\"hljs-string\"\u003e\"yolov8\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e매개변수와 파일 경로를 지정한 다음, 모델 훈련을 시작하세요.\u003c/p\u003e\n\u003cp\u003e!yolo task=detect mode=train epochs=25 batch=32 plots=True \u003cbr\u003e\nmodel='/content/-q/yolov10n.pt' \u003cbr\u003e\ndata='/content/X-Ray-Baggage-3/data.yaml'\u003c/p\u003e\n\u003cp\u003e예시 data.yaml 파일\u003c/p\u003e\n\u003cp\u003enames:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGun\u003c/li\u003e\n\u003cli\u003eKnife\u003c/li\u003e\n\u003cli\u003ePliers\u003c/li\u003e\n\u003cli\u003eScissors\u003c/li\u003e\n\u003cli\u003eWrench\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003enc: 5\u003c/p\u003e\n\u003cp\u003eroboflow:\nlicense: CC BY 4.0\nproject: x-ray-baggage\nurl: \u003ca href=\"https://universe.roboflow.com/vladutc/x-ray-baggage/dataset/3\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://universe.roboflow.com/vladutc/x-ray-baggage/dataset/3\u003c/a\u003e\nversion: 3\nworkspace: vladutc\u003c/p\u003e\n\u003cp\u003etest: /content/X-Ray-Baggage-3/test/images\ntrain: /content/X-Ray-Baggage-3/train/images\nval: /content/X-Ray-Baggage-3/valid/images\u003c/p\u003e\n\u003cp\u003e결과를 살펴봅시다.\u003c/p\u003e\n\u003cp\u003emd\n\u003cimg src=\"/content/runs/detect/train/results.png\" alt=\"Training results\"\u003e{width=1000}\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-YOLOv10CustomObjectDetection_5.png\" alt=\"Prediction results\"\u003e\u003c/p\u003e\n\u003cp\u003e테스트 데이터를 예측하고 결과를 5x2 그리드로 표시합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e ultralytics \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e YOLOv10\n\nmodel_path = \u003cspan class=\"hljs-string\"\u003e'/content/runs/detect/train/weights/best.pt'\u003c/span\u003e\nmodel = YOLOv10(model_path)\nresults = model(source=\u003cspan class=\"hljs-string\"\u003e'/content/X-Ray-Baggage-3/test/images'\u003c/span\u003e, conf=\u003cspan class=\"hljs-number\"\u003e0.25\u003c/span\u003e, save=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e glob\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003eimage\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e mpimg\n\nimages = glob.\u003cspan class=\"hljs-title function_\"\u003eglob\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'/content/runs/detect/predict/*.jpg'\u003c/span\u003e)\n\nimages_to_display = images[:\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e]\n\nfig, axes = plt.\u003cspan class=\"hljs-title function_\"\u003esubplots\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i, ax \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eenumerate\u003c/span\u003e(axes.\u003cspan class=\"hljs-property\"\u003eflat\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e i \u0026#x3C; \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(images_to_display):\n        img = mpimg.\u003cspan class=\"hljs-title function_\"\u003eimread\u003c/span\u003e(images_to_display[i])\n        ax.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(img)\n        ax.\u003cspan class=\"hljs-title function_\"\u003eaxis\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'off'\u003c/span\u003e)  \n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        ax.\u003cspan class=\"hljs-title function_\"\u003eaxis\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'off'\u003c/span\u003e)  \n\nplt.\u003cspan class=\"hljs-title function_\"\u003etight_layout\u003c/span\u003e()\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e결론 및 권장 사항\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e이 글을 작성하는 동안 여러 데이터셋에서 YOLOv10n 모델을 학습하여 Colab의 15GB 무료 T4 GPU 한도를 고갈시켰습니다. Colab 환경에서 모델을 학습할 때 한도를 초과하면 T4 GPU에 제한이 있습니다. 이 문제를 해결하기 위해 다른 구글 계정으로 로그인할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e기술이 빠르게 발전함에 따라 컴퓨터 비전과 대형 언어 모델 양쪽에서 단일 기술에 갇히지 않고 주요 개념을 배우는 것이 유익하다고 생각됩니다. 이를 적응하기 위해 이러한 기술의 개발자들로부터 배우는 것이 도움이 됩니다. Ultralytics와 Roboflow의 콘텐츠는 이 분야에서 매우 가치 있으며, 그들을 팔로우하는 것이 바람직합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e참고 자료\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e공식 레포: \u003ca href=\"https://github.com/THU-MIG/yolov10\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/THU-MIG/yolov10\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e울트라리틱스 (Ultralytics)\u003c/li\u003e\n\u003cli\u003e로보플로우 (Roboflow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e@article{\u003cspan class=\"hljs-variable constant_\"\u003eTHU\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eMIGyolov10\u003c/span\u003e,\n  title={\u003cspan class=\"hljs-title class_\"\u003eYOLOv10\u003c/span\u003e: 실시간 엔드 투 엔드 객체 검출},\n  author={\u003cspan class=\"hljs-title class_\"\u003eAo\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eWang\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eHui\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eChen\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eLihao\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eLiu\u003c/span\u003e 등},\n  journal={arXiv 사전 인쇄 \u003cspan class=\"hljs-attr\"\u003earXiv\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e2405.14458\u003c/span\u003e},\n  year={\u003cspan class=\"hljs-number\"\u003e2024\u003c/span\u003e},\n  institution={\u003cspan class=\"hljs-title class_\"\u003eTsinghua\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUniversity\u003c/span\u003e},\n  license={\u003cspan class=\"hljs-variable constant_\"\u003eAGPL\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3.0\u003c/span\u003e}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e@misc{\nx-ray-baggage_dataset,\ntitle={X-레이 수하물 데이터셋},\ntype={오픈 소스 데이터셋},\nauthor={vladutc},\nhowpublished={\\url{ \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//universe.roboflow.com/vladutc/x-ray-baggage }},\u003c/span\u003e\nurl={\u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//universe.roboflow.com/vladutc/x-ray-baggage},\u003c/span\u003e\njournal={\u003cspan class=\"hljs-title class_\"\u003eRoboflow\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUniverse\u003c/span\u003e},\npublisher={\u003cspan class=\"hljs-title class_\"\u003eRoboflow\u003c/span\u003e},\nyear={\u003cspan class=\"hljs-number\"\u003e2024\u003c/span\u003e},\nmonth={\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e},\nnote={방문일: \u003cspan class=\"hljs-number\"\u003e2024\u003c/span\u003e년 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e월 \u003cspan class=\"hljs-number\"\u003e26\u003c/span\u003e일},\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e저는 청화 대학교의 연구원들, Ultralytics와 Roboflow 팀, 그리고 오픈 소스 커뮤니티의 모든 기여자들에게 감사드립니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-27-YOLOv10CustomObjectDetection"},"buildId":"t9N7vwmpvBMQnO2PSctoH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>