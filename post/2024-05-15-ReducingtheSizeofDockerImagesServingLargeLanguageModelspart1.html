<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1" data-gatsby-head="true"/><meta name="twitter:title" content="대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 11:42" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-985df180e46efe53.js" defer=""></script><script src="/_next/static/z1a6VTi5qHH9JJH7jaxL3/_buildManifest.js" defer=""></script><script src="/_next/static/z1a6VTi5qHH9JJH7jaxL3/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/assets/profile.jpg"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><img src="/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png"/>
<h1>소개</h1>
<p>BERT, RoBERTa 또는 T5와 같은 Transformer 기반 모델은 자연어 처리에서 맞춤 문제에 대한 최신 솔루션을 제공합니다. 제품에서 모델을 제공하는 보편적인 방법은 모델에 대한 API를 제공하는 Docker 이미지를 빌드하는 것입니다. 이미지는 필요한 종속성, 모델 자체 및 모델로 입력 데이터를 처리하는 코드를 캡슐화합니다. 큰 생성 모델 (GenAI)과 비교하면, 이러한 모델은 상대적으로 작아서 0.5~2GB 정도입니다. 그러나 모델을 Docker 이미지로 배포하는 간단한 방법을 따를 때, 이미지 크기가 8GB에 이를 수 있음에 놀랐을 수도 있습니다. 대상 이미지가 왜 그렇게 큰지, 그리고 이미지 크기를 줄일 수 있는 방법이 있는지 궁금했던 적이 있나요? 이 이야기에서는 Docker 이미지가 왜 그렇게 큰지 설명하고 그 크기를 줄이는 방법에 대해 논의하겠습니다.</p>
<p>이 이야기에서 사용된 Python 스크립트 및 Docker 파일의 예시는 [1]에서도 확인할 수 있습니다:</p>
<h1>기본 도커 이미지</h1>
<p>언어 감지 모델을 위한 간단한 도커 이미지를 만들어 봅시다. 모델을 구축하기 위한 몇 가지 전제사항은 다음과 같습니다:</p>
<ul>
<li>훈련된 모델을 사용할 것입니다: papluca/xlm-roberta-base-language-detection [2].</li>
<li>가능한 최상의 성능을 얻기 위해 GPU를 활용할 것입니다.</li>
<li>단일 텍스트를 처리하는 간단한 엔드포인트를 제공하기 위해 FastAPI를 사용할 것입니다.</li>
</ul>
<p>다음은 이미지를 빌드하기 위한 Dockerfile입니다:</p>
<p>모델을 로드하고 추론을 수행하는 데 사용된 코드는 다음과 같습니다:</p>
<p>다음은 이미지를 빌드하는 데 사용된 명령어입니다:</p>
<pre><code class="hljs language-js">docker build -t language_detection_cuda . -f <span class="hljs-title class_">Dockerfile</span>_cuda
</code></pre>
<p>... 그리고 이미지를 실행하세요.</p>
<pre><code class="hljs language-js">도커 실행 --gpus <span class="hljs-number">0</span> -p <span class="hljs-number">8000</span>:<span class="hljs-number">8000</span> language_detection_cuda
</code></pre>
<p>... 엔드포인트를 테스트해 보겠습니다:</p>
<pre><code class="hljs language-js">시간 curl -X <span class="hljs-string">&#x27;POST&#x27;</span>   <span class="hljs-string">&#x27;http://localhost:8000/process&#x27;</span>   -H <span class="hljs-string">&#x27;accept: application/json&#x27;</span>   -H <span class="hljs-string">&#x27;Content-Type: application/json&#x27;</span>   -d <span class="hljs-string">&#x27;{
  &quot;text&quot;: &quot;Certo ci sono stati dei problemi - problemi che dovremo risolvere in vista, per esempio, dell&#x27;</span>\<span class="hljs-string">&#x27;&#x27;</span>ampliamento - ma a volte ne esageriamo il lato negativo.<span class="hljs-string">&quot;
}&#x27;
</span></code></pre>
<p>다음 출력을 받았습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-string">&quot;it&quot;</span>
</code></pre>
<p>지금까지 특별한 것은 없어요. 엔드포인트가 할 일을 잘 수행하고 있어요.</p>
<p>모델의 크기는 1.11GB입니다 (model.safetensors 파일), 토크나이저를 위한 추가 10MB가 있어요. 이제 도커 이미지의 크기를 보겠습니다:</p>
<pre><code class="hljs language-js">docker images | grep language_detection_cuda
</code></pre>
<p>… 출력물은:</p>
<pre><code class="hljs language-js">language_detection_cuda    최신 버전   47f4c1c0de2d   <span class="hljs-number">33</span>분 전   <span class="hljs-number">7.</span>05GB
</code></pre>
<p>도커 이미지의 총 용량은 7.05GB입니다. 와우, 상당히 많죠? 하지만 왜 이미지가 그렇게 큰 걸까요? 이제 컨테이너로 들어가서 내부를 확인해 보겠습니다.</p>
<pre><code class="hljs language-js">docker run -it --gpus <span class="hljs-number">0</span> -p <span class="hljs-number">8000</span>:<span class="hljs-number">8000</span> --entrypoint <span class="hljs-string">&quot;/bin/bash&quot;</span>  language_detection_cuda
</code></pre>
<p>이미지 크기를 분석하기 위해 du명령어를 사용하고 가장 큰 폴더를 추적할 것입니다.</p>
<pre><code class="hljs language-shell">du -h --max-depth 1 /
</code></pre>
<p>루트 디렉토리의 출력을 포함하여 가장 큰 폴더들:</p>
<pre><code class="hljs language-shell">5.9G    /usr
1.1G    /workspace
 ...
</code></pre>
<p>Workspace 폴더에는 모델과 Python 스크립트가 포함되어 있으며, 주로 model.safetensors 파일의 크기입니다. 여기서 놀라운 점은 없어요.</p>
<p>usr 폴더에는 Python 코드를 실행하는 데 필요한 종속성이 포함되어 있어요. 폴더 안에 무엇이 있는지 살펴보겠습니다.</p>
<ul>
<li>5.3G /usr/local/lib/python3.9/dist-packages/</li>
<li>2.9G /usr/local/lib/python3.9/dist-packages/nvidia</li>
<li>1.6G /usr/local/lib/python3.9/dist-packages/torch</li>
<li>439M /usr/local/lib/python3.9/dist-packages/triton</li>
<li>77M /usr/local/lib/python3.9/dist-packages/transformers</li>
<li>53M /usr/local/lib/python3.9/dist-packages/sympy</li>
<li>...</li>
</ul>
<p>5.9G 중 5.3G는 Python 모듈을 위한 것입니다. 가장 큰 패키지는 다음과 같습니다:</p>
<ul>
<li>3.0 GB — nvidia (cuda, cudnn, cublas, 등)</li>
<li>1.6 GB — torch</li>
<li>0.4 GB — triton</li>
</ul>
<p>nvidia와 triton 모듈은 torch에 종속됩니다. GPU에서 추론을 실행하려면 nvidia 모듈이 필요합니다. 다시 말해, transformers 모듈을 실행하기 위해서 torch 모듈이 필요합니다. 아래 다이어그램은 언급된 모듈이 전체 이미지에 기여하는 방식을 보여줍니다.</p>
<p><img src="/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_1.png" alt="Diagram"/></p>
<p>GPU에서 추론을 실행하려면 이미지 크기를 크게 줄일 수 있는 방법은 없습니다. 그러나 GPU 추론 대신 ONNX [4] 및 양자화를 사용하여 Docker 이미지 크기를 최대 10배 줄일 수 있습니다.</p>
<h1>ONNX 모델을 포함한 Docker 이미지</h1>
<p>int8 양자화가 적용된 ONNX는 성능 손실이 거의 없는 채 모델 크기를 4배로 축소할 수 있습니다 [5]. 다른 장점은 Docker 이미지의 크기를 최대 10배 줄일 수 있다는 것입니다. 이것이 어떻게 가능한 걸까요? ONNX 모델의 Docker 이미지를 빌드하는 데 필요한 작업을 살펴보겠습니다:</p>
<p>다음은 onnxruntime을 사용하여 추론을 실행하는 Python 코드입니다:</p>
<p>먼저, 이미지를 빌드하고 크기를 비교해 보겠습니다. 그런 다음, 이와 이전 이미지 간의 차이를 분석하겠습니다.</p>
<pre><code class="hljs language-js">도커 빌드 -t language_detection_onnx . -f <span class="hljs-title class_">Dockerfile</span>_onnx
</code></pre>
<p>… 그리고 이미지를 실행하세요:</p>
<pre><code class="hljs language-js">도커 실행 -p <span class="hljs-number">8000</span>:<span class="hljs-number">8000</span> language_detection_onnx
</code></pre>
<p>이미지의 크기를 비교해봅시다:</p>
<pre><code class="hljs language-js">도커 이미지 | grep language_detection
</code></pre>
<p>출력:</p>
<pre><code class="hljs language-js">language_detection_cuda    latest   47f4c1c0de2d   <span class="hljs-number">33</span>분 전   <span class="hljs-number">7.</span>05GB
language_detection_onnx    latest   3086089bd994   <span class="hljs-number">9</span>시간 전    699MB
</code></pre>
<p>7.05 GB 대 699 MB — 이것은 정말로 10배 작은 도커 이미지입니다. 이게 어떻게 가능했을까요?</p>
<p>세 이미지 사이에는 세 가지 주요 차이점이 있습니다.</p>
<h2>1. 베이스 도커 이미지</h2>
<p>대신 nvidia/cuda:11.8.0-base-ubuntu22.04를 사용하는 대신에 훨씬 작은 베이스 도커 이미지 python:3.9-slim을 사용했습니다. 첫 번째 이미지에는 GPU에서 추론을 실행하는 데 필요한 모든 Nvidia 라이브러리가 포함되어 있습니다 (CUDA, cuDNN, cuBLAS). ONNX 및 양자화된 모델로 추론을 실행하기 위해서는 GPU가 필요하지 않습니다. 따라서 Nvidia 라이브러리가 필요하지 않습니다.</p>
<h2>2. Python 모듈</h2>
<p>토치 대신 NVIDIA와 Triton 모듈이 필요 없는 ONNX Runtime을 사용했습니다. 이렇게 하면 세 개의 큰 Python 모듈을 제거할 수 있었어요.</p>
<h2>3. ONNX 형식의 양자화된 모델</h2>
<p>마지막으로 중요한 차이점은 ONNX 형식으로 변환된 양자화된 모델을 사용했다는 것입니다 [3]. model_quantized.onnx 파일은 279 MB로, 원본 모델 크기의 4분의 1 크기입니다.</p>
<h1>결론</h1>
<p>양자화된 ONNX를 사용하면 제품 이미지의 크기를 최대 10배까지 줄일 수 있어요.</p>
<p>어떤 경우에는 제품 모델의 크기가 과도한 모델 성능보다 중요할 수 있어요. 그런 경우에는 모델 양자화와 ONNX 형식으로 전환하는 것이 도움이 될 수 있어요. 양자화는 Docker 이미지의 크기를 줄일 뿐만 아니라 CPU를 사용하는 인스턴스보다 GPU를 사용하는 인스턴스보다 비용을 줄일 수 있어요. 그럼에도 결정은 여러 요인에 기반해야 해요 - 해결해야 하는 문제, 예상 성능, 예상 추론 시간, 양자화된 모델에 대한 성능 손실, 그리고 제품 환경의 구성 등을 고려해야 해요.</p>
<h1>문제 해결</h1>
<p>도커 이미지를 --gpus 매개변수와 함께 실행하는 중 문제가 발생하면 다음을 확인해보세요:</p>
<ul>
<li>Nvidia 컨테이너 툴킷 설치</li>
</ul>
<pre><code class="hljs language-js">sudo apt install nvidia-container-toolkit
</code></pre>
<ol start="2">
<li>도커 서비스 재시작</li>
</ol>
<pre><code class="hljs language-js">sudo systemctl restart docker
</code></pre>
<p>다음 명령은 GPU에 관한 정보를 출력해야 합니다:</p>
<pre><code class="hljs language-js">docker run --gpus all nvidia/<span class="hljs-attr">cuda</span>:<span class="hljs-number">11.8</span><span class="hljs-number">.0</span>-base-ubuntu22<span class="hljs-number">.04</span> nvidia-smi
</code></pre>
<h1>참조</h1>
<p>[1] https://github.com/CodeNLP/codenlp-docker-ml</p>
<p>[2] <a href="https://huggingface.co/papluca/xlm-roberta-base-language-detection">papluca/xlm-roberta-base-language-detection</a></p>
<p>[3] <a href="https://huggingface.co/protectai/xlm-roberta-base-language-detection-onnx">protectai/xlm-roberta-base-language-detection-onnx</a></p>
<p>[4] <a href="https://onnx.ai/">ONNX</a></p>
<p>[5] <a href="https://medium.com/codenlp/reducing-inference-time-of-t5-models-76e996523fb2?sk=f02379f5a8363d2de73a332fcef55f78">Reducing Inference Time of T5 Models</a></p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"대용량 언어 모델을 제공하는 도커 이미지 크기를 줄이기 파트 1","description":"","date":"2024-05-15 11:42","slug":"2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1","content":"\n\n\u003cimg src=\"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png\" /\u003e\n\n# 소개\n\nBERT, RoBERTa 또는 T5와 같은 Transformer 기반 모델은 자연어 처리에서 맞춤 문제에 대한 최신 솔루션을 제공합니다. 제품에서 모델을 제공하는 보편적인 방법은 모델에 대한 API를 제공하는 Docker 이미지를 빌드하는 것입니다. 이미지는 필요한 종속성, 모델 자체 및 모델로 입력 데이터를 처리하는 코드를 캡슐화합니다. 큰 생성 모델 (GenAI)과 비교하면, 이러한 모델은 상대적으로 작아서 0.5~2GB 정도입니다. 그러나 모델을 Docker 이미지로 배포하는 간단한 방법을 따를 때, 이미지 크기가 8GB에 이를 수 있음에 놀랐을 수도 있습니다. 대상 이미지가 왜 그렇게 큰지, 그리고 이미지 크기를 줄일 수 있는 방법이 있는지 궁금했던 적이 있나요? 이 이야기에서는 Docker 이미지가 왜 그렇게 큰지 설명하고 그 크기를 줄이는 방법에 대해 논의하겠습니다.\n\n이 이야기에서 사용된 Python 스크립트 및 Docker 파일의 예시는 [1]에서도 확인할 수 있습니다:\n\n\n\n# 기본 도커 이미지\n\n언어 감지 모델을 위한 간단한 도커 이미지를 만들어 봅시다. 모델을 구축하기 위한 몇 가지 전제사항은 다음과 같습니다:\n\n- 훈련된 모델을 사용할 것입니다: papluca/xlm-roberta-base-language-detection [2].\n- 가능한 최상의 성능을 얻기 위해 GPU를 활용할 것입니다.\n- 단일 텍스트를 처리하는 간단한 엔드포인트를 제공하기 위해 FastAPI를 사용할 것입니다.\n\n다음은 이미지를 빌드하기 위한 Dockerfile입니다:\n\n\n\n모델을 로드하고 추론을 수행하는 데 사용된 코드는 다음과 같습니다:\n\n다음은 이미지를 빌드하는 데 사용된 명령어입니다:\n\n```js\ndocker build -t language_detection_cuda . -f Dockerfile_cuda\n```\n\n... 그리고 이미지를 실행하세요.\n\n\n\n```js\n도커 실행 --gpus 0 -p 8000:8000 language_detection_cuda\n```\n\n... 엔드포인트를 테스트해 보겠습니다:\n\n```js\n시간 curl -X 'POST'   'http://localhost:8000/process'   -H 'accept: application/json'   -H 'Content-Type: application/json'   -d '{\n  \"text\": \"Certo ci sono stati dei problemi - problemi che dovremo risolvere in vista, per esempio, dell'\\''ampliamento - ma a volte ne esageriamo il lato negativo.\"\n}'\n```\n\n다음 출력을 받았습니다:\n\n\n\n```js\n\"it\"\n```\n\n지금까지 특별한 것은 없어요. 엔드포인트가 할 일을 잘 수행하고 있어요.\n\n모델의 크기는 1.11GB입니다 (model.safetensors 파일), 토크나이저를 위한 추가 10MB가 있어요. 이제 도커 이미지의 크기를 보겠습니다:\n\n```js\ndocker images | grep language_detection_cuda\n```\n\n\n\n… 출력물은:\n\n```js\nlanguage_detection_cuda    최신 버전   47f4c1c0de2d   33분 전   7.05GB\n```\n\n도커 이미지의 총 용량은 7.05GB입니다. 와우, 상당히 많죠? 하지만 왜 이미지가 그렇게 큰 걸까요? 이제 컨테이너로 들어가서 내부를 확인해 보겠습니다.\n\n```js\ndocker run -it --gpus 0 -p 8000:8000 --entrypoint \"/bin/bash\"  language_detection_cuda\n```\n\n\n\n이미지 크기를 분석하기 위해 du명령어를 사용하고 가장 큰 폴더를 추적할 것입니다.\n\n```shell\ndu -h --max-depth 1 /\n```\n\n루트 디렉토리의 출력을 포함하여 가장 큰 폴더들:\n\n```shell\n5.9G    /usr\n1.1G    /workspace\n ...\n```\n\n\n\nWorkspace 폴더에는 모델과 Python 스크립트가 포함되어 있으며, 주로 model.safetensors 파일의 크기입니다. 여기서 놀라운 점은 없어요.\n\nusr 폴더에는 Python 코드를 실행하는 데 필요한 종속성이 포함되어 있어요. 폴더 안에 무엇이 있는지 살펴보겠습니다.\n\n\n- 5.3G /usr/local/lib/python3.9/dist-packages/\n- 2.9G /usr/local/lib/python3.9/dist-packages/nvidia\n- 1.6G /usr/local/lib/python3.9/dist-packages/torch\n- 439M /usr/local/lib/python3.9/dist-packages/triton\n- 77M /usr/local/lib/python3.9/dist-packages/transformers\n- 53M /usr/local/lib/python3.9/dist-packages/sympy\n- ...\n\n\n5.9G 중 5.3G는 Python 모듈을 위한 것입니다. 가장 큰 패키지는 다음과 같습니다:\n\n\n\n- 3.0 GB — nvidia (cuda, cudnn, cublas, 등)\n- 1.6 GB — torch\n- 0.4 GB — triton\n\nnvidia와 triton 모듈은 torch에 종속됩니다. GPU에서 추론을 실행하려면 nvidia 모듈이 필요합니다. 다시 말해, transformers 모듈을 실행하기 위해서 torch 모듈이 필요합니다. 아래 다이어그램은 언급된 모듈이 전체 이미지에 기여하는 방식을 보여줍니다.\n\n![Diagram](/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_1.png)\n\nGPU에서 추론을 실행하려면 이미지 크기를 크게 줄일 수 있는 방법은 없습니다. 그러나 GPU 추론 대신 ONNX [4] 및 양자화를 사용하여 Docker 이미지 크기를 최대 10배 줄일 수 있습니다.\n\n\n\n# ONNX 모델을 포함한 Docker 이미지\n\nint8 양자화가 적용된 ONNX는 성능 손실이 거의 없는 채 모델 크기를 4배로 축소할 수 있습니다 [5]. 다른 장점은 Docker 이미지의 크기를 최대 10배 줄일 수 있다는 것입니다. 이것이 어떻게 가능한 걸까요? ONNX 모델의 Docker 이미지를 빌드하는 데 필요한 작업을 살펴보겠습니다:\n\n다음은 onnxruntime을 사용하여 추론을 실행하는 Python 코드입니다:\n\n먼저, 이미지를 빌드하고 크기를 비교해 보겠습니다. 그런 다음, 이와 이전 이미지 간의 차이를 분석하겠습니다.\n\n\n\n```js\n도커 빌드 -t language_detection_onnx . -f Dockerfile_onnx\n```\n\n… 그리고 이미지를 실행하세요:\n\n```js\n도커 실행 -p 8000:8000 language_detection_onnx\n```\n\n이미지의 크기를 비교해봅시다:\n\n\n\n```js\n도커 이미지 | grep language_detection\n```\n\n출력:\n\n```js\nlanguage_detection_cuda    latest   47f4c1c0de2d   33분 전   7.05GB\nlanguage_detection_onnx    latest   3086089bd994   9시간 전    699MB\n```\n\n7.05 GB 대 699 MB — 이것은 정말로 10배 작은 도커 이미지입니다. 이게 어떻게 가능했을까요?\n\n\n\n세 이미지 사이에는 세 가지 주요 차이점이 있습니다.\n\n## 1. 베이스 도커 이미지\n\n대신 nvidia/cuda:11.8.0-base-ubuntu22.04를 사용하는 대신에 훨씬 작은 베이스 도커 이미지 python:3.9-slim을 사용했습니다. 첫 번째 이미지에는 GPU에서 추론을 실행하는 데 필요한 모든 Nvidia 라이브러리가 포함되어 있습니다 (CUDA, cuDNN, cuBLAS). ONNX 및 양자화된 모델로 추론을 실행하기 위해서는 GPU가 필요하지 않습니다. 따라서 Nvidia 라이브러리가 필요하지 않습니다.\n\n## 2. Python 모듈\n\n\n\n토치 대신 NVIDIA와 Triton 모듈이 필요 없는 ONNX Runtime을 사용했습니다. 이렇게 하면 세 개의 큰 Python 모듈을 제거할 수 있었어요.\n\n## 3. ONNX 형식의 양자화된 모델\n\n마지막으로 중요한 차이점은 ONNX 형식으로 변환된 양자화된 모델을 사용했다는 것입니다 [3]. model_quantized.onnx 파일은 279 MB로, 원본 모델 크기의 4분의 1 크기입니다.\n\n# 결론\n\n\n\n양자화된 ONNX를 사용하면 제품 이미지의 크기를 최대 10배까지 줄일 수 있어요.\n\n어떤 경우에는 제품 모델의 크기가 과도한 모델 성능보다 중요할 수 있어요. 그런 경우에는 모델 양자화와 ONNX 형식으로 전환하는 것이 도움이 될 수 있어요. 양자화는 Docker 이미지의 크기를 줄일 뿐만 아니라 CPU를 사용하는 인스턴스보다 GPU를 사용하는 인스턴스보다 비용을 줄일 수 있어요. 그럼에도 결정은 여러 요인에 기반해야 해요 - 해결해야 하는 문제, 예상 성능, 예상 추론 시간, 양자화된 모델에 대한 성능 손실, 그리고 제품 환경의 구성 등을 고려해야 해요.\n\n# 문제 해결\n\n도커 이미지를 --gpus 매개변수와 함께 실행하는 중 문제가 발생하면 다음을 확인해보세요:\n\n\n\n- Nvidia 컨테이너 툴킷 설치\n\n```js\nsudo apt install nvidia-container-toolkit\n```\n\n2. 도커 서비스 재시작\n\n```js\nsudo systemctl restart docker\n```\n\n\n\n다음 명령은 GPU에 관한 정보를 출력해야 합니다:\n\n```js\ndocker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\n```\n\n# 참조\n\n[1] https://github.com/CodeNLP/codenlp-docker-ml\n\n\n\n[2] [papluca/xlm-roberta-base-language-detection](https://huggingface.co/papluca/xlm-roberta-base-language-detection)\n\n[3] [protectai/xlm-roberta-base-language-detection-onnx](https://huggingface.co/protectai/xlm-roberta-base-language-detection-onnx)\n\n[4] [ONNX](https://onnx.ai/)\n\n[5] [Reducing Inference Time of T5 Models](https://medium.com/codenlp/reducing-inference-time-of-t5-models-76e996523fb2?sk=f02379f5a8363d2de73a332fcef55f78)","ogImage":{"url":"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png"},"coverImage":"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png","tag":["Tech"],"readingTime":6},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h1: \"h1\",\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\",\n    img: \"img\",\n    h2: \"h2\",\n    ol: \"ol\",\n    a: \"a\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(\"img\", {\n      src: \"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_0.png\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"소개\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"BERT, RoBERTa 또는 T5와 같은 Transformer 기반 모델은 자연어 처리에서 맞춤 문제에 대한 최신 솔루션을 제공합니다. 제품에서 모델을 제공하는 보편적인 방법은 모델에 대한 API를 제공하는 Docker 이미지를 빌드하는 것입니다. 이미지는 필요한 종속성, 모델 자체 및 모델로 입력 데이터를 처리하는 코드를 캡슐화합니다. 큰 생성 모델 (GenAI)과 비교하면, 이러한 모델은 상대적으로 작아서 0.5~2GB 정도입니다. 그러나 모델을 Docker 이미지로 배포하는 간단한 방법을 따를 때, 이미지 크기가 8GB에 이를 수 있음에 놀랐을 수도 있습니다. 대상 이미지가 왜 그렇게 큰지, 그리고 이미지 크기를 줄일 수 있는 방법이 있는지 궁금했던 적이 있나요? 이 이야기에서는 Docker 이미지가 왜 그렇게 큰지 설명하고 그 크기를 줄이는 방법에 대해 논의하겠습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 이야기에서 사용된 Python 스크립트 및 Docker 파일의 예시는 [1]에서도 확인할 수 있습니다:\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"기본 도커 이미지\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"언어 감지 모델을 위한 간단한 도커 이미지를 만들어 봅시다. 모델을 구축하기 위한 몇 가지 전제사항은 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"훈련된 모델을 사용할 것입니다: papluca/xlm-roberta-base-language-detection [2].\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"가능한 최상의 성능을 얻기 위해 GPU를 활용할 것입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"단일 텍스트를 처리하는 간단한 엔드포인트를 제공하기 위해 FastAPI를 사용할 것입니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음은 이미지를 빌드하기 위한 Dockerfile입니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델을 로드하고 추론을 수행하는 데 사용된 코드는 다음과 같습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음은 이미지를 빌드하는 데 사용된 명령어입니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"docker build -t language_detection_cuda . -f \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Dockerfile\"\n        }), \"_cuda\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"... 그리고 이미지를 실행하세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"도커 실행 --gpus \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0\"\n        }), \" -p \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \" language_detection_cuda\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"... 엔드포인트를 테스트해 보겠습니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"시간 curl -X \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'POST'\"\n        }), \"   \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'http://localhost:8000/process'\"\n        }), \"   -H \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'accept: application/json'\"\n        }), \"   -H \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Content-Type: application/json'\"\n        }), \"   -d \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'{\\n  \\\"text\\\": \\\"Certo ci sono stati dei problemi - problemi che dovremo risolvere in vista, per esempio, dell'\"\n        }), \"\\\\\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"ampliamento - ma a volte ne esageriamo il lato negativo.\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n}'\\n\"\n        })]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음 출력을 받았습니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"it\\\"\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"지금까지 특별한 것은 없어요. 엔드포인트가 할 일을 잘 수행하고 있어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델의 크기는 1.11GB입니다 (model.safetensors 파일), 토크나이저를 위한 추가 10MB가 있어요. 이제 도커 이미지의 크기를 보겠습니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"docker images | grep language_detection_cuda\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"… 출력물은:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"language_detection_cuda    최신 버전   47f4c1c0de2d   \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"33\"\n        }), \"분 전   \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"7.\"\n        }), \"05GB\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"도커 이미지의 총 용량은 7.05GB입니다. 와우, 상당히 많죠? 하지만 왜 이미지가 그렇게 큰 걸까요? 이제 컨테이너로 들어가서 내부를 확인해 보겠습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"docker run -it --gpus \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0\"\n        }), \" -p \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \" --entrypoint \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"/bin/bash\\\"\"\n        }), \"  language_detection_cuda\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이미지 크기를 분석하기 위해 du명령어를 사용하고 가장 큰 폴더를 추적할 것입니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-shell\",\n        children: \"du -h --max-depth 1 /\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"루트 디렉토리의 출력을 포함하여 가장 큰 폴더들:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-shell\",\n        children: \"5.9G    /usr\\n1.1G    /workspace\\n ...\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Workspace 폴더에는 모델과 Python 스크립트가 포함되어 있으며, 주로 model.safetensors 파일의 크기입니다. 여기서 놀라운 점은 없어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"usr 폴더에는 Python 코드를 실행하는 데 필요한 종속성이 포함되어 있어요. 폴더 안에 무엇이 있는지 살펴보겠습니다.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"5.3G /usr/local/lib/python3.9/dist-packages/\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"2.9G /usr/local/lib/python3.9/dist-packages/nvidia\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"1.6G /usr/local/lib/python3.9/dist-packages/torch\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"439M /usr/local/lib/python3.9/dist-packages/triton\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"77M /usr/local/lib/python3.9/dist-packages/transformers\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"53M /usr/local/lib/python3.9/dist-packages/sympy\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"...\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"5.9G 중 5.3G는 Python 모듈을 위한 것입니다. 가장 큰 패키지는 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"3.0 GB — nvidia (cuda, cudnn, cublas, 등)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"1.6 GB — torch\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"0.4 GB — triton\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"nvidia와 triton 모듈은 torch에 종속됩니다. GPU에서 추론을 실행하려면 nvidia 모듈이 필요합니다. 다시 말해, transformers 모듈을 실행하기 위해서 torch 모듈이 필요합니다. 아래 다이어그램은 언급된 모듈이 전체 이미지에 기여하는 방식을 보여줍니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1_1.png\",\n        alt: \"Diagram\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"GPU에서 추론을 실행하려면 이미지 크기를 크게 줄일 수 있는 방법은 없습니다. 그러나 GPU 추론 대신 ONNX [4] 및 양자화를 사용하여 Docker 이미지 크기를 최대 10배 줄일 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"ONNX 모델을 포함한 Docker 이미지\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"int8 양자화가 적용된 ONNX는 성능 손실이 거의 없는 채 모델 크기를 4배로 축소할 수 있습니다 [5]. 다른 장점은 Docker 이미지의 크기를 최대 10배 줄일 수 있다는 것입니다. 이것이 어떻게 가능한 걸까요? ONNX 모델의 Docker 이미지를 빌드하는 데 필요한 작업을 살펴보겠습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음은 onnxruntime을 사용하여 추론을 실행하는 Python 코드입니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"먼저, 이미지를 빌드하고 크기를 비교해 보겠습니다. 그런 다음, 이와 이전 이미지 간의 차이를 분석하겠습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"도커 빌드 -t language_detection_onnx . -f \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Dockerfile\"\n        }), \"_onnx\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"… 그리고 이미지를 실행하세요:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"도커 실행 -p \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \" language_detection_onnx\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이미지의 크기를 비교해봅시다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"도커 이미지 | grep language_detection\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"출력:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"language_detection_cuda    latest   47f4c1c0de2d   \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"33\"\n        }), \"분 전   \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"7.\"\n        }), \"05GB\\nlanguage_detection_onnx    latest   3086089bd994   \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"9\"\n        }), \"시간 전    699MB\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"7.05 GB 대 699 MB — 이것은 정말로 10배 작은 도커 이미지입니다. 이게 어떻게 가능했을까요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"세 이미지 사이에는 세 가지 주요 차이점이 있습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"1. 베이스 도커 이미지\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"대신 nvidia/cuda:11.8.0-base-ubuntu22.04를 사용하는 대신에 훨씬 작은 베이스 도커 이미지 python:3.9-slim을 사용했습니다. 첫 번째 이미지에는 GPU에서 추론을 실행하는 데 필요한 모든 Nvidia 라이브러리가 포함되어 있습니다 (CUDA, cuDNN, cuBLAS). ONNX 및 양자화된 모델로 추론을 실행하기 위해서는 GPU가 필요하지 않습니다. 따라서 Nvidia 라이브러리가 필요하지 않습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"2. Python 모듈\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토치 대신 NVIDIA와 Triton 모듈이 필요 없는 ONNX Runtime을 사용했습니다. 이렇게 하면 세 개의 큰 Python 모듈을 제거할 수 있었어요.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3. ONNX 형식의 양자화된 모델\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마지막으로 중요한 차이점은 ONNX 형식으로 변환된 양자화된 모델을 사용했다는 것입니다 [3]. model_quantized.onnx 파일은 279 MB로, 원본 모델 크기의 4분의 1 크기입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결론\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"양자화된 ONNX를 사용하면 제품 이미지의 크기를 최대 10배까지 줄일 수 있어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"어떤 경우에는 제품 모델의 크기가 과도한 모델 성능보다 중요할 수 있어요. 그런 경우에는 모델 양자화와 ONNX 형식으로 전환하는 것이 도움이 될 수 있어요. 양자화는 Docker 이미지의 크기를 줄일 뿐만 아니라 CPU를 사용하는 인스턴스보다 GPU를 사용하는 인스턴스보다 비용을 줄일 수 있어요. 그럼에도 결정은 여러 요인에 기반해야 해요 - 해결해야 하는 문제, 예상 성능, 예상 추론 시간, 양자화된 모델에 대한 성능 손실, 그리고 제품 환경의 구성 등을 고려해야 해요.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"문제 해결\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"도커 이미지를 --gpus 매개변수와 함께 실행하는 중 문제가 발생하면 다음을 확인해보세요:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Nvidia 컨테이너 툴킷 설치\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"sudo apt install nvidia-container-toolkit\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"도커 서비스 재시작\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"sudo systemctl restart docker\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음 명령은 GPU에 관한 정보를 출력해야 합니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"docker run --gpus all nvidia/\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"cuda\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"11.8\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".0\"\n        }), \"-base-ubuntu22\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".04\"\n        }), \" nvidia-smi\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"참조\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[1] https://github.com/CodeNLP/codenlp-docker-ml\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[2] \", _jsx(_components.a, {\n        href: \"https://huggingface.co/papluca/xlm-roberta-base-language-detection\",\n        children: \"papluca/xlm-roberta-base-language-detection\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[3] \", _jsx(_components.a, {\n        href: \"https://huggingface.co/protectai/xlm-roberta-base-language-detection-onnx\",\n        children: \"protectai/xlm-roberta-base-language-detection-onnx\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[4] \", _jsx(_components.a, {\n        href: \"https://onnx.ai/\",\n        children: \"ONNX\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[5] \", _jsx(_components.a, {\n        href: \"https://medium.com/codenlp/reducing-inference-time-of-t5-models-76e996523fb2?sk=f02379f5a8363d2de73a332fcef55f78\",\n        children: \"Reducing Inference Time of T5 Models\"\n      })]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-ReducingtheSizeofDockerImagesServingLargeLanguageModelspart1"},"buildId":"z1a6VTi5qHH9JJH7jaxL3","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>