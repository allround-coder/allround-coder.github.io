<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>EKS에서 자체 호스팅 LLM을 배포하는 방법 및 그 이유 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="EKS에서 자체 호스팅 LLM을 배포하는 방법 및 그 이유 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="EKS에서 자체 호스팅 LLM을 배포하는 방법 및 그 이유 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould" data-gatsby-head="true"/><meta name="twitter:title" content="EKS에서 자체 호스팅 LLM을 배포하는 방법 및 그 이유 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 03:39" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-985df180e46efe53.js" defer=""></script><script src="/_next/static/K-h7XvEVBqnNx_uXMgZoe/_buildManifest.js" defer=""></script><script src="/_next/static/K-h7XvEVBqnNx_uXMgZoe/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">EKS에서 자체 호스팅 LLM을 배포하는 방법 및 그 이유</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="EKS에서 자체 호스팅 LLM을 배포하는 방법 및 그 이유" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/assets/profile.jpg"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>생산 환경에서 가격이 폭등하는 토큰에 대해 계속 걱정하고 있나요? 외부 업체가 민감한 데이터를 어떻게 처리하는지 걱정되나요? 이 게시물은 자체 호스팅 LLM을 EKS(Elastic Kubernetes Service)에 배포하는 방법을 안내해 드립니다. 이를 통해 제어권과 비용 효율성을 높일 수 있습니다. 우리는 자체 호스팅을 원하는 이유부터 설정에 필수적인 도구 및 지표에 이르기까지 모든 것을 탐구할 것입니다. 게다가 모델과 상호 작용할 수 있는 간단한 채팅 애플리케이션을 설정하는 방법을 따라해 보겠습니다.</p>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_0.png" alt="이미지"/></p>
<h1>자체 호스팅이란?</h1>
<p>OpenAI나 Anthropic과 같은 공급업체의 고급 언어 모델은 매우 인상적이지만 항상 지갑 친화적이라고 할 수 없습니다. 실험 및 개발은 금융 오퍼레이션팀의 주의를 끌지 않을 수 있지만, 프로덕션으로 전환하면 토큰당 요금 체계와 관련된 비용이 빠르게 누적될 수 있습니다.</p>
<p>크고 비싼 모델들이 있긴 하지만, 언제나 당신의 요구 사항을 평가하고 단순히 큰 모델만을 공략하는 것이 아니라는 점을 염두에 두어야 합니다. 하지만 규모에 따라, 작은 모델도 결국에는 비실 것입니다.</p>
<p>하지만, 돈만을 생각할 필요는 없어요. 자체 호스팅은 다음과 같은 중요한 이점들을 제공합니다:</p>
<ul>
<li>데이터 보안 — 모든 민감한 정보, 특히 개인 식별 정보(PII)는 우리 네트워크 내에서 안전하게 보관됩니다. 이 설정은 데이터를 외부로 보내거나 외부 공급 업체가 데이터를 어떻게 사용할지 걱정할 필요가 없게 합니다.</li>
<li>개발자 자유 — 자체 호스팅은 개발자들이 치솟는 비용과 외부 데이터 개인 정보 보호 우려 없이 탐구하고 혁신할 수 있는 자유를 제공합니다. 이 자유는 기술적 실험을 장려하는 창의적인 환경을 지원하여 더 혁신적인 솔루션을 도출하게 됩니다.</li>
</ul>
<p>GPT-4와 맞먹는 오픈 소스 모델은 많이 찾기 어려울지 몰라도, 보통 GPT-3.5에서 다루는 작업에 적합한 수많은 대안이 있습니다. 이러한 모델 중 일부는 심지어 더 나은 성능을 발휘하면서 더 저렴한 비용으로 제공되고 있습니다. 이를 네트워크에 배포하여 데이터를 제어할 수 있습니다. 무엇보다 중요한 것은 사용량 당 비용을 지불하는 대신 고정 컴퓨팅 가격을 지불하므로 비용을 더 예측 가능하고 관리하기 쉽게 만들 수 있습니다.</p>
<h1>필요한 도구는 무엇인가요? (그리고 몇 가지 다른 고려 사항)</h1>
<p>AWS 및 EKS에 익숙하다고 가정하고, LLM 모델을 제공하는 데 필요한 다른 구성 요소에 초점을 맞추겠습니다. 고려해야 할 주요 영역은 Compute, 추론 및 모델입니다.</p>
<h2>Compute</h2>
<p>LLM 추론을 설정할 때 고려해야 할 주요 자원은 GPU입니다. 특히 GPU의 종류와 수량을 고려해야 합니다. 이는 전체 모델이 GPU의 메모리 (VRAM)에 로드되며, 모든 LLM 계산이 GPU에서 수행되기 때문입니다.</p>
<p>VRAM 양을 추정하려면 이 안내서를 확인하거나 다음과 같은 간단한 생각의 척도를 따르세요: 모델의 매개변수 수(10억 개 단위)를 두 배하여 기본 요구 사항을 얻은 다음, 캐싱과 오버헤드를 커버하기 위해 20%를 추가하세요. 예를 들어, 70억 개의 매개변수를 가진 모델을 이용하려면 VRAM 약 17GB(7 x 2 x 1.2 = 약 16.8 GB)가 한 개 또는 여러 개의 GPU에서 필요합니다.</p>
<h2>추론</h2>
<p>서빙 프레임워크로는 vLLM을 사용할 것입니다. 이는 LLM 모델을 OpenAI 호환 API 서버로 제공하는 데 설계된 오픈 소스 프레임워크입니다. vLLM은 연속 배치를 지원하며, 다중 동시 요청 및 높은 부하를 처리하기에 이상적입니다. 게다가, vLLM은 분산 서빙을 지원하며, 모델을 여러 GPU 또는 노드에서 실행해야하는 경우를 대비합니다. 분산 서빙을 위해 백엔드로 Ray를 사용하며, 이는 대규모 ML 애플리케이션을 실행하기 위한 또 다른 오픈 소스 프레임워크입니다.</p>
<h2>모델</h2>
<p>수백 개의 모델이 Hugging Face에 있습니다. Foundation 모델부터 더 구체적이고 특정 문제를 해결하기 위해 디자인된 Feat-Tuned 버전까지 말이죠. Hugging Face를 인공지능(AI) 및 기계학습(ML) 애플리케이션의 &quot;GitHub&quot;이라고 생각해보세요. 필요한 어떤 모델이나 데이터셋이든 찾을 수 있는 중요한 장소입니다.</p>
<p>모델을 선택할 때 라이선스를 꼭 확인해주세요. Mistral과 같은 일부 모델은 Apache 라이선스 하에 완전히 오픈 소스입니다. 그러나 많은 모델은 상용 라이선스가 적용됩니다. 이 조항을 검토하는 것은 귀하의 법적 및 운영 계획에 부합하는지 확인하는 데 중요합니다.</p>
<h1>모두 함께 하기 (데모 시간)</h1>
<p>이 데모에서 Mistral 7b instruct 0.2 모델을 사용할 것이며, 이는 Apache 라이선스에 따라 완전히 오픈 소스입니다. 이 모델을 처리하기 위해 주로 15센트 미만의 비용이 드는 AWS g6.xlarge 인스턴스에서 실행할 것입니다. 이 인스턴스는 우리가 상의한 VRAM 견적 규칙에 따라 처리하기에 완벽하게 적합한 24GB VRAM을 갖춘 Nvidia L4 GPU가 장착되어 있습니다.</p>
<p>데모 중에는 미국 서부-2 지역에 VPC를 배포하고, Fargate에서 Karpenter가 있는 EKS 클러스터, GPU를 위한 Karpenter 프로바이더 하나 및 표준 노드를 위한 또 다른 하나, Kubernetes에서 GPU를 사용할 수 있도록 Nvidia Driver 플러그인, 그리고 모니터링을 위한 Prometheus와 Grafana가 설정될 것입니다. 이 모든 리소스는 Terraform을 사용하여 설정될 것입니다.</p>
<p>비용에 관해서는, 이 데모 실행에 예상되는 비용은 시간당 약 30~40센트입니다. NAT 게이트웨이, EKS 제어 평면 및 GPU가 장착된 노드를 포함한 모든 노드에 대한 요금이 포함됩니다.</p>
<h2>0. 전제 조건</h2>
<p>데모에 들어가기 전에, 다음을 준비해 두세요:</p>
<ul>
<li>AWS 계정 — 데모에서 설치해야 할 VPC, EKS 클러스터 등을 구성할 충분한 권한이 있는 AWS 계정이 필요합니다.</li>
<li>AWS 자격 증명 — 로컬 환경에서 자격 증명이 올바르게 구성되어 있는지 확인하세요.</li>
<li>Terraform — 여러분의 컴퓨터에 Terraform이 설치되어 있어야 합니다. 데모에 필요한 AWS 리소스를 프로비저닝하고 관리하는 데 사용될 것입니다.</li>
<li>Kubectl — Kubernetes 리소스를 관리하기 때문에 Kubectl이 설치되어 있는지 확인하세요.</li>
<li>HuggingFace 액세스 토큰 — 이 가이드를 따라 Hugging Face에서 모델을 가져오기 위한 API 액세스 토큰을 생성하세요.</li>
</ul>
<h2>1. 기본 인프라</h2>
<ul>
<li>터미널을 열고 다음을 실행하여 저장소를 클론합니다:</li>
</ul>
<pre><code class="hljs language-js">git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/eliran89c/self-hosted-llm-on-eks</span>
</code></pre>
<ol start="2">
<li>디렉토리를 변경하세요:</li>
</ol>
<pre><code class="hljs language-js">cd self-hosted-llm-on-eks
</code></pre>
<ol start="3">
<li>
<p>(선택사항) 필요에 따라 Terraform 코드를 조정하여 설정을 사용자 정의하세요.</p>
</li>
<li>
<p>Terraform을 초기화하고 인프라를 배포하기 위해 Terraform 구성을 적용하세요 (EKS 클러스터를 배포하는 데 최대 30분 소요될 수 있습니다).</p>
</li>
</ol>
<pre><code class="hljs language-js">terraform init
terraform apply
</code></pre>
<ol start="5">
<li>새로 생성된 EKS 클러스터와 상호 작용하기 위해 Kubectl을 설정하세요.</li>
</ol>
<pre><code class="hljs language-js">aws eks update-kubeconfig --region us-west-<span class="hljs-number">2</span> \
    --name self-hosted-llm \
    --alias self-hosted-llm
</code></pre>
<ol start="6">
<li>Karpenter와 CoreDNS가 실행 중인지 확인하세요.</li>
</ol>
<pre><code class="hljs language-js">kubectl get pods --all-namespaces
</code></pre>
<p>기대되는 출력은 다음과 같아야 합니다:</p>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_1.png" alt="image"/></p>
<ol start="7">
<li>Karpenter 제공자가 올바르게 설치되었는지 확인하세요:</li>
</ol>
<pre><code class="hljs language-js">kubectl get ec2nodeclasses.<span class="hljs-property">karpenter</span>.<span class="hljs-property">k8s</span>.<span class="hljs-property">aws</span>
</code></pre>
<p>원하는 결과는 사용 가능한 노드 클래스를 표시해야 합니다:</p>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_2.png" alt="Available Node Classes"/></p>
<h2>2. vLLM 배포 및 모델 제공</h2>
<ul>
<li>HuggingFace 모델 페이지로 이동하셔서 모델 약관에 동의해주세요</li>
</ul>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_3.png" alt="이미지"/></p>
<ol start="2">
<li>HuggingFace API 액세스 토큰을 사용하여 비밀을 생성하세요:</li>
</ol>
<pre><code class="hljs language-js">kubectl create secret generic huggingface-token \
    --<span class="hljs-keyword">from</span>-literal=token=&lt;your_hugging_face_token&gt;
</code></pre>
<ul>
<li>&#x27;your_hugging_face_token&#x27;을 실제 Hugging Face API 액세스 토큰으로 대체해주세요.</li>
</ul>
<ol start="3">
<li>
<p>(선택 사항) 배포 파일을 검토하고, 특히 배포 인수 섹션을 확인하세요. 필요에 따라 엔진 인수를 수정하여 특정 요구 사항에 더 잘 맞도록 설정할 수 있습니다. 모든 사용 가능한 엔진 인수 목록을 확인하려면 여기를 참조하세요.</p>
</li>
<li>
<p>vLLM 배포하기:</p>
</li>
</ol>
<pre><code class="hljs language-js">kubectl apply -f vllm.<span class="hljs-property">yaml</span>
</code></pre>
<ol start="5">
<li>프로메테우스가 vLLM에서 메트릭을 수집하도록 하려면 ServiceMonitor을 배포하십시오.</li>
</ol>
<pre><code class="hljs language-js">kubectl apply -f serviceMonitor.<span class="hljs-property">yaml</span>
</code></pre>
<ol start="6">
<li>vLLM을 배포한 후에는 일반적으로 GPU로 모델을 다운로드하고 로드하는 데 2-3분 정도가 소요됩니다. 이 초기화 단계 중에 무엇이 발생하는지 모니터링하려면 로그를 직접 확인할 수 있습니다.
먼저, 파드가 실행 중인지 확인하십시오:</li>
</ol>
<pre><code class="hljs language-js">kubectl get pods
</code></pre>
<p>아래와 같이 로그를 확인하세요:</p>
<pre><code class="hljs language-js">kubectl logs -f -l app=vllm
</code></pre>
<p>모델이 로딩되고 준비되면, 로그에 다음 메시지가 표시될 것을 기대할 수 있습니다:</p>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_4.png" alt="이미지"/></p>
<ol start="7">
<li>새 터미널을 열고 포트 포워딩을 설정하여 포트 8000에서 OpenAI 호환 API 엔드포인트와 상호 작용할 수 있습니다:</li>
</ol>
<pre><code class="hljs language-js">kubectl port-forward svc/vllm <span class="hljs-number">8000</span>:<span class="hljs-number">8000</span>
</code></pre>
<ol start="8">
<li>모든 준비가 되었으므로, LLM을 테스트하기 위해 표준 OpenAI curl 명령을 사용하여 쿼리를 보내보세요. 아래는 예시입니다:</li>
</ol>
<pre><code class="hljs language-js">curl -X <span class="hljs-variable constant_">POST</span> <span class="hljs-attr">http</span>:<span class="hljs-comment">//localhost:8000/v1/chat/completions \</span>
-H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \
-d <span class="hljs-string">&#x27;{
      &quot;model&quot;: &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;,
      &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;프랑스의 수도는 무엇인가요?&quot;}]
    }&#x27;</span>
</code></pre>
<p>정보를 성공적으로 검색했는지 확인하기 위해 예상 결과는 다음과 같습니다:</p>
<img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_5.png"/>
<h2>3. 모델과 상호작용하는 간단한 채팅 애플리케이션 설정</h2>
<ul>
<li>다음을 실행하여 새 Python 가상 환경을 만듭니다:</li>
</ul>
<pre><code class="hljs language-js">python3 -m venv .<span class="hljs-property">venv</span>
</code></pre>
<ol start="2">
<li>가상 환경을 활성화합니다:</li>
</ol>
<pre><code class="hljs language-js">source .<span class="hljs-property">venv</span>/bin/activate   # 리눅스 또는 맥<span class="hljs-variable constant_">OS</span>
.<span class="hljs-property">venv</span>\<span class="hljs-title class_">Scripts</span>\activate      # 윈도우
</code></pre>
<ol start="3">
<li>필요한 파이썬 패키지를 설치하려면 다음을 실행하세요:</li>
</ol>
<pre><code class="hljs language-bash">pip install -r requirements.txt
</code></pre>
<p>이 패키지에는 API 요청을 위한 OpenAI Python 클라이언트와 웹 인터페이스를 만들기 위한 Gradio가 포함되어 있습니다.</p>
<ol start="4">
<li>아래 명령어를 실행하여 애플리케이션을 시작하세요:</li>
</ol>
<pre><code class="hljs language-bash">python chat.py
</code></pre>
<ol start="5">
<li>어플리케이션이 실행되면 웹 브라우저를 열고 http://localhost:7860/ 으로 이동하세요.</li>
</ol>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_6.png" alt="이미지"/></p>
<h1>모델 모니터링</h1>
<p>LLM에서는 모델의 대기 시간(Latency) 및 처리량(Throughput)을 모니터링하고 측정하는 여러 중요한 지표가 있습니다. 이러한 지표들은 성능을 최적화하고 모델이 효율적으로 작동하는 것을 보장하기 위해 중요합니다. 아래는 고려해야 할 주요 지표입니다:</p>
<p>첫 번째 토큰 생성 시간(TFFT) - 이 지표는 요청을 제출한 후 모델이 응답의 첫 번째 토큰을 생성하는 데 걸리는 시간을 측정합니다. 이는 모델의 초기 반응성을 나타내는 중요한 지표로, 사용자 상호 작용 애플리케이션에서 응답 시간이 사용자 경험에 영향을 미치는 경우에 특히 중요합니다.</p>
<p>출력 토큰 시간(TFOT) - 위와 유사하게, 이 지표는 처음 토큰 생성 후 각 후속 토큰을 생성하는 데 걸리는 시간을 추적합니다. 이는 모델이 시작된 후 계속해서 내용을 처리하고 생성하는 데 효율성을 이해하는 데 도움이 됩니다. 이로써 실행량 성능에 대한 통찰을 얻을 수 있습니다.</p>
<p>프롬프트/생성 토큰 초당 - 이 지표는 모델이 초당 처리하거나 생성하는 토큰 수를 측정합니다. 이는 모델의 처리량 용량을 평가하는 데 필수적인 지표이며, 높은 비율은 더 효율적인 모델을 나타내며 더 많은 입력을 처리하거나 시간이 적게 소요되면서 더 많은 컨텐츠를 생성할 수 있는 것을 의미합니다.</p>
<p>vLLM은 이러한 지표(및 기타 지표)를 /metrics/endpoint를 통해 내보냅니다. 이미 Prometheus에 스크래퍼를 구성했으며, 이제 Grafana에서 대시보드를 설정하여 이러한 지표를 시각화하고 모델의 성능을 실시간으로 더 잘 이해할 수 있도록 합시다.</p>
<h2>LLM 메트릭을 모니터링하기 위한 Grafana 대시보드 설정</h2>
<ul>
<li>브라우저를 통해 Grafana 대시보드에 액세스하려면 먼저 Grafana 팟을 포트 포워딩해야 합니다. 터미널에서 다음 명령을 입력하세요:</li>
</ul>
<pre><code class="hljs language-js">kubectl port-forward -n kube-prometheus-stack \
  service/kube-prometheus-stack-grafana <span class="hljs-number">8080</span>:<span class="hljs-number">80</span>
</code></pre>
<ol start="2">
<li>
<p>웹 브라우저를 열고 http://localhost:8080으로 이동하세요. 로그인 페이지가 나타날 것입니다. 사용자 이름은 &quot;admin&quot;이고 기본 비밀번호는 &quot;prom-operator&quot;입니다.</p>
</li>
<li>
<p>한 번 로그인한 후, 오른쪽 상단 막대의 “+&quot; 아이콘을 클릭하고 “대시보드 가져오기&quot;를 선택합니다.</p>
</li>
</ol>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_7.png" alt="image"/></p>
<ol start="4">
<li>GitHub 저장소의 루트 폴더에 있는 grafana-dashboard.json이라는 JSON 파일을 업로드하고 “가져오기&quot;를 클릭합니다.</li>
</ol>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_8.png" alt="image"/></p>
<ol start="5">
<li>대시보드 상단 좌측에 있는 드롭다운 필터를 사용하여 원하는 모델을 선택해 모니터링할 수 있습니다:</li>
</ol>
<p><img src="/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_9.png" alt="이미지"/></p>
<h2>환경 철거</h2>
<p>셋업을 제거하고 리소스를 해제하려면 다음 단계를 따르세요:</p>
<ul>
<li>vLLM 배포를 제거하세요</li>
</ul>
<pre><code class="hljs language-js">kubectl <span class="hljs-keyword">delete</span> -f vllm.<span class="hljs-property">yaml</span>
</code></pre>
<ol start="2">
<li>이제, Terraform을 사용하여 생성된 모든 인프라 리소스를 삭제하세요. 아래 명령어를 실행하세요:</li>
</ol>
<pre><code class="hljs language-js">terraform destroy
</code></pre>
<h1>결론</h1>
<p>이 게시물에서는 중요한 혜택을 제공하는 자체 호스팅 LLM 설정을 살펴보았습니다. 특히 비용 절감과 데이터 제어 측면에서 큰 이점을 제공합니다. 이 설정은 GPT-4와 같이 가장 고급 모델이 필요하지 않거나, 작고 자원 소모가 적은 모델로도 충분한 경우에 특히 유용합니다.</p>
<p>제품 사용을 위한 데모가 아님을 참고해주시기 바랍니다. 제품 환경에서 이 설정을 구현하기 위해 클러스터 건강 상태의 지속적인 모니터링이 매우 중요합니다. 또한 로드를 관리하고 서비스 가용성을 효과적으로 유지하기 위해 인그레스 및 스케일링 정책을 실행하는 것이 중요합니다.</p>
<p>여러 노드에서 실행해야 하는 대규모 모델이 필요한 사용 사례의 경우, Ray operator인 KubeRay를 사용하는 것을 강력히 추천합니다. KubeRay는 복잡한 분산 시스템의 스케일링과 관리를 크게 용이하게 합니다. 큰 규모의 배포에서 KubeRay를 활용하는 방법에 대해 더 깊게 알고 싶으시면, 미래 게시물에서 KubeRay를 활용하는 것에 대해 자세히 다루도록 하겠습니다. 그게 궁금하신 경우 댓글에서 알려주시면 감사하겠습니다!</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"EKS에서 자체 호스팅 LLM을 배포하는 방법 및 그 이유","description":"","date":"2024-05-16 03:39","slug":"2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould","content":"\n\n생산 환경에서 가격이 폭등하는 토큰에 대해 계속 걱정하고 있나요? 외부 업체가 민감한 데이터를 어떻게 처리하는지 걱정되나요? 이 게시물은 자체 호스팅 LLM을 EKS(Elastic Kubernetes Service)에 배포하는 방법을 안내해 드립니다. 이를 통해 제어권과 비용 효율성을 높일 수 있습니다. 우리는 자체 호스팅을 원하는 이유부터 설정에 필수적인 도구 및 지표에 이르기까지 모든 것을 탐구할 것입니다. 게다가 모델과 상호 작용할 수 있는 간단한 채팅 애플리케이션을 설정하는 방법을 따라해 보겠습니다.\n\n![이미지](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_0.png)\n\n# 자체 호스팅이란?\n\nOpenAI나 Anthropic과 같은 공급업체의 고급 언어 모델은 매우 인상적이지만 항상 지갑 친화적이라고 할 수 없습니다. 실험 및 개발은 금융 오퍼레이션팀의 주의를 끌지 않을 수 있지만, 프로덕션으로 전환하면 토큰당 요금 체계와 관련된 비용이 빠르게 누적될 수 있습니다.\n\n\n\n크고 비싼 모델들이 있긴 하지만, 언제나 당신의 요구 사항을 평가하고 단순히 큰 모델만을 공략하는 것이 아니라는 점을 염두에 두어야 합니다. 하지만 규모에 따라, 작은 모델도 결국에는 비실 것입니다.\n\n하지만, 돈만을 생각할 필요는 없어요. 자체 호스팅은 다음과 같은 중요한 이점들을 제공합니다:\n\n- 데이터 보안 — 모든 민감한 정보, 특히 개인 식별 정보(PII)는 우리 네트워크 내에서 안전하게 보관됩니다. 이 설정은 데이터를 외부로 보내거나 외부 공급 업체가 데이터를 어떻게 사용할지 걱정할 필요가 없게 합니다.\n- 개발자 자유 — 자체 호스팅은 개발자들이 치솟는 비용과 외부 데이터 개인 정보 보호 우려 없이 탐구하고 혁신할 수 있는 자유를 제공합니다. 이 자유는 기술적 실험을 장려하는 창의적인 환경을 지원하여 더 혁신적인 솔루션을 도출하게 됩니다.\n\nGPT-4와 맞먹는 오픈 소스 모델은 많이 찾기 어려울지 몰라도, 보통 GPT-3.5에서 다루는 작업에 적합한 수많은 대안이 있습니다. 이러한 모델 중 일부는 심지어 더 나은 성능을 발휘하면서 더 저렴한 비용으로 제공되고 있습니다. 이를 네트워크에 배포하여 데이터를 제어할 수 있습니다. 무엇보다 중요한 것은 사용량 당 비용을 지불하는 대신 고정 컴퓨팅 가격을 지불하므로 비용을 더 예측 가능하고 관리하기 쉽게 만들 수 있습니다.\n\n\n\n# 필요한 도구는 무엇인가요? (그리고 몇 가지 다른 고려 사항)\n\nAWS 및 EKS에 익숙하다고 가정하고, LLM 모델을 제공하는 데 필요한 다른 구성 요소에 초점을 맞추겠습니다. 고려해야 할 주요 영역은 Compute, 추론 및 모델입니다.\n\n## Compute\n\nLLM 추론을 설정할 때 고려해야 할 주요 자원은 GPU입니다. 특히 GPU의 종류와 수량을 고려해야 합니다. 이는 전체 모델이 GPU의 메모리 (VRAM)에 로드되며, 모든 LLM 계산이 GPU에서 수행되기 때문입니다.\n\n\n\n VRAM 양을 추정하려면 이 안내서를 확인하거나 다음과 같은 간단한 생각의 척도를 따르세요: 모델의 매개변수 수(10억 개 단위)를 두 배하여 기본 요구 사항을 얻은 다음, 캐싱과 오버헤드를 커버하기 위해 20%를 추가하세요. 예를 들어, 70억 개의 매개변수를 가진 모델을 이용하려면 VRAM 약 17GB(7 x 2 x 1.2 = 약 16.8 GB)가 한 개 또는 여러 개의 GPU에서 필요합니다.\n\n## 추론\n\n서빙 프레임워크로는 vLLM을 사용할 것입니다. 이는 LLM 모델을 OpenAI 호환 API 서버로 제공하는 데 설계된 오픈 소스 프레임워크입니다. vLLM은 연속 배치를 지원하며, 다중 동시 요청 및 높은 부하를 처리하기에 이상적입니다. 게다가, vLLM은 분산 서빙을 지원하며, 모델을 여러 GPU 또는 노드에서 실행해야하는 경우를 대비합니다. 분산 서빙을 위해 백엔드로 Ray를 사용하며, 이는 대규모 ML 애플리케이션을 실행하기 위한 또 다른 오픈 소스 프레임워크입니다.\n\n## 모델\n\n\n\n수백 개의 모델이 Hugging Face에 있습니다. Foundation 모델부터 더 구체적이고 특정 문제를 해결하기 위해 디자인된 Feat-Tuned 버전까지 말이죠. Hugging Face를 인공지능(AI) 및 기계학습(ML) 애플리케이션의 \"GitHub\"이라고 생각해보세요. 필요한 어떤 모델이나 데이터셋이든 찾을 수 있는 중요한 장소입니다.\n\n모델을 선택할 때 라이선스를 꼭 확인해주세요. Mistral과 같은 일부 모델은 Apache 라이선스 하에 완전히 오픈 소스입니다. 그러나 많은 모델은 상용 라이선스가 적용됩니다. 이 조항을 검토하는 것은 귀하의 법적 및 운영 계획에 부합하는지 확인하는 데 중요합니다.\n\n# 모두 함께 하기 (데모 시간)\n\n이 데모에서 Mistral 7b instruct 0.2 모델을 사용할 것이며, 이는 Apache 라이선스에 따라 완전히 오픈 소스입니다. 이 모델을 처리하기 위해 주로 15센트 미만의 비용이 드는 AWS g6.xlarge 인스턴스에서 실행할 것입니다. 이 인스턴스는 우리가 상의한 VRAM 견적 규칙에 따라 처리하기에 완벽하게 적합한 24GB VRAM을 갖춘 Nvidia L4 GPU가 장착되어 있습니다.\n\n\n\n데모 중에는 미국 서부-2 지역에 VPC를 배포하고, Fargate에서 Karpenter가 있는 EKS 클러스터, GPU를 위한 Karpenter 프로바이더 하나 및 표준 노드를 위한 또 다른 하나, Kubernetes에서 GPU를 사용할 수 있도록 Nvidia Driver 플러그인, 그리고 모니터링을 위한 Prometheus와 Grafana가 설정될 것입니다. 이 모든 리소스는 Terraform을 사용하여 설정될 것입니다.\n\n비용에 관해서는, 이 데모 실행에 예상되는 비용은 시간당 약 30~40센트입니다. NAT 게이트웨이, EKS 제어 평면 및 GPU가 장착된 노드를 포함한 모든 노드에 대한 요금이 포함됩니다.\n\n## 0. 전제 조건\n\n데모에 들어가기 전에, 다음을 준비해 두세요:\n\n\n\n- AWS 계정 — 데모에서 설치해야 할 VPC, EKS 클러스터 등을 구성할 충분한 권한이 있는 AWS 계정이 필요합니다.\n- AWS 자격 증명 — 로컬 환경에서 자격 증명이 올바르게 구성되어 있는지 확인하세요.\n- Terraform — 여러분의 컴퓨터에 Terraform이 설치되어 있어야 합니다. 데모에 필요한 AWS 리소스를 프로비저닝하고 관리하는 데 사용될 것입니다.\n- Kubectl — Kubernetes 리소스를 관리하기 때문에 Kubectl이 설치되어 있는지 확인하세요.\n- HuggingFace 액세스 토큰 — 이 가이드를 따라 Hugging Face에서 모델을 가져오기 위한 API 액세스 토큰을 생성하세요.\n\n## 1. 기본 인프라\n\n- 터미널을 열고 다음을 실행하여 저장소를 클론합니다:\n\n```js\ngit clone https://github.com/eliran89c/self-hosted-llm-on-eks\n```\n\n\n\n2. 디렉토리를 변경하세요:\n\n```js\ncd self-hosted-llm-on-eks\n```\n\n3. (선택사항) 필요에 따라 Terraform 코드를 조정하여 설정을 사용자 정의하세요.\n\n4. Terraform을 초기화하고 인프라를 배포하기 위해 Terraform 구성을 적용하세요 (EKS 클러스터를 배포하는 데 최대 30분 소요될 수 있습니다).\n\n\n\n```js\nterraform init\nterraform apply\n```\n\n5. 새로 생성된 EKS 클러스터와 상호 작용하기 위해 Kubectl을 설정하세요.\n\n```js\naws eks update-kubeconfig --region us-west-2 \\\n    --name self-hosted-llm \\\n    --alias self-hosted-llm\n```\n\n6. Karpenter와 CoreDNS가 실행 중인지 확인하세요.\n\n\n\n```js\nkubectl get pods --all-namespaces\n```\n\n기대되는 출력은 다음과 같아야 합니다:\n\n![image](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_1.png)\n\n7. Karpenter 제공자가 올바르게 설치되었는지 확인하세요:\n\n\n\n```js\nkubectl get ec2nodeclasses.karpenter.k8s.aws\n```\n\n원하는 결과는 사용 가능한 노드 클래스를 표시해야 합니다:\n\n![Available Node Classes](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_2.png)\n\n## 2. vLLM 배포 및 모델 제공\n\n\n\n- HuggingFace 모델 페이지로 이동하셔서 모델 약관에 동의해주세요\n\n![이미지](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_3.png)\n\n2. HuggingFace API 액세스 토큰을 사용하여 비밀을 생성하세요:\n\n```js\nkubectl create secret generic huggingface-token \\\n    --from-literal=token=\u003cyour_hugging_face_token\u003e\n```\n\n\n\n- 'your_hugging_face_token'을 실제 Hugging Face API 액세스 토큰으로 대체해주세요.\n\n3. (선택 사항) 배포 파일을 검토하고, 특히 배포 인수 섹션을 확인하세요. 필요에 따라 엔진 인수를 수정하여 특정 요구 사항에 더 잘 맞도록 설정할 수 있습니다. 모든 사용 가능한 엔진 인수 목록을 확인하려면 여기를 참조하세요.\n\n4. vLLM 배포하기:\n\n```js\nkubectl apply -f vllm.yaml\n```\n\n\n\n5. 프로메테우스가 vLLM에서 메트릭을 수집하도록 하려면 ServiceMonitor을 배포하십시오.\n\n```js\nkubectl apply -f serviceMonitor.yaml\n```\n\n6. vLLM을 배포한 후에는 일반적으로 GPU로 모델을 다운로드하고 로드하는 데 2-3분 정도가 소요됩니다. 이 초기화 단계 중에 무엇이 발생하는지 모니터링하려면 로그를 직접 확인할 수 있습니다.\n먼저, 파드가 실행 중인지 확인하십시오:\n\n```js\nkubectl get pods\n```\n\n\n\n아래와 같이 로그를 확인하세요:\n\n```js\nkubectl logs -f -l app=vllm\n```\n\n모델이 로딩되고 준비되면, 로그에 다음 메시지가 표시될 것을 기대할 수 있습니다:\n\n![이미지](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_4.png)\n\n\n\n7. 새 터미널을 열고 포트 포워딩을 설정하여 포트 8000에서 OpenAI 호환 API 엔드포인트와 상호 작용할 수 있습니다:\n\n```js\nkubectl port-forward svc/vllm 8000:8000\n```\n\n8. 모든 준비가 되었으므로, LLM을 테스트하기 위해 표준 OpenAI curl 명령을 사용하여 쿼리를 보내보세요. 아래는 예시입니다:\n\n```js\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n      \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"프랑스의 수도는 무엇인가요?\"}]\n    }'\n```\n\n\n\n정보를 성공적으로 검색했는지 확인하기 위해 예상 결과는 다음과 같습니다:\n\n\u003cimg src=\"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_5.png\" /\u003e\n\n## 3. 모델과 상호작용하는 간단한 채팅 애플리케이션 설정\n\n- 다음을 실행하여 새 Python 가상 환경을 만듭니다:\n\n\n\n```js\npython3 -m venv .venv\n```\n\n2. 가상 환경을 활성화합니다:\n\n```js\nsource .venv/bin/activate   # 리눅스 또는 맥OS\n.venv\\Scripts\\activate      # 윈도우\n```\n\n3. 필요한 파이썬 패키지를 설치하려면 다음을 실행하세요:\n\n\n\n\n```bash\npip install -r requirements.txt\n```\n\n이 패키지에는 API 요청을 위한 OpenAI Python 클라이언트와 웹 인터페이스를 만들기 위한 Gradio가 포함되어 있습니다.\n\n4. 아래 명령어를 실행하여 애플리케이션을 시작하세요:\n\n```bash\npython chat.py\n```\n\n\n\n5. 어플리케이션이 실행되면 웹 브라우저를 열고 http://localhost:7860/ 으로 이동하세요.\n\n![이미지](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_6.png)\n\n# 모델 모니터링\n\nLLM에서는 모델의 대기 시간(Latency) 및 처리량(Throughput)을 모니터링하고 측정하는 여러 중요한 지표가 있습니다. 이러한 지표들은 성능을 최적화하고 모델이 효율적으로 작동하는 것을 보장하기 위해 중요합니다. 아래는 고려해야 할 주요 지표입니다:\n\n\n\n첫 번째 토큰 생성 시간(TFFT) - 이 지표는 요청을 제출한 후 모델이 응답의 첫 번째 토큰을 생성하는 데 걸리는 시간을 측정합니다. 이는 모델의 초기 반응성을 나타내는 중요한 지표로, 사용자 상호 작용 애플리케이션에서 응답 시간이 사용자 경험에 영향을 미치는 경우에 특히 중요합니다.\n\n출력 토큰 시간(TFOT) - 위와 유사하게, 이 지표는 처음 토큰 생성 후 각 후속 토큰을 생성하는 데 걸리는 시간을 추적합니다. 이는 모델이 시작된 후 계속해서 내용을 처리하고 생성하는 데 효율성을 이해하는 데 도움이 됩니다. 이로써 실행량 성능에 대한 통찰을 얻을 수 있습니다.\n\n프롬프트/생성 토큰 초당 - 이 지표는 모델이 초당 처리하거나 생성하는 토큰 수를 측정합니다. 이는 모델의 처리량 용량을 평가하는 데 필수적인 지표이며, 높은 비율은 더 효율적인 모델을 나타내며 더 많은 입력을 처리하거나 시간이 적게 소요되면서 더 많은 컨텐츠를 생성할 수 있는 것을 의미합니다.\n\nvLLM은 이러한 지표(및 기타 지표)를 /metrics/endpoint를 통해 내보냅니다. 이미 Prometheus에 스크래퍼를 구성했으며, 이제 Grafana에서 대시보드를 설정하여 이러한 지표를 시각화하고 모델의 성능을 실시간으로 더 잘 이해할 수 있도록 합시다.\n\n\n\n## LLM 메트릭을 모니터링하기 위한 Grafana 대시보드 설정\n\n- 브라우저를 통해 Grafana 대시보드에 액세스하려면 먼저 Grafana 팟을 포트 포워딩해야 합니다. 터미널에서 다음 명령을 입력하세요:\n\n```js\nkubectl port-forward -n kube-prometheus-stack \\\n  service/kube-prometheus-stack-grafana 8080:80\n```\n\n2. 웹 브라우저를 열고 http://localhost:8080으로 이동하세요. 로그인 페이지가 나타날 것입니다. 사용자 이름은 \"admin\"이고 기본 비밀번호는 \"prom-operator\"입니다.\n\n\n\n3. 한 번 로그인한 후, 오른쪽 상단 막대의 “+\" 아이콘을 클릭하고 “대시보드 가져오기\"를 선택합니다.\n\n![image](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_7.png)\n\n4. GitHub 저장소의 루트 폴더에 있는 grafana-dashboard.json이라는 JSON 파일을 업로드하고 “가져오기\"를 클릭합니다.\n\n![image](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_8.png)\n\n\n\n5. 대시보드 상단 좌측에 있는 드롭다운 필터를 사용하여 원하는 모델을 선택해 모니터링할 수 있습니다:\n\n![이미지](/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_9.png)\n\n## 환경 철거\n\n셋업을 제거하고 리소스를 해제하려면 다음 단계를 따르세요:\n\n\n\n- vLLM 배포를 제거하세요\n\n```js\nkubectl delete -f vllm.yaml\n```\n\n2. 이제, Terraform을 사용하여 생성된 모든 인프라 리소스를 삭제하세요. 아래 명령어를 실행하세요:\n\n```js\nterraform destroy\n```\n\n\n\n# 결론\n\n이 게시물에서는 중요한 혜택을 제공하는 자체 호스팅 LLM 설정을 살펴보았습니다. 특히 비용 절감과 데이터 제어 측면에서 큰 이점을 제공합니다. 이 설정은 GPT-4와 같이 가장 고급 모델이 필요하지 않거나, 작고 자원 소모가 적은 모델로도 충분한 경우에 특히 유용합니다.\n\n제품 사용을 위한 데모가 아님을 참고해주시기 바랍니다. 제품 환경에서 이 설정을 구현하기 위해 클러스터 건강 상태의 지속적인 모니터링이 매우 중요합니다. 또한 로드를 관리하고 서비스 가용성을 효과적으로 유지하기 위해 인그레스 및 스케일링 정책을 실행하는 것이 중요합니다.\n\n여러 노드에서 실행해야 하는 대규모 모델이 필요한 사용 사례의 경우, Ray operator인 KubeRay를 사용하는 것을 강력히 추천합니다. KubeRay는 복잡한 분산 시스템의 스케일링과 관리를 크게 용이하게 합니다. 큰 규모의 배포에서 KubeRay를 활용하는 방법에 대해 더 깊게 알고 싶으시면, 미래 게시물에서 KubeRay를 활용하는 것에 대해 자세히 다루도록 하겠습니다. 그게 궁금하신 경우 댓글에서 알려주시면 감사하겠습니다!","ogImage":{"url":"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_0.png"},"coverImage":"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_0.png","tag":["Tech"],"readingTime":9},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h1: \"h1\",\n    ul: \"ul\",\n    li: \"li\",\n    h2: \"h2\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\",\n    ol: \"ol\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"생산 환경에서 가격이 폭등하는 토큰에 대해 계속 걱정하고 있나요? 외부 업체가 민감한 데이터를 어떻게 처리하는지 걱정되나요? 이 게시물은 자체 호스팅 LLM을 EKS(Elastic Kubernetes Service)에 배포하는 방법을 안내해 드립니다. 이를 통해 제어권과 비용 효율성을 높일 수 있습니다. 우리는 자체 호스팅을 원하는 이유부터 설정에 필수적인 도구 및 지표에 이르기까지 모든 것을 탐구할 것입니다. 게다가 모델과 상호 작용할 수 있는 간단한 채팅 애플리케이션을 설정하는 방법을 따라해 보겠습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_0.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"자체 호스팅이란?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"OpenAI나 Anthropic과 같은 공급업체의 고급 언어 모델은 매우 인상적이지만 항상 지갑 친화적이라고 할 수 없습니다. 실험 및 개발은 금융 오퍼레이션팀의 주의를 끌지 않을 수 있지만, 프로덕션으로 전환하면 토큰당 요금 체계와 관련된 비용이 빠르게 누적될 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"크고 비싼 모델들이 있긴 하지만, 언제나 당신의 요구 사항을 평가하고 단순히 큰 모델만을 공략하는 것이 아니라는 점을 염두에 두어야 합니다. 하지만 규모에 따라, 작은 모델도 결국에는 비실 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만, 돈만을 생각할 필요는 없어요. 자체 호스팅은 다음과 같은 중요한 이점들을 제공합니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"데이터 보안 — 모든 민감한 정보, 특히 개인 식별 정보(PII)는 우리 네트워크 내에서 안전하게 보관됩니다. 이 설정은 데이터를 외부로 보내거나 외부 공급 업체가 데이터를 어떻게 사용할지 걱정할 필요가 없게 합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"개발자 자유 — 자체 호스팅은 개발자들이 치솟는 비용과 외부 데이터 개인 정보 보호 우려 없이 탐구하고 혁신할 수 있는 자유를 제공합니다. 이 자유는 기술적 실험을 장려하는 창의적인 환경을 지원하여 더 혁신적인 솔루션을 도출하게 됩니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"GPT-4와 맞먹는 오픈 소스 모델은 많이 찾기 어려울지 몰라도, 보통 GPT-3.5에서 다루는 작업에 적합한 수많은 대안이 있습니다. 이러한 모델 중 일부는 심지어 더 나은 성능을 발휘하면서 더 저렴한 비용으로 제공되고 있습니다. 이를 네트워크에 배포하여 데이터를 제어할 수 있습니다. 무엇보다 중요한 것은 사용량 당 비용을 지불하는 대신 고정 컴퓨팅 가격을 지불하므로 비용을 더 예측 가능하고 관리하기 쉽게 만들 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"필요한 도구는 무엇인가요? (그리고 몇 가지 다른 고려 사항)\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"AWS 및 EKS에 익숙하다고 가정하고, LLM 모델을 제공하는 데 필요한 다른 구성 요소에 초점을 맞추겠습니다. 고려해야 할 주요 영역은 Compute, 추론 및 모델입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Compute\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM 추론을 설정할 때 고려해야 할 주요 자원은 GPU입니다. 특히 GPU의 종류와 수량을 고려해야 합니다. 이는 전체 모델이 GPU의 메모리 (VRAM)에 로드되며, 모든 LLM 계산이 GPU에서 수행되기 때문입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"VRAM 양을 추정하려면 이 안내서를 확인하거나 다음과 같은 간단한 생각의 척도를 따르세요: 모델의 매개변수 수(10억 개 단위)를 두 배하여 기본 요구 사항을 얻은 다음, 캐싱과 오버헤드를 커버하기 위해 20%를 추가하세요. 예를 들어, 70억 개의 매개변수를 가진 모델을 이용하려면 VRAM 약 17GB(7 x 2 x 1.2 = 약 16.8 GB)가 한 개 또는 여러 개의 GPU에서 필요합니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"추론\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"서빙 프레임워크로는 vLLM을 사용할 것입니다. 이는 LLM 모델을 OpenAI 호환 API 서버로 제공하는 데 설계된 오픈 소스 프레임워크입니다. vLLM은 연속 배치를 지원하며, 다중 동시 요청 및 높은 부하를 처리하기에 이상적입니다. 게다가, vLLM은 분산 서빙을 지원하며, 모델을 여러 GPU 또는 노드에서 실행해야하는 경우를 대비합니다. 분산 서빙을 위해 백엔드로 Ray를 사용하며, 이는 대규모 ML 애플리케이션을 실행하기 위한 또 다른 오픈 소스 프레임워크입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"모델\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"수백 개의 모델이 Hugging Face에 있습니다. Foundation 모델부터 더 구체적이고 특정 문제를 해결하기 위해 디자인된 Feat-Tuned 버전까지 말이죠. Hugging Face를 인공지능(AI) 및 기계학습(ML) 애플리케이션의 \\\"GitHub\\\"이라고 생각해보세요. 필요한 어떤 모델이나 데이터셋이든 찾을 수 있는 중요한 장소입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델을 선택할 때 라이선스를 꼭 확인해주세요. Mistral과 같은 일부 모델은 Apache 라이선스 하에 완전히 오픈 소스입니다. 그러나 많은 모델은 상용 라이선스가 적용됩니다. 이 조항을 검토하는 것은 귀하의 법적 및 운영 계획에 부합하는지 확인하는 데 중요합니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"모두 함께 하기 (데모 시간)\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 데모에서 Mistral 7b instruct 0.2 모델을 사용할 것이며, 이는 Apache 라이선스에 따라 완전히 오픈 소스입니다. 이 모델을 처리하기 위해 주로 15센트 미만의 비용이 드는 AWS g6.xlarge 인스턴스에서 실행할 것입니다. 이 인스턴스는 우리가 상의한 VRAM 견적 규칙에 따라 처리하기에 완벽하게 적합한 24GB VRAM을 갖춘 Nvidia L4 GPU가 장착되어 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데모 중에는 미국 서부-2 지역에 VPC를 배포하고, Fargate에서 Karpenter가 있는 EKS 클러스터, GPU를 위한 Karpenter 프로바이더 하나 및 표준 노드를 위한 또 다른 하나, Kubernetes에서 GPU를 사용할 수 있도록 Nvidia Driver 플러그인, 그리고 모니터링을 위한 Prometheus와 Grafana가 설정될 것입니다. 이 모든 리소스는 Terraform을 사용하여 설정될 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"비용에 관해서는, 이 데모 실행에 예상되는 비용은 시간당 약 30~40센트입니다. NAT 게이트웨이, EKS 제어 평면 및 GPU가 장착된 노드를 포함한 모든 노드에 대한 요금이 포함됩니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"0. 전제 조건\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데모에 들어가기 전에, 다음을 준비해 두세요:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"AWS 계정 — 데모에서 설치해야 할 VPC, EKS 클러스터 등을 구성할 충분한 권한이 있는 AWS 계정이 필요합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"AWS 자격 증명 — 로컬 환경에서 자격 증명이 올바르게 구성되어 있는지 확인하세요.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Terraform — 여러분의 컴퓨터에 Terraform이 설치되어 있어야 합니다. 데모에 필요한 AWS 리소스를 프로비저닝하고 관리하는 데 사용될 것입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Kubectl — Kubernetes 리소스를 관리하기 때문에 Kubectl이 설치되어 있는지 확인하세요.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"HuggingFace 액세스 토큰 — 이 가이드를 따라 Hugging Face에서 모델을 가져오기 위한 API 액세스 토큰을 생성하세요.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"1. 기본 인프라\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"터미널을 열고 다음을 실행하여 저장소를 클론합니다:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"git clone \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//github.com/eliran89c/self-hosted-llm-on-eks\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"디렉토리를 변경하세요:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"cd self-hosted-llm-on-eks\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"(선택사항) 필요에 따라 Terraform 코드를 조정하여 설정을 사용자 정의하세요.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Terraform을 초기화하고 인프라를 배포하기 위해 Terraform 구성을 적용하세요 (EKS 클러스터를 배포하는 데 최대 30분 소요될 수 있습니다).\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"terraform init\\nterraform apply\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"5\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"새로 생성된 EKS 클러스터와 상호 작용하기 위해 Kubectl을 설정하세요.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"aws eks update-kubeconfig --region us-west-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \" \\\\\\n    --name self-hosted-llm \\\\\\n    --alias self-hosted-llm\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"6\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Karpenter와 CoreDNS가 실행 중인지 확인하세요.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"kubectl get pods --all-namespaces\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기대되는 출력은 다음과 같아야 합니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_1.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"7\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Karpenter 제공자가 올바르게 설치되었는지 확인하세요:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"kubectl get ec2nodeclasses.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"karpenter\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"k8s\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"aws\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"원하는 결과는 사용 가능한 노드 클래스를 표시해야 합니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_2.png\",\n        alt: \"Available Node Classes\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"2. vLLM 배포 및 모델 제공\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"HuggingFace 모델 페이지로 이동하셔서 모델 약관에 동의해주세요\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_3.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"HuggingFace API 액세스 토큰을 사용하여 비밀을 생성하세요:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"kubectl create secret generic huggingface-token \\\\\\n    --\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \"-literal=token=\u003cyour_hugging_face_token\u003e\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"'your_hugging_face_token'을 실제 Hugging Face API 액세스 토큰으로 대체해주세요.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"(선택 사항) 배포 파일을 검토하고, 특히 배포 인수 섹션을 확인하세요. 필요에 따라 엔진 인수를 수정하여 특정 요구 사항에 더 잘 맞도록 설정할 수 있습니다. 모든 사용 가능한 엔진 인수 목록을 확인하려면 여기를 참조하세요.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"vLLM 배포하기:\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"kubectl apply -f vllm.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"yaml\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"5\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"프로메테우스가 vLLM에서 메트릭을 수집하도록 하려면 ServiceMonitor을 배포하십시오.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"kubectl apply -f serviceMonitor.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"yaml\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"6\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"vLLM을 배포한 후에는 일반적으로 GPU로 모델을 다운로드하고 로드하는 데 2-3분 정도가 소요됩니다. 이 초기화 단계 중에 무엇이 발생하는지 모니터링하려면 로그를 직접 확인할 수 있습니다.\\n먼저, 파드가 실행 중인지 확인하십시오:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"kubectl get pods\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래와 같이 로그를 확인하세요:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"kubectl logs -f -l app=vllm\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델이 로딩되고 준비되면, 로그에 다음 메시지가 표시될 것을 기대할 수 있습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_4.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"7\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"새 터미널을 열고 포트 포워딩을 설정하여 포트 8000에서 OpenAI 호환 API 엔드포인트와 상호 작용할 수 있습니다:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"kubectl port-forward svc/vllm \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8000\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"8\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"모든 준비가 되었으므로, LLM을 테스트하기 위해 표준 OpenAI curl 명령을 사용하여 쿼리를 보내보세요. 아래는 예시입니다:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"curl -X \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"POST\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"http\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//localhost:8000/v1/chat/completions \\\\\"\n        }), \"\\n-H \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Content-Type: application/json\\\"\"\n        }), \" \\\\\\n-d \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'{\\n      \\\"model\\\": \\\"mistralai/Mistral-7B-Instruct-v0.2\\\",\\n      \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"프랑스의 수도는 무엇인가요?\\\"}]\\n    }'\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"정보를 성공적으로 검색했는지 확인하기 위해 예상 결과는 다음과 같습니다:\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_5.png\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3. 모델과 상호작용하는 간단한 채팅 애플리케이션 설정\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"다음을 실행하여 새 Python 가상 환경을 만듭니다:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"python3 -m venv .\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"venv\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"가상 환경을 활성화합니다:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"source .\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"venv\"\n        }), \"/bin/activate   # 리눅스 또는 맥\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"OS\"\n        }), \"\\n.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"venv\"\n        }), \"\\\\\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Scripts\"\n        }), \"\\\\activate      # 윈도우\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"필요한 파이썬 패키지를 설치하려면 다음을 실행하세요:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-bash\",\n        children: \"pip install -r requirements.txt\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 패키지에는 API 요청을 위한 OpenAI Python 클라이언트와 웹 인터페이스를 만들기 위한 Gradio가 포함되어 있습니다.\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"4\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"아래 명령어를 실행하여 애플리케이션을 시작하세요:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-bash\",\n        children: \"python chat.py\\n\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"5\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"어플리케이션이 실행되면 웹 브라우저를 열고 http://localhost:7860/ 으로 이동하세요.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_6.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"모델 모니터링\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM에서는 모델의 대기 시간(Latency) 및 처리량(Throughput)을 모니터링하고 측정하는 여러 중요한 지표가 있습니다. 이러한 지표들은 성능을 최적화하고 모델이 효율적으로 작동하는 것을 보장하기 위해 중요합니다. 아래는 고려해야 할 주요 지표입니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"첫 번째 토큰 생성 시간(TFFT) - 이 지표는 요청을 제출한 후 모델이 응답의 첫 번째 토큰을 생성하는 데 걸리는 시간을 측정합니다. 이는 모델의 초기 반응성을 나타내는 중요한 지표로, 사용자 상호 작용 애플리케이션에서 응답 시간이 사용자 경험에 영향을 미치는 경우에 특히 중요합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"출력 토큰 시간(TFOT) - 위와 유사하게, 이 지표는 처음 토큰 생성 후 각 후속 토큰을 생성하는 데 걸리는 시간을 추적합니다. 이는 모델이 시작된 후 계속해서 내용을 처리하고 생성하는 데 효율성을 이해하는 데 도움이 됩니다. 이로써 실행량 성능에 대한 통찰을 얻을 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"프롬프트/생성 토큰 초당 - 이 지표는 모델이 초당 처리하거나 생성하는 토큰 수를 측정합니다. 이는 모델의 처리량 용량을 평가하는 데 필수적인 지표이며, 높은 비율은 더 효율적인 모델을 나타내며 더 많은 입력을 처리하거나 시간이 적게 소요되면서 더 많은 컨텐츠를 생성할 수 있는 것을 의미합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"vLLM은 이러한 지표(및 기타 지표)를 /metrics/endpoint를 통해 내보냅니다. 이미 Prometheus에 스크래퍼를 구성했으며, 이제 Grafana에서 대시보드를 설정하여 이러한 지표를 시각화하고 모델의 성능을 실시간으로 더 잘 이해할 수 있도록 합시다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"LLM 메트릭을 모니터링하기 위한 Grafana 대시보드 설정\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"브라우저를 통해 Grafana 대시보드에 액세스하려면 먼저 Grafana 팟을 포트 포워딩해야 합니다. 터미널에서 다음 명령을 입력하세요:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"kubectl port-forward -n kube-prometheus-stack \\\\\\n  service/kube-prometheus-stack-grafana \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8080\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"80\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"웹 브라우저를 열고 http://localhost:8080으로 이동하세요. 로그인 페이지가 나타날 것입니다. 사용자 이름은 \\\"admin\\\"이고 기본 비밀번호는 \\\"prom-operator\\\"입니다.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"한 번 로그인한 후, 오른쪽 상단 막대의 “+\\\" 아이콘을 클릭하고 “대시보드 가져오기\\\"를 선택합니다.\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_7.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"4\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"GitHub 저장소의 루트 폴더에 있는 grafana-dashboard.json이라는 JSON 파일을 업로드하고 “가져오기\\\"를 클릭합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_8.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"5\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"대시보드 상단 좌측에 있는 드롭다운 필터를 사용하여 원하는 모델을 선택해 모니터링할 수 있습니다:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould_9.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"환경 철거\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"셋업을 제거하고 리소스를 해제하려면 다음 단계를 따르세요:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"vLLM 배포를 제거하세요\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"kubectl \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"delete\"\n        }), \" -f vllm.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"yaml\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"이제, Terraform을 사용하여 생성된 모든 인프라 리소스를 삭제하세요. 아래 명령어를 실행하세요:\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"terraform destroy\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결론\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 게시물에서는 중요한 혜택을 제공하는 자체 호스팅 LLM 설정을 살펴보았습니다. 특히 비용 절감과 데이터 제어 측면에서 큰 이점을 제공합니다. 이 설정은 GPT-4와 같이 가장 고급 모델이 필요하지 않거나, 작고 자원 소모가 적은 모델로도 충분한 경우에 특히 유용합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"제품 사용을 위한 데모가 아님을 참고해주시기 바랍니다. 제품 환경에서 이 설정을 구현하기 위해 클러스터 건강 상태의 지속적인 모니터링이 매우 중요합니다. 또한 로드를 관리하고 서비스 가용성을 효과적으로 유지하기 위해 인그레스 및 스케일링 정책을 실행하는 것이 중요합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"여러 노드에서 실행해야 하는 대규모 모델이 필요한 사용 사례의 경우, Ray operator인 KubeRay를 사용하는 것을 강력히 추천합니다. KubeRay는 복잡한 분산 시스템의 스케일링과 관리를 크게 용이하게 합니다. 큰 규모의 배포에서 KubeRay를 활용하는 방법에 대해 더 깊게 알고 싶으시면, 미래 게시물에서 KubeRay를 활용하는 것에 대해 자세히 다루도록 하겠습니다. 그게 궁금하신 경우 댓글에서 알려주시면 감사하겠습니다!\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-HowtoDeployaSelf-HostedLLMonEKSandWhyYouShould"},"buildId":"K-h7XvEVBqnNx_uXMgZoe","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>