<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-17-5HacksToImproveYourRAGApplication" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-17-5HacksToImproveYourRAGApplication" data-gatsby-head="true"/><meta name="twitter:title" content="여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-17 04:20" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/Y-fCAg8BUV7y2HNFwX9AA/_buildManifest.js" defer=""></script><script src="/_next/static/Y-fCAg8BUV7y2HNFwX9AA/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 17, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-17-5HacksToImproveYourRAGApplication&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>RAG는 기업 및 비즈니스에서 Gen AI 기능을 사용자 지정 데이터와 통합하는 데 중요한 도구가 되었습니다.</p>
<p><img src="/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png" alt="image"></p>
<p>다음은 RAG 애플리케이션을 개선하는 몇 가지 팁입니다.</p>
<ul>
<li>쿼리 보강</li>
<li>문서 청킹</li>
<li>결과 재랭킹</li>
<li>임베딩 어댑터</li>
<li>가상 문서 임베딩</li>
</ul>
<div class="content-ad"></div>
<p>쿼리 확장:</p>
<p>관련 데이터를 검색하고 정확한 응답을 얻기 위해 프롬프트와 함께 보강하는 것이 중요합니다.</p>
<p>단계:</p>
<ul>
<li>코사인 유사도나 유클리드 거리를 사용하여 벡터 임베딩 데이터베이스를 사용하여 사용자 쿼리를 기반으로 문서를 검색합니다.</li>
<li>검색된 데이터/문서와 프롬프트를 결합합니다.</li>
<li>LLM(언어 모델)을 사용하여 하이브리드 데이터로부터 데이터를 생성합니다.</li>
</ul>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> chromadb
<span class="hljs-keyword">import</span> openai

<span class="hljs-keyword">def</span> <span class="hljs-title function_">augmented_query_creator</span>(<span class="hljs-params">user_query, retrieved_documents</span>):
    information = <span class="hljs-string">"\n\n"</span>.join(retrieved_documents)
    prompt = (<span class="hljs-string">f'You are a movie critic.\n'</span>
    <span class="hljs-string">f'Your users are asking questions about movie review.\n'</span>
    <span class="hljs-string">f'You will be shown the user\'s question, and the relevant information from the movie.\n'</span>
    <span class="hljs-string">f'Answer the user\'s question using only this information.\n\n'</span>
    <span class="hljs-string">f'Question: <span class="hljs-subst">{query}</span>. \n Information: <span class="hljs-subst">{information}</span>'</span>)
    <span class="hljs-keyword">return</span> prompt

<span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_answer</span>(<span class="hljs-params">prompt</span>):
    openai.api_key = <span class="hljs-string">"YOUR_OPENAI_API_KEY"</span>  
    response = openai.Completion.create(
        engine=<span class="hljs-string">"text-davinci-003"</span>,  
        prompt=prompt,
        max_tokens=<span class="hljs-number">1024</span>, 
        n=<span class="hljs-number">1</span>,
        stop=<span class="hljs-literal">None</span>,
        temperature=<span class="hljs-number">0.7</span>
    )
    <span class="hljs-keyword">return</span> response.choices[<span class="hljs-number">0</span>].text.strip()

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    query = <span class="hljs-string">"What is the review of the movie?"</span>
    
    <span class="hljs-comment"># 1 Retrive relevant documents</span>
    results = chroma_collection.query(query_texts=[query], n_results=<span class="hljs-number">5</span>)
    retrieved_documents = results[<span class="hljs-string">'documents'</span>][<span class="hljs-number">0</span>]
    
    <span class="hljs-comment"># 2 Augmented query generation</span>
    augmented_query = augmented_query_creator(query,retrieved_documents)

    <span class="hljs-comment"># 3 Response for augmented query</span>
    result = generate_answer(augmented_query)
</code></pre>
<p>문서 청크 데이터 중복 :</p>
<p>다양한 문서에 대한 벡터 데이터베이스를 구축할 때, 토큰 제한으로 인해 데이터 손실이 발생할 수 있습니다. 이 문제를 해결하기 위해 데이터를 작은 세그먼트로 분할하는 것이 해결책입니다.</p>
<p>하지만 이러한 청크를 사용하더라도 한 문서와 다른 문서 사이의 의미와 연속성 손실이 발생할 수 있습니다. 이 문제를 완화하기 위해 데이터의 일관성과 흐름을 유지하기 위해 청크 사이에 중첩을 도입하는 것이 중요합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain.<span class="hljs-property">vectorstores</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Chroma</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">embeddings</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">OpenAIEmbeddings</span>  # 귀하의 선택한 임베딩 모델로 대체하십시오
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">text_splitter</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">RecursiveCharacterTextSplitter</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">document_loaders</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">TextLoader</span>

# 문서 경로와 임베딩 모델 정의 (귀하의 것으로 대체하십시오)
document_path = <span class="hljs-string">"your_document.txt"</span>
embedding_model = <span class="hljs-title class_">OpenAIEmbeddings</span>

# 청크 크기 및 선택적인 오버랩 설정
chunk_size = <span class="hljs-number">500</span>
chunk_overlap = <span class="hljs-number">100</span>

# 문서를 로드하고 <span class="hljs-title class_">RecursiveCharacterTextSplitter</span>로 분할합니다.
text_loader = <span class="hljs-title class_">TextLoader</span>(document_path)
documents = text_loader.<span class="hljs-title function_">load</span>()
splitter = <span class="hljs-title class_">RecursiveCharacterTextSplitter</span>(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
texts = splitter.<span class="hljs-title function_">split_documents</span>(documents)

# 임베딩을 사용하여 <span class="hljs-title class_">ChromaDB</span> 생성
persist_directory = <span class="hljs-string">"chroma_db"</span>
chroma_collection = <span class="hljs-title class_">Chroma</span>.<span class="hljs-title function_">from_documents</span>(
    documents=texts, embedding=<span class="hljs-title function_">embedding_model</span>(), persist_directory=persist_directory
)
</code></pre>
<p>재랭킹</p>
<p>결과를 재랭킹하는 것은 검색된 문서를 검색기에 의해 검색된 후 특정 기준에 따라 다시 정렬하는 것을 의미합니다. 응답을 생성하기 전에 검색된 문서의 관련성을 더욱 정제하는 데 유용할 수 있습니다.</p>
<p>우리는 문서의 관련성 순서를 변경하기 위해 코사인 유사성 대신 크로스 인코더 모델을 사용합니다.</p>
<div class="content-ad"></div>
<p>Step 1: 크로스-인코더 모델을 로드합니다.</p>
<p>Step 2: 관련성 점수를 변경하는 재랭크 함수입니다.</p>
<p>Step 3: 문서를 정렬하고 반환합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer

<span class="hljs-comment"># ChromaDB와 크로스-인코더 모델을 로드합니다.</span>
chromadb = Chroma.load(<span class="hljs-string">"chroma_db"</span>)  
cross_encoder = SentenceTransformer(<span class="hljs-string">"all-mpnet-base-v2"</span>)  

<span class="hljs-keyword">def</span> <span class="hljs-title function_">re_rank_results</span>(<span class="hljs-params">query, retrieved_chunks, k=<span class="hljs-number">3</span></span>):
  
  scored_chunks = []
  <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> retrieved_chunks:
    score = cross_encoder.compute_similarity([query], [chunk])[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]
    scored_chunks.append({<span class="hljs-string">"chunk"</span>: chunk, <span class="hljs-string">"score"</span>: score})

  <span class="hljs-comment"># 점수를 기준으로 (내림차순으로) 정렬하고 상위 k개 결과를 반환합니다.</span>
  sorted_chunks = <span class="hljs-built_in">sorted</span>(scored_chunks, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">"score"</span>], reverse=<span class="hljs-literal">True</span>)
  <span class="hljs-keyword">return</span> sorted_chunks[:k]
</code></pre>
<div class="content-ad"></div>
<p>임베딩 어댑터:</p>
<p>임베딩 어댑터는 초기 임베딩 프로세스와 검색 단계 간의 세세한 조정 단계로 작용하는 소규모 신경망 모듈입니다. 그 목적은 쿼리의 임베딩과 지식베이스에 저장된 문서 표현의 정렬을 개선하는 것입니다.</p>
<p>단계 1: 임베딩 생성</p>
<p>단계 2: 임베딩 어댑터로 섬세하게 조정하기</p>
<div class="content-ad"></div>
<p>3단계: 개선된 검색</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain.<span class="hljs-property">vectorstores</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Chroma</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">text_embeddings</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SentenceTransformerEmbeddings</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">text_encoders</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">IdentityEncoder</span>  # 원본 텍스트 보존
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">document_loaders</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">TextLoader</span>

# 문서 경로 및 임베딩 모델 정의
document_path = <span class="hljs-string">"your_document.txt"</span>
embedding_model = <span class="hljs-title class_">SentenceTransformerEmbeddings</span>(<span class="hljs-string">"all-mpnet-base-v2"</span>)

# 문서 로드
text_loader = <span class="hljs-title class_">TextLoader</span>(document_path)
documents = text_loader.<span class="hljs-title function_">load</span>()

# 텍스트 인코더를 사용하여 <span class="hljs-title class_">ChromaDB</span> 생성 (선택 사항)
persist_directory = <span class="hljs-string">"chroma_db"</span>  
text_encoder = <span class="hljs-title class_">IdentityEncoder</span>()  

vectordb = <span class="hljs-title class_">Chroma</span>.<span class="hljs-title function_">from_documents</span>(
    documents=documents,
    embedding=<span class="hljs-title function_">embedding_model</span>(),
    text_encoder=text_encoder,
    persist_directory=persist_directory
)

# 선택적 지속성
vectordb.<span class="hljs-title function_">persist</span>()

# 텍스트 검색 예시
query = <span class="hljs-string">"북극 해는 어디에 있나요?"</span>

# 인코딩된 텍스트를 기반으로 검색 (임베딩 아님)
results = vectordb.<span class="hljs-title function_">search</span>(query, k=<span class="hljs-number">5</span>)  # 상위 <span class="hljs-number">5</span>개 결과 가져오기

# 검색된 문서 출력
<span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> <span class="hljs-attr">results</span>:
    <span class="hljs-title function_">print</span>(doc)

<span class="hljs-title function_">print</span>(<span class="hljs-string">"ChromaDB 검색 완료!"</span>)
</code></pre>
<p>가상 문서 임베딩:</p>
<p>HyDE는 대형 언어 모델(Large Language Models, LLMs)을 활용하여 문서로부터 정보 검색을 개선하는 기술입니다.</p>
<div class="content-ad"></div>
<p>Step 1: Query 이해하기: 모든 것은 사용자 쿼리로 시작됩니다. HyDE는 이 쿼리를 입력으로 받습니다.</p>
<p>Step 2: 가상 문서 생성: HyDE는 GPT-3과 같은 LLM을 사용하여 사용자 쿼리에 완벽한 답변이 될 것으로 믿는 가상 문서를 생성합니다. 이 문서에는 사실적인 정보 뿐만 아니라 창의적인 요소나 사용자 의도에 부합하는 설명이 포함될 수 있습니다.</p>
<p>Step 3: 가설 인코딩: 가상 문서가 생성된 후, HyDE는 문서 자체를 사용하지 않습니다. 대신, 문서의 의미를 수학적 벡터 표현으로 인코딩합니다. 이 벡터는 가상 답변 내의 핵심 개념과 정보를 포착합니다.</p>
<p>Step 4: 유사 문서 찾기: 이제 검색 과정이 시작됩니다. HyDE는 가상 문서를 나타내는 벡터를 사용하여 방대한 문서 컬렉션(보통 미리 인코딩된)을 검색합니다. 이것은 가상 문서의 벡터와 유사한 실제 문서를 탐색합니다. 유사성은 이 실제 문서들이 가상 답변과 유사한 방법으로 사용자 쿼리에 대응한다는 것을 나타냅니다.</p>
<div class="content-ad"></div>
<p>5단계: 검색된 문서를 활용하기: HyDE 프로세스를 기반으로 가장 관련성 높은 것으로 간주된 이러한 검색된 문서는 이후 RAG 시스템에 공급됩니다. RAG 내의 LLM은 이 문서들을 사용하여 사용자의 초기 쿼리에 대한 더 포괄적이고 유익한 응답을 생성할 수 있습니다.</p>
<p>참고 자료:</p>
<ul>
<li><a href="https://platform.openai.com/docs/assistants/overview" rel="nofollow" target="_blank">https://platform.openai.com/docs/assistants/overview</a></li>
<li>LinkedIn GitHub</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"여러분의 RAG 어플리케이션 성능 향상을 위한 5가지 꿀팁","description":"","date":"2024-05-17 04:20","slug":"2024-05-17-5HacksToImproveYourRAGApplication","content":"\n\nRAG는 기업 및 비즈니스에서 Gen AI 기능을 사용자 지정 데이터와 통합하는 데 중요한 도구가 되었습니다.\n\n![image](/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png)\n\n다음은 RAG 애플리케이션을 개선하는 몇 가지 팁입니다.\n\n- 쿼리 보강\n- 문서 청킹\n- 결과 재랭킹\n- 임베딩 어댑터\n- 가상 문서 임베딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n쿼리 확장:\n\n관련 데이터를 검색하고 정확한 응답을 얻기 위해 프롬프트와 함께 보강하는 것이 중요합니다.\n\n단계:\n\n- 코사인 유사도나 유클리드 거리를 사용하여 벡터 임베딩 데이터베이스를 사용하여 사용자 쿼리를 기반으로 문서를 검색합니다.\n- 검색된 데이터/문서와 프롬프트를 결합합니다.\n- LLM(언어 모델)을 사용하여 하이브리드 데이터로부터 데이터를 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport chromadb\nimport openai\n\ndef augmented_query_creator(user_query, retrieved_documents):\n    information = \"\\n\\n\".join(retrieved_documents)\n    prompt = (f'You are a movie critic.\\n'\n    f'Your users are asking questions about movie review.\\n'\n    f'You will be shown the user\\'s question, and the relevant information from the movie.\\n'\n    f'Answer the user\\'s question using only this information.\\n\\n'\n    f'Question: {query}. \\n Information: {information}')\n    return prompt\n\ndef generate_answer(prompt):\n    openai.api_key = \"YOUR_OPENAI_API_KEY\"  \n    response = openai.Completion.create(\n        engine=\"text-davinci-003\",  \n        prompt=prompt,\n        max_tokens=1024, \n        n=1,\n        stop=None,\n        temperature=0.7\n    )\n    return response.choices[0].text.strip()\n\nif __name__ == \"__main__\":\n    query = \"What is the review of the movie?\"\n    \n    # 1 Retrive relevant documents\n    results = chroma_collection.query(query_texts=[query], n_results=5)\n    retrieved_documents = results['documents'][0]\n    \n    # 2 Augmented query generation\n    augmented_query = augmented_query_creator(query,retrieved_documents)\n\n    # 3 Response for augmented query\n    result = generate_answer(augmented_query)\n```\n\n문서 청크 데이터 중복 :\n\n다양한 문서에 대한 벡터 데이터베이스를 구축할 때, 토큰 제한으로 인해 데이터 손실이 발생할 수 있습니다. 이 문제를 해결하기 위해 데이터를 작은 세그먼트로 분할하는 것이 해결책입니다.\n\n하지만 이러한 청크를 사용하더라도 한 문서와 다른 문서 사이의 의미와 연속성 손실이 발생할 수 있습니다. 이 문제를 완화하기 위해 데이터의 일관성과 흐름을 유지하기 위해 청크 사이에 중첩을 도입하는 것이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings  # 귀하의 선택한 임베딩 모델로 대체하십시오\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import TextLoader\n\n# 문서 경로와 임베딩 모델 정의 (귀하의 것으로 대체하십시오)\ndocument_path = \"your_document.txt\"\nembedding_model = OpenAIEmbeddings\n\n# 청크 크기 및 선택적인 오버랩 설정\nchunk_size = 500\nchunk_overlap = 100\n\n# 문서를 로드하고 RecursiveCharacterTextSplitter로 분할합니다.\ntext_loader = TextLoader(document_path)\ndocuments = text_loader.load()\nsplitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\ntexts = splitter.split_documents(documents)\n\n# 임베딩을 사용하여 ChromaDB 생성\npersist_directory = \"chroma_db\"\nchroma_collection = Chroma.from_documents(\n    documents=texts, embedding=embedding_model(), persist_directory=persist_directory\n)\n```\n\n재랭킹\n\n결과를 재랭킹하는 것은 검색된 문서를 검색기에 의해 검색된 후 특정 기준에 따라 다시 정렬하는 것을 의미합니다. 응답을 생성하기 전에 검색된 문서의 관련성을 더욱 정제하는 데 유용할 수 있습니다.\n\n우리는 문서의 관련성 순서를 변경하기 위해 코사인 유사성 대신 크로스 인코더 모델을 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 1: 크로스-인코더 모델을 로드합니다.\n\nStep 2: 관련성 점수를 변경하는 재랭크 함수입니다.\n\nStep 3: 문서를 정렬하고 반환합니다.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\n# ChromaDB와 크로스-인코더 모델을 로드합니다.\nchromadb = Chroma.load(\"chroma_db\")  \ncross_encoder = SentenceTransformer(\"all-mpnet-base-v2\")  \n\ndef re_rank_results(query, retrieved_chunks, k=3):\n  \n  scored_chunks = []\n  for chunk in retrieved_chunks:\n    score = cross_encoder.compute_similarity([query], [chunk])[0][0]\n    scored_chunks.append({\"chunk\": chunk, \"score\": score})\n\n  # 점수를 기준으로 (내림차순으로) 정렬하고 상위 k개 결과를 반환합니다.\n  sorted_chunks = sorted(scored_chunks, key=lambda x: x[\"score\"], reverse=True)\n  return sorted_chunks[:k]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임베딩 어댑터:\n\n임베딩 어댑터는 초기 임베딩 프로세스와 검색 단계 간의 세세한 조정 단계로 작용하는 소규모 신경망 모듈입니다. 그 목적은 쿼리의 임베딩과 지식베이스에 저장된 문서 표현의 정렬을 개선하는 것입니다.\n\n단계 1: 임베딩 생성\n\n단계 2: 임베딩 어댑터로 섬세하게 조정하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3단계: 개선된 검색\n\n```js\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_embeddings import SentenceTransformerEmbeddings\nfrom langchain.text_encoders import IdentityEncoder  # 원본 텍스트 보존\nfrom langchain.document_loaders import TextLoader\n\n# 문서 경로 및 임베딩 모델 정의\ndocument_path = \"your_document.txt\"\nembedding_model = SentenceTransformerEmbeddings(\"all-mpnet-base-v2\")\n\n# 문서 로드\ntext_loader = TextLoader(document_path)\ndocuments = text_loader.load()\n\n# 텍스트 인코더를 사용하여 ChromaDB 생성 (선택 사항)\npersist_directory = \"chroma_db\"  \ntext_encoder = IdentityEncoder()  \n\nvectordb = Chroma.from_documents(\n    documents=documents,\n    embedding=embedding_model(),\n    text_encoder=text_encoder,\n    persist_directory=persist_directory\n)\n\n# 선택적 지속성\nvectordb.persist()\n\n# 텍스트 검색 예시\nquery = \"북극 해는 어디에 있나요?\"\n\n# 인코딩된 텍스트를 기반으로 검색 (임베딩 아님)\nresults = vectordb.search(query, k=5)  # 상위 5개 결과 가져오기\n\n# 검색된 문서 출력\nfor doc in results:\n    print(doc)\n\nprint(\"ChromaDB 검색 완료!\")\n```\n\n가상 문서 임베딩:\n\nHyDE는 대형 언어 모델(Large Language Models, LLMs)을 활용하여 문서로부터 정보 검색을 개선하는 기술입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 1: Query 이해하기: 모든 것은 사용자 쿼리로 시작됩니다. HyDE는 이 쿼리를 입력으로 받습니다.\n\nStep 2: 가상 문서 생성: HyDE는 GPT-3과 같은 LLM을 사용하여 사용자 쿼리에 완벽한 답변이 될 것으로 믿는 가상 문서를 생성합니다. 이 문서에는 사실적인 정보 뿐만 아니라 창의적인 요소나 사용자 의도에 부합하는 설명이 포함될 수 있습니다.\n\nStep 3: 가설 인코딩: 가상 문서가 생성된 후, HyDE는 문서 자체를 사용하지 않습니다. 대신, 문서의 의미를 수학적 벡터 표현으로 인코딩합니다. 이 벡터는 가상 답변 내의 핵심 개념과 정보를 포착합니다.\n\nStep 4: 유사 문서 찾기: 이제 검색 과정이 시작됩니다. HyDE는 가상 문서를 나타내는 벡터를 사용하여 방대한 문서 컬렉션(보통 미리 인코딩된)을 검색합니다. 이것은 가상 문서의 벡터와 유사한 실제 문서를 탐색합니다. 유사성은 이 실제 문서들이 가상 답변과 유사한 방법으로 사용자 쿼리에 대응한다는 것을 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5단계: 검색된 문서를 활용하기: HyDE 프로세스를 기반으로 가장 관련성 높은 것으로 간주된 이러한 검색된 문서는 이후 RAG 시스템에 공급됩니다. RAG 내의 LLM은 이 문서들을 사용하여 사용자의 초기 쿼리에 대한 더 포괄적이고 유익한 응답을 생성할 수 있습니다.\n\n참고 자료:\n\n- https://platform.openai.com/docs/assistants/overview\n- LinkedIn GitHub","ogImage":{"url":"/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png"},"coverImage":"/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eRAG는 기업 및 비즈니스에서 Gen AI 기능을 사용자 지정 데이터와 통합하는 데 중요한 도구가 되었습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-5HacksToImproveYourRAGApplication_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e다음은 RAG 애플리케이션을 개선하는 몇 가지 팁입니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e쿼리 보강\u003c/li\u003e\n\u003cli\u003e문서 청킹\u003c/li\u003e\n\u003cli\u003e결과 재랭킹\u003c/li\u003e\n\u003cli\u003e임베딩 어댑터\u003c/li\u003e\n\u003cli\u003e가상 문서 임베딩\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e쿼리 확장:\u003c/p\u003e\n\u003cp\u003e관련 데이터를 검색하고 정확한 응답을 얻기 위해 프롬프트와 함께 보강하는 것이 중요합니다.\u003c/p\u003e\n\u003cp\u003e단계:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e코사인 유사도나 유클리드 거리를 사용하여 벡터 임베딩 데이터베이스를 사용하여 사용자 쿼리를 기반으로 문서를 검색합니다.\u003c/li\u003e\n\u003cli\u003e검색된 데이터/문서와 프롬프트를 결합합니다.\u003c/li\u003e\n\u003cli\u003eLLM(언어 모델)을 사용하여 하이브리드 데이터로부터 데이터를 생성합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e chromadb\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e openai\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eaugmented_query_creator\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003euser_query, retrieved_documents\u003c/span\u003e):\n    information = \u003cspan class=\"hljs-string\"\u003e\"\\n\\n\"\u003c/span\u003e.join(retrieved_documents)\n    prompt = (\u003cspan class=\"hljs-string\"\u003ef'You are a movie critic.\\n'\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003ef'Your users are asking questions about movie review.\\n'\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003ef'You will be shown the user\\'s question, and the relevant information from the movie.\\n'\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003ef'Answer the user\\'s question using only this information.\\n\\n'\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003ef'Question: \u003cspan class=\"hljs-subst\"\u003e{query}\u003c/span\u003e. \\n Information: \u003cspan class=\"hljs-subst\"\u003e{information}\u003c/span\u003e'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e prompt\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003egenerate_answer\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eprompt\u003c/span\u003e):\n    openai.api_key = \u003cspan class=\"hljs-string\"\u003e\"YOUR_OPENAI_API_KEY\"\u003c/span\u003e  \n    response = openai.Completion.create(\n        engine=\u003cspan class=\"hljs-string\"\u003e\"text-davinci-003\"\u003c/span\u003e,  \n        prompt=prompt,\n        max_tokens=\u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e, \n        n=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\n        stop=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e,\n        temperature=\u003cspan class=\"hljs-number\"\u003e0.7\u003c/span\u003e\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e response.choices[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].text.strip()\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e\"__main__\"\u003c/span\u003e:\n    query = \u003cspan class=\"hljs-string\"\u003e\"What is the review of the movie?\"\u003c/span\u003e\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 1 Retrive relevant documents\u003c/span\u003e\n    results = chroma_collection.query(query_texts=[query], n_results=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n    retrieved_documents = results[\u003cspan class=\"hljs-string\"\u003e'documents'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n    \n    \u003cspan class=\"hljs-comment\"\u003e# 2 Augmented query generation\u003c/span\u003e\n    augmented_query = augmented_query_creator(query,retrieved_documents)\n\n    \u003cspan class=\"hljs-comment\"\u003e# 3 Response for augmented query\u003c/span\u003e\n    result = generate_answer(augmented_query)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e문서 청크 데이터 중복 :\u003c/p\u003e\n\u003cp\u003e다양한 문서에 대한 벡터 데이터베이스를 구축할 때, 토큰 제한으로 인해 데이터 손실이 발생할 수 있습니다. 이 문제를 해결하기 위해 데이터를 작은 세그먼트로 분할하는 것이 해결책입니다.\u003c/p\u003e\n\u003cp\u003e하지만 이러한 청크를 사용하더라도 한 문서와 다른 문서 사이의 의미와 연속성 손실이 발생할 수 있습니다. 이 문제를 완화하기 위해 데이터의 일관성과 흐름을 유지하기 위해 청크 사이에 중첩을 도입하는 것이 중요합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003evectorstores\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eChroma\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003eembeddings\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOpenAIEmbeddings\u003c/span\u003e  # 귀하의 선택한 임베딩 모델로 대체하십시오\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003etext_splitter\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003edocument_loaders\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTextLoader\u003c/span\u003e\n\n# 문서 경로와 임베딩 모델 정의 (귀하의 것으로 대체하십시오)\ndocument_path = \u003cspan class=\"hljs-string\"\u003e\"your_document.txt\"\u003c/span\u003e\nembedding_model = \u003cspan class=\"hljs-title class_\"\u003eOpenAIEmbeddings\u003c/span\u003e\n\n# 청크 크기 및 선택적인 오버랩 설정\nchunk_size = \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\nchunk_overlap = \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e\n\n# 문서를 로드하고 \u003cspan class=\"hljs-title class_\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e로 분할합니다.\ntext_loader = \u003cspan class=\"hljs-title class_\"\u003eTextLoader\u003c/span\u003e(document_path)\ndocuments = text_loader.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e()\nsplitter = \u003cspan class=\"hljs-title class_\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\ntexts = splitter.\u003cspan class=\"hljs-title function_\"\u003esplit_documents\u003c/span\u003e(documents)\n\n# 임베딩을 사용하여 \u003cspan class=\"hljs-title class_\"\u003eChromaDB\u003c/span\u003e 생성\npersist_directory = \u003cspan class=\"hljs-string\"\u003e\"chroma_db\"\u003c/span\u003e\nchroma_collection = \u003cspan class=\"hljs-title class_\"\u003eChroma\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_documents\u003c/span\u003e(\n    documents=texts, embedding=\u003cspan class=\"hljs-title function_\"\u003eembedding_model\u003c/span\u003e(), persist_directory=persist_directory\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e재랭킹\u003c/p\u003e\n\u003cp\u003e결과를 재랭킹하는 것은 검색된 문서를 검색기에 의해 검색된 후 특정 기준에 따라 다시 정렬하는 것을 의미합니다. 응답을 생성하기 전에 검색된 문서의 관련성을 더욱 정제하는 데 유용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e우리는 문서의 관련성 순서를 변경하기 위해 코사인 유사성 대신 크로스 인코더 모델을 사용합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eStep 1: 크로스-인코더 모델을 로드합니다.\u003c/p\u003e\n\u003cp\u003eStep 2: 관련성 점수를 변경하는 재랭크 함수입니다.\u003c/p\u003e\n\u003cp\u003eStep 3: 문서를 정렬하고 반환합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sentence_transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SentenceTransformer\n\n\u003cspan class=\"hljs-comment\"\u003e# ChromaDB와 크로스-인코더 모델을 로드합니다.\u003c/span\u003e\nchromadb = Chroma.load(\u003cspan class=\"hljs-string\"\u003e\"chroma_db\"\u003c/span\u003e)  \ncross_encoder = SentenceTransformer(\u003cspan class=\"hljs-string\"\u003e\"all-mpnet-base-v2\"\u003c/span\u003e)  \n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ere_rank_results\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003equery, retrieved_chunks, k=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\u003c/span\u003e):\n  \n  scored_chunks = []\n  \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e chunk \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e retrieved_chunks:\n    score = cross_encoder.compute_similarity([query], [chunk])[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n    scored_chunks.append({\u003cspan class=\"hljs-string\"\u003e\"chunk\"\u003c/span\u003e: chunk, \u003cspan class=\"hljs-string\"\u003e\"score\"\u003c/span\u003e: score})\n\n  \u003cspan class=\"hljs-comment\"\u003e# 점수를 기준으로 (내림차순으로) 정렬하고 상위 k개 결과를 반환합니다.\u003c/span\u003e\n  sorted_chunks = \u003cspan class=\"hljs-built_in\"\u003esorted\u003c/span\u003e(scored_chunks, key=\u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e x: x[\u003cspan class=\"hljs-string\"\u003e\"score\"\u003c/span\u003e], reverse=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e sorted_chunks[:k]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e임베딩 어댑터:\u003c/p\u003e\n\u003cp\u003e임베딩 어댑터는 초기 임베딩 프로세스와 검색 단계 간의 세세한 조정 단계로 작용하는 소규모 신경망 모듈입니다. 그 목적은 쿼리의 임베딩과 지식베이스에 저장된 문서 표현의 정렬을 개선하는 것입니다.\u003c/p\u003e\n\u003cp\u003e단계 1: 임베딩 생성\u003c/p\u003e\n\u003cp\u003e단계 2: 임베딩 어댑터로 섬세하게 조정하기\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e3단계: 개선된 검색\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003evectorstores\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eChroma\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003etext_embeddings\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSentenceTransformerEmbeddings\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003etext_encoders\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eIdentityEncoder\u003c/span\u003e  # 원본 텍스트 보존\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003edocument_loaders\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTextLoader\u003c/span\u003e\n\n# 문서 경로 및 임베딩 모델 정의\ndocument_path = \u003cspan class=\"hljs-string\"\u003e\"your_document.txt\"\u003c/span\u003e\nembedding_model = \u003cspan class=\"hljs-title class_\"\u003eSentenceTransformerEmbeddings\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"all-mpnet-base-v2\"\u003c/span\u003e)\n\n# 문서 로드\ntext_loader = \u003cspan class=\"hljs-title class_\"\u003eTextLoader\u003c/span\u003e(document_path)\ndocuments = text_loader.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e()\n\n# 텍스트 인코더를 사용하여 \u003cspan class=\"hljs-title class_\"\u003eChromaDB\u003c/span\u003e 생성 (선택 사항)\npersist_directory = \u003cspan class=\"hljs-string\"\u003e\"chroma_db\"\u003c/span\u003e  \ntext_encoder = \u003cspan class=\"hljs-title class_\"\u003eIdentityEncoder\u003c/span\u003e()  \n\nvectordb = \u003cspan class=\"hljs-title class_\"\u003eChroma\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_documents\u003c/span\u003e(\n    documents=documents,\n    embedding=\u003cspan class=\"hljs-title function_\"\u003eembedding_model\u003c/span\u003e(),\n    text_encoder=text_encoder,\n    persist_directory=persist_directory\n)\n\n# 선택적 지속성\nvectordb.\u003cspan class=\"hljs-title function_\"\u003epersist\u003c/span\u003e()\n\n# 텍스트 검색 예시\nquery = \u003cspan class=\"hljs-string\"\u003e\"북극 해는 어디에 있나요?\"\u003c/span\u003e\n\n# 인코딩된 텍스트를 기반으로 검색 (임베딩 아님)\nresults = vectordb.\u003cspan class=\"hljs-title function_\"\u003esearch\u003c/span\u003e(query, k=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)  # 상위 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e개 결과 가져오기\n\n# 검색된 문서 출력\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e doc \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eresults\u003c/span\u003e:\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(doc)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"ChromaDB 검색 완료!\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e가상 문서 임베딩:\u003c/p\u003e\n\u003cp\u003eHyDE는 대형 언어 모델(Large Language Models, LLMs)을 활용하여 문서로부터 정보 검색을 개선하는 기술입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eStep 1: Query 이해하기: 모든 것은 사용자 쿼리로 시작됩니다. HyDE는 이 쿼리를 입력으로 받습니다.\u003c/p\u003e\n\u003cp\u003eStep 2: 가상 문서 생성: HyDE는 GPT-3과 같은 LLM을 사용하여 사용자 쿼리에 완벽한 답변이 될 것으로 믿는 가상 문서를 생성합니다. 이 문서에는 사실적인 정보 뿐만 아니라 창의적인 요소나 사용자 의도에 부합하는 설명이 포함될 수 있습니다.\u003c/p\u003e\n\u003cp\u003eStep 3: 가설 인코딩: 가상 문서가 생성된 후, HyDE는 문서 자체를 사용하지 않습니다. 대신, 문서의 의미를 수학적 벡터 표현으로 인코딩합니다. 이 벡터는 가상 답변 내의 핵심 개념과 정보를 포착합니다.\u003c/p\u003e\n\u003cp\u003eStep 4: 유사 문서 찾기: 이제 검색 과정이 시작됩니다. HyDE는 가상 문서를 나타내는 벡터를 사용하여 방대한 문서 컬렉션(보통 미리 인코딩된)을 검색합니다. 이것은 가상 문서의 벡터와 유사한 실제 문서를 탐색합니다. 유사성은 이 실제 문서들이 가상 답변과 유사한 방법으로 사용자 쿼리에 대응한다는 것을 나타냅니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e5단계: 검색된 문서를 활용하기: HyDE 프로세스를 기반으로 가장 관련성 높은 것으로 간주된 이러한 검색된 문서는 이후 RAG 시스템에 공급됩니다. RAG 내의 LLM은 이 문서들을 사용하여 사용자의 초기 쿼리에 대한 더 포괄적이고 유익한 응답을 생성할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e참고 자료:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://platform.openai.com/docs/assistants/overview\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://platform.openai.com/docs/assistants/overview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLinkedIn GitHub\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-17-5HacksToImproveYourRAGApplication"},"buildId":"Y-fCAg8BUV7y2HNFwX9AA","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>