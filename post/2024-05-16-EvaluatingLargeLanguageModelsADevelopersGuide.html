<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>대형 언어 모델 평가 개발자를 위한 안내 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="대형 언어 모델 평가 개발자를 위한 안내 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="대형 언어 모델 평가 개발자를 위한 안내 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide" data-gatsby-head="true"/><meta name="twitter:title" content="대형 언어 모델 평가 개발자를 위한 안내 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 04:18" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/Y-fCAg8BUV7y2HNFwX9AA/_buildManifest.js" defer=""></script><script src="/_next/static/Y-fCAg8BUV7y2HNFwX9AA/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">대형 언어 모델 평가 개발자를 위한 안내</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="대형 언어 모델 평가 개발자를 위한 안내" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png" alt="image"></p>
<p>대형 언어 모델 (LLM)인 GPT-4, Claude, LLama 및 Gemini는 AI 커뮤니티에 많은 기여를 했습니다. 기관들이 견고한 LLM 기반 애플리케이션을 구축하는 데 도움을 주었죠. 그럼에도 불구하고, LLM은 환각을 하며 종종 진실 같은 자신만의 이야기를 만들어 냅니다. AI에 대한 안전하고 안정적이며 책임감 있는 LLM 사용에 준수하는 것은 중요해졌습니다. 속도 뿐만 아니라 정확성과 성능 면에서 이러한 LLM을 평가하는 것이 권장됩니다.</p>
<p>오늘은 간단한 자습서를 통해 더 나은 성능을 위해 이러한 LLM을 어떻게 평가할 수 있는지 살펴볼 것입니다. 하지만 우선, LLM 평가가 무엇인지에 대해 먼저 이해해 보겠습니다.</p>
<h1>LLM 평가란?</h1>
<p>LLM 평가는 LLM의 성능을 얼마나 잘 이해하는지에 중요합니다. 이는 개발자가 모델의 강점과 약점을 파악하여 실제 응용 프로그램에서 효과적으로 작동하도록 보장합니다. 이 평가 프로세스는 편향된 또는 오도하는 콘텐츠와 같은 위험을 완화하는 데도 도움이 됩니다. LLM 평가에는 두 가지 주요 유형이 있습니다:</p>
<ul>
<li>모델 평가: LLM 자체의 핵심 능력을 평가합니다.</li>
<li>시스템 평가: 특정 프로그램 내에서 또는 사용자 입력과 함께 수행되는 방식을 살펴봅니다.</li>
</ul>
<h1>LLM 평가 지표</h1>
<p>다음은 제품화하기 전에 고려해야 할 가장 중요한 평가 지표 목록입니다.</p>
<p>LLM(Large Language Model)를 평가하는 데 중요한 것은 적절한 측정 지표를 갖추는 것입니다. 이러한 지표는 주어진 기준에 따라 LLM의 출력물을 평가하는 점수 메커니즘으로 작용합니다. 일반적인 지표 및 기준은 다음과 같습니다:</p>
<ul>
<li>응답 완성도 및 간결성: LLM 응답이 사용자 쿼리를 완벽하게 해결하는지 여부를 결정합니다. 간결성은 생성된 응답이 얼마나 관련성이 있는지를 결정합니다.</li>
<li>텍스트 유사성 지표: 생성된 텍스트를 참조나 기준 텍스트와 비교하여 그들이 얼마나 유사한지를 측정합니다. 그런 다음 특정 LLM이 어떻게 수행했는지 이해할 수 있도록 점수가 부여됩니다.</li>
<li>질문 응답 정확도: LLM이 사실적인 정확성 기준에 따라 제기된 질문에 얼마나 잘 대답하는지 측정합니다.</li>
<li>관련성: 주어진 프롬프트나 사용자 질문에 대한 LLM 응답의 적절성을 결정합니다.</li>
<li>망상 지표: LLM이 정보를 얼마나 만들어 내거나 특정 프롬프트에 대해 편향된 출력을 공유하는지 식별합니다.</li>
<li>유해성: LLM의 출력물에서 모욕적이거나 해로운 언어의 백분율을 결정합니다.</li>
<li>작업별 지표: 요약, 번역 등 작업 유형 및 응용 프로그램에 따라 다양한 지표가 존재합니다(BLEU 점수 등).</li>
</ul>
<p>LLM 평가 프레임워크 및 도구</p>
<p>LLM 평가 프레임워크와 도구는 언어 모델의 성능, 신뢰성 및 공정성을 측정하고 향상시키는 데 표준화된 벤치마크를 제공하기 때문에 중요합니다. 다음은 LLM 평가 프레임워크와 도구 중 일부입니다:</p>
<ul>
<li>DeepEval은 기업이 LLM 애플리케이션을 평가할 수 있도록 돕는 오픈 소스 프레임워크입니다. 주요 메트릭인 문맥 기억, 답변 관련성 및 충실도 등 다양한 중요 메트릭에 대한 성능을 측정합니다.</li>
<li>promptfoo는 LLM 출력 품질과 성능을 평가하기 위한 CLI 및 라이브러리입니다. promptfoo를 사용하면 사전 정의된 테스트를 사용하여 프롬프트와 모델을 체계적으로 테스트할 수 있습니다.</li>
<li>EleutherAI LM Eval은 최소한의 세밀한 조정으로 다양한 작업에 걸쳐 소량 평가와 성능을 수행합니다.</li>
<li>MMLU는 제로샷 및 원샷 설정에서 다양한 주제에 대해 모델을 테스트하는 LLM 평가 프레임워크입니다.</li>
<li>BLEU(BiLingual Evaluation Understudy)는 기계 번역된 텍스트의 유사성을 이미 벤치마킹된 고품질 참조 번역과 측정하는 메트릭입니다. 평가는 0에서 1까지의 범위로 이루어집니다.</li>
<li>SQuAD(Stanford Question Answering Dataset)는 질문 응답 작업을 위해 LLM을 평가하기 위한 데이터셋입니다. 특정 답변과 관련된 문맥 패스 및 해당하는 질문이 포함됩니다.</li>
<li>OpenAI Evals는 OpenAI에 의해 LLM을 평가하기 위한 표준 프레임워크이자 벤치마크의 오픈 소스 레지스트리입니다. 이 프레임워크는 LLM 모델의 정확성을 보장하기 위해 사용됩니다.</li>
<li>UpTrain은 오픈 소스 LLM 평가 도구입니다. 정확성, 환각 및 독성을 포함한 다양한 측면에서 LLM 응답을 확인하기 위한 미리 작성된 메트릭을 제공합니다.</li>
<li>H2O LLM EvalGPT는 다양한 작업과 벤치마크를 통해 모델의 성능을 이해하는 오픈 도구입니다.</li>
</ul>
<h1>UpTrain을 사용한 LLM 평가: 노트북 자습서</h1>
<p>만약 아직 하지 않았다면, 무료 SingleStore 평가판에 가입하여 자습서에 따라 진행해 보세요. SingleStore 노트북을 사용하게 될 것인데, 이는 Jupyter 노트북과 유사하지만 통합 데이터베이스의 추가 기능과 혜택을 갖추고 있습니다.</p>
<p>가입하면 워크스페이스를 생성해야 합니다.</p>
<img src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_1.png">
<p>메인 대시보드로 이동하여 개발 탭을 클릭하세요.</p>
<img src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_2.png">
<p>새 노트북을 만들고 원하는 이름을 지정하세요.</p>
<img src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_3.png">
<p>이제 시작할 수 있어요. 여기에 표시된 모든 코드를 생성한 노트북에 추가하세요.</p>
<p>'evaluate_llm'이라는 데이터베이스를 생성하세요.</p>
<pre><code class="hljs language-js">%%sql

<span class="hljs-variable constant_">DROP</span> <span class="hljs-variable constant_">DATABASE</span> <span class="hljs-variable constant_">IF</span> <span class="hljs-variable constant_">EXISTS</span> evaluate_llm;
<span class="hljs-variable constant_">CREATE</span> <span class="hljs-variable constant_">DATABASE</span> evaluate_llm;
</code></pre>
<p>필요한 패키지를 설치하세요</p>
<pre><code class="hljs language-js">!pip install uptrain==<span class="hljs-number">0.5</span><span class="hljs-number">.0</span> openai==<span class="hljs-number">1.3</span><span class="hljs-number">.3</span> langchain==<span class="hljs-number">0.1</span><span class="hljs-number">.4</span> tiktoken==<span class="hljs-number">0.5</span><span class="hljs-number">.2</span> --quiet
</code></pre>
<p>다음 단계는 필요한 환경 변수를 설정하는 것입니다 — 주로 openai 키(응답 생성을 위해), <code>singlestoredb</code>(컨텍스트 검색을 위해) 그리고 <code>uptrain API 키</code>(응답 평가를 위해)입니다. UpTrain에 계정을 생성하고 무료로 API 키를 생성할 수 있습니다.</p>
<p>자세한 내용은 <a href="https://uptrain.ai/" rel="nofollow" target="_blank">https://uptrain.ai/</a> 를 방문해주세요.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> getpass
<span class="hljs-keyword">import</span> os

os.environ[<span class="hljs-string">'OPENAI_API_KEY'</span>] = getpass.getpass(<span class="hljs-string">'OpenAI API Key: '</span>)

<span class="hljs-keyword">import</span> openai

client = openai.OpenAI()
</code></pre>
<p>Add the UpTrain API key.</p>
<pre><code class="hljs language-python">UPTRAIN_API_KEY = getpass.getpass(<span class="hljs-string">'Uptrain API Key: '</span>)
</code></pre>
<p>Import necessary modules</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> singlestoredb
<span class="hljs-keyword">from</span> uptrain <span class="hljs-keyword">import</span> <span class="hljs-title class_">APIClient</span>, <span class="hljs-title class_">Evals</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">vectorstores</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SingleStoreDB</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">embeddings</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">OpenAIEmbeddings</span>
</code></pre>
<p>웹에서 데이터를 로드합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain.<span class="hljs-property">document_loaders</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">WebBaseLoader</span>

loader = <span class="hljs-title class_">WebBaseLoader</span>(<span class="hljs-string">'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/generative-ai-studio'</span>)
data = loader.<span class="hljs-title function_">load</span>()
</code></pre>
<p>다음으로 데이터를 분할합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain.<span class="hljs-property">text_splitter</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">RecursiveCharacterTextSplitter</span>

text_splitter = <span class="hljs-title class_">RecursiveCharacterTextSplitter</span>(chunk_size=<span class="hljs-number">200</span>, chunk_overlap=<span class="hljs-number">0</span>)
all_splits = text_splitter.<span class="hljs-title function_">split_documents</span>(data)
</code></pre>
<p>OpenAI 임베딩을 사용하여 SingleStore 데이터베이스를 설정합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">vectorstores</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SingleStoreDB</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">embeddings</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">OpenAIEmbeddings</span>
<span class="hljs-keyword">from</span> singlestoredb <span class="hljs-keyword">import</span> create_engine

conn = <span class="hljs-title function_">create_engine</span>().<span class="hljs-title function_">connect</span>()

vectorstore = <span class="hljs-title class_">SingleStoreDB</span>.<span class="hljs-title function_">from_documents</span>(documents=all_splits,
                                           embedding=<span class="hljs-title class_">OpenAIEmbeddings</span>(),
                                           table_name=<span class="hljs-string">'vertex_ai_docs_chunk_size_200'</span>)
</code></pre>
<p>완전한 단계별 노트북 코드는 저희 스페이스에 있습니다.</p>
<p>마침내 오픈 소스 LLM 평가 도구인 UpTrain을 사용하여 평가를 실행할 것입니다. UpTrain 대시보드에 액세스하여 평가 결과를 확인할 수 있을 겁니다.</p>
<p>다양한 청크 크기로 실험해 보면 다른 결과를 확인할 수 있을 겁니다.</p>
<p><img src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_4.png" alt="이미지"></p>
<p>UpTrain의 API 클라이언트는 또한 입력 데이터를 가져와 실행할 체크 목록과 실험에 연결된 열의 이름과 함께 해당 데이터를 평가하는 <code>evaluate_experiments</code> 메서드를 제공합니다.</p>
<p><img src="/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_5.png" alt="image"></p>
<p>튜토리얼에서 보여준 LLM 평가 접근 방식과 도구를 따라가면, LLM의 장단점을 보다 깊게 이해할 수 있습니다. 이를 통해 우리는 그들의 능력을 책임 있게 활용하여 사실 불일치와 편향과 관련된 잠재적 위험을 완화할 수 있습니다. 궁극적으로는, 효과적인 LLM 평가는 다양한 LLM 기반 응용 프로그램에서 인공 지능의 윤리적 발전을 위한 신뢰 구축과 증진을 위한 길을 열어줍니다.</p>
<p>오늘 'UpTrain과 함께 LLM 평가하기' 노트북을 시도해보세요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"대형 언어 모델 평가 개발자를 위한 안내","description":"","date":"2024-05-16 04:18","slug":"2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide","content":"\n\n![image](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png)\n\n대형 언어 모델 (LLM)인 GPT-4, Claude, LLama 및 Gemini는 AI 커뮤니티에 많은 기여를 했습니다. 기관들이 견고한 LLM 기반 애플리케이션을 구축하는 데 도움을 주었죠. 그럼에도 불구하고, LLM은 환각을 하며 종종 진실 같은 자신만의 이야기를 만들어 냅니다. AI에 대한 안전하고 안정적이며 책임감 있는 LLM 사용에 준수하는 것은 중요해졌습니다. 속도 뿐만 아니라 정확성과 성능 면에서 이러한 LLM을 평가하는 것이 권장됩니다.\n\n오늘은 간단한 자습서를 통해 더 나은 성능을 위해 이러한 LLM을 어떻게 평가할 수 있는지 살펴볼 것입니다. 하지만 우선, LLM 평가가 무엇인지에 대해 먼저 이해해 보겠습니다.\n\n# LLM 평가란?\n\n\n\nLLM 평가는 LLM의 성능을 얼마나 잘 이해하는지에 중요합니다. 이는 개발자가 모델의 강점과 약점을 파악하여 실제 응용 프로그램에서 효과적으로 작동하도록 보장합니다. 이 평가 프로세스는 편향된 또는 오도하는 콘텐츠와 같은 위험을 완화하는 데도 도움이 됩니다. LLM 평가에는 두 가지 주요 유형이 있습니다:\n\n- 모델 평가: LLM 자체의 핵심 능력을 평가합니다.\n- 시스템 평가: 특정 프로그램 내에서 또는 사용자 입력과 함께 수행되는 방식을 살펴봅니다.\n\n# LLM 평가 지표\n\n다음은 제품화하기 전에 고려해야 할 가장 중요한 평가 지표 목록입니다.\n\n\n\nLLM(Large Language Model)를 평가하는 데 중요한 것은 적절한 측정 지표를 갖추는 것입니다. 이러한 지표는 주어진 기준에 따라 LLM의 출력물을 평가하는 점수 메커니즘으로 작용합니다. 일반적인 지표 및 기준은 다음과 같습니다:\n\n- 응답 완성도 및 간결성: LLM 응답이 사용자 쿼리를 완벽하게 해결하는지 여부를 결정합니다. 간결성은 생성된 응답이 얼마나 관련성이 있는지를 결정합니다.\n- 텍스트 유사성 지표: 생성된 텍스트를 참조나 기준 텍스트와 비교하여 그들이 얼마나 유사한지를 측정합니다. 그런 다음 특정 LLM이 어떻게 수행했는지 이해할 수 있도록 점수가 부여됩니다.\n- 질문 응답 정확도: LLM이 사실적인 정확성 기준에 따라 제기된 질문에 얼마나 잘 대답하는지 측정합니다.\n- 관련성: 주어진 프롬프트나 사용자 질문에 대한 LLM 응답의 적절성을 결정합니다.\n- 망상 지표: LLM이 정보를 얼마나 만들어 내거나 특정 프롬프트에 대해 편향된 출력을 공유하는지 식별합니다.\n- 유해성: LLM의 출력물에서 모욕적이거나 해로운 언어의 백분율을 결정합니다.\n- 작업별 지표: 요약, 번역 등 작업 유형 및 응용 프로그램에 따라 다양한 지표가 존재합니다(BLEU 점수 등).\n\nLLM 평가 프레임워크 및 도구\n\nLLM 평가 프레임워크와 도구는 언어 모델의 성능, 신뢰성 및 공정성을 측정하고 향상시키는 데 표준화된 벤치마크를 제공하기 때문에 중요합니다. 다음은 LLM 평가 프레임워크와 도구 중 일부입니다:\n\n\n\n- DeepEval은 기업이 LLM 애플리케이션을 평가할 수 있도록 돕는 오픈 소스 프레임워크입니다. 주요 메트릭인 문맥 기억, 답변 관련성 및 충실도 등 다양한 중요 메트릭에 대한 성능을 측정합니다.\n- promptfoo는 LLM 출력 품질과 성능을 평가하기 위한 CLI 및 라이브러리입니다. promptfoo를 사용하면 사전 정의된 테스트를 사용하여 프롬프트와 모델을 체계적으로 테스트할 수 있습니다.\n- EleutherAI LM Eval은 최소한의 세밀한 조정으로 다양한 작업에 걸쳐 소량 평가와 성능을 수행합니다.\n- MMLU는 제로샷 및 원샷 설정에서 다양한 주제에 대해 모델을 테스트하는 LLM 평가 프레임워크입니다.\n- BLEU(BiLingual Evaluation Understudy)는 기계 번역된 텍스트의 유사성을 이미 벤치마킹된 고품질 참조 번역과 측정하는 메트릭입니다. 평가는 0에서 1까지의 범위로 이루어집니다.\n- SQuAD(Stanford Question Answering Dataset)는 질문 응답 작업을 위해 LLM을 평가하기 위한 데이터셋입니다. 특정 답변과 관련된 문맥 패스 및 해당하는 질문이 포함됩니다.\n- OpenAI Evals는 OpenAI에 의해 LLM을 평가하기 위한 표준 프레임워크이자 벤치마크의 오픈 소스 레지스트리입니다. 이 프레임워크는 LLM 모델의 정확성을 보장하기 위해 사용됩니다.\n- UpTrain은 오픈 소스 LLM 평가 도구입니다. 정확성, 환각 및 독성을 포함한 다양한 측면에서 LLM 응답을 확인하기 위한 미리 작성된 메트릭을 제공합니다.\n- H2O LLM EvalGPT는 다양한 작업과 벤치마크를 통해 모델의 성능을 이해하는 오픈 도구입니다.\n\n# UpTrain을 사용한 LLM 평가: 노트북 자습서\n\n\n만약 아직 하지 않았다면, 무료 SingleStore 평가판에 가입하여 자습서에 따라 진행해 보세요. SingleStore 노트북을 사용하게 될 것인데, 이는 Jupyter 노트북과 유사하지만 통합 데이터베이스의 추가 기능과 혜택을 갖추고 있습니다.\n\n가입하면 워크스페이스를 생성해야 합니다.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_1.png\" /\u003e\n\n메인 대시보드로 이동하여 개발 탭을 클릭하세요.\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_2.png\" /\u003e\n\n새 노트북을 만들고 원하는 이름을 지정하세요.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_3.png\" /\u003e\n\n이제 시작할 수 있어요. 여기에 표시된 모든 코드를 생성한 노트북에 추가하세요.\n\n'evaluate_llm'이라는 데이터베이스를 생성하세요.\n\n```js\n%%sql\n\nDROP DATABASE IF EXISTS evaluate_llm;\nCREATE DATABASE evaluate_llm;\n```\n\n\n\n필요한 패키지를 설치하세요\n\n```js\n!pip install uptrain==0.5.0 openai==1.3.3 langchain==0.1.4 tiktoken==0.5.2 --quiet\n```\n\n다음 단계는 필요한 환경 변수를 설정하는 것입니다 — 주로 openai 키(응답 생성을 위해), `singlestoredb`(컨텍스트 검색을 위해) 그리고 `uptrain API 키`(응답 평가를 위해)입니다. UpTrain에 계정을 생성하고 무료로 API 키를 생성할 수 있습니다.\n\n자세한 내용은 https://uptrain.ai/ 를 방문해주세요.\n\n\n\n```python\nimport getpass\nimport os\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n\nimport openai\n\nclient = openai.OpenAI()\n```\n\nAdd the UpTrain API key.\n\n```python\nUPTRAIN_API_KEY = getpass.getpass('Uptrain API Key: ')\n```\n\nImport necessary modules\n\n\n\n\n```js\nimport singlestoredb\nfrom uptrain import APIClient, Evals\nfrom langchain.vectorstores import SingleStoreDB\nfrom langchain.embeddings import OpenAIEmbeddings\n```\n\n웹에서 데이터를 로드합니다.\n\n```js\nfrom langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader('https://cloud.google.com/vertex-ai/docs/generative-ai/learn/generative-ai-studio')\ndata = loader.load()\n```\n\n다음으로 데이터를 분할합니다.\n\n\n\n```js\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n```\n\nOpenAI 임베딩을 사용하여 SingleStore 데이터베이스를 설정합니다.\n\n```js\nimport os\nfrom langchain.vectorstores import SingleStoreDB\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom singlestoredb import create_engine\n\nconn = create_engine().connect()\n\nvectorstore = SingleStoreDB.from_documents(documents=all_splits,\n                                           embedding=OpenAIEmbeddings(),\n                                           table_name='vertex_ai_docs_chunk_size_200')\n```\n\n완전한 단계별 노트북 코드는 저희 스페이스에 있습니다.\n\n\n\n마침내 오픈 소스 LLM 평가 도구인 UpTrain을 사용하여 평가를 실행할 것입니다. UpTrain 대시보드에 액세스하여 평가 결과를 확인할 수 있을 겁니다.\n\n다양한 청크 크기로 실험해 보면 다른 결과를 확인할 수 있을 겁니다.\n\n![이미지](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_4.png)\n\nUpTrain의 API 클라이언트는 또한 입력 데이터를 가져와 실행할 체크 목록과 실험에 연결된 열의 이름과 함께 해당 데이터를 평가하는 `evaluate_experiments` 메서드를 제공합니다.\n\n\n\n![image](/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_5.png)\n\n튜토리얼에서 보여준 LLM 평가 접근 방식과 도구를 따라가면, LLM의 장단점을 보다 깊게 이해할 수 있습니다. 이를 통해 우리는 그들의 능력을 책임 있게 활용하여 사실 불일치와 편향과 관련된 잠재적 위험을 완화할 수 있습니다. 궁극적으로는, 효과적인 LLM 평가는 다양한 LLM 기반 응용 프로그램에서 인공 지능의 윤리적 발전을 위한 신뢰 구축과 증진을 위한 길을 열어줍니다.\n\n오늘 'UpTrain과 함께 LLM 평가하기' 노트북을 시도해보세요!","ogImage":{"url":"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png"},"coverImage":"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e대형 언어 모델 (LLM)인 GPT-4, Claude, LLama 및 Gemini는 AI 커뮤니티에 많은 기여를 했습니다. 기관들이 견고한 LLM 기반 애플리케이션을 구축하는 데 도움을 주었죠. 그럼에도 불구하고, LLM은 환각을 하며 종종 진실 같은 자신만의 이야기를 만들어 냅니다. AI에 대한 안전하고 안정적이며 책임감 있는 LLM 사용에 준수하는 것은 중요해졌습니다. 속도 뿐만 아니라 정확성과 성능 면에서 이러한 LLM을 평가하는 것이 권장됩니다.\u003c/p\u003e\n\u003cp\u003e오늘은 간단한 자습서를 통해 더 나은 성능을 위해 이러한 LLM을 어떻게 평가할 수 있는지 살펴볼 것입니다. 하지만 우선, LLM 평가가 무엇인지에 대해 먼저 이해해 보겠습니다.\u003c/p\u003e\n\u003ch1\u003eLLM 평가란?\u003c/h1\u003e\n\u003cp\u003eLLM 평가는 LLM의 성능을 얼마나 잘 이해하는지에 중요합니다. 이는 개발자가 모델의 강점과 약점을 파악하여 실제 응용 프로그램에서 효과적으로 작동하도록 보장합니다. 이 평가 프로세스는 편향된 또는 오도하는 콘텐츠와 같은 위험을 완화하는 데도 도움이 됩니다. LLM 평가에는 두 가지 주요 유형이 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e모델 평가: LLM 자체의 핵심 능력을 평가합니다.\u003c/li\u003e\n\u003cli\u003e시스템 평가: 특정 프로그램 내에서 또는 사용자 입력과 함께 수행되는 방식을 살펴봅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eLLM 평가 지표\u003c/h1\u003e\n\u003cp\u003e다음은 제품화하기 전에 고려해야 할 가장 중요한 평가 지표 목록입니다.\u003c/p\u003e\n\u003cp\u003eLLM(Large Language Model)를 평가하는 데 중요한 것은 적절한 측정 지표를 갖추는 것입니다. 이러한 지표는 주어진 기준에 따라 LLM의 출력물을 평가하는 점수 메커니즘으로 작용합니다. 일반적인 지표 및 기준은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e응답 완성도 및 간결성: LLM 응답이 사용자 쿼리를 완벽하게 해결하는지 여부를 결정합니다. 간결성은 생성된 응답이 얼마나 관련성이 있는지를 결정합니다.\u003c/li\u003e\n\u003cli\u003e텍스트 유사성 지표: 생성된 텍스트를 참조나 기준 텍스트와 비교하여 그들이 얼마나 유사한지를 측정합니다. 그런 다음 특정 LLM이 어떻게 수행했는지 이해할 수 있도록 점수가 부여됩니다.\u003c/li\u003e\n\u003cli\u003e질문 응답 정확도: LLM이 사실적인 정확성 기준에 따라 제기된 질문에 얼마나 잘 대답하는지 측정합니다.\u003c/li\u003e\n\u003cli\u003e관련성: 주어진 프롬프트나 사용자 질문에 대한 LLM 응답의 적절성을 결정합니다.\u003c/li\u003e\n\u003cli\u003e망상 지표: LLM이 정보를 얼마나 만들어 내거나 특정 프롬프트에 대해 편향된 출력을 공유하는지 식별합니다.\u003c/li\u003e\n\u003cli\u003e유해성: LLM의 출력물에서 모욕적이거나 해로운 언어의 백분율을 결정합니다.\u003c/li\u003e\n\u003cli\u003e작업별 지표: 요약, 번역 등 작업 유형 및 응용 프로그램에 따라 다양한 지표가 존재합니다(BLEU 점수 등).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLLM 평가 프레임워크 및 도구\u003c/p\u003e\n\u003cp\u003eLLM 평가 프레임워크와 도구는 언어 모델의 성능, 신뢰성 및 공정성을 측정하고 향상시키는 데 표준화된 벤치마크를 제공하기 때문에 중요합니다. 다음은 LLM 평가 프레임워크와 도구 중 일부입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeepEval은 기업이 LLM 애플리케이션을 평가할 수 있도록 돕는 오픈 소스 프레임워크입니다. 주요 메트릭인 문맥 기억, 답변 관련성 및 충실도 등 다양한 중요 메트릭에 대한 성능을 측정합니다.\u003c/li\u003e\n\u003cli\u003epromptfoo는 LLM 출력 품질과 성능을 평가하기 위한 CLI 및 라이브러리입니다. promptfoo를 사용하면 사전 정의된 테스트를 사용하여 프롬프트와 모델을 체계적으로 테스트할 수 있습니다.\u003c/li\u003e\n\u003cli\u003eEleutherAI LM Eval은 최소한의 세밀한 조정으로 다양한 작업에 걸쳐 소량 평가와 성능을 수행합니다.\u003c/li\u003e\n\u003cli\u003eMMLU는 제로샷 및 원샷 설정에서 다양한 주제에 대해 모델을 테스트하는 LLM 평가 프레임워크입니다.\u003c/li\u003e\n\u003cli\u003eBLEU(BiLingual Evaluation Understudy)는 기계 번역된 텍스트의 유사성을 이미 벤치마킹된 고품질 참조 번역과 측정하는 메트릭입니다. 평가는 0에서 1까지의 범위로 이루어집니다.\u003c/li\u003e\n\u003cli\u003eSQuAD(Stanford Question Answering Dataset)는 질문 응답 작업을 위해 LLM을 평가하기 위한 데이터셋입니다. 특정 답변과 관련된 문맥 패스 및 해당하는 질문이 포함됩니다.\u003c/li\u003e\n\u003cli\u003eOpenAI Evals는 OpenAI에 의해 LLM을 평가하기 위한 표준 프레임워크이자 벤치마크의 오픈 소스 레지스트리입니다. 이 프레임워크는 LLM 모델의 정확성을 보장하기 위해 사용됩니다.\u003c/li\u003e\n\u003cli\u003eUpTrain은 오픈 소스 LLM 평가 도구입니다. 정확성, 환각 및 독성을 포함한 다양한 측면에서 LLM 응답을 확인하기 위한 미리 작성된 메트릭을 제공합니다.\u003c/li\u003e\n\u003cli\u003eH2O LLM EvalGPT는 다양한 작업과 벤치마크를 통해 모델의 성능을 이해하는 오픈 도구입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eUpTrain을 사용한 LLM 평가: 노트북 자습서\u003c/h1\u003e\n\u003cp\u003e만약 아직 하지 않았다면, 무료 SingleStore 평가판에 가입하여 자습서에 따라 진행해 보세요. SingleStore 노트북을 사용하게 될 것인데, 이는 Jupyter 노트북과 유사하지만 통합 데이터베이스의 추가 기능과 혜택을 갖추고 있습니다.\u003c/p\u003e\n\u003cp\u003e가입하면 워크스페이스를 생성해야 합니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_1.png\"\u003e\n\u003cp\u003e메인 대시보드로 이동하여 개발 탭을 클릭하세요.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_2.png\"\u003e\n\u003cp\u003e새 노트북을 만들고 원하는 이름을 지정하세요.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_3.png\"\u003e\n\u003cp\u003e이제 시작할 수 있어요. 여기에 표시된 모든 코드를 생성한 노트북에 추가하세요.\u003c/p\u003e\n\u003cp\u003e'evaluate_llm'이라는 데이터베이스를 생성하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e%%sql\n\n\u003cspan class=\"hljs-variable constant_\"\u003eDROP\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eDATABASE\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eIF\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eEXISTS\u003c/span\u003e evaluate_llm;\n\u003cspan class=\"hljs-variable constant_\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eDATABASE\u003c/span\u003e evaluate_llm;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e필요한 패키지를 설치하세요\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!pip install uptrain==\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e openai==\u003cspan class=\"hljs-number\"\u003e1.3\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003e langchain==\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.4\u003c/span\u003e tiktoken==\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003e --quiet\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음 단계는 필요한 환경 변수를 설정하는 것입니다 — 주로 openai 키(응답 생성을 위해), \u003ccode\u003esinglestoredb\u003c/code\u003e(컨텍스트 검색을 위해) 그리고 \u003ccode\u003euptrain API 키\u003c/code\u003e(응답 평가를 위해)입니다. UpTrain에 계정을 생성하고 무료로 API 키를 생성할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e자세한 내용은 \u003ca href=\"https://uptrain.ai/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://uptrain.ai/\u003c/a\u003e 를 방문해주세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e getpass\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\nos.environ[\u003cspan class=\"hljs-string\"\u003e'OPENAI_API_KEY'\u003c/span\u003e] = getpass.getpass(\u003cspan class=\"hljs-string\"\u003e'OpenAI API Key: '\u003c/span\u003e)\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e openai\n\nclient = openai.OpenAI()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAdd the UpTrain API key.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003eUPTRAIN_API_KEY = getpass.getpass(\u003cspan class=\"hljs-string\"\u003e'Uptrain API Key: '\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eImport necessary modules\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e singlestoredb\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e uptrain \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAPIClient\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eEvals\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003evectorstores\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSingleStoreDB\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003eembeddings\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOpenAIEmbeddings\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e웹에서 데이터를 로드합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003edocument_loaders\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eWebBaseLoader\u003c/span\u003e\n\nloader = \u003cspan class=\"hljs-title class_\"\u003eWebBaseLoader\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/generative-ai-studio'\u003c/span\u003e)\ndata = loader.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음으로 데이터를 분할합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003etext_splitter\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e\n\ntext_splitter = \u003cspan class=\"hljs-title class_\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e(chunk_size=\u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e, chunk_overlap=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\nall_splits = text_splitter.\u003cspan class=\"hljs-title function_\"\u003esplit_documents\u003c/span\u003e(data)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOpenAI 임베딩을 사용하여 SingleStore 데이터베이스를 설정합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003evectorstores\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSingleStoreDB\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003eembeddings\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOpenAIEmbeddings\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e singlestoredb \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e create_engine\n\nconn = \u003cspan class=\"hljs-title function_\"\u003ecreate_engine\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003econnect\u003c/span\u003e()\n\nvectorstore = \u003cspan class=\"hljs-title class_\"\u003eSingleStoreDB\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_documents\u003c/span\u003e(documents=all_splits,\n                                           embedding=\u003cspan class=\"hljs-title class_\"\u003eOpenAIEmbeddings\u003c/span\u003e(),\n                                           table_name=\u003cspan class=\"hljs-string\"\u003e'vertex_ai_docs_chunk_size_200'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e완전한 단계별 노트북 코드는 저희 스페이스에 있습니다.\u003c/p\u003e\n\u003cp\u003e마침내 오픈 소스 LLM 평가 도구인 UpTrain을 사용하여 평가를 실행할 것입니다. UpTrain 대시보드에 액세스하여 평가 결과를 확인할 수 있을 겁니다.\u003c/p\u003e\n\u003cp\u003e다양한 청크 크기로 실험해 보면 다른 결과를 확인할 수 있을 겁니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eUpTrain의 API 클라이언트는 또한 입력 데이터를 가져와 실행할 체크 목록과 실험에 연결된 열의 이름과 함께 해당 데이터를 평가하는 \u003ccode\u003eevaluate_experiments\u003c/code\u003e 메서드를 제공합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e튜토리얼에서 보여준 LLM 평가 접근 방식과 도구를 따라가면, LLM의 장단점을 보다 깊게 이해할 수 있습니다. 이를 통해 우리는 그들의 능력을 책임 있게 활용하여 사실 불일치와 편향과 관련된 잠재적 위험을 완화할 수 있습니다. 궁극적으로는, 효과적인 LLM 평가는 다양한 LLM 기반 응용 프로그램에서 인공 지능의 윤리적 발전을 위한 신뢰 구축과 증진을 위한 길을 열어줍니다.\u003c/p\u003e\n\u003cp\u003e오늘 'UpTrain과 함께 LLM 평가하기' 노트북을 시도해보세요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-EvaluatingLargeLanguageModelsADevelopersGuide"},"buildId":"Y-fCAg8BUV7y2HNFwX9AA","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>