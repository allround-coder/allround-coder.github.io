<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>오디오 데이터에 대한 대화형 감정 분석 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-06-20-ConversationalSentimentAnalysisonAudioData" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="오디오 데이터에 대한 대화형 감정 분석 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="오디오 데이터에 대한 대화형 감정 분석 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-06-20-ConversationalSentimentAnalysisonAudioData" data-gatsby-head="true"/><meta name="twitter:title" content="오디오 데이터에 대한 대화형 감정 분석 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 04:41" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/Y-fCAg8BUV7y2HNFwX9AA/_buildManifest.js" defer=""></script><script src="/_next/static/Y-fCAg8BUV7y2HNFwX9AA/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">오디오 데이터에 대한 대화형 감정 분석</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="오디오 데이터에 대한 대화형 감정 분석" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-ConversationalSentimentAnalysisonAudioData&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_0.png">
<p>감성 분석 또는 의견 분석은 자연어 처리(NLP)에서 널리 사용되는 작업입니다. NLP 기술을 텍스트 데이터에 특히 적용하는 맥락에서, 주요 목표는 주어진 텍스트를 다양한 감성 클래스에 분류할 수 있는 모델을 훈련하는 것입니다. 감성 분류기의 고수준 개요는 아래 이미지에 나와 있습니다.</p>
<img src="/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_1.png">
<p>예를 들어, 세 가지 클래스 분류 문제의 클래스는 긍정적, 부정적 및 중립일 수 있습니다. 세 가지 클래스의 감성 분석 문제의 예는 인기 있는 Twitter 감성 분석 데이터 세트입니다. 이 데이터는 트위터 사용자들이 게시한 다국어 트윗에 대한 Entity-level 감성 분석 작업입니다.</p>
<div class="content-ad"></div>
<p>과거의 자연어처리(NLP) 연구 및 개발 대부분은 주로 텍스트에 감성 분석을 적용하는 데 중점을 두었습니다. 그러나 최근에는 사용자들 사이에서 음성 기반 상호 작용 도구의 대규모 채택과 인기를 볼 수 있었으며, 이는 연구자들과 기관들을 음성 영역에서 감성 분류기를 구축하도록 이끕니다.</p>
<p>따라서 이 게시물에서는 AssemblyAI API와 Python을 사용하여 대화 데이터에 감성 분석 시스템을 구축하는 방법을 보여줄 것입니다. 이 종단 간 시스템은 엄격한 고객 지원 및 피드백 평가와 관련이 있는 영역에서 광범위한 적용 가능성을 지니며, 특히 음성 도메인에서 중요하고 가치 있는 문제를 해결하는 데 도움이 됩니다. 마지막으로 얻은 결과물을 이해하기 쉽게 향상시키고 데이터에서 적절한 통찰을 얻기 위한 분석을 보여줄 것입니다.</p>
<p>이 글의 코드는 여기에서 찾을 수 있습니다. 게시물의 주요 내용은 다음과 같습니다:</p>
<ul>
<li>대화형 오디오 데이터에 대한 감성 분석</li>
<li>감성 분석 결과</li>
<li>감성 분석 통찰력</li>
</ul>
<div class="content-ad"></div>
<h1>대화 오디오 데이터에 대한 감정 분석</h1>
<p>이 섹션에서는, 녹음된 음성 대화 조각에서 개별 문장을 세 가지 감정 클래스로 분류하는 AssemblyAI API의 사용을 보여드리겠습니다: 긍정적, 부정적 및 중립적.</p>
<p><img src="/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_2.png" alt="이미지"></p>
<h2>단계 1: 요구 사항 설치</h2>
<div class="content-ad"></div>
<p>감정 분류기를 구축하는 데 필요한 요소가 매우 적습니다. Python 라이브러리 관점에서 requests 패키지만 필요합니다. 아래와 같이 수행할 수 있습니다:</p>
<pre><code class="hljs language-js">pip install requests
</code></pre>
<h2>단계 2: API 토큰 생성</h2>
<p>다음 단계는 AssemblyAI 웹사이트에 계정을 생성하는 것입니다. 이 과정은 무료로 진행할 수 있습니다. 계정을 생성하면 프라이빗 API 액세스 키를 받게 되는데, 이것을 사용하여 음성을 텍스트로 변환하는 모델에 접근할 것입니다.</p>
<div class="content-ad"></div>
<h2>단계 3: 오디오 파일 업로드</h2>
<p>이 튜토리얼의 목적을 위해, 두 사람 간의 미리 녹음된 오디오 대화를 사용하여 감정 분석을 수행하겠습니다. API 키를 획득하셨다면, 미리 녹음된 오디오 파일에 대한 감정 분류 작업을 진행할 수 있습니다.</p>
<p>그러나 그 전에, 오디오 파일을 업로드하여 URL을 통해 액세스할 수 있도록 해야 합니다. AWS S3 버킷에 업로드하거나 SoundCloud 또는 AssemblyAI의 셀프-호스팅 서비스와 같은 오디오 호스팅 서비스에 업로드하는 옵션이 있습니다. 저는 오디오 파일을 SoundCloud에 업로드하여 아래에서 액세스할 수 있도록 했습니다.</p>
<p>만약 AssemblyAI의 호스팅 서비스에 오디오 파일을 직접 업로드하고 싶다면, 그것도 가능합니다. 저는 코드 블록 안에서 이 단계별 절차를 보여드렸습니다.</p>
<div class="content-ad"></div>
<h2>단계 3.1: 요구 사항 가져오기</h2>
<p>프로젝트에 필요한 요구 사항을 가져오는 것으로 시작합니다.</p>
<h2>단계 3.2: 파일 위치 및 API 키 지정</h2>
<p>다음으로, 로컬 컴퓨터에서 오디오 파일의 위치와 가입 후 얻은 API 키를 지정해야 합니다.</p>
<div class="content-ad"></div>
<h2>단계 3.3: 업로드 엔드포인트 지정</h2>
<ul>
<li>엔드포인트: 여기서 사용할 서비스인 "upload" 서비스를 지정합니다.</li>
<li>헤더: API 키 및 콘텐츠 유형을 보유합니다.</li>
</ul>
<h2>단계 3.4: 업로드 함수 정의</h2>
<p>오디오 파일은 한 번에 5 MB(5,242,880 바이트)까지만 업로드할 수 있습니다. 따라서 데이터를 청크 단위로 업로드해야 합니다. 이후에 서비스 엔드포인트에서 이들을 합칩니다. 따라서 많은 URL을 처리할 필요가 없어집니다.</p>
<div class="content-ad"></div>
<h2>단계 3.5: 업로드</h2>
<p>마지막 단계는 POST 요청을 호출하는 것입니다. POST 요청의 응답은 오디오 파일의 업로드 URL을 보유한 JSON입니다. 이 URL을 사용하여 오디오의 감정 분류를 실행하는 다음 단계에 사용할 것입니다.</p>
<h2>단계 4: 감정 분석</h2>
<p>이제 이 단계에서 오디오 파일에 대해 감정 분석 작업을 수행하기 위한 필요한 모든 전제조건을 충족했습니다. 이제 우리는 API를 호출하여 원하는 결과를 가져오는 것으로 계속할 수 있습니다. 이는 아래 소목록에서 설명되는 2단계 프로세스입니다.</p>
<div class="content-ad"></div>
<h2>단계 4.1: 전사용 파일 제출</h2>
<p>첫 번째 단계는 HTTP POST 요청을 호출하는 것입니다. 이는 기본으로 실행되는 AI 모델에 오디오 파일을 보내 전사를 위임하고, 전사된 텍스트에 대해 감정 분석을 수행하도록 지시하는 것입니다.</p>
<p>POST 요청에 전달되는 인수는 다음과 같습니다:</p>
<ul>
<li>endpoint: 호출할 전사 서비스를 지정합니다.</li>
<li>json: 오디오 파일의 URL을 audio_url 키로 포함합니다. 대화 데이터에 대한 감정 분석을 수행하려면 sentiment_analysis 플래그와 speaker_labels를 True로 설정합니다.</li>
<li>headers: 허가 키와 콘텐츠 유형을 보유합니다.</li>
</ul>
<div class="content-ad"></div>
<p>포스트 요청의 현재 상태는 JSON 응답으로 받았을 때 대기 중인 상태입니다. 이는 현재 오디오가 변환 중임을 나타냅니다.</p>
<p>또한, JSON 응답에서 sentiment_analysis 플래그도 True로 나와 있습니다. 그러나 sentiment_analysis_results 키에 해당하는 값은 상태가 대기 중이기 때문에 None입니다.</p>
<h2>단계 4.2: 변환 결과 가져오기</h2>
<p>POST 요청의 상태를 확인하려면 위에서 받은 JSON 응답의 id 키를 사용하여 GET 요청을 해야 합니다.</p>
<div class="content-ad"></div>
<p>다음으로, 아래 코드 블록에 나와 있는 것처럼 GET 요청을 진행할 수 있습니다.</p>
<p>GET 요청에 전달되는 인수는 다음과 같습니다:</p>
<ul>
<li>endpoint: 이는 호출된 서비스를 지정하며 id 키를 사용하여 결정된 API 호출 식별자를 나타냅니다.</li>
<li>headers: 이는 귀하의 고유한 API 키를 보유합니다.</li>
</ul>
<p>여기서 중요한 점은 상태 키가 completed로 변경될 때까지 전사 결과가 준비되지 않는다는 것입니다. 전사에 걸리는 시간은 입력 오디오 파일의 길이에 따라 다릅니다. 따라서 전사 상태를 확인하기 위해 규칙적인 간격으로 반복적인 GET 요청을 수행해야 합니다. 이를 위한 간단한 방법을 아래 구현하였습니다:</p>
<div class="content-ad"></div>
<h1>감정 분석 결과</h1>
<p>일단 상태가 완료로 변경되면 아래와 유사한 응답을 받게 될 것입니다.</p>
<ul>
<li>JSON 응답에서의 상태가 완료로 표시됩니다. 이는 오디오 전사에 오류가 없었음을 나타냅니다.</li>
<li>텍스트 키에는 입력 오디오 대화의 전체 전사가 포함되어 있으며, 총 22개 문장이 포함됩니다.</li>
<li>오디오 파일은 여러 화자로 구성되어 있기 때문에, 단어 키 내의 모든 화자 키를 Not Null로 볼 수 있습니다. 화자 키는 "A" 또는 "B"일 수 있습니다.</li>
<li>모든 개별 단어와 전체 전사 텍스트에 대한 확신 점수를 볼 수 있습니다. 이 점수는 0부터 1까지 범위를 가지며, 0이 가장 낮고 1이 가장 높습니다.</li>
<li>오디오의 각각 22개 문장에 대한 감정 분석 결과는 JSON 응답의 sentiment_analysis_results 키를 사용하여 액세스할 수 있습니다.</li>
<li>각 문장에 대응하여, 4번 항목과 유사한 확신 점수를 얻을 수 있습니다.</li>
<li>각 문장의 감정은 문장 사전의 sentiment 키를 사용하여 검색할 수 있습니다. 두 번째 문장에 대한 감정 분석 결과가 아래에 표시되어 있습니다:</li>
</ul>
<h1>감정 분석 인사이트</h1>
<div class="content-ad"></div>
<p>JSON은 보통 읽고 해석하기 어렵습니다. 그래서 데이터를 시각적으로 보기 좋게 만들고 추가 분석을 수행하기 위해 위의 감정 분석 결과를 DataFrame으로 변환합시다. 우리는 문장의 텍스트, 지속 시간, 스피커, 그리고 문장의 감정을 저장할 것입니다. 이를 아래에서 구현하겠습니다:</p>
<p>위 코드 스니펫으로 생성된 DataFrame은 아래 이미지에 표시됩니다. 여기서 대화 중 발화된 22개의 문장과 해당하는 스피커 레이블("A"와 "B"), 문장의 지속 시간(초), 그리고 모델이 예측한 문장의 감정이 포함되어 있습니다.</p>
<img src="/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_3.png">
<h2>#1 스피커 분포</h2>
<div class="content-ad"></div>
<p>각 화자가 말한 문장 수는 아래와 같이 value_counts() 메소드를 사용하여 계산할 수 있습니다:</p>
<p>화자들의 백분율 분포를 보려면 다음과 같이 value_counts() 메소드에 normalize = True를 전달할 수 있습니다:</p>
<p>“A”와 “B” 두 화자 모두 문장 수 측면에서 대화에 동등하게 기여했습니다.</p>
<h2>#2 화자 지속 시간 분포</h2>
<div class="content-ad"></div>
<p>다음으로 대화 참가자 각각의 기여도를 계산해 봅시다. 아래에서 확인할 수 있습니다:</p>
<p>groupby() 메서드를 사용하여 각 발화자의 발화 시간을 총합하여 계산했습니다. 발화 시간 측면에서 발화자 A가 우세한 발화자입니다.</p>
<h2>#3 감정 분포</h2>
<p>대화 중 총 22문장 중 부정 감정으로 분류된 문장은 3개뿐이었습니다. 또한 양의 감정으로 분류된 문장은 없었습니다.</p>
<div class="content-ad"></div>
<p>정규화된 분포는 다음과 같이 계산할 수 있습니다:</p>
<h2>스피커 레벨에서 #4 감정 분포</h2>
<p>마지막으로, 개별 스피커 간의 감정 분포를 계산해 봅시다. 여기서는 groupby() 메서드 대신 더 나은 시각화를 위해 crosstab()을 사용할 것입니다. 아래에서 이를 시연합니다:</p>
<p>"A" 스피커가 한 부정적 문장의 비율이 "B" 스피커보다 더 많았습니다.</p>
<div class="content-ad"></div>
<h2>#5 감정 수준별 평균 문장 지속 시간</h2>
<p>마지막으로, 우리는 개별 감정 클래스에 속하는 문장의 평균 지속 시간을 계산할 것입니다. 이는 아래의 groupby() 메서드를 사용하여 구현되었습니다:</p>
<p>부정적인 문장의 평균 지속 시간은 중립 문장보다 작습니다.</p>
<p>마무리로, 이 글에서는 AssemblyAI API의 특정 NLP 사용 사례에 대해 논의했습니다. 구체적으로, 여러 화자로 구성된 미리 녹음된 오디오 파일에서 감정 분류 모듈을 구축하는 방법을 살펴보았습니다. 마지막으로, 감정 분석 결과에 대해 철저한 분석을 수행했습니다. API로부터 얻은 결과는 입력 오디오 파일의 22개의 개별 문장의 감정을 강조했습니다.</p>
<div class="content-ad"></div>
<p>이 글의 코드는 여기서 찾을 수 있어요.</p>
<p>다음 게시물에서는 어셈블리 AI API의 더 많은 사용 사례에 대해 논의할 거예요. Entity Detection, Content Moderation 등 기술적, 실용적 관점에서 더 자세하게 다루겠습니다.</p>
<p>다음에 또 봐요. 읽어 주셔서 감사해요.</p>
<p>🚀 매일 뉴스레터를 구독하면 320개 이상의 글이 실린 데이터 과학 PDF(550페이지)를 무료로 받을 수 있어요:</p>
<div class="content-ad"></div>
<img src="https://miro.medium.com/v2/resize:fit:1400/0*QXJuDEr_pCNDtj4D.gif"><table>
  
</table>
<p>DDI 중간 게시글 바닥 링크(DDI)</p>
<p>DataDrivenInvestor.com에서 방문하세요</p>
<p>DDIntel을 여기에서 구독하세요.</p>
<div class="content-ad"></div>
<p>주요 기사:</p>
<p>여기서 우리의 창조자 생태계에 참여하세요.</p>
<p>DDI 공식 텔레그램 채널: <a href="https://t.me/+tafUp6ecEys4YjQ1" rel="nofollow" target="_blank">https://t.me/+tafUp6ecEys4YjQ1</a></p>
<p>LinkedIn, Twitter, YouTube, 그리고 Facebook에서 팔로우해보세요.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"오디오 데이터에 대한 대화형 감정 분석","description":"","date":"2024-06-20 04:41","slug":"2024-06-20-ConversationalSentimentAnalysisonAudioData","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_0.png\" /\u003e\n\n감성 분석 또는 의견 분석은 자연어 처리(NLP)에서 널리 사용되는 작업입니다. NLP 기술을 텍스트 데이터에 특히 적용하는 맥락에서, 주요 목표는 주어진 텍스트를 다양한 감성 클래스에 분류할 수 있는 모델을 훈련하는 것입니다. 감성 분류기의 고수준 개요는 아래 이미지에 나와 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_1.png\" /\u003e\n\n예를 들어, 세 가지 클래스 분류 문제의 클래스는 긍정적, 부정적 및 중립일 수 있습니다. 세 가지 클래스의 감성 분석 문제의 예는 인기 있는 Twitter 감성 분석 데이터 세트입니다. 이 데이터는 트위터 사용자들이 게시한 다국어 트윗에 대한 Entity-level 감성 분석 작업입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과거의 자연어처리(NLP) 연구 및 개발 대부분은 주로 텍스트에 감성 분석을 적용하는 데 중점을 두었습니다. 그러나 최근에는 사용자들 사이에서 음성 기반 상호 작용 도구의 대규모 채택과 인기를 볼 수 있었으며, 이는 연구자들과 기관들을 음성 영역에서 감성 분류기를 구축하도록 이끕니다.\n\n따라서 이 게시물에서는 AssemblyAI API와 Python을 사용하여 대화 데이터에 감성 분석 시스템을 구축하는 방법을 보여줄 것입니다. 이 종단 간 시스템은 엄격한 고객 지원 및 피드백 평가와 관련이 있는 영역에서 광범위한 적용 가능성을 지니며, 특히 음성 도메인에서 중요하고 가치 있는 문제를 해결하는 데 도움이 됩니다. 마지막으로 얻은 결과물을 이해하기 쉽게 향상시키고 데이터에서 적절한 통찰을 얻기 위한 분석을 보여줄 것입니다.\n\n이 글의 코드는 여기에서 찾을 수 있습니다. 게시물의 주요 내용은 다음과 같습니다:\n\n- 대화형 오디오 데이터에 대한 감성 분석\n- 감성 분석 결과\n- 감성 분석 통찰력\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 대화 오디오 데이터에 대한 감정 분석\n\n이 섹션에서는, 녹음된 음성 대화 조각에서 개별 문장을 세 가지 감정 클래스로 분류하는 AssemblyAI API의 사용을 보여드리겠습니다: 긍정적, 부정적 및 중립적.\n\n![이미지](/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_2.png)\n\n## 단계 1: 요구 사항 설치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n감정 분류기를 구축하는 데 필요한 요소가 매우 적습니다. Python 라이브러리 관점에서 requests 패키지만 필요합니다. 아래와 같이 수행할 수 있습니다:\n\n```js\npip install requests\n```\n\n## 단계 2: API 토큰 생성\n\n다음 단계는 AssemblyAI 웹사이트에 계정을 생성하는 것입니다. 이 과정은 무료로 진행할 수 있습니다. 계정을 생성하면 프라이빗 API 액세스 키를 받게 되는데, 이것을 사용하여 음성을 텍스트로 변환하는 모델에 접근할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 3: 오디오 파일 업로드\n\n이 튜토리얼의 목적을 위해, 두 사람 간의 미리 녹음된 오디오 대화를 사용하여 감정 분석을 수행하겠습니다. API 키를 획득하셨다면, 미리 녹음된 오디오 파일에 대한 감정 분류 작업을 진행할 수 있습니다.\n\n그러나 그 전에, 오디오 파일을 업로드하여 URL을 통해 액세스할 수 있도록 해야 합니다. AWS S3 버킷에 업로드하거나 SoundCloud 또는 AssemblyAI의 셀프-호스팅 서비스와 같은 오디오 호스팅 서비스에 업로드하는 옵션이 있습니다. 저는 오디오 파일을 SoundCloud에 업로드하여 아래에서 액세스할 수 있도록 했습니다.\n\n만약 AssemblyAI의 호스팅 서비스에 오디오 파일을 직접 업로드하고 싶다면, 그것도 가능합니다. 저는 코드 블록 안에서 이 단계별 절차를 보여드렸습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 3.1: 요구 사항 가져오기\n\n프로젝트에 필요한 요구 사항을 가져오는 것으로 시작합니다.\n\n## 단계 3.2: 파일 위치 및 API 키 지정\n\n다음으로, 로컬 컴퓨터에서 오디오 파일의 위치와 가입 후 얻은 API 키를 지정해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 3.3: 업로드 엔드포인트 지정\n\n- 엔드포인트: 여기서 사용할 서비스인 \"upload\" 서비스를 지정합니다.\n- 헤더: API 키 및 콘텐츠 유형을 보유합니다.\n\n## 단계 3.4: 업로드 함수 정의\n\n오디오 파일은 한 번에 5 MB(5,242,880 바이트)까지만 업로드할 수 있습니다. 따라서 데이터를 청크 단위로 업로드해야 합니다. 이후에 서비스 엔드포인트에서 이들을 합칩니다. 따라서 많은 URL을 처리할 필요가 없어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 3.5: 업로드\n\n마지막 단계는 POST 요청을 호출하는 것입니다. POST 요청의 응답은 오디오 파일의 업로드 URL을 보유한 JSON입니다. 이 URL을 사용하여 오디오의 감정 분류를 실행하는 다음 단계에 사용할 것입니다.\n\n## 단계 4: 감정 분석\n\n이제 이 단계에서 오디오 파일에 대해 감정 분석 작업을 수행하기 위한 필요한 모든 전제조건을 충족했습니다. 이제 우리는 API를 호출하여 원하는 결과를 가져오는 것으로 계속할 수 있습니다. 이는 아래 소목록에서 설명되는 2단계 프로세스입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 4.1: 전사용 파일 제출\n\n첫 번째 단계는 HTTP POST 요청을 호출하는 것입니다. 이는 기본으로 실행되는 AI 모델에 오디오 파일을 보내 전사를 위임하고, 전사된 텍스트에 대해 감정 분석을 수행하도록 지시하는 것입니다.\n\nPOST 요청에 전달되는 인수는 다음과 같습니다:\n\n- endpoint: 호출할 전사 서비스를 지정합니다.\n- json: 오디오 파일의 URL을 audio_url 키로 포함합니다. 대화 데이터에 대한 감정 분석을 수행하려면 sentiment_analysis 플래그와 speaker_labels를 True로 설정합니다.\n- headers: 허가 키와 콘텐츠 유형을 보유합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n포스트 요청의 현재 상태는 JSON 응답으로 받았을 때 대기 중인 상태입니다. 이는 현재 오디오가 변환 중임을 나타냅니다.\n\n또한, JSON 응답에서 sentiment_analysis 플래그도 True로 나와 있습니다. 그러나 sentiment_analysis_results 키에 해당하는 값은 상태가 대기 중이기 때문에 None입니다.\n\n## 단계 4.2: 변환 결과 가져오기\n\nPOST 요청의 상태를 확인하려면 위에서 받은 JSON 응답의 id 키를 사용하여 GET 요청을 해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 아래 코드 블록에 나와 있는 것처럼 GET 요청을 진행할 수 있습니다.\n\nGET 요청에 전달되는 인수는 다음과 같습니다:\n\n- endpoint: 이는 호출된 서비스를 지정하며 id 키를 사용하여 결정된 API 호출 식별자를 나타냅니다.\n- headers: 이는 귀하의 고유한 API 키를 보유합니다.\n\n여기서 중요한 점은 상태 키가 completed로 변경될 때까지 전사 결과가 준비되지 않는다는 것입니다. 전사에 걸리는 시간은 입력 오디오 파일의 길이에 따라 다릅니다. 따라서 전사 상태를 확인하기 위해 규칙적인 간격으로 반복적인 GET 요청을 수행해야 합니다. 이를 위한 간단한 방법을 아래 구현하였습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 감정 분석 결과\n\n일단 상태가 완료로 변경되면 아래와 유사한 응답을 받게 될 것입니다.\n\n- JSON 응답에서의 상태가 완료로 표시됩니다. 이는 오디오 전사에 오류가 없었음을 나타냅니다.\n- 텍스트 키에는 입력 오디오 대화의 전체 전사가 포함되어 있으며, 총 22개 문장이 포함됩니다.\n- 오디오 파일은 여러 화자로 구성되어 있기 때문에, 단어 키 내의 모든 화자 키를 Not Null로 볼 수 있습니다. 화자 키는 \"A\" 또는 \"B\"일 수 있습니다.\n- 모든 개별 단어와 전체 전사 텍스트에 대한 확신 점수를 볼 수 있습니다. 이 점수는 0부터 1까지 범위를 가지며, 0이 가장 낮고 1이 가장 높습니다.\n- 오디오의 각각 22개 문장에 대한 감정 분석 결과는 JSON 응답의 sentiment_analysis_results 키를 사용하여 액세스할 수 있습니다.\n- 각 문장에 대응하여, 4번 항목과 유사한 확신 점수를 얻을 수 있습니다.\n- 각 문장의 감정은 문장 사전의 sentiment 키를 사용하여 검색할 수 있습니다. 두 번째 문장에 대한 감정 분석 결과가 아래에 표시되어 있습니다:\n\n# 감정 분석 인사이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nJSON은 보통 읽고 해석하기 어렵습니다. 그래서 데이터를 시각적으로 보기 좋게 만들고 추가 분석을 수행하기 위해 위의 감정 분석 결과를 DataFrame으로 변환합시다. 우리는 문장의 텍스트, 지속 시간, 스피커, 그리고 문장의 감정을 저장할 것입니다. 이를 아래에서 구현하겠습니다:\n\n위 코드 스니펫으로 생성된 DataFrame은 아래 이미지에 표시됩니다. 여기서 대화 중 발화된 22개의 문장과 해당하는 스피커 레이블(\"A\"와 \"B\"), 문장의 지속 시간(초), 그리고 모델이 예측한 문장의 감정이 포함되어 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_3.png\" /\u003e\n\n## #1 스피커 분포\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 화자가 말한 문장 수는 아래와 같이 value_counts() 메소드를 사용하여 계산할 수 있습니다:\n\n화자들의 백분율 분포를 보려면 다음과 같이 value_counts() 메소드에 normalize = True를 전달할 수 있습니다:\n\n“A”와 “B” 두 화자 모두 문장 수 측면에서 대화에 동등하게 기여했습니다.\n\n## #2 화자 지속 시간 분포\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로 대화 참가자 각각의 기여도를 계산해 봅시다. 아래에서 확인할 수 있습니다:\n\ngroupby() 메서드를 사용하여 각 발화자의 발화 시간을 총합하여 계산했습니다. 발화 시간 측면에서 발화자 A가 우세한 발화자입니다.\n\n## #3 감정 분포\n\n대화 중 총 22문장 중 부정 감정으로 분류된 문장은 3개뿐이었습니다. 또한 양의 감정으로 분류된 문장은 없었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정규화된 분포는 다음과 같이 계산할 수 있습니다:\n\n## 스피커 레벨에서 #4 감정 분포\n\n마지막으로, 개별 스피커 간의 감정 분포를 계산해 봅시다. 여기서는 groupby() 메서드 대신 더 나은 시각화를 위해 crosstab()을 사용할 것입니다. 아래에서 이를 시연합니다:\n\n\"A\" 스피커가 한 부정적 문장의 비율이 \"B\" 스피커보다 더 많았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## #5 감정 수준별 평균 문장 지속 시간\n\n마지막으로, 우리는 개별 감정 클래스에 속하는 문장의 평균 지속 시간을 계산할 것입니다. 이는 아래의 groupby() 메서드를 사용하여 구현되었습니다:\n\n부정적인 문장의 평균 지속 시간은 중립 문장보다 작습니다.\n\n마무리로, 이 글에서는 AssemblyAI API의 특정 NLP 사용 사례에 대해 논의했습니다. 구체적으로, 여러 화자로 구성된 미리 녹음된 오디오 파일에서 감정 분류 모듈을 구축하는 방법을 살펴보았습니다. 마지막으로, 감정 분석 결과에 대해 철저한 분석을 수행했습니다. API로부터 얻은 결과는 입력 오디오 파일의 22개의 개별 문장의 감정을 강조했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글의 코드는 여기서 찾을 수 있어요.\n\n다음 게시물에서는 어셈블리 AI API의 더 많은 사용 사례에 대해 논의할 거예요. Entity Detection, Content Moderation 등 기술적, 실용적 관점에서 더 자세하게 다루겠습니다.\n\n다음에 또 봐요. 읽어 주셔서 감사해요.\n\n🚀 매일 뉴스레터를 구독하면 320개 이상의 글이 실린 데이터 과학 PDF(550페이지)를 무료로 받을 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003ctable\u003e\n  \u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/0*QXJuDEr_pCNDtj4D.gif\" /\u003e\n\u003c/table\u003e\n\nDDI 중간 게시글 바닥 링크(DDI)\n\nDataDrivenInvestor.com에서 방문하세요\n\nDDIntel을 여기에서 구독하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주요 기사:\n\n여기서 우리의 창조자 생태계에 참여하세요.\n\nDDI 공식 텔레그램 채널: [https://t.me/+tafUp6ecEys4YjQ1](https://t.me/+tafUp6ecEys4YjQ1)\n\nLinkedIn, Twitter, YouTube, 그리고 Facebook에서 팔로우해보세요.","ogImage":{"url":"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_0.png"},"coverImage":"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_0.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_0.png\"\u003e\n\u003cp\u003e감성 분석 또는 의견 분석은 자연어 처리(NLP)에서 널리 사용되는 작업입니다. NLP 기술을 텍스트 데이터에 특히 적용하는 맥락에서, 주요 목표는 주어진 텍스트를 다양한 감성 클래스에 분류할 수 있는 모델을 훈련하는 것입니다. 감성 분류기의 고수준 개요는 아래 이미지에 나와 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_1.png\"\u003e\n\u003cp\u003e예를 들어, 세 가지 클래스 분류 문제의 클래스는 긍정적, 부정적 및 중립일 수 있습니다. 세 가지 클래스의 감성 분석 문제의 예는 인기 있는 Twitter 감성 분석 데이터 세트입니다. 이 데이터는 트위터 사용자들이 게시한 다국어 트윗에 대한 Entity-level 감성 분석 작업입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e과거의 자연어처리(NLP) 연구 및 개발 대부분은 주로 텍스트에 감성 분석을 적용하는 데 중점을 두었습니다. 그러나 최근에는 사용자들 사이에서 음성 기반 상호 작용 도구의 대규모 채택과 인기를 볼 수 있었으며, 이는 연구자들과 기관들을 음성 영역에서 감성 분류기를 구축하도록 이끕니다.\u003c/p\u003e\n\u003cp\u003e따라서 이 게시물에서는 AssemblyAI API와 Python을 사용하여 대화 데이터에 감성 분석 시스템을 구축하는 방법을 보여줄 것입니다. 이 종단 간 시스템은 엄격한 고객 지원 및 피드백 평가와 관련이 있는 영역에서 광범위한 적용 가능성을 지니며, 특히 음성 도메인에서 중요하고 가치 있는 문제를 해결하는 데 도움이 됩니다. 마지막으로 얻은 결과물을 이해하기 쉽게 향상시키고 데이터에서 적절한 통찰을 얻기 위한 분석을 보여줄 것입니다.\u003c/p\u003e\n\u003cp\u003e이 글의 코드는 여기에서 찾을 수 있습니다. 게시물의 주요 내용은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e대화형 오디오 데이터에 대한 감성 분석\u003c/li\u003e\n\u003cli\u003e감성 분석 결과\u003c/li\u003e\n\u003cli\u003e감성 분석 통찰력\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e대화 오디오 데이터에 대한 감정 분석\u003c/h1\u003e\n\u003cp\u003e이 섹션에서는, 녹음된 음성 대화 조각에서 개별 문장을 세 가지 감정 클래스로 분류하는 AssemblyAI API의 사용을 보여드리겠습니다: 긍정적, 부정적 및 중립적.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e단계 1: 요구 사항 설치\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e감정 분류기를 구축하는 데 필요한 요소가 매우 적습니다. Python 라이브러리 관점에서 requests 패키지만 필요합니다. 아래와 같이 수행할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install requests\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e단계 2: API 토큰 생성\u003c/h2\u003e\n\u003cp\u003e다음 단계는 AssemblyAI 웹사이트에 계정을 생성하는 것입니다. 이 과정은 무료로 진행할 수 있습니다. 계정을 생성하면 프라이빗 API 액세스 키를 받게 되는데, 이것을 사용하여 음성을 텍스트로 변환하는 모델에 접근할 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 3: 오디오 파일 업로드\u003c/h2\u003e\n\u003cp\u003e이 튜토리얼의 목적을 위해, 두 사람 간의 미리 녹음된 오디오 대화를 사용하여 감정 분석을 수행하겠습니다. API 키를 획득하셨다면, 미리 녹음된 오디오 파일에 대한 감정 분류 작업을 진행할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그러나 그 전에, 오디오 파일을 업로드하여 URL을 통해 액세스할 수 있도록 해야 합니다. AWS S3 버킷에 업로드하거나 SoundCloud 또는 AssemblyAI의 셀프-호스팅 서비스와 같은 오디오 호스팅 서비스에 업로드하는 옵션이 있습니다. 저는 오디오 파일을 SoundCloud에 업로드하여 아래에서 액세스할 수 있도록 했습니다.\u003c/p\u003e\n\u003cp\u003e만약 AssemblyAI의 호스팅 서비스에 오디오 파일을 직접 업로드하고 싶다면, 그것도 가능합니다. 저는 코드 블록 안에서 이 단계별 절차를 보여드렸습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 3.1: 요구 사항 가져오기\u003c/h2\u003e\n\u003cp\u003e프로젝트에 필요한 요구 사항을 가져오는 것으로 시작합니다.\u003c/p\u003e\n\u003ch2\u003e단계 3.2: 파일 위치 및 API 키 지정\u003c/h2\u003e\n\u003cp\u003e다음으로, 로컬 컴퓨터에서 오디오 파일의 위치와 가입 후 얻은 API 키를 지정해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 3.3: 업로드 엔드포인트 지정\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e엔드포인트: 여기서 사용할 서비스인 \"upload\" 서비스를 지정합니다.\u003c/li\u003e\n\u003cli\u003e헤더: API 키 및 콘텐츠 유형을 보유합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e단계 3.4: 업로드 함수 정의\u003c/h2\u003e\n\u003cp\u003e오디오 파일은 한 번에 5 MB(5,242,880 바이트)까지만 업로드할 수 있습니다. 따라서 데이터를 청크 단위로 업로드해야 합니다. 이후에 서비스 엔드포인트에서 이들을 합칩니다. 따라서 많은 URL을 처리할 필요가 없어집니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 3.5: 업로드\u003c/h2\u003e\n\u003cp\u003e마지막 단계는 POST 요청을 호출하는 것입니다. POST 요청의 응답은 오디오 파일의 업로드 URL을 보유한 JSON입니다. 이 URL을 사용하여 오디오의 감정 분류를 실행하는 다음 단계에 사용할 것입니다.\u003c/p\u003e\n\u003ch2\u003e단계 4: 감정 분석\u003c/h2\u003e\n\u003cp\u003e이제 이 단계에서 오디오 파일에 대해 감정 분석 작업을 수행하기 위한 필요한 모든 전제조건을 충족했습니다. 이제 우리는 API를 호출하여 원하는 결과를 가져오는 것으로 계속할 수 있습니다. 이는 아래 소목록에서 설명되는 2단계 프로세스입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 4.1: 전사용 파일 제출\u003c/h2\u003e\n\u003cp\u003e첫 번째 단계는 HTTP POST 요청을 호출하는 것입니다. 이는 기본으로 실행되는 AI 모델에 오디오 파일을 보내 전사를 위임하고, 전사된 텍스트에 대해 감정 분석을 수행하도록 지시하는 것입니다.\u003c/p\u003e\n\u003cp\u003ePOST 요청에 전달되는 인수는 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eendpoint: 호출할 전사 서비스를 지정합니다.\u003c/li\u003e\n\u003cli\u003ejson: 오디오 파일의 URL을 audio_url 키로 포함합니다. 대화 데이터에 대한 감정 분석을 수행하려면 sentiment_analysis 플래그와 speaker_labels를 True로 설정합니다.\u003c/li\u003e\n\u003cli\u003eheaders: 허가 키와 콘텐츠 유형을 보유합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e포스트 요청의 현재 상태는 JSON 응답으로 받았을 때 대기 중인 상태입니다. 이는 현재 오디오가 변환 중임을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e또한, JSON 응답에서 sentiment_analysis 플래그도 True로 나와 있습니다. 그러나 sentiment_analysis_results 키에 해당하는 값은 상태가 대기 중이기 때문에 None입니다.\u003c/p\u003e\n\u003ch2\u003e단계 4.2: 변환 결과 가져오기\u003c/h2\u003e\n\u003cp\u003ePOST 요청의 상태를 확인하려면 위에서 받은 JSON 응답의 id 키를 사용하여 GET 요청을 해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음으로, 아래 코드 블록에 나와 있는 것처럼 GET 요청을 진행할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eGET 요청에 전달되는 인수는 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eendpoint: 이는 호출된 서비스를 지정하며 id 키를 사용하여 결정된 API 호출 식별자를 나타냅니다.\u003c/li\u003e\n\u003cli\u003eheaders: 이는 귀하의 고유한 API 키를 보유합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e여기서 중요한 점은 상태 키가 completed로 변경될 때까지 전사 결과가 준비되지 않는다는 것입니다. 전사에 걸리는 시간은 입력 오디오 파일의 길이에 따라 다릅니다. 따라서 전사 상태를 확인하기 위해 규칙적인 간격으로 반복적인 GET 요청을 수행해야 합니다. 이를 위한 간단한 방법을 아래 구현하였습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e감정 분석 결과\u003c/h1\u003e\n\u003cp\u003e일단 상태가 완료로 변경되면 아래와 유사한 응답을 받게 될 것입니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJSON 응답에서의 상태가 완료로 표시됩니다. 이는 오디오 전사에 오류가 없었음을 나타냅니다.\u003c/li\u003e\n\u003cli\u003e텍스트 키에는 입력 오디오 대화의 전체 전사가 포함되어 있으며, 총 22개 문장이 포함됩니다.\u003c/li\u003e\n\u003cli\u003e오디오 파일은 여러 화자로 구성되어 있기 때문에, 단어 키 내의 모든 화자 키를 Not Null로 볼 수 있습니다. 화자 키는 \"A\" 또는 \"B\"일 수 있습니다.\u003c/li\u003e\n\u003cli\u003e모든 개별 단어와 전체 전사 텍스트에 대한 확신 점수를 볼 수 있습니다. 이 점수는 0부터 1까지 범위를 가지며, 0이 가장 낮고 1이 가장 높습니다.\u003c/li\u003e\n\u003cli\u003e오디오의 각각 22개 문장에 대한 감정 분석 결과는 JSON 응답의 sentiment_analysis_results 키를 사용하여 액세스할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e각 문장에 대응하여, 4번 항목과 유사한 확신 점수를 얻을 수 있습니다.\u003c/li\u003e\n\u003cli\u003e각 문장의 감정은 문장 사전의 sentiment 키를 사용하여 검색할 수 있습니다. 두 번째 문장에 대한 감정 분석 결과가 아래에 표시되어 있습니다:\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e감정 분석 인사이트\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eJSON은 보통 읽고 해석하기 어렵습니다. 그래서 데이터를 시각적으로 보기 좋게 만들고 추가 분석을 수행하기 위해 위의 감정 분석 결과를 DataFrame으로 변환합시다. 우리는 문장의 텍스트, 지속 시간, 스피커, 그리고 문장의 감정을 저장할 것입니다. 이를 아래에서 구현하겠습니다:\u003c/p\u003e\n\u003cp\u003e위 코드 스니펫으로 생성된 DataFrame은 아래 이미지에 표시됩니다. 여기서 대화 중 발화된 22개의 문장과 해당하는 스피커 레이블(\"A\"와 \"B\"), 문장의 지속 시간(초), 그리고 모델이 예측한 문장의 감정이 포함되어 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-20-ConversationalSentimentAnalysisonAudioData_3.png\"\u003e\n\u003ch2\u003e#1 스피커 분포\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e각 화자가 말한 문장 수는 아래와 같이 value_counts() 메소드를 사용하여 계산할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e화자들의 백분율 분포를 보려면 다음과 같이 value_counts() 메소드에 normalize = True를 전달할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e“A”와 “B” 두 화자 모두 문장 수 측면에서 대화에 동등하게 기여했습니다.\u003c/p\u003e\n\u003ch2\u003e#2 화자 지속 시간 분포\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음으로 대화 참가자 각각의 기여도를 계산해 봅시다. 아래에서 확인할 수 있습니다:\u003c/p\u003e\n\u003cp\u003egroupby() 메서드를 사용하여 각 발화자의 발화 시간을 총합하여 계산했습니다. 발화 시간 측면에서 발화자 A가 우세한 발화자입니다.\u003c/p\u003e\n\u003ch2\u003e#3 감정 분포\u003c/h2\u003e\n\u003cp\u003e대화 중 총 22문장 중 부정 감정으로 분류된 문장은 3개뿐이었습니다. 또한 양의 감정으로 분류된 문장은 없었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e정규화된 분포는 다음과 같이 계산할 수 있습니다:\u003c/p\u003e\n\u003ch2\u003e스피커 레벨에서 #4 감정 분포\u003c/h2\u003e\n\u003cp\u003e마지막으로, 개별 스피커 간의 감정 분포를 계산해 봅시다. 여기서는 groupby() 메서드 대신 더 나은 시각화를 위해 crosstab()을 사용할 것입니다. 아래에서 이를 시연합니다:\u003c/p\u003e\n\u003cp\u003e\"A\" 스피커가 한 부정적 문장의 비율이 \"B\" 스피커보다 더 많았습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e#5 감정 수준별 평균 문장 지속 시간\u003c/h2\u003e\n\u003cp\u003e마지막으로, 우리는 개별 감정 클래스에 속하는 문장의 평균 지속 시간을 계산할 것입니다. 이는 아래의 groupby() 메서드를 사용하여 구현되었습니다:\u003c/p\u003e\n\u003cp\u003e부정적인 문장의 평균 지속 시간은 중립 문장보다 작습니다.\u003c/p\u003e\n\u003cp\u003e마무리로, 이 글에서는 AssemblyAI API의 특정 NLP 사용 사례에 대해 논의했습니다. 구체적으로, 여러 화자로 구성된 미리 녹음된 오디오 파일에서 감정 분류 모듈을 구축하는 방법을 살펴보았습니다. 마지막으로, 감정 분석 결과에 대해 철저한 분석을 수행했습니다. API로부터 얻은 결과는 입력 오디오 파일의 22개의 개별 문장의 감정을 강조했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 글의 코드는 여기서 찾을 수 있어요.\u003c/p\u003e\n\u003cp\u003e다음 게시물에서는 어셈블리 AI API의 더 많은 사용 사례에 대해 논의할 거예요. Entity Detection, Content Moderation 등 기술적, 실용적 관점에서 더 자세하게 다루겠습니다.\u003c/p\u003e\n\u003cp\u003e다음에 또 봐요. 읽어 주셔서 감사해요.\u003c/p\u003e\n\u003cp\u003e🚀 매일 뉴스레터를 구독하면 320개 이상의 글이 실린 데이터 과학 PDF(550페이지)를 무료로 받을 수 있어요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/0*QXJuDEr_pCNDtj4D.gif\"\u003e\u003ctable\u003e\n  \n\u003c/table\u003e\n\u003cp\u003eDDI 중간 게시글 바닥 링크(DDI)\u003c/p\u003e\n\u003cp\u003eDataDrivenInvestor.com에서 방문하세요\u003c/p\u003e\n\u003cp\u003eDDIntel을 여기에서 구독하세요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e주요 기사:\u003c/p\u003e\n\u003cp\u003e여기서 우리의 창조자 생태계에 참여하세요.\u003c/p\u003e\n\u003cp\u003eDDI 공식 텔레그램 채널: \u003ca href=\"https://t.me/+tafUp6ecEys4YjQ1\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://t.me/+tafUp6ecEys4YjQ1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eLinkedIn, Twitter, YouTube, 그리고 Facebook에서 팔로우해보세요.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-ConversationalSentimentAnalysisonAudioData"},"buildId":"Y-fCAg8BUV7y2HNFwX9AA","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>