<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>점들로부터 평면까지 Vision-Language Subspace Prompting | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="점들로부터 평면까지 Vision-Language Subspace Prompting | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="점들로부터 평면까지 Vision-Language Subspace Prompting | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting" data-gatsby-head="true"/><meta name="twitter:title" content="점들로부터 평면까지 Vision-Language Subspace Prompting | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 17:39" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-a8eda6c93e0b14fe.js" defer=""></script><script src="/_next/static/R94iUTCf1NWeBC_VXjTJG/_buildManifest.js" defer=""></script><script src="/_next/static/R94iUTCf1NWeBC_VXjTJG/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">점들로부터 평면까지 Vision-Language Subspace Prompting</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="점들로부터 평면까지 Vision-Language Subspace Prompting" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/assets/profile.jpg"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">5<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p><img src="/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png" alt="lecture image"/></p>
<p>최근에 고급 컴퓨터 비전 수업인 Dr. 이 제가 이끄는 Dr. 송 이제의 흥미로운 게스트 강연을 듣게 되었습니다. Dr. Li는 삼성 AI의 연구 과학자로, 도메인 일반화, 연합 학습 및 PEFT 방법과 같은 인공 지능의 중요 주제를 다루었습니다. 특히, 그의 최신 연구인 &quot;Vision-Language Sub Space Prompting&quot;에 초점을 맞춘 프레젠테이션은 정보 전달 뿐만 아니라 유사성 검색과 같은 작업에서 진화하는 방법론을 구체화하는 데 있어서 흥미롭고 사유를 제공했습니다. 그의 통찰력에 영감을 받아, 저는 그의 최신 연구 작업에 대한 직관에 대한 생각을 이 블로그를 통해 공유할 필요성을 느꼈습니다.</p>
<p>Sub-space prompting의 복잡성에 뛰어들기 전에, 대규모 언어 모델(LLMs)과 시각-언어 모델(VLMs)의 영역에서 우리를 이 시점으로 이끌어 온 혁신의 역사를 잠깐 돌아봅시다. 여러분, 정말 대단한 여정이었습니다!</p>
<p>LLMs의 초기 발전은 Word2Vec 및 GloVe와 같은 모델의 등장으로 시작되었고, 이 모델은 단어의 밀도 있는 벡터 표현을 생성했습니다. 이 임베딩은 단어 간 의미와 관계를 포착해, 더 정교한 모델을 위한 기초를 마련했습니다. 이는 예전 페이스북에서 누군가가 우리를 &quot;찌르다&quot;라는 말만 들어도 정말 즐거웠던 소셜 미디어 초기 시절과 닮았네요.</p>
<div class="content-ad"></div>
<p>그 다음에는 트랜스포머의 시대가 도래했습니다. 특히 2017년 Vaswani 등이 소개한 트랜스포머 아키텍처는 해당 분야를 완전히 혁신했습니다. 트랜스포머는 셀프 어텐션 메커니즘을 활용하여 모델이 문장이나 문서 전체를 병렬로 처리하고 맥락적 관계를 더 효과적으로 포착할 수 있게 했습니다.</p>
<p>이어서 BERT(Bidirectional Encoder Representations from Transformers)와 GPT(Generative Pre-trained Transformer)가 등장했습니다. BERT는 모델이 양방향으로 컨텍스트를 학습하는 이중향 교육 개념을 소개했고, GPT는 비지도 학습과 자기회귀 언어 생성의 힘을 보여줌으로써 언어 모델링 분야에서 새로운 기준을 세웠습니다. GPT가 마치 AI 세계를 황홀한 롤러코스터 여행에 초대하며 기록을 깨고 모두를 감탄시켰다고 상상해보세요.</p>
<p>한편 VLM(Visual Language Models)도 주목을 받고 있었습니다. VisualBERT와 ViLBERT와 같은 모델은 시각적 및 텍스트 데이터를 결합하여 이미지 캡션 달기, 시각적 질문 응답 등 양쪽 모드를 이해해야 하는 작업을 가능하게 했습니다. AI의 동적인 듀오로 생각해보세요. 함께 세계를 보고 설명하며 복잡한 퍼즐을 해결합니다.</p>
<p>이어서 OpenAI의 CLIP(Contrastive Language–Image Pre-training)이 등장합니다. CLIP은 대규모 이미지-텍스트 쌍 데이터셋에서 훈련하여 이미지와 텍스트를 연관시키는 방법을 학습함으로써 제로샷 분류 작업에서 놀라운 성능을 달성했습니다. CLIP은 마치 AI 파티에 등장해 파티 분위기를 살려 이미지와 텍스트를 완벽하게 이해하는 능력으로 모든 이들을 감탄시켰다고 생각해보세요.</p>
<div class="content-ad"></div>
<p>서브스페이스 프롬프팅의 구체적인 내용을 살펴보면, 기술의 진화를 이해하는 것이 매우 중요합니다. 각 혁신은 이전 성공과 교훈을 기반으로 구축되어 오늘날 우리가 보는 정교한 모델과 방법론으로 이어졌습니다.</p>
<h1>비전-언어 모델 이해: CLIP 사례</h1>
<p><img src="/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_1.png" alt="이미지"/></p>
<p>비전-언어 모델(VLM)에 대해 생각할 때, 그것들을 AI 세계의 스위스 아미 나이프로 상상해보세요. 다재다능하고 소형이면서 예상치 못하게 강력합니다. 이 분야에서 빛나는 별 한 개는 OpenAI가 개발한 CLIP입니다. 수백만 개의 세심하게 레이블이 붙은 이미지를 사탕 가게에서 사탕을 먹는 아이처럼 먹이는 전통적 모델들이 있는 세계에서, CLIP은 자가 감독 학습 방식으로 젠의 접근을 취합니다. 이 방법은 대조 손실 함수를 사용하여 멋지기만 한 것이 아니라 놀랍도록 효과적으로 작동하여 CLIP이 많은 지도 학습 모델들을 앞설 수 있게 합니다.</p>
<div class="content-ad"></div>
<p>CLIP은 그냥 어떤 모델이 아닙니다. 그것은 다양한 작업을 동시에 수행하는 마에스트로입니다. 이 모델은 우수한 특성 표현을 만들어내는데, 이는 단순히 좋을 뿐만 아니라 수많은 하위 작업들을 해결하는 금빛 열쇠 같습니다. 예를 들어, CLIP에서 임베딩을 사용하면 순식간에 이미지 또는 텍스트 분류 시스템을 만들 수 있습니다. 또는 이 임베딩을 사용하여 텍스트와 이미지를 결합한 가장 가까운 이웃 검색을 수행할 수 있습니다. 마치 화가가 캔버스에 색을 섞는 것처럼요.</p>
<h1>프롬프트 엔지니어링의 진화</h1>
<p>훌륭한 임베딩이면 큰 책임도 따라옵니다. 즉, 그것들을 효과적으로 활용하는 방법을 알아내야 합니다. 이것이바로 프롬프트 엔지니어링, 모델로부터 최상의 답변을 도출하기 위해 완벽한 질문을 만드는 예술입니다. 이는 데이터를 위한 매치메이킹과 같습니다. 이를 통해 질문과 모델이 완벽히 맞는지를 확인합니다. 연구자들은 구문을 다듬어 순수한 요청인 &quot;&#x27;CLASS&#x27;의 사진.&quot;과 같은 것을 &quot;새 종류 중 &#x27;CLASS&#x27;의 사진.&quot;과 같이 더 화려하게 변환하여 결과물을 향상시키고 있습니다.</p>
<div class="content-ad"></div>
<p>그러나 수동으로 이러한 프롬프트를 제작하는 것은 설명서 없이 가구를 조립하는 것만큼 지루합니다. 이에 프롬프트 학습 분야가 등장해 밝은 기사갑옷을 입은 기사처럼 나타났습니다. 이는 강한, 수동으로 만들어진 프롬프트를 더 유연한 것으로 대체합니다 — Coop 및 CoCoOp과 같은 작품에서 나타나는 학습 가능한 임베딩, 즉 컨텍스트 최적화(즉, 소프트 프롬프팅이나 소프트 프롬프터라고도 함). 이러한 동적 임베딩은 특정 작업의 맛을 향상시킬 수 있도록 맞춤형 양념 조합과 같습니다. 예를 들어, 정의된 일련의 범주를 분류하는 것과 같은 특정 작업에 더욱 특화된 작업에 특화된 작업에 대한 임베딩입니다.</p>
<h1>Vision-Language Sub Space Prompting</h1>
<img src="/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_3.png"/>
<p>그들의 논문인 “Vision-Language Sub Space Prompting”에서 컨텍스트 최적화의 일반적인 문제점들이 지적됩니다. 일반적으로 학습 가능한 소프트 프롬프트는 너무 열정적인 학생들처럼 작용합니다 — 익숙한 주제에서 뛰어나지만 새로운 자료에 직면할 때 성능이 떨어지며 새로운 클래스에 마주했을 때 성능이 떨어집니다. 전통적으로 해결책은 수동으로 만들어진 프롬프트 조미료를 약간 뿌리는 것이었지만, 이는 모델이 초기에 학습한 작업에 대해 빛나던 빛깔을 둔화시키는 경우가 많았습니다. 이는 주로 여기서 특징 공간에서 2개의 점 간 거리가 계산되고 점 단독이 항상 수업의 충분한 의미를 포착할 수 없기 때문입니다.</p>
<div class="content-ad"></div>
<p>이를 해결하기 위해 SuPr (Subspace Prompting) 개념이 제안되었습니다. 클래스를 하나의 공간 점으로 나타내는 대신 전체 하위 공간으로 나타내는 새로운 차원으로의 건너뛰기와 같다고 생각해보세요. 일반적인 소프트 프롬프트 세트를 더 짧은 길이로 분할하여 파라미터 수를 증가시키지 않고도 앙상블을 유지할 수 있습니다. 각 세트는 토큰 시퀀스를 생성하여 텍스트 인코더에 공급하여 임베딩의 꽃다발을 만들어내는 하위 공간을 생성합니다 - 그들은 이를 하위 공간의 지원 지점이라고 부릅니다.</p>
<p><img src="/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_4.png" alt="image alt text"/></p>
<p>기존의 유클리드 메트릭을 뒤바꾸는 포인트-서브스페이스 거리를 사용하여 (누가 평범한 유클리드를 좋아할까요?). 이 방법은 단순히 새로운 레이어를 추가하는 것이 아니라 하위 공간이 VLM(비전-언어 모델)과 상호 작용하는 방식을 변경하여 시각적 의미의 더 풍부한 스펙트럼을 포괄할 수 있는 지평을 넓힙니다.</p>
<p><img src="/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_5.png" alt="image alt text"/></p>
<div class="content-ad"></div>
<p>이 방법론을 통해 다른 다중 모달에 적용할 수 있는 가능성이 있으며, 여러 기본 모델에 기반을 둔 차별적인 방식으로 공동 내재 표현이 학습될 것으로 예상됩니다.</p>
<p>참고문헌</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"점들로부터 평면까지 Vision-Language Subspace Prompting","description":"","date":"2024-05-16 17:39","slug":"2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting","content":"\n\n\n![lecture image](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png)\n\n최근에 고급 컴퓨터 비전 수업인 Dr. 이 제가 이끄는 Dr. 송 이제의 흥미로운 게스트 강연을 듣게 되었습니다. Dr. Li는 삼성 AI의 연구 과학자로, 도메인 일반화, 연합 학습 및 PEFT 방법과 같은 인공 지능의 중요 주제를 다루었습니다. 특히, 그의 최신 연구인 \"Vision-Language Sub Space Prompting\"에 초점을 맞춘 프레젠테이션은 정보 전달 뿐만 아니라 유사성 검색과 같은 작업에서 진화하는 방법론을 구체화하는 데 있어서 흥미롭고 사유를 제공했습니다. 그의 통찰력에 영감을 받아, 저는 그의 최신 연구 작업에 대한 직관에 대한 생각을 이 블로그를 통해 공유할 필요성을 느꼈습니다.\n\nSub-space prompting의 복잡성에 뛰어들기 전에, 대규모 언어 모델(LLMs)과 시각-언어 모델(VLMs)의 영역에서 우리를 이 시점으로 이끌어 온 혁신의 역사를 잠깐 돌아봅시다. 여러분, 정말 대단한 여정이었습니다!\n\nLLMs의 초기 발전은 Word2Vec 및 GloVe와 같은 모델의 등장으로 시작되었고, 이 모델은 단어의 밀도 있는 벡터 표현을 생성했습니다. 이 임베딩은 단어 간 의미와 관계를 포착해, 더 정교한 모델을 위한 기초를 마련했습니다. 이는 예전 페이스북에서 누군가가 우리를 \"찌르다\"라는 말만 들어도 정말 즐거웠던 소셜 미디어 초기 시절과 닮았네요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 다음에는 트랜스포머의 시대가 도래했습니다. 특히 2017년 Vaswani 등이 소개한 트랜스포머 아키텍처는 해당 분야를 완전히 혁신했습니다. 트랜스포머는 셀프 어텐션 메커니즘을 활용하여 모델이 문장이나 문서 전체를 병렬로 처리하고 맥락적 관계를 더 효과적으로 포착할 수 있게 했습니다.\n\n이어서 BERT(Bidirectional Encoder Representations from Transformers)와 GPT(Generative Pre-trained Transformer)가 등장했습니다. BERT는 모델이 양방향으로 컨텍스트를 학습하는 이중향 교육 개념을 소개했고, GPT는 비지도 학습과 자기회귀 언어 생성의 힘을 보여줌으로써 언어 모델링 분야에서 새로운 기준을 세웠습니다. GPT가 마치 AI 세계를 황홀한 롤러코스터 여행에 초대하며 기록을 깨고 모두를 감탄시켰다고 상상해보세요.\n\n한편 VLM(Visual Language Models)도 주목을 받고 있었습니다. VisualBERT와 ViLBERT와 같은 모델은 시각적 및 텍스트 데이터를 결합하여 이미지 캡션 달기, 시각적 질문 응답 등 양쪽 모드를 이해해야 하는 작업을 가능하게 했습니다. AI의 동적인 듀오로 생각해보세요. 함께 세계를 보고 설명하며 복잡한 퍼즐을 해결합니다.\n\n이어서 OpenAI의 CLIP(Contrastive Language–Image Pre-training)이 등장합니다. CLIP은 대규모 이미지-텍스트 쌍 데이터셋에서 훈련하여 이미지와 텍스트를 연관시키는 방법을 학습함으로써 제로샷 분류 작업에서 놀라운 성능을 달성했습니다. CLIP은 마치 AI 파티에 등장해 파티 분위기를 살려 이미지와 텍스트를 완벽하게 이해하는 능력으로 모든 이들을 감탄시켰다고 생각해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서브스페이스 프롬프팅의 구체적인 내용을 살펴보면, 기술의 진화를 이해하는 것이 매우 중요합니다. 각 혁신은 이전 성공과 교훈을 기반으로 구축되어 오늘날 우리가 보는 정교한 모델과 방법론으로 이어졌습니다.\n\n# 비전-언어 모델 이해: CLIP 사례\n\n![이미지](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_1.png)\n\n비전-언어 모델(VLM)에 대해 생각할 때, 그것들을 AI 세계의 스위스 아미 나이프로 상상해보세요. 다재다능하고 소형이면서 예상치 못하게 강력합니다. 이 분야에서 빛나는 별 한 개는 OpenAI가 개발한 CLIP입니다. 수백만 개의 세심하게 레이블이 붙은 이미지를 사탕 가게에서 사탕을 먹는 아이처럼 먹이는 전통적 모델들이 있는 세계에서, CLIP은 자가 감독 학습 방식으로 젠의 접근을 취합니다. 이 방법은 대조 손실 함수를 사용하여 멋지기만 한 것이 아니라 놀랍도록 효과적으로 작동하여 CLIP이 많은 지도 학습 모델들을 앞설 수 있게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCLIP은 그냥 어떤 모델이 아닙니다. 그것은 다양한 작업을 동시에 수행하는 마에스트로입니다. 이 모델은 우수한 특성 표현을 만들어내는데, 이는 단순히 좋을 뿐만 아니라 수많은 하위 작업들을 해결하는 금빛 열쇠 같습니다. 예를 들어, CLIP에서 임베딩을 사용하면 순식간에 이미지 또는 텍스트 분류 시스템을 만들 수 있습니다. 또는 이 임베딩을 사용하여 텍스트와 이미지를 결합한 가장 가까운 이웃 검색을 수행할 수 있습니다. 마치 화가가 캔버스에 색을 섞는 것처럼요.\n\n# 프롬프트 엔지니어링의 진화\n\n훌륭한 임베딩이면 큰 책임도 따라옵니다. 즉, 그것들을 효과적으로 활용하는 방법을 알아내야 합니다. 이것이바로 프롬프트 엔지니어링, 모델로부터 최상의 답변을 도출하기 위해 완벽한 질문을 만드는 예술입니다. 이는 데이터를 위한 매치메이킹과 같습니다. 이를 통해 질문과 모델이 완벽히 맞는지를 확인합니다. 연구자들은 구문을 다듬어 순수한 요청인 \"'CLASS'의 사진.\"과 같은 것을 \"새 종류 중 'CLASS'의 사진.\"과 같이 더 화려하게 변환하여 결과물을 향상시키고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 수동으로 이러한 프롬프트를 제작하는 것은 설명서 없이 가구를 조립하는 것만큼 지루합니다. 이에 프롬프트 학습 분야가 등장해 밝은 기사갑옷을 입은 기사처럼 나타났습니다. 이는 강한, 수동으로 만들어진 프롬프트를 더 유연한 것으로 대체합니다 — Coop 및 CoCoOp과 같은 작품에서 나타나는 학습 가능한 임베딩, 즉 컨텍스트 최적화(즉, 소프트 프롬프팅이나 소프트 프롬프터라고도 함). 이러한 동적 임베딩은 특정 작업의 맛을 향상시킬 수 있도록 맞춤형 양념 조합과 같습니다. 예를 들어, 정의된 일련의 범주를 분류하는 것과 같은 특정 작업에 더욱 특화된 작업에 특화된 작업에 대한 임베딩입니다.\n\n# Vision-Language Sub Space Prompting\n\n\u003cimg src=\"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_3.png\" /\u003e\n\n그들의 논문인 “Vision-Language Sub Space Prompting”에서 컨텍스트 최적화의 일반적인 문제점들이 지적됩니다. 일반적으로 학습 가능한 소프트 프롬프트는 너무 열정적인 학생들처럼 작용합니다 — 익숙한 주제에서 뛰어나지만 새로운 자료에 직면할 때 성능이 떨어지며 새로운 클래스에 마주했을 때 성능이 떨어집니다. 전통적으로 해결책은 수동으로 만들어진 프롬프트 조미료를 약간 뿌리는 것이었지만, 이는 모델이 초기에 학습한 작업에 대해 빛나던 빛깔을 둔화시키는 경우가 많았습니다. 이는 주로 여기서 특징 공간에서 2개의 점 간 거리가 계산되고 점 단독이 항상 수업의 충분한 의미를 포착할 수 없기 때문입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이를 해결하기 위해 SuPr (Subspace Prompting) 개념이 제안되었습니다. 클래스를 하나의 공간 점으로 나타내는 대신 전체 하위 공간으로 나타내는 새로운 차원으로의 건너뛰기와 같다고 생각해보세요. 일반적인 소프트 프롬프트 세트를 더 짧은 길이로 분할하여 파라미터 수를 증가시키지 않고도 앙상블을 유지할 수 있습니다. 각 세트는 토큰 시퀀스를 생성하여 텍스트 인코더에 공급하여 임베딩의 꽃다발을 만들어내는 하위 공간을 생성합니다 - 그들은 이를 하위 공간의 지원 지점이라고 부릅니다.\n\n![image alt text](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_4.png)\n\n기존의 유클리드 메트릭을 뒤바꾸는 포인트-서브스페이스 거리를 사용하여 (누가 평범한 유클리드를 좋아할까요?). 이 방법은 단순히 새로운 레이어를 추가하는 것이 아니라 하위 공간이 VLM(비전-언어 모델)과 상호 작용하는 방식을 변경하여 시각적 의미의 더 풍부한 스펙트럼을 포괄할 수 있는 지평을 넓힙니다.\n\n![image alt text](/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법론을 통해 다른 다중 모달에 적용할 수 있는 가능성이 있으며, 여러 기본 모델에 기반을 둔 차별적인 방식으로 공동 내재 표현이 학습될 것으로 예상됩니다.\n\n참고문헌","ogImage":{"url":"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png"},"coverImage":"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png","tag":["Tech"],"readingTime":5},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h1: \"h1\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_0.png\",\n        alt: \"lecture image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"최근에 고급 컴퓨터 비전 수업인 Dr. 이 제가 이끄는 Dr. 송 이제의 흥미로운 게스트 강연을 듣게 되었습니다. Dr. Li는 삼성 AI의 연구 과학자로, 도메인 일반화, 연합 학습 및 PEFT 방법과 같은 인공 지능의 중요 주제를 다루었습니다. 특히, 그의 최신 연구인 \\\"Vision-Language Sub Space Prompting\\\"에 초점을 맞춘 프레젠테이션은 정보 전달 뿐만 아니라 유사성 검색과 같은 작업에서 진화하는 방법론을 구체화하는 데 있어서 흥미롭고 사유를 제공했습니다. 그의 통찰력에 영감을 받아, 저는 그의 최신 연구 작업에 대한 직관에 대한 생각을 이 블로그를 통해 공유할 필요성을 느꼈습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Sub-space prompting의 복잡성에 뛰어들기 전에, 대규모 언어 모델(LLMs)과 시각-언어 모델(VLMs)의 영역에서 우리를 이 시점으로 이끌어 온 혁신의 역사를 잠깐 돌아봅시다. 여러분, 정말 대단한 여정이었습니다!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLMs의 초기 발전은 Word2Vec 및 GloVe와 같은 모델의 등장으로 시작되었고, 이 모델은 단어의 밀도 있는 벡터 표현을 생성했습니다. 이 임베딩은 단어 간 의미와 관계를 포착해, 더 정교한 모델을 위한 기초를 마련했습니다. 이는 예전 페이스북에서 누군가가 우리를 \\\"찌르다\\\"라는 말만 들어도 정말 즐거웠던 소셜 미디어 초기 시절과 닮았네요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그 다음에는 트랜스포머의 시대가 도래했습니다. 특히 2017년 Vaswani 등이 소개한 트랜스포머 아키텍처는 해당 분야를 완전히 혁신했습니다. 트랜스포머는 셀프 어텐션 메커니즘을 활용하여 모델이 문장이나 문서 전체를 병렬로 처리하고 맥락적 관계를 더 효과적으로 포착할 수 있게 했습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이어서 BERT(Bidirectional Encoder Representations from Transformers)와 GPT(Generative Pre-trained Transformer)가 등장했습니다. BERT는 모델이 양방향으로 컨텍스트를 학습하는 이중향 교육 개념을 소개했고, GPT는 비지도 학습과 자기회귀 언어 생성의 힘을 보여줌으로써 언어 모델링 분야에서 새로운 기준을 세웠습니다. GPT가 마치 AI 세계를 황홀한 롤러코스터 여행에 초대하며 기록을 깨고 모두를 감탄시켰다고 상상해보세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"한편 VLM(Visual Language Models)도 주목을 받고 있었습니다. VisualBERT와 ViLBERT와 같은 모델은 시각적 및 텍스트 데이터를 결합하여 이미지 캡션 달기, 시각적 질문 응답 등 양쪽 모드를 이해해야 하는 작업을 가능하게 했습니다. AI의 동적인 듀오로 생각해보세요. 함께 세계를 보고 설명하며 복잡한 퍼즐을 해결합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이어서 OpenAI의 CLIP(Contrastive Language–Image Pre-training)이 등장합니다. CLIP은 대규모 이미지-텍스트 쌍 데이터셋에서 훈련하여 이미지와 텍스트를 연관시키는 방법을 학습함으로써 제로샷 분류 작업에서 놀라운 성능을 달성했습니다. CLIP은 마치 AI 파티에 등장해 파티 분위기를 살려 이미지와 텍스트를 완벽하게 이해하는 능력으로 모든 이들을 감탄시켰다고 생각해보세요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"서브스페이스 프롬프팅의 구체적인 내용을 살펴보면, 기술의 진화를 이해하는 것이 매우 중요합니다. 각 혁신은 이전 성공과 교훈을 기반으로 구축되어 오늘날 우리가 보는 정교한 모델과 방법론으로 이어졌습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"비전-언어 모델 이해: CLIP 사례\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_1.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"비전-언어 모델(VLM)에 대해 생각할 때, 그것들을 AI 세계의 스위스 아미 나이프로 상상해보세요. 다재다능하고 소형이면서 예상치 못하게 강력합니다. 이 분야에서 빛나는 별 한 개는 OpenAI가 개발한 CLIP입니다. 수백만 개의 세심하게 레이블이 붙은 이미지를 사탕 가게에서 사탕을 먹는 아이처럼 먹이는 전통적 모델들이 있는 세계에서, CLIP은 자가 감독 학습 방식으로 젠의 접근을 취합니다. 이 방법은 대조 손실 함수를 사용하여 멋지기만 한 것이 아니라 놀랍도록 효과적으로 작동하여 CLIP이 많은 지도 학습 모델들을 앞설 수 있게 합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CLIP은 그냥 어떤 모델이 아닙니다. 그것은 다양한 작업을 동시에 수행하는 마에스트로입니다. 이 모델은 우수한 특성 표현을 만들어내는데, 이는 단순히 좋을 뿐만 아니라 수많은 하위 작업들을 해결하는 금빛 열쇠 같습니다. 예를 들어, CLIP에서 임베딩을 사용하면 순식간에 이미지 또는 텍스트 분류 시스템을 만들 수 있습니다. 또는 이 임베딩을 사용하여 텍스트와 이미지를 결합한 가장 가까운 이웃 검색을 수행할 수 있습니다. 마치 화가가 캔버스에 색을 섞는 것처럼요.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"프롬프트 엔지니어링의 진화\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훌륭한 임베딩이면 큰 책임도 따라옵니다. 즉, 그것들을 효과적으로 활용하는 방법을 알아내야 합니다. 이것이바로 프롬프트 엔지니어링, 모델로부터 최상의 답변을 도출하기 위해 완벽한 질문을 만드는 예술입니다. 이는 데이터를 위한 매치메이킹과 같습니다. 이를 통해 질문과 모델이 완벽히 맞는지를 확인합니다. 연구자들은 구문을 다듬어 순수한 요청인 \\\"'CLASS'의 사진.\\\"과 같은 것을 \\\"새 종류 중 'CLASS'의 사진.\\\"과 같이 더 화려하게 변환하여 결과물을 향상시키고 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 수동으로 이러한 프롬프트를 제작하는 것은 설명서 없이 가구를 조립하는 것만큼 지루합니다. 이에 프롬프트 학습 분야가 등장해 밝은 기사갑옷을 입은 기사처럼 나타났습니다. 이는 강한, 수동으로 만들어진 프롬프트를 더 유연한 것으로 대체합니다 — Coop 및 CoCoOp과 같은 작품에서 나타나는 학습 가능한 임베딩, 즉 컨텍스트 최적화(즉, 소프트 프롬프팅이나 소프트 프롬프터라고도 함). 이러한 동적 임베딩은 특정 작업의 맛을 향상시킬 수 있도록 맞춤형 양념 조합과 같습니다. 예를 들어, 정의된 일련의 범주를 분류하는 것과 같은 특정 작업에 더욱 특화된 작업에 특화된 작업에 대한 임베딩입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"Vision-Language Sub Space Prompting\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_3.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그들의 논문인 “Vision-Language Sub Space Prompting”에서 컨텍스트 최적화의 일반적인 문제점들이 지적됩니다. 일반적으로 학습 가능한 소프트 프롬프트는 너무 열정적인 학생들처럼 작용합니다 — 익숙한 주제에서 뛰어나지만 새로운 자료에 직면할 때 성능이 떨어지며 새로운 클래스에 마주했을 때 성능이 떨어집니다. 전통적으로 해결책은 수동으로 만들어진 프롬프트 조미료를 약간 뿌리는 것이었지만, 이는 모델이 초기에 학습한 작업에 대해 빛나던 빛깔을 둔화시키는 경우가 많았습니다. 이는 주로 여기서 특징 공간에서 2개의 점 간 거리가 계산되고 점 단독이 항상 수업의 충분한 의미를 포착할 수 없기 때문입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이를 해결하기 위해 SuPr (Subspace Prompting) 개념이 제안되었습니다. 클래스를 하나의 공간 점으로 나타내는 대신 전체 하위 공간으로 나타내는 새로운 차원으로의 건너뛰기와 같다고 생각해보세요. 일반적인 소프트 프롬프트 세트를 더 짧은 길이로 분할하여 파라미터 수를 증가시키지 않고도 앙상블을 유지할 수 있습니다. 각 세트는 토큰 시퀀스를 생성하여 텍스트 인코더에 공급하여 임베딩의 꽃다발을 만들어내는 하위 공간을 생성합니다 - 그들은 이를 하위 공간의 지원 지점이라고 부릅니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_4.png\",\n        alt: \"image alt text\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기존의 유클리드 메트릭을 뒤바꾸는 포인트-서브스페이스 거리를 사용하여 (누가 평범한 유클리드를 좋아할까요?). 이 방법은 단순히 새로운 레이어를 추가하는 것이 아니라 하위 공간이 VLM(비전-언어 모델)과 상호 작용하는 방식을 변경하여 시각적 의미의 더 풍부한 스펙트럼을 포괄할 수 있는 지평을 넓힙니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting_5.png\",\n        alt: \"image alt text\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 방법론을 통해 다른 다중 모달에 적용할 수 있는 가능성이 있으며, 여러 기본 모델에 기반을 둔 차별적인 방식으로 공동 내재 표현이 학습될 것으로 예상됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"참고문헌\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-FromPointstoPlanesVision-LanguageSubspacePrompting"},"buildId":"R94iUTCf1NWeBC_VXjTJG","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>