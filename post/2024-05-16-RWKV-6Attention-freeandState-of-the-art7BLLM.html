<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM" data-gatsby-head="true"/><meta name="twitter:title" content="RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 17:38" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/OFpTzInQeZKWBaqJEukNX/_buildManifest.js" defer=""></script><script src="/_next/static/OFpTzInQeZKWBaqJEukNX/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">1<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>RWKV 신경 구조는 주의를 사용하지 않습니다. 이는 시퀀스 길이에 대해 제곱으로 증가하는 어텐션 계산 비용을 갖는 트랜스포머 아키텍처보다 추론에서 훨씬 더 효율적입니다. 이 글에서 RWKV를 설명하고 사용하는 방법을 보여드렸어요:</p>
<p>RWKV를 개발한 팀은 주기적으로 아키텍처를 개선하고 새로운 모델을 출시합니다. 현재 RWKV의 여섯 번째 버전이 출시되었으며 Hugging Face Hub에서 7B RWKV-6이 공개되었습니다:</p>
<ul>
<li>BlinkDL/rwkv-6-world (Apache 2.0 라이선스)</li>
</ul>
<p>이 모델은 100개 이상의 언어를 지원하며 2.5T 토큰에 대해 사전 훈련되었습니다. 이 사이즈의 LLM 중 비영어권 언어에 대해 최고의 성능을 보여준다고 하네요. 저희 자체 평가에 따르면요:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png" alt="2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png"></p>
<p>Llama 3 8B보다 더 좋아 보입니다. 또한 RWKV-5보다 현저히 더 좋습니다. 그러나 영어 작업에 대해서는 Mistral 7B와 Llama 3 8B 대부분의 벤치마크에서 성과가 낮습니다. 아마도 아키텍처 때문이 아니라 단순히 영어 토큰을 훨씬 적게 학습했기 때문일 것입니다.</p>
<p>제 작업을 지원하려면 AI의 최근 발전에 대한 더 많은 기사/튜토리얼을 제공하는 뉴스레터를 구독해 주시기 바랍니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"RWKV-6 주목할 필요 없이 최신 기술을 활용한 7B LLM","description":"","date":"2024-05-16 17:38","slug":"2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM","content":"\n\nRWKV 신경 구조는 주의를 사용하지 않습니다. 이는 시퀀스 길이에 대해 제곱으로 증가하는 어텐션 계산 비용을 갖는 트랜스포머 아키텍처보다 추론에서 훨씬 더 효율적입니다. 이 글에서 RWKV를 설명하고 사용하는 방법을 보여드렸어요:\n\nRWKV를 개발한 팀은 주기적으로 아키텍처를 개선하고 새로운 모델을 출시합니다. 현재 RWKV의 여섯 번째 버전이 출시되었으며 Hugging Face Hub에서 7B RWKV-6이 공개되었습니다:\n\n- BlinkDL/rwkv-6-world (Apache 2.0 라이선스)\n\n이 모델은 100개 이상의 언어를 지원하며 2.5T 토큰에 대해 사전 훈련되었습니다. 이 사이즈의 LLM 중 비영어권 언어에 대해 최고의 성능을 보여준다고 하네요. 저희 자체 평가에 따르면요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png](/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png)\n\nLlama 3 8B보다 더 좋아 보입니다. 또한 RWKV-5보다 현저히 더 좋습니다. 그러나 영어 작업에 대해서는 Mistral 7B와 Llama 3 8B 대부분의 벤치마크에서 성과가 낮습니다. 아마도 아키텍처 때문이 아니라 단순히 영어 토큰을 훨씬 적게 학습했기 때문일 것입니다.\n\n제 작업을 지원하려면 AI의 최근 발전에 대한 더 많은 기사/튜토리얼을 제공하는 뉴스레터를 구독해 주시기 바랍니다.","ogImage":{"url":"/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png"},"coverImage":"/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png","tag":["Tech"],"readingTime":1},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eRWKV 신경 구조는 주의를 사용하지 않습니다. 이는 시퀀스 길이에 대해 제곱으로 증가하는 어텐션 계산 비용을 갖는 트랜스포머 아키텍처보다 추론에서 훨씬 더 효율적입니다. 이 글에서 RWKV를 설명하고 사용하는 방법을 보여드렸어요:\u003c/p\u003e\n\u003cp\u003eRWKV를 개발한 팀은 주기적으로 아키텍처를 개선하고 새로운 모델을 출시합니다. 현재 RWKV의 여섯 번째 버전이 출시되었으며 Hugging Face Hub에서 7B RWKV-6이 공개되었습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBlinkDL/rwkv-6-world (Apache 2.0 라이선스)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 모델은 100개 이상의 언어를 지원하며 2.5T 토큰에 대해 사전 훈련되었습니다. 이 사이즈의 LLM 중 비영어권 언어에 대해 최고의 성능을 보여준다고 하네요. 저희 자체 평가에 따르면요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png\" alt=\"2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM_0.png\"\u003e\u003c/p\u003e\n\u003cp\u003eLlama 3 8B보다 더 좋아 보입니다. 또한 RWKV-5보다 현저히 더 좋습니다. 그러나 영어 작업에 대해서는 Mistral 7B와 Llama 3 8B 대부분의 벤치마크에서 성과가 낮습니다. 아마도 아키텍처 때문이 아니라 단순히 영어 토큰을 훨씬 적게 학습했기 때문일 것입니다.\u003c/p\u003e\n\u003cp\u003e제 작업을 지원하려면 AI의 최근 발전에 대한 더 많은 기사/튜토리얼을 제공하는 뉴스레터를 구독해 주시기 바랍니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-RWKV-6Attention-freeandState-of-the-art7BLLM"},"buildId":"OFpTzInQeZKWBaqJEukNX","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>