<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>HTTP 요청 로그를 통해 봇 트래픽 측정 및 감사하는 방법 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="HTTP 요청 로그를 통해 봇 트래픽 측정 및 감사하는 방법 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="HTTP 요청 로그를 통해 봇 트래픽 측정 및 감사하는 방법 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs" data-gatsby-head="true"/><meta name="twitter:title" content="HTTP 요청 로그를 통해 봇 트래픽 측정 및 감사하는 방법 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 13:31" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/Rv-NbbtWUaja2joH5WkO_/_buildManifest.js" defer=""></script><script src="/_next/static/Rv-NbbtWUaja2joH5WkO_/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">HTTP 요청 로그를 통해 봇 트래픽 측정 및 감사하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="HTTP 요청 로그를 통해 봇 트래픽 측정 및 감사하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">5<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>현재 인터넷 트래픽의 약 30%는 봇, 스파이더 및 크롤러에 의해 생성됩니다. 이들은 다양한 목적으로 웹을 스캔하는 자동화 프로그램입니다.</p>
<p>이 프로그램 중 일부는 다음과 같은 중요한 인터넷 작업을 수행합니다:</p>
<ul>
<li>검색 엔진 인덱싱</li>
<li>성능 모니터링</li>
<li>인터넷 매핑</li>
<li>선의의 취약점 스캔</li>
</ul>
<p>해당 주제에서 제가 찾을 수 있는 가장 포괄적인 목록은 클라우드플레어의 목록입니다.</p>
<div class="content-ad"></div>
<h1>합법적인 또는 조작된 것인가요?</h1>
<p>좋은 크롤러 외에도, 다른 많은 봇들은 단순히 자원을 낭비하거나 비패치된 취약점을 악용하려 하거나 웹사이트의 콘텐츠를 훔치려고 시도하는 경우가 많습니다.</p>
<p>좋은 봇과 나쁜 봇을 구분하는 것은 어려운 주제입니다. 몇몇 영리한 봇은 사용자 에이전트 필드를 조정하여 실제로 보이도록 속이려고 합니다. 사용자 에이전트 필드는 브라우저가 웹사이트로 보내는 것이며, 자기 자신을 식별하는 방법입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-title class_">User</span>-<span class="hljs-title class_">Agent</span>: <span class="hljs-title class_">Mozilla</span>/<span class="hljs-number">5.0</span> (<span class="hljs-variable constant_">X11</span>; <span class="hljs-title class_">Ubuntu</span>; <span class="hljs-title class_">Linux</span> x86_64; <span class="hljs-attr">rv</span>:<span class="hljs-number">35.0</span>) <span class="hljs-title class_">Gecko</span>/<span class="hljs-number">20100101</span> <span class="hljs-title class_">Firefox</span>/<span class="hljs-number">35.0</span>
</code></pre>
<div class="content-ad"></div>
<p>그래도 프로그래밍으로 요청을 보낼 때, 어떤 개발자든 해당 요청에 원하는 사용자 에이전트를 설정할 수 있어요. 그래서 HTTP 요청이 필수적인 봇에서 시작되었는지를 확인하는 방법은 사용자 에이전트 값을 살펴보는 것 뿐이에요.</p>
<h2>robots.txt는 어떤가요?</h2>
<p>웹사이트 루트에 위치한 robots.txt 파일은 자동화된 요청에 대한 해당 웹사이트의 정책을 나타냅니다. 크롤러(또는 프로그램)가 지시 사항을 존중하고 이행하는 것은 그들에 달려 있어요.</p>
<h1>원본 확인</h1>
<div class="content-ad"></div>
<p>다행히도, 주요 인터넷 업체들은 명확한 확인 지침을 제공하고 최신 IP 목록을 유지하고 있습니다. 이를 통해 누구든지 특정 서비스에서 오는 임의의 요청인지 여부를 확인할 수 있습니다.</p>
<p>지금 당장, 이 정보를 공유하는 기업들 사이에 공통 표준이 없습니다. 이 공통 표준의 부재는 괜찮습니다; 웹에는 더 많은 대화가 필요합니다 — 더 많은 규제가 아닌요.</p>
<p>아래를 읽어보세요.</p>
<h2>Google</h2>
<div class="content-ad"></div>
<p>Google은 사전 정의된 URL에서 제공되는 JSON 형식(기계가 읽을 수 있는 형식)으로 봇 IP 범위를 공개합니다. Google은 현재 세 가지 주요 크롤러를 운영 중입니다:</p>
<ul>
<li>GoogleBot — Google 검색용 초기 크롤러</li>
<li>GoogleBot Special + AdsBot — 광고 및 기타 서비스</li>
<li>GoogleBot User Triggered — 사용자 요청에 따른 색인 작업</li>
</ul>
<h2>Bing</h2>
<p>Microsoft는 검색 결과를 개선하기 위해 BingBot을 사용하며, Google과 동일한 JSON 형식으로 IP 범위를 나열합니다.</p>
<div class="content-ad"></div>
<h2>OpenAI</h2>
<p>GPTBot은 OpenAI의 봇으로, 웹사이트 콘텐츠를 다운로드하는 데 사용될 가능성이 높습니다. 이를 통해 ChatGPT와 같은 AI 모델을 훈련할 수 있습니다. IP 범위는 JSON 형식으로 제공됩니다. robots.txt를 사용하여 이들을 차단할 수 있습니다.</p>
<h2>DuckDuckGo</h2>
<p>DuckDuckGo는 Google의 대체로서 개인정보 보호에 중점을 둔 검색 엔진입니다. 이 페이지에서 각 개별 봇의 IP를 공개합니다.</p>
<div class="content-ad"></div>
<h2>메타</h2>
<p>메타는 웹사이트에 접근하기 위해 Facebook 크롤러를 사용합니다. IP 범위는 radb.net 레지스트리를 사용하여 공개됩니다.</p>
<h2>Amazon, Apple, Baidu, Sogou, Yahoo, Yandex</h2>
<p>이 기업들은 인터넷을 스캔하는 데 사용되는 IP 주소의 고정된 목록이나 자동 업데이트되는 목록을 공개하지 않습니다. 그러나, bot 트래픽이 자신들의 시스템에서 유래되었는지 확인하는 문서화된 방법을 갖고 있습니다. host 명령을 사용하여 확인할 수 있습니다.</p>
<div class="content-ad"></div>
<p>$ host 17.58.101.179
179.101.58.17.in-addr.arpa domain name pointer 17-58-101-179.applebot.apple.com.</p>
<h2>인터넷 아카이브</h2>
<p>인터넷 아카이브 봇은 웨이백 머신/아카이브 닷오르그 프로젝트를 위한 크롤러입니다. 그들은 웹의 콘텐츠를 역사적인 이유로 저장합니다. 웹사이트는 변화하고 페이지는 매일 생성되고 사라집니다.</p>
<p>IABot은 host 명령을 사용하여 확인할 수 있습니다. robots.txt의 내용을 무시하기 때문에 차단하기 어렵습니다.</p>
<div class="content-ad"></div>
<h2>CommonCrawl</h2>
<p>커먼크롤(CommonCrawl)은 인터넷 아카이브와 유사한 비영리 기관입니다. 전체 인터넷의 주기적인 스냅샷 또는 데이터셋을 생성하고 이를 공개적으로 제공합니다.</p>
<p>커먼크롤은 해당 주제에 대한 정보를 과도하게 공유하지 않으며, 실제 CCBot를 식별하는 것은 대규모 온디맨드 AWS 인프라에서 실행되므로 더 복잡합니다.</p>
<p>한 가지 방법은 사용자 에이전트와 호스트를 모두 확인하는 것입니다. 이는 완벽한 방법은 아니지만, 모든 EC2 클라우드 인스턴스가 같은 .amazonaws.com 호스트 이름을 공유하기 때문에 복잡합니다. 그러나 두 가지가 일치하면, 요청이 실제 CCBot에서 시작되었을 가능성이 있습니다.</p>
<div class="content-ad"></div>
<h1>이 스크립트로 변환하기</h1>
<p>GitHub에는 자체 업데이트되는 좋은 봇 IP 목록 및 프로젝트가 많이 있습니다. 주요 단점은 이러한 프로젝트 관리자에게 의존한다는 것입니다. 이러한 프로젝트의 계획 또는 EOL(수익화 종료) 날짜는 알려져 있지 않으며 라이선스도 명확하지 않습니다.</p>
<p>그것을 고려하면 아래 스크립트는 생성자에서 IP 범위에 관한 모든 정보를 다운로드합니다. 그 정보는 직접 제공되는 서비스에서 가져옵니다. 따라서 HTTP 요청 로그 항목의 IP 및 사용자 에이전트를 확인할 수 있습니다.</p>
<p>안타깝게도 이 스니펫은 완전한 Python 패키지로 발전시키기 위해 추가 작업이 필요하므로 현재는 Gist 상태입니다. 호스트 호출로 인해 실시간 사용에 대한 도움 클래스는 충분히 빠르지 않을 것입니다. 50,000개의 로그 항목을 확인하는 데 약 15분이 소요됩니다. 서브프로세스 호출에 대해 교차 플랫폼 작업이 필요할 수 있습니다. IPv6 지원은 없습니다. 가독성을 우선시합니다.</p>
<div class="content-ad"></div>
<p>웹 응용 프로그램 방화벽(WAF)가 실시간으로 이러한 요청을 차단해야 한다면 CloudFlare, RunCloud 등의 검증된 서비스를 선택하세요.</p>
<p>현재 클래스는 과거의 HTTP 서버 로그 항목을 테스트할 수 있습니다. 서버가 받는 건강한 봇/크롤러/스파이더 트래픽의 양을 확인하는 데 사용할 수 있습니다.</p>
<h1>사용 방법</h1>
<p>저는 NGINX 서버의 봇 트래픽을 확인하기 위해 아래 스크립트를 사용했습니다.
아래의 사용 예는 하위 폴더 내에있는 모든 access.log[.*.gz] 파일을 구문 분석합니다.</p>
<div class="content-ad"></div>
<h1>NGINX 로그 파일에서 모든 데이터를 pandas DataFrame으로 로드합니다</h1>
<p>NGINX 로그 파일에서 모든 데이터를 pandas DataFrame으로로드합니다.</p>
<h1>tqdm을 사용하여 요청 정보를 확인하고 진행 상황을 보고합니다</h1>
<p>tqdm을 사용하여 요청 정보를 확인하고 진행 상황을 보고합니다. 이 단계는 시간이 걸릴 수 있으므로 주의해주세요.</p>
<h1>디스크에 보고서 저장 및 요약 출력</h1>
<p>디스크에 보고서를 저장하고 요약을 출력합니다.</p>
<p>이 스크립트는 NGINX 로그 파일에 작동합니다. NGINX는 인그레스 컨트롤러로 가장 인기 있는 선택지이기 때문에 잘 작동합니다.<br>
위 코드 조각들은 어떤 데이터 분석 사용 사례 및 다른 인그레스 컨트롤러에 대한 확장이 가능하도록 충분히 일반적입니다.</p>
<div class="content-ad"></div>
<h1>결론</h1>
<p>사실, 로봇 요청 중에서 4%만 진짜 서비스에서 시작된 것으로 나타났습니다. 즉, 25개 중 1개만이 진짜 요청입니다.</p>
<p>나머지는 단지 대역폭과 자원을 낭비하는 것 뿐입니다.</p>
<p>읽어주셔서 감사합니다!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"HTTP 요청 로그를 통해 봇 트래픽 측정 및 감사하는 방법","description":"","date":"2024-06-23 13:31","slug":"2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs","content":"\n\n현재 인터넷 트래픽의 약 30%는 봇, 스파이더 및 크롤러에 의해 생성됩니다. 이들은 다양한 목적으로 웹을 스캔하는 자동화 프로그램입니다.\n\n이 프로그램 중 일부는 다음과 같은 중요한 인터넷 작업을 수행합니다:\n\n- 검색 엔진 인덱싱\n- 성능 모니터링\n- 인터넷 매핑\n- 선의의 취약점 스캔\n\n해당 주제에서 제가 찾을 수 있는 가장 포괄적인 목록은 클라우드플레어의 목록입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 합법적인 또는 조작된 것인가요?\n\n좋은 크롤러 외에도, 다른 많은 봇들은 단순히 자원을 낭비하거나 비패치된 취약점을 악용하려 하거나 웹사이트의 콘텐츠를 훔치려고 시도하는 경우가 많습니다.\n\n좋은 봇과 나쁜 봇을 구분하는 것은 어려운 주제입니다. 몇몇 영리한 봇은 사용자 에이전트 필드를 조정하여 실제로 보이도록 속이려고 합니다. 사용자 에이전트 필드는 브라우저가 웹사이트로 보내는 것이며, 자기 자신을 식별하는 방법입니다.\n\n```js\nUser-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:35.0) Gecko/20100101 Firefox/35.0\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래도 프로그래밍으로 요청을 보낼 때, 어떤 개발자든 해당 요청에 원하는 사용자 에이전트를 설정할 수 있어요. 그래서 HTTP 요청이 필수적인 봇에서 시작되었는지를 확인하는 방법은 사용자 에이전트 값을 살펴보는 것 뿐이에요.\n\n## robots.txt는 어떤가요?\n\n웹사이트 루트에 위치한 robots.txt 파일은 자동화된 요청에 대한 해당 웹사이트의 정책을 나타냅니다. 크롤러(또는 프로그램)가 지시 사항을 존중하고 이행하는 것은 그들에 달려 있어요.\n\n# 원본 확인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다행히도, 주요 인터넷 업체들은 명확한 확인 지침을 제공하고 최신 IP 목록을 유지하고 있습니다. 이를 통해 누구든지 특정 서비스에서 오는 임의의 요청인지 여부를 확인할 수 있습니다.\n\n지금 당장, 이 정보를 공유하는 기업들 사이에 공통 표준이 없습니다. 이 공통 표준의 부재는 괜찮습니다; 웹에는 더 많은 대화가 필요합니다 — 더 많은 규제가 아닌요.\n\n아래를 읽어보세요.\n## Google\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle은 사전 정의된 URL에서 제공되는 JSON 형식(기계가 읽을 수 있는 형식)으로 봇 IP 범위를 공개합니다. Google은 현재 세 가지 주요 크롤러를 운영 중입니다:\n\n- GoogleBot — Google 검색용 초기 크롤러\n- GoogleBot Special + AdsBot — 광고 및 기타 서비스\n- GoogleBot User Triggered — 사용자 요청에 따른 색인 작업\n\n## Bing\n\nMicrosoft는 검색 결과를 개선하기 위해 BingBot을 사용하며, Google과 동일한 JSON 형식으로 IP 범위를 나열합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## OpenAI\n\nGPTBot은 OpenAI의 봇으로, 웹사이트 콘텐츠를 다운로드하는 데 사용될 가능성이 높습니다. 이를 통해 ChatGPT와 같은 AI 모델을 훈련할 수 있습니다. IP 범위는 JSON 형식으로 제공됩니다. robots.txt를 사용하여 이들을 차단할 수 있습니다.\n\n## DuckDuckGo\n\nDuckDuckGo는 Google의 대체로서 개인정보 보호에 중점을 둔 검색 엔진입니다. 이 페이지에서 각 개별 봇의 IP를 공개합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 메타\n\n메타는 웹사이트에 접근하기 위해 Facebook 크롤러를 사용합니다. IP 범위는 radb.net 레지스트리를 사용하여 공개됩니다.\n\n## Amazon, Apple, Baidu, Sogou, Yahoo, Yandex\n\n이 기업들은 인터넷을 스캔하는 데 사용되는 IP 주소의 고정된 목록이나 자동 업데이트되는 목록을 공개하지 않습니다. 그러나, bot 트래픽이 자신들의 시스템에서 유래되었는지 확인하는 문서화된 방법을 갖고 있습니다. host 명령을 사용하여 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n$ host 17.58.101.179\n179.101.58.17.in-addr.arpa domain name pointer 17-58-101-179.applebot.apple.com.\n\n\n## 인터넷 아카이브\n\n인터넷 아카이브 봇은 웨이백 머신/아카이브 닷오르그 프로젝트를 위한 크롤러입니다. 그들은 웹의 콘텐츠를 역사적인 이유로 저장합니다. 웹사이트는 변화하고 페이지는 매일 생성되고 사라집니다.\n\nIABot은 host 명령을 사용하여 확인할 수 있습니다. robots.txt의 내용을 무시하기 때문에 차단하기 어렵습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## CommonCrawl\n\n커먼크롤(CommonCrawl)은 인터넷 아카이브와 유사한 비영리 기관입니다. 전체 인터넷의 주기적인 스냅샷 또는 데이터셋을 생성하고 이를 공개적으로 제공합니다.\n\n커먼크롤은 해당 주제에 대한 정보를 과도하게 공유하지 않으며, 실제 CCBot를 식별하는 것은 대규모 온디맨드 AWS 인프라에서 실행되므로 더 복잡합니다.\n\n한 가지 방법은 사용자 에이전트와 호스트를 모두 확인하는 것입니다. 이는 완벽한 방법은 아니지만, 모든 EC2 클라우드 인스턴스가 같은 .amazonaws.com 호스트 이름을 공유하기 때문에 복잡합니다. 그러나 두 가지가 일치하면, 요청이 실제 CCBot에서 시작되었을 가능성이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 이 스크립트로 변환하기\n\nGitHub에는 자체 업데이트되는 좋은 봇 IP 목록 및 프로젝트가 많이 있습니다. 주요 단점은 이러한 프로젝트 관리자에게 의존한다는 것입니다. 이러한 프로젝트의 계획 또는 EOL(수익화 종료) 날짜는 알려져 있지 않으며 라이선스도 명확하지 않습니다.\n\n그것을 고려하면 아래 스크립트는 생성자에서 IP 범위에 관한 모든 정보를 다운로드합니다. 그 정보는 직접 제공되는 서비스에서 가져옵니다. 따라서 HTTP 요청 로그 항목의 IP 및 사용자 에이전트를 확인할 수 있습니다.\n\n안타깝게도 이 스니펫은 완전한 Python 패키지로 발전시키기 위해 추가 작업이 필요하므로 현재는 Gist 상태입니다. 호스트 호출로 인해 실시간 사용에 대한 도움 클래스는 충분히 빠르지 않을 것입니다. 50,000개의 로그 항목을 확인하는 데 약 15분이 소요됩니다. 서브프로세스 호출에 대해 교차 플랫폼 작업이 필요할 수 있습니다. IPv6 지원은 없습니다. 가독성을 우선시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n웹 응용 프로그램 방화벽(WAF)가 실시간으로 이러한 요청을 차단해야 한다면 CloudFlare, RunCloud 등의 검증된 서비스를 선택하세요.\n\n현재 클래스는 과거의 HTTP 서버 로그 항목을 테스트할 수 있습니다. 서버가 받는 건강한 봇/크롤러/스파이더 트래픽의 양을 확인하는 데 사용할 수 있습니다.\n\n# 사용 방법\n\n저는 NGINX 서버의 봇 트래픽을 확인하기 위해 아래 스크립트를 사용했습니다.\n아래의 사용 예는 하위 폴더 내에있는 모든 access.log[.*.gz] 파일을 구문 분석합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# NGINX 로그 파일에서 모든 데이터를 pandas DataFrame으로 로드합니다  \nNGINX 로그 파일에서 모든 데이터를 pandas DataFrame으로로드합니다.  \n\n# tqdm을 사용하여 요청 정보를 확인하고 진행 상황을 보고합니다\ntqdm을 사용하여 요청 정보를 확인하고 진행 상황을 보고합니다. 이 단계는 시간이 걸릴 수 있으므로 주의해주세요.  \n\n# 디스크에 보고서 저장 및 요약 출력  \n디스크에 보고서를 저장하고 요약을 출력합니다.  \n\n이 스크립트는 NGINX 로그 파일에 작동합니다. NGINX는 인그레스 컨트롤러로 가장 인기 있는 선택지이기 때문에 잘 작동합니다.  \n위 코드 조각들은 어떤 데이터 분석 사용 사례 및 다른 인그레스 컨트롤러에 대한 확장이 가능하도록 충분히 일반적입니다.  \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n사실, 로봇 요청 중에서 4%만 진짜 서비스에서 시작된 것으로 나타났습니다. 즉, 25개 중 1개만이 진짜 요청입니다.\n\n나머지는 단지 대역폭과 자원을 낭비하는 것 뿐입니다.\n\n읽어주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs_0.png"},"coverImage":"/assets/img/2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs_0.png","tag":["Tech"],"readingTime":5},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e현재 인터넷 트래픽의 약 30%는 봇, 스파이더 및 크롤러에 의해 생성됩니다. 이들은 다양한 목적으로 웹을 스캔하는 자동화 프로그램입니다.\u003c/p\u003e\n\u003cp\u003e이 프로그램 중 일부는 다음과 같은 중요한 인터넷 작업을 수행합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e검색 엔진 인덱싱\u003c/li\u003e\n\u003cli\u003e성능 모니터링\u003c/li\u003e\n\u003cli\u003e인터넷 매핑\u003c/li\u003e\n\u003cli\u003e선의의 취약점 스캔\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e해당 주제에서 제가 찾을 수 있는 가장 포괄적인 목록은 클라우드플레어의 목록입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e합법적인 또는 조작된 것인가요?\u003c/h1\u003e\n\u003cp\u003e좋은 크롤러 외에도, 다른 많은 봇들은 단순히 자원을 낭비하거나 비패치된 취약점을 악용하려 하거나 웹사이트의 콘텐츠를 훔치려고 시도하는 경우가 많습니다.\u003c/p\u003e\n\u003cp\u003e좋은 봇과 나쁜 봇을 구분하는 것은 어려운 주제입니다. 몇몇 영리한 봇은 사용자 에이전트 필드를 조정하여 실제로 보이도록 속이려고 합니다. 사용자 에이전트 필드는 브라우저가 웹사이트로 보내는 것이며, 자기 자신을 식별하는 방법입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eUser\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eAgent\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eMozilla\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e5.0\u003c/span\u003e (\u003cspan class=\"hljs-variable constant_\"\u003eX11\u003c/span\u003e; \u003cspan class=\"hljs-title class_\"\u003eUbuntu\u003c/span\u003e; \u003cspan class=\"hljs-title class_\"\u003eLinux\u003c/span\u003e x86_64; \u003cspan class=\"hljs-attr\"\u003erv\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e35.0\u003c/span\u003e) \u003cspan class=\"hljs-title class_\"\u003eGecko\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e20100101\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFirefox\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e35.0\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그래도 프로그래밍으로 요청을 보낼 때, 어떤 개발자든 해당 요청에 원하는 사용자 에이전트를 설정할 수 있어요. 그래서 HTTP 요청이 필수적인 봇에서 시작되었는지를 확인하는 방법은 사용자 에이전트 값을 살펴보는 것 뿐이에요.\u003c/p\u003e\n\u003ch2\u003erobots.txt는 어떤가요?\u003c/h2\u003e\n\u003cp\u003e웹사이트 루트에 위치한 robots.txt 파일은 자동화된 요청에 대한 해당 웹사이트의 정책을 나타냅니다. 크롤러(또는 프로그램)가 지시 사항을 존중하고 이행하는 것은 그들에 달려 있어요.\u003c/p\u003e\n\u003ch1\u003e원본 확인\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다행히도, 주요 인터넷 업체들은 명확한 확인 지침을 제공하고 최신 IP 목록을 유지하고 있습니다. 이를 통해 누구든지 특정 서비스에서 오는 임의의 요청인지 여부를 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e지금 당장, 이 정보를 공유하는 기업들 사이에 공통 표준이 없습니다. 이 공통 표준의 부재는 괜찮습니다; 웹에는 더 많은 대화가 필요합니다 — 더 많은 규제가 아닌요.\u003c/p\u003e\n\u003cp\u003e아래를 읽어보세요.\u003c/p\u003e\n\u003ch2\u003eGoogle\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eGoogle은 사전 정의된 URL에서 제공되는 JSON 형식(기계가 읽을 수 있는 형식)으로 봇 IP 범위를 공개합니다. Google은 현재 세 가지 주요 크롤러를 운영 중입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGoogleBot — Google 검색용 초기 크롤러\u003c/li\u003e\n\u003cli\u003eGoogleBot Special + AdsBot — 광고 및 기타 서비스\u003c/li\u003e\n\u003cli\u003eGoogleBot User Triggered — 사용자 요청에 따른 색인 작업\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBing\u003c/h2\u003e\n\u003cp\u003eMicrosoft는 검색 결과를 개선하기 위해 BingBot을 사용하며, Google과 동일한 JSON 형식으로 IP 범위를 나열합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eOpenAI\u003c/h2\u003e\n\u003cp\u003eGPTBot은 OpenAI의 봇으로, 웹사이트 콘텐츠를 다운로드하는 데 사용될 가능성이 높습니다. 이를 통해 ChatGPT와 같은 AI 모델을 훈련할 수 있습니다. IP 범위는 JSON 형식으로 제공됩니다. robots.txt를 사용하여 이들을 차단할 수 있습니다.\u003c/p\u003e\n\u003ch2\u003eDuckDuckGo\u003c/h2\u003e\n\u003cp\u003eDuckDuckGo는 Google의 대체로서 개인정보 보호에 중점을 둔 검색 엔진입니다. 이 페이지에서 각 개별 봇의 IP를 공개합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e메타\u003c/h2\u003e\n\u003cp\u003e메타는 웹사이트에 접근하기 위해 Facebook 크롤러를 사용합니다. IP 범위는 radb.net 레지스트리를 사용하여 공개됩니다.\u003c/p\u003e\n\u003ch2\u003eAmazon, Apple, Baidu, Sogou, Yahoo, Yandex\u003c/h2\u003e\n\u003cp\u003e이 기업들은 인터넷을 스캔하는 데 사용되는 IP 주소의 고정된 목록이나 자동 업데이트되는 목록을 공개하지 않습니다. 그러나, bot 트래픽이 자신들의 시스템에서 유래되었는지 확인하는 문서화된 방법을 갖고 있습니다. host 명령을 사용하여 확인할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e$ host 17.58.101.179\n179.101.58.17.in-addr.arpa domain name pointer 17-58-101-179.applebot.apple.com.\u003c/p\u003e\n\u003ch2\u003e인터넷 아카이브\u003c/h2\u003e\n\u003cp\u003e인터넷 아카이브 봇은 웨이백 머신/아카이브 닷오르그 프로젝트를 위한 크롤러입니다. 그들은 웹의 콘텐츠를 역사적인 이유로 저장합니다. 웹사이트는 변화하고 페이지는 매일 생성되고 사라집니다.\u003c/p\u003e\n\u003cp\u003eIABot은 host 명령을 사용하여 확인할 수 있습니다. robots.txt의 내용을 무시하기 때문에 차단하기 어렵습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003eCommonCrawl\u003c/h2\u003e\n\u003cp\u003e커먼크롤(CommonCrawl)은 인터넷 아카이브와 유사한 비영리 기관입니다. 전체 인터넷의 주기적인 스냅샷 또는 데이터셋을 생성하고 이를 공개적으로 제공합니다.\u003c/p\u003e\n\u003cp\u003e커먼크롤은 해당 주제에 대한 정보를 과도하게 공유하지 않으며, 실제 CCBot를 식별하는 것은 대규모 온디맨드 AWS 인프라에서 실행되므로 더 복잡합니다.\u003c/p\u003e\n\u003cp\u003e한 가지 방법은 사용자 에이전트와 호스트를 모두 확인하는 것입니다. 이는 완벽한 방법은 아니지만, 모든 EC2 클라우드 인스턴스가 같은 .amazonaws.com 호스트 이름을 공유하기 때문에 복잡합니다. 그러나 두 가지가 일치하면, 요청이 실제 CCBot에서 시작되었을 가능성이 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e이 스크립트로 변환하기\u003c/h1\u003e\n\u003cp\u003eGitHub에는 자체 업데이트되는 좋은 봇 IP 목록 및 프로젝트가 많이 있습니다. 주요 단점은 이러한 프로젝트 관리자에게 의존한다는 것입니다. 이러한 프로젝트의 계획 또는 EOL(수익화 종료) 날짜는 알려져 있지 않으며 라이선스도 명확하지 않습니다.\u003c/p\u003e\n\u003cp\u003e그것을 고려하면 아래 스크립트는 생성자에서 IP 범위에 관한 모든 정보를 다운로드합니다. 그 정보는 직접 제공되는 서비스에서 가져옵니다. 따라서 HTTP 요청 로그 항목의 IP 및 사용자 에이전트를 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e안타깝게도 이 스니펫은 완전한 Python 패키지로 발전시키기 위해 추가 작업이 필요하므로 현재는 Gist 상태입니다. 호스트 호출로 인해 실시간 사용에 대한 도움 클래스는 충분히 빠르지 않을 것입니다. 50,000개의 로그 항목을 확인하는 데 약 15분이 소요됩니다. 서브프로세스 호출에 대해 교차 플랫폼 작업이 필요할 수 있습니다. IPv6 지원은 없습니다. 가독성을 우선시합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e웹 응용 프로그램 방화벽(WAF)가 실시간으로 이러한 요청을 차단해야 한다면 CloudFlare, RunCloud 등의 검증된 서비스를 선택하세요.\u003c/p\u003e\n\u003cp\u003e현재 클래스는 과거의 HTTP 서버 로그 항목을 테스트할 수 있습니다. 서버가 받는 건강한 봇/크롤러/스파이더 트래픽의 양을 확인하는 데 사용할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e사용 방법\u003c/h1\u003e\n\u003cp\u003e저는 NGINX 서버의 봇 트래픽을 확인하기 위해 아래 스크립트를 사용했습니다.\n아래의 사용 예는 하위 폴더 내에있는 모든 access.log[.*.gz] 파일을 구문 분석합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003eNGINX 로그 파일에서 모든 데이터를 pandas DataFrame으로 로드합니다\u003c/h1\u003e\n\u003cp\u003eNGINX 로그 파일에서 모든 데이터를 pandas DataFrame으로로드합니다.\u003c/p\u003e\n\u003ch1\u003etqdm을 사용하여 요청 정보를 확인하고 진행 상황을 보고합니다\u003c/h1\u003e\n\u003cp\u003etqdm을 사용하여 요청 정보를 확인하고 진행 상황을 보고합니다. 이 단계는 시간이 걸릴 수 있으므로 주의해주세요.\u003c/p\u003e\n\u003ch1\u003e디스크에 보고서 저장 및 요약 출력\u003c/h1\u003e\n\u003cp\u003e디스크에 보고서를 저장하고 요약을 출력합니다.\u003c/p\u003e\n\u003cp\u003e이 스크립트는 NGINX 로그 파일에 작동합니다. NGINX는 인그레스 컨트롤러로 가장 인기 있는 선택지이기 때문에 잘 작동합니다.\u003cbr\u003e\n위 코드 조각들은 어떤 데이터 분석 사용 사례 및 다른 인그레스 컨트롤러에 대한 확장이 가능하도록 충분히 일반적입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e사실, 로봇 요청 중에서 4%만 진짜 서비스에서 시작된 것으로 나타났습니다. 즉, 25개 중 1개만이 진짜 요청입니다.\u003c/p\u003e\n\u003cp\u003e나머지는 단지 대역폭과 자원을 낭비하는 것 뿐입니다.\u003c/p\u003e\n\u003cp\u003e읽어주셔서 감사합니다!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-HowtoMeasureandAuditBotTrafficbyHTTPRequestLogs"},"buildId":"Rv-NbbtWUaja2joH5WkO_","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>