<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>파이토치를 처음부터 다시 만들어보기 GPU 지원 및 자동 미분 기능 포함 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="파이토치를 처음부터 다시 만들어보기 GPU 지원 및 자동 미분 기능 포함 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="파이토치를 처음부터 다시 만들어보기 GPU 지원 및 자동 미분 기능 포함 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation" data-gatsby-head="true"/><meta name="twitter:title" content="파이토치를 처음부터 다시 만들어보기 GPU 지원 및 자동 미분 기능 포함 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 10:33" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/t9N7vwmpvBMQnO2PSctoH/_buildManifest.js" defer=""></script><script src="/_next/static/t9N7vwmpvBMQnO2PSctoH/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">파이토치를 처음부터 다시 만들어보기 GPU 지원 및 자동 미분 기능 포함</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="파이토치를 처음부터 다시 만들어보기 GPU 지원 및 자동 미분 기능 포함" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">40<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>C/C++, CUDA, 및 Python을 기반으로 한 고유의 딥 러닝 프레임워크를 구축해 보세요. GPU 지원과 자동 미분을 제공합니다</h2>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png" alt="image"></p>
<h1>소개</h1>
<p>여러 해 동안 PyTorch를 사용하여 딥 러닝 모델을 구축하고 훈련해 왔습니다. 그럼에도 불구하고, 그 문법과 규칙을 익히고도, 제 궁금증을 자극하던 것이 있었습니다: 이러한 작업 중에 내부에서 어떤 일이 일어나고 있는 걸까요? 이 모든 것이 어떻게 작동할까요?</p>
<p>여기까지 오셨다면, 아마도 비슷한 질문을 가지고 계실 것입니다. 파이토치(PyTorch)에서 모델을 생성하고 훈련하는 방법을 물어본다면 아마도 아래 코드와 비슷한 것을 생각해볼 것입니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.<span class="hljs-property">nn</span> <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.<span class="hljs-property">optim</span> <span class="hljs-keyword">as</span> optim

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.<span class="hljs-property">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self):
        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">MyModel</span>, self).<span class="hljs-title function_">__init__</span>()
        self.<span class="hljs-property">fc1</span> = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>)
        self.<span class="hljs-property">sigmoid</span> = nn.<span class="hljs-title class_">Sigmoid</span>()
        self.<span class="hljs-property">fc2</span> = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">10</span>, <span class="hljs-number">1</span>)

    def <span class="hljs-title function_">forward</span>(self, x):
        out = self.<span class="hljs-title function_">fc1</span>(x)
        out = self.<span class="hljs-title function_">sigmoid</span>(out)
        out = self.<span class="hljs-title function_">fc2</span>(out)
        
        <span class="hljs-keyword">return</span> out

...

model = <span class="hljs-title class_">MyModel</span>().<span class="hljs-title function_">to</span>(device)
criterion = nn.<span class="hljs-title class_">MSELoss</span>()
optimizer = optim.<span class="hljs-title function_">SGD</span>(model.<span class="hljs-title function_">parameters</span>(), lr=<span class="hljs-number">0.001</span>)

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(epochs):
    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> ...
        
        x = x.<span class="hljs-title function_">to</span>(device)
        y = y.<span class="hljs-title function_">to</span>(device)

        outputs = <span class="hljs-title function_">model</span>(x)
        loss = <span class="hljs-title function_">criterion</span>(outputs, y)
        
        optimizer.<span class="hljs-title function_">zero_grad</span>()
        loss.<span class="hljs-title function_">backward</span>()
        optimizer.<span class="hljs-title function_">step</span>()
</code></pre>
<p>하지만 이번에 역전파(backward) 단계가 어떻게 작동하는지 물어본다면 어떨까요? 또는 예를 들어, 텐서를 재구성할 때 무슨 일이 일어나는지 궁금하시다면요? 내부에서 데이터가 재배치되나요? 그런 일이 어떻게 일어나나요? 왜 PyTorch는 빠른가요? PyTorch가 GPU 연산을 어떻게 처리하는지요? 이런 질문들이 항상 저를 호기심 가득하게 만들었고, 여러분도 마찬가지로 호기심이 드실 것이라고 상상합니다. 그래서 이러한 개념을 더 잘 이해하기 위해 스스로 텐서 라이브러리를 처음부터 구축해보는 것이 무엇보다 좋을까요? 이 글에서 여러분이 배우게 될 것이 바로 그겁니다!</p>
<h2>#1 — 텐서</h2>
<p>텐서 라이브러리를 구축하기 위해 가장 먼저 알아야 할 개념은 무엇이 텐서인지에 대한 명백한 개념입니다.</p>
<p>텐서는 몇 가지 숫자를 포함하는 n차원 데이터 구조의 수학적 개념이라는 직관적인 생각을 가지고 있을 수 있습니다. 그러나 여기서는 이 데이터 구조를 계산적 관점에서 어떻게 모델링할지 이해해야 합니다. 텐서는 데이터 자체뿐만 아니라 모양이나 텐서가 있는 장치(예: CPU 메모리, GPU 메모리)와 같은 측면을 설명하는 메타데이터로 구성된다고 생각할 수 있습니다.</p>
<p>텐서의 내부를 이해하는 데 매우 중요한 개념인 stride라는 잘 알려지지 않은 메타데이터도 있습니다. 따라서 텐서 데이터 재배열의 내부를 이해하기 위해 약간 더 이에 대해 논의해야 합니다.</p>
<p>2-D 텐서의 모양이 [4, 8]인 경우를 상상해보세요.</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_2.png" alt="텐서"></p>
<p>텐서의 데이터(즉, 부동 소수점 수)는 실제로 메모리에 1차원 배열로 저장됩니다.</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_3.png" alt="데이터"></p>
<p>그러면 이 1차원 배열을 N차원 텐서로 나타내려면 스트라이드를 사용합니다. 기본 아이디어는 다음과 같습니다:</p>
<p>4행 8열의 행렬이 있습니다. 그 행렬의 모든 원소가 1차원 배열의 행에 의해 구성되어 있다고 가정할 때, 위치 [2, 3]의 값을 액세스하려면 2행(각 행에 8개의 요소)을 횡단해야 하며 추가로 3개의 위치를 지나야 합니다. 수학적으로 표현하면 1차원 배열에서 3 + 2 * 8 요소를 횡단해야 합니다.</p>
<p>따라서, '8'은 두 번째 차원의 스트라이드입니다. 이 경우, 배열에서 다른 위치로 "점프"하기 위해 몇 개의 요소를 횡단해야 하는지를 나타내는 정보입니다.</p>
<p>따라서, 모양이 [shape_0, shape_1]인 2차원 텐서의 요소 [i, j]에 액세스하려면, 기본적으로 j + i * shape_1 위치에 있는 요소에 액세스해야 합니다.</p>
<p>이제 3차원 텐서를 상상해보겠습니다:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_5.png" alt="image"></p>
<p>이 3차원 텐서를 행렬의 시퀀스로 생각할 수 있습니다. 예를 들어, 이 [5, 4, 8] 텐서를 [4, 8] 모양의 5개 행렬로 생각할 수 있습니다.</p>
<p>이제 [1, 3, 7] 위치에 있는 요소에 액세스하기 위해 [4,8] 형태의 행렬을 1개 완전히 횡단하고, [8] 형태의 행을 2개, [1] 형태의 열을 7개 횡단해야 합니다. 따라서 1차원 배열에서 (1 * 4 * 8) + (2 * 8) + (7 * 1) 위치를 횡단해야 합니다.</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_6.png" alt="image"></p>
<p>따라서, [shape_0, shape_1, shape_2] 모양의 3차원 텐서에서 1차원 데이터 배열에서 [i][j][k] 요소에 액세스하는 방법은 다음과 같습니다:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_7.png" alt="image"></p>
<p>이 shape_1 * shape_2가 첫 번째 차원의 stride이고, shape_2는 두 번째 차원의 stride이며 1은 세 번째 차원의 stride입니다.</p>
<p>그런 다음, 일반화하기 위해서는:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_8.png" alt="image"></p>
<p>각 차원의 stride는 다음 차원 텐서 모양의 곱을 사용하여 계산할 수 있습니다:</p>
<p>그런 다음 stride[n-1] = 1로 설정합니다.</p>
<p>우리의 형태의 텐서 예제 [5, 4, 8]에서 strides = [4*8, 8, 1] = [32, 8, 1]일 것입니다.</p>
<p>여러분들도 직접 테스트할 수 있어요:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> torch

torch.<span class="hljs-title function_">rand</span>([<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>]).<span class="hljs-title function_">stride</span>()
#(<span class="hljs-number">32</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>)
</code></pre>
<p>알겠어요, 그런데 왜 모양과 스트라이드가 필요한 건가요? N차원 텐서의 요소에 접근하는 것을 넘어, 이 개념은 텐서 배열을 매우 쉽게 조작하는 데 사용될 수 있어요.</p>
<p>예를 들어, 텐서를 재구성하려면 새로운 모양을 설정하고 새로운 스트라이드를 계산하면 됩니다! (새로운 모양은 동일한 요소 수를 보장하므로)</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> torch

t = torch.<span class="hljs-title function_">rand</span>([<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>])

<span class="hljs-title function_">print</span>(t.<span class="hljs-property">shape</span>)
# [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>]

<span class="hljs-title function_">print</span>(t.<span class="hljs-title function_">stride</span>())
# [<span class="hljs-number">32</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>]

new_t = t.<span class="hljs-title function_">reshape</span>([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])

<span class="hljs-title function_">print</span>(new_t.<span class="hljs-property">shape</span>)
# [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]

<span class="hljs-title function_">print</span>(new_t.<span class="hljs-title function_">stride</span>())
# [<span class="hljs-number">40</span>, <span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]
</code></pre>
<p>텐서 내부에서는 여전히 동일한 1차원 배열로 저장됩니다. reshape 메서드는 배열 내 요소의 순서를 변경하지 않았습니다! 대단하지 않나요? 😁</p>
<p>다음 함수를 사용하여 PyTorch에서 내부 1차원 배열에 액세스하는 함수를 사용하여 직접 확인할 수 있습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> ctypes

def <span class="hljs-title function_">print_internal</span>(<span class="hljs-attr">t</span>: torch.<span class="hljs-property">Tensor</span>):
    <span class="hljs-title function_">print</span>(
        torch.<span class="hljs-title function_">frombuffer</span>(
            ctypes.<span class="hljs-title function_">string_at</span>(t.<span class="hljs-title function_">data_ptr</span>(), t.<span class="hljs-title function_">storage</span>().<span class="hljs-title function_">nbytes</span>()), dtype=t.<span class="hljs-property">dtype</span>
        )
    )

<span class="hljs-title function_">print_internal</span>(t)
# [<span class="hljs-number">0.0752</span>, <span class="hljs-number">0.5898</span>, <span class="hljs-number">0.3930</span>, <span class="hljs-number">0.9577</span>, <span class="hljs-number">0.2276</span>, <span class="hljs-number">0.9786</span>, <span class="hljs-number">0.1009</span>, <span class="hljs-number">0.138</span>, ...

<span class="hljs-title function_">print_internal</span>(new_t)
# [<span class="hljs-number">0.0752</span>, <span class="hljs-number">0.5898</span>, <span class="hljs-number">0.3930</span>, <span class="hljs-number">0.9577</span>, <span class="hljs-number">0.2276</span>, <span class="hljs-number">0.9786</span>, <span class="hljs-number">0.1009</span>, <span class="hljs-number">0.138</span>, ...
</code></pre>
<p>예를 들어 두 축을 전치하려면 내부적으로 해당 스트라이드를 단순히 바꾸어 주면 됩니다!</p>
<pre><code class="hljs language-js">t = torch.<span class="hljs-title function_">arange</span>(<span class="hljs-number">0</span>, <span class="hljs-number">24</span>).<span class="hljs-title function_">reshape</span>(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
<span class="hljs-title function_">print</span>(t)
# [[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],
#   [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>],
#   [ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]],
 
#  [[<span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],
#   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>],
#   [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]]]

<span class="hljs-title function_">print</span>(t.<span class="hljs-property">shape</span>)
# [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]

<span class="hljs-title function_">print</span>(t.<span class="hljs-title function_">stride</span>())
# [<span class="hljs-number">12</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>]

new_t = t.<span class="hljs-title function_">transpose</span>(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
<span class="hljs-title function_">print</span>(new_t)
# [[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],
#   [<span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>]],

#  [[ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>],
#   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>]],

#  [[ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>],
#   [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]]]

<span class="hljs-title function_">print</span>(new_t.<span class="hljs-property">shape</span>)
# [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]

<span class="hljs-title function_">print</span>(new_t.<span class="hljs-title function_">stride</span>())
# [<span class="hljs-number">4</span>, <span class="hljs-number">12</span>, <span class="hljs-number">1</span>]
</code></pre>
<p>내부 배열을 출력하면 두 값 모두 동일합니다:</p>
<pre><code class="hljs language-js"><span class="hljs-title function_">print_internal</span>(t)
# [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]

<span class="hljs-title function_">print_internal</span>(new_t)
# [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]
</code></pre>
<p>그러나 new_t의 스트라이드는 이제 위의 식과 일치하지 않습니다. 이것은 텐서가 이제 연속적이지 않기 때문에 발생합니다. 즉, 내부 배열은 동일하지만 메모리 내의 값의 순서가 텐서의 실제 순서와 일치하지 않는다는 것을 의미합니다.</p>
<pre><code class="hljs language-js">t.<span class="hljs-title function_">is_contiguous</span>()
# <span class="hljs-title class_">True</span>

new_t.<span class="hljs-title function_">is_contiguous</span>()
# <span class="hljs-title class_">False</span>
</code></pre>
<p>이는 연속되지 않는 요소에 연속적으로 액세스하는 것이 효율적이지 않다는 것을 의미합니다 (실제 텐서 요소는 메모리 상에서 순서대로 정렬되어 있지 않기 때문입니다). 이를 해결하기 위해 다음을 수행할 수 있습니다:</p>
<pre><code class="hljs language-js">new_t_contiguous = new_t.<span class="hljs-title function_">contiguous</span>()

<span class="hljs-title function_">print</span>(new_t_contiguous.<span class="hljs-title function_">is_contiguous</span>())
# <span class="hljs-title class_">True</span>
</code></pre>
<p>내부 배열을 분석하면 이제 순서가 실제 텐서 순서와 일치하여 더 나은 메모리 액세스 효율을 제공할 수 있습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-title function_">print</span>(new_t)
# [[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],
#   [<span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>]],

#  [[ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>],
#   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>]],

#  [[ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>],
#   [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]]]

<span class="hljs-title function_">print_internal</span>(new_t)
# [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]

<span class="hljs-title function_">print_internal</span>(new_t_contiguous)
# [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>]
</code></pre>
<p>이제 우리는 텐서가 어떻게 모델링되는지 이해했으니, 라이브러리 생성을 시작해 봅시다!</p>
<p>내가 만들 라이브러리 이름은 Norch입니다. PyTorch가 아닌 (NOT PyTorch)을 의미하며, 성(Nogueira)을 암시하기도 합니다. 😁</p>
<p>첫 번째로 알아야 할 것은 PyTorch가 Python을 통해 사용되지만 내부적으로는 C/C++로 실행된다는 것입니다. 그래서 먼저 내부 C/C++ 함수를 만들 것입니다.</p>
<p>먼저 텐서를 데이터와 메타데이터를 저장하는 구조체로 정의하고 이를 만들기 위한 함수를 생성할 수 있습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/tensor.cpp</span>

#include &#x3C;stdio.<span class="hljs-property">h</span>>
#include &#x3C;stdlib.<span class="hljs-property">h</span>>
#include &#x3C;string.<span class="hljs-property">h</span>>
#include &#x3C;math.<span class="hljs-property">h</span>>

typedef struct {
    float* data;
    int* strides;
    int* shape;
    int ndim;
    int size;
    char* device;
} <span class="hljs-title class_">Tensor</span>;

<span class="hljs-title class_">Tensor</span>* <span class="hljs-title function_">create_tensor</span>(<span class="hljs-params">float* data, int* shape, int ndim</span>) {
    
    <span class="hljs-title class_">Tensor</span>* tensor = (<span class="hljs-title class_">Tensor</span>*)<span class="hljs-title function_">malloc</span>(<span class="hljs-title function_">sizeof</span>(<span class="hljs-title class_">Tensor</span>));
    <span class="hljs-keyword">if</span> (tensor == <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }
    tensor->data = data;
    tensor->shape = shape;
    tensor->ndim = ndim;

    tensor->size = <span class="hljs-number">1</span>;
    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; ndim; i++) {
        tensor->size *= shape[i];
    }

    tensor->strides = (int*)<span class="hljs-title function_">malloc</span>(ndim * <span class="hljs-title function_">sizeof</span>(int));
    <span class="hljs-keyword">if</span> (tensor->strides == <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }
    int stride = <span class="hljs-number">1</span>;
    <span class="hljs-keyword">for</span> (int i = ndim - <span class="hljs-number">1</span>; i >= <span class="hljs-number">0</span>; i--) {
        tensor->strides[i] = stride;
        stride *= shape[i];
    }
    
    <span class="hljs-keyword">return</span> tensor;
}
</code></pre>
<p>일부 요소에 접근하기 위해서는 앞서 배웠던 스트라이드(strides)를 활용할 수 있습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/tensor.cpp</span>

float <span class="hljs-title function_">get_item</span>(<span class="hljs-params">Tensor* tensor, int* indices</span>) {
    int index = <span class="hljs-number">0</span>;
    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; tensor->ndim; i++) {
        index += indices[i] * tensor->strides[i];
    }

    float result;
    result = tensor->data[index];

    <span class="hljs-keyword">return</span> result;
}
</code></pre>
<p>이제 텐서 작업을 만들 수 있습니다. 몇 가지 예제를 보여드리겠고, 이 글 끝에 링크된 저장소에서 완전한 버전을 찾을 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/cpu.cpp</span>

<span class="hljs-keyword">void</span> <span class="hljs-title function_">add_tensor_cpu</span>(<span class="hljs-params">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {
    
    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; tensor1->size; i++) {
        result_data[i] = tensor1->data[i] + tensor2->data[i];
    }
}

<span class="hljs-keyword">void</span> <span class="hljs-title function_">sub_tensor_cpu</span>(<span class="hljs-params">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {
    
    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; tensor1->size; i++) {
        result_data[i] = tensor1->data[i] - tensor2->data[i];
    }
}

<span class="hljs-keyword">void</span> <span class="hljs-title function_">elementwise_mul_tensor_cpu</span>(<span class="hljs-params">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {
    
    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; tensor1->size; i++) {
        result_data[i] = tensor1->data[i] * tensor2->data[i];
    }
}

<span class="hljs-keyword">void</span> <span class="hljs-title function_">assign_tensor_cpu</span>(<span class="hljs-params">Tensor* tensor, float* result_data</span>) {

    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; tensor->size; i++) {
        result_data[i] = tensor->data[i];
    }
}

...
</code></pre>
<p>그 다음에, 이러한 작업들을 호출할 텐서 다른 함수를 만들 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/tensor.cpp</span>

<span class="hljs-title class_">Tensor</span>* <span class="hljs-title function_">add_tensor</span>(<span class="hljs-params">Tensor* tensor1, Tensor* tensor2</span>) {
    <span class="hljs-keyword">if</span> (tensor1->ndim != tensor2->ndim) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"덧셈을 위해서 텐서는 동일한 차원 수여야 합니다 %d 와 %d\n"</span>, tensor1->ndim, tensor2->ndim);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }

    int ndim = tensor1->ndim;
    int* shape = (int*)<span class="hljs-title function_">malloc</span>(ndim * <span class="hljs-title function_">sizeof</span>(int));
    <span class="hljs-keyword">if</span> (shape == <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }

    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; ndim; i++) {
        <span class="hljs-keyword">if</span> (tensor1->shape[i] != tensor2->shape[i]) {
            <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"덧셈을 위해서 텐서는 동일한 모양이어야 합니다 %d 와 %d 인덱스 %d에서\n"</span>, tensor1->shape[i], tensor2->shape[i], i);
            <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
        }
        shape[i] = tensor1->shape[i];
    }        
    float* result_data = (float*)<span class="hljs-title function_">malloc</span>(tensor1->size * <span class="hljs-title function_">sizeof</span>(float));
    <span class="hljs-keyword">if</span> (result_data == <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }
    <span class="hljs-title function_">add_tensor_cpu</span>(tensor1, tensor2, result_data);
    
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">create_tensor</span>(result_data, shape, ndim, device);
}
</code></pre>
<p>이전에 언급한 대로, 텐서 재구성은 내부 데이터 배열을 수정하지 않습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/tensor.cpp</span>

<span class="hljs-title class_">Tensor</span>* <span class="hljs-title function_">reshape_tensor</span>(<span class="hljs-params">Tensor* tensor, int* new_shape, int new_ndim</span>) {

    int ndim = new_ndim;
    int* shape = (int*)<span class="hljs-title function_">malloc</span>(ndim * <span class="hljs-title function_">sizeof</span>(int));
    <span class="hljs-keyword">if</span> (shape == <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }

    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; ndim; i++) {
        shape[i] = new_shape[i];
    }

    <span class="hljs-comment">// 새 모양의 요소 총 수 계산</span>
    int size = <span class="hljs-number">1</span>;
    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; new_ndim; i++) {
        size *= shape[i];
    }

    <span class="hljs-comment">// 총 요소 수가 현재 텐서의 크기와 일치하는지 확인</span>
    <span class="hljs-keyword">if</span> (size != tensor->size) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"텐서를 재구성할 수 없습니다. 새 모양의 요소 총 수가 현재 텐서의 크기와 일치하지 않습니다.\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }

    float* result_data = (float*)<span class="hljs-title function_">malloc</span>(tensor->size * <span class="hljs-title function_">sizeof</span>(float));
    <span class="hljs-keyword">if</span> (result_data == <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }
    <span class="hljs-title function_">assign_tensor_cpu</span>(tensor, result_data);
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">create_tensor</span>(result_data, shape, ndim, device);
}
</code></pre>
<p>이제 일부 텐서 작업을 수행할 수 있지만, 누구나 C/C++을 사용하여 실행해야 하는 것은 아닙니다. 이제 Python 래퍼를 만들어 봅시다!</p>
<p>Python을 사용하여 C/C++ 코드를 실행할 수 있는 다양한 옵션이 있습니다. Pybind11과 Cython 등이 있습니다. 이 예시에서는 ctypes를 사용할 것입니다.</p>
<p>아래는 ctypes의 기본적인 구조입니다:</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//C 코드</span>
#include &#x3C;stdio.<span class="hljs-property">h</span>>

float <span class="hljs-title function_">add_floats</span>(<span class="hljs-params">float a, float b</span>) {
    <span class="hljs-keyword">return</span> a + b;
}
</code></pre>
<pre><code class="hljs language-js"># 컴파일
gcc -shared -o add_floats.<span class="hljs-property">so</span> -fPIC add_floats.<span class="hljs-property">c</span>
</code></pre>
<pre><code class="hljs language-js"># <span class="hljs-title class_">Python</span> 코드
<span class="hljs-keyword">import</span> ctypes

# 공유 라이브러리 로드
lib = ctypes.<span class="hljs-title function_">CDLL</span>(<span class="hljs-string">'./add_floats.so'</span>)

# 함수의 인자와 반환 유형 정의
lib.<span class="hljs-property">add_floats</span>.<span class="hljs-property">argtypes</span> = [ctypes.<span class="hljs-property">c_float</span>, ctypes.<span class="hljs-property">c_float</span>]
lib.<span class="hljs-property">add_floats</span>.<span class="hljs-property">restype</span> = ctypes.<span class="hljs-property">c_float</span>

# 파이썬 float 값을 c_float 유형으로 변환
a = ctypes.<span class="hljs-title function_">c_float</span>(<span class="hljs-number">3.5</span>)
b = ctypes.<span class="hljs-title function_">c_float</span>(<span class="hljs-number">2.2</span>)

# C 함수 호출
result = lib.<span class="hljs-title function_">add_floats</span>(a, b)
<span class="hljs-title function_">print</span>(result)
# <span class="hljs-number">5.7</span>
</code></pre>
<p>보시다시피 매우 직관적입니다. C/C++ 코드를 컴파일한 후 Python에서 ctypes를 사용하면 매우 쉽게 사용할 수 있습니다. 함수의 매개변수 및 반환 c_types를 정의하고, 변수를 해당 c_types로 변환하고 함수를 호출하기만 하면 됩니다. 배열(부동 소수점 목록)과 같은 보다 복잡한 유형의 경우 포인터를 사용할 수 있습니다.</p>
<pre><code class="hljs language-js">data = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>]
data_ctype = (ctypes.<span class="hljs-property">c_float</span> * <span class="hljs-title function_">len</span>(data))(*data)

lib.<span class="hljs-property">some_array_func</span>.<span class="hljs-property">argstypes</span> = [ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_float</span>)]

...

lib.<span class="hljs-title function_">some_array_func</span>(data)
</code></pre>
<p>그리고 구조체 유형의 경우 직접 c_type을 만들 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomType</span>(ctypes.<span class="hljs-property">Structure</span>):
    _fields_ = [
        (<span class="hljs-string">'field1'</span>, ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_float</span>)),
        (<span class="hljs-string">'field2'</span>, ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_int</span>)),
        (<span class="hljs-string">'field3'</span>, ctypes.<span class="hljs-property">c_int</span>),
    ]

# ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CustomType</span>)로 사용할 수 있습니다.
</code></pre>
<p>간단히 설명하고, 텐서 C/C++ 라이브러리를 위한 Python 래퍼를 만들어 보겠습니다!</p>
<pre><code class="hljs language-js"># norch/tensor.<span class="hljs-property">py</span>

<span class="hljs-keyword">import</span> ctypes

<span class="hljs-keyword">class</span> <span class="hljs-title class_">CTensor</span>(ctypes.<span class="hljs-property">Structure</span>):
    _fields_ = [
        (<span class="hljs-string">'data'</span>, ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_float</span>)),
        (<span class="hljs-string">'strides'</span>, ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_int</span>)),
        (<span class="hljs-string">'shape'</span>, ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_int</span>)),
        (<span class="hljs-string">'ndim'</span>, ctypes.<span class="hljs-property">c_int</span>),
        (<span class="hljs-string">'size'</span>, ctypes.<span class="hljs-property">c_int</span>),
    ]

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Tensor</span>:
    os.<span class="hljs-property">path</span>.<span class="hljs-title function_">abspath</span>(os.<span class="hljs-property">curdir</span>)
    _C = ctypes.<span class="hljs-title function_">CDLL</span>(<span class="hljs-string">"COMPILED_LIB.so"</span>)

    def <span class="hljs-title function_">__init__</span>(self):
        
        data, shape = self.<span class="hljs-title function_">flatten</span>(data)
        self.<span class="hljs-property">data_ctype</span> = (ctypes.<span class="hljs-property">c_float</span> * <span class="hljs-title function_">len</span>(data))(*data)
        self.<span class="hljs-property">shape_ctype</span> = (ctypes.<span class="hljs-property">c_int</span> * <span class="hljs-title function_">len</span>(shape))(*shape)
        self.<span class="hljs-property">ndim_ctype</span> = ctypes.<span class="hljs-title function_">c_int</span>(<span class="hljs-title function_">len</span>(shape))
       
        self.<span class="hljs-property">shape</span> = shape
        self.<span class="hljs-property">ndim</span> = <span class="hljs-title function_">len</span>(shape)

        <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">create_tensor</span>.<span class="hljs-property">argtypes</span> = [ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_float</span>), ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_int</span>), ctypes.<span class="hljs-property">c_int</span>]
        <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">create_tensor</span>.<span class="hljs-property">restype</span> = ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>)

        self.<span class="hljs-property">tensor</span> = <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-title function_">create_tensor</span>(
            self.<span class="hljs-property">data_ctype</span>,
            self.<span class="hljs-property">shape_ctype</span>,
            self.<span class="hljs-property">ndim_ctype</span>,
        )
        
    def <span class="hljs-title function_">flatten</span>(self, nested_list):
        <span class="hljs-string">""</span><span class="hljs-string">"
        This method simply convert a list type tensor to a flatten tensor with its shape
        
        Example:
        
        Arguments:  
            nested_list: [[1, 2, 3], [-5, 2, 0]]
        Return:
            flat_data: [1, 2, 3, -5, 2, 0]
            shape: [2, 3]
        "</span><span class="hljs-string">""</span>
        def <span class="hljs-title function_">flatten_recursively</span>(nested_list):
            flat_data = []
            shape = []
            <span class="hljs-keyword">if</span> <span class="hljs-title function_">isinstance</span>(nested_list, list):
                <span class="hljs-keyword">for</span> sublist <span class="hljs-keyword">in</span> <span class="hljs-attr">nested_list</span>:
                    inner_data, inner_shape = <span class="hljs-title function_">flatten_recursively</span>(sublist)
                    flat_data.<span class="hljs-title function_">extend</span>(inner_data)
                shape.<span class="hljs-title function_">append</span>(<span class="hljs-title function_">len</span>(nested_list))
                shape.<span class="hljs-title function_">extend</span>(inner_shape)
            <span class="hljs-attr">else</span>:
                flat_data.<span class="hljs-title function_">append</span>(nested_list)
            <span class="hljs-keyword">return</span> flat_data, shape
        
        flat_data, shape = <span class="hljs-title function_">flatten_recursively</span>(nested_list)
        <span class="hljs-keyword">return</span> flat_data, shape
</code></pre>
<p>이제 Python 텐서 작업을 포함하여 C/C++ 작업을 호출할 수 있습니다.</p>
<pre><code class="hljs language-js"># norch/tensor.<span class="hljs-property">py</span>

def <span class="hljs-title function_">__getitem__</span>(self, indices):
    <span class="hljs-string">""</span><span class="hljs-string">"
    index 텐서를 사용하여 텐서에 액세스 tensor[i, j, k...]
    "</span><span class="hljs-string">""</span>

    <span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(indices) != self.<span class="hljs-property">ndim</span>:
        raise <span class="hljs-title class_">ValueError</span>(<span class="hljs-string">"인덱스 수가 차원 수와 일치해야 함"</span>)
    
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">get_item</span>.<span class="hljs-property">argtypes</span> = [ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>), ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_int</span>)]
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">get_item</span>.<span class="hljs-property">restype</span> = ctypes.<span class="hljs-property">c_float</span>
                                       
    indices = (ctypes.<span class="hljs-property">c_int</span> * <span class="hljs-title function_">len</span>(indices))(*indices)
    value = <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-title function_">get_item</span>(self.<span class="hljs-property">tensor</span>, indices)  
    
    <span class="hljs-keyword">return</span> value

def <span class="hljs-title function_">reshape</span>(self, new_shape):
    <span class="hljs-string">""</span><span class="hljs-string">"
    텐서를 재구성합니다
    result = tensor.reshape([1,2])
    "</span><span class="hljs-string">""</span>
    new_shape_ctype = (ctypes.<span class="hljs-property">c_int</span> * <span class="hljs-title function_">len</span>(new_shape))(*new_shape)
    new_ndim_ctype = ctypes.<span class="hljs-title function_">c_int</span>(<span class="hljs-title function_">len</span>(new_shape))
    
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">reshape_tensor</span>.<span class="hljs-property">argtypes</span> = [ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>), ctypes.<span class="hljs-title function_">POINTER</span>(ctypes.<span class="hljs-property">c_int</span>), ctypes.<span class="hljs-property">c_int</span>]
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">reshape_tensor</span>.<span class="hljs-property">restype</span> = ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>)
    result_tensor_ptr = <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-title function_">reshape_tensor</span>(self.<span class="hljs-property">tensor</span>, new_shape_ctype, new_ndim_ctype)   

    result_data = <span class="hljs-title class_">Tensor</span>()
    result_data.<span class="hljs-property">tensor</span> = result_tensor_ptr
    result_data.<span class="hljs-property">shape</span> = new_shape.<span class="hljs-title function_">copy</span>()
    result_data.<span class="hljs-property">ndim</span> = <span class="hljs-title function_">len</span>(new_shape)
    result_data.<span class="hljs-property">device</span> = self.<span class="hljs-property">device</span>

    <span class="hljs-keyword">return</span> result_data

def <span class="hljs-title function_">__add__</span>(self, other):
    <span class="hljs-string">""</span><span class="hljs-string">"
    텐서를 더합니다
    result = tensor1 + tensor2
    "</span><span class="hljs-string">""</span>
  
    <span class="hljs-keyword">if</span> self.<span class="hljs-property">shape</span> != other.<span class="hljs-property">shape</span>:
        raise <span class="hljs-title class_">ValueError</span>(<span class="hljs-string">"덧셈을 위해서 텐서들은 동일한 모양이어야 함"</span>)
    
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">add_tensor</span>.<span class="hljs-property">argtypes</span> = [ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>), ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>)]
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">add_tensor</span>.<span class="hljs-property">restype</span> = ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>)

    result_tensor_ptr = <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-title function_">add_tensor</span>(self.<span class="hljs-property">tensor</span>, other.<span class="hljs-property">tensor</span>)

    result_data = <span class="hljs-title class_">Tensor</span>()
    result_data.<span class="hljs-property">tensor</span> = result_tensor_ptr
    result_data.<span class="hljs-property">shape</span> = self.<span class="hljs-property">shape</span>.<span class="hljs-title function_">copy</span>()
    result_data.<span class="hljs-property">ndim</span> = self.<span class="hljs-property">ndim</span>
    result_data.<span class="hljs-property">device</span> = self.<span class="hljs-property">device</span>

    <span class="hljs-keyword">return</span> result_data

# 기타 연산 포함:
# __str__
# __sub__ (-)
# __mul__ (*)
# __matmul__ (@)
# __pow__ (**)
# __truediv__ (/)
# log
# ...
</code></pre>
<p>여기까지 오신 것을 환영합니다! 이제 코드를 실행하고 텐서 작업을 시작할 수 있는 능력이 생겼습니다!</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> norch

tensor1 = norch.<span class="hljs-title class_">Tensor</span>([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]])
tensor2 = norch.<span class="hljs-title class_">Tensor</span>([[<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])

result = tensor1 + tensor2
<span class="hljs-title function_">print</span>(result[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>])
# <span class="hljs-number">4</span> 
</code></pre>
<h1>#2 — GPU 지원</h1>
<p>우리 라이브러리의 기본 구조를 만든 후, 이제 새로운 수준으로 끌어올릴 것입니다. 데이터를 GPU로 전송하고 수학 연산을 빠르게 실행하기 위해 <code>.to("cuda")</code>를 호출할 수 있다는 것은 잘 알려져 있습니다. CUDA가 어떻게 작동하는지 기본 지식이 있을 것으로 가정하겠습니다만, 그렇지 않은 경우 다른 기사인 'CUDA 튜토리얼'을 읽어볼 수 있습니다. 여기서 기다릴게요. 😊</p>
<p>...</p>
<p>급한 사람들을 위해, 간단한 소개가 여기 있어요:</p>
<p>기본적으로, 지금까지의 모든 코드는 CPU 메모리에서 실행되고 있어요. 하나의 작업에 대해서는 CPU가 빠르지만, GPU의 장점은 병렬화 능력에 있어요. CPU 디자인은 연산(스레드)을 빠르게 실행하도록 목표를 한 반면, GPU 디자인은 수백만 개의 연산을 병렬로 실행하도록 목표를 해요 (개별 스레드의 성능을 희생하며).</p>
<p>그래서 우리는 이 능력을 활용하여 병렬 연산을 수행할 수 있어요. 예를 들어, 백만 개의 요소로 구성된 텐서를 추가할 때, 반복문 내에서 각 색인의 요소를 순차적으로 추가하는 대신, GPU를 사용하여 한꺼번에 모두를 병렬로 추가할 수 있어요. 이를 위해 NVIDIA에서 개발한 개발자들이 GPU 지원을 소프트웨어 애플리케이션에 통합할 수 있게 하는 플랫폼인 CUDA를 사용할 수 있어요.</p>
<p>그걸 하려면, 특정 GPU 작업(예: CPU 메모리에서 GPU 메모리로 데이터 복사)을 실행하기 위해 설계된 간단한 C/C++ 기반 인터페이스 인 CUDA C/C++를 사용할 수 있습니다.</p>
<p>아래 코드는 기본적으로 CPU에서 GPU로 데이터를 복사하고 배열의 각 요소를 추가하는 AddTwoArrays 함수(커널이라고도 함)를 N개의 GPU 스레드에서 병렬로 실행하는 몇 가지 CUDA C/C++ 함수를 사용합니다.</p>
<pre><code class="hljs language-c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&#x3C;stdio.h></span></span>

<span class="hljs-comment">// CPU 버전(비교용)</span>
<span class="hljs-type">void</span> <span class="hljs-title function_">AddTwoArrays_CPU</span><span class="hljs-params">(flaot A[], <span class="hljs-type">float</span> B[], <span class="hljs-type">float</span> C[])</span> {
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &#x3C; N; i++) {
        C[i] = A[i] + B[i];
    }
}

<span class="hljs-comment">// 커널 정의</span>
__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">AddTwoArrays_GPU</span><span class="hljs-params">(<span class="hljs-type">float</span> A[], <span class="hljs-type">float</span> B[], <span class="hljs-type">float</span> C[])</span> {
    <span class="hljs-type">int</span> i = threadIdx.x;
    C[i] = A[i] + B[i];
}

<span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> {

    <span class="hljs-type">int</span> N = <span class="hljs-number">1000</span>; <span class="hljs-comment">// 배열 크기</span>
    <span class="hljs-type">float</span> A[N], B[N], C[N]; <span class="hljs-comment">// 배열 A, B, C</span>

    ...

    <span class="hljs-type">float</span> *d_A, *d_B, *d_C; <span class="hljs-comment">// 배열 A, B, C의 장치 포인터</span>

    <span class="hljs-comment">// 배열 A, B, C에 대한 장치에서의 메모리 할당</span>
    cudaMalloc((<span class="hljs-type">void</span> **)&#x26;d_A, N * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));
    cudaMalloc((<span class="hljs-type">void</span> **)&#x26;d_B, N * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));
    cudaMalloc((<span class="hljs-type">void</span> **)&#x26;d_C, N * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));

    <span class="hljs-comment">// 호스트에서 장치로 배열 A 및 B 복사</span>
    cudaMemcpy(d_A, A, N * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, N * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);

    <span class="hljs-comment">// N개의 스레드를 사용하여 커널 호출</span>
    AddTwoArrays_GPU&#x3C;&#x3C;&#x3C;<span class="hljs-number">1</span>, N>>>(d_A, d_B, d_C);
    
    <span class="hljs-comment">// 장치에서 호스트로 벡터 C 복사</span>
    cudaMemcpy(C, d_C, N * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyDeviceToHost);

}
</code></pre>
<p>주목할 점은 각 요소 쌍을 각각 추가하는 대신 모든 덧셈 작업을 병렬로 실행하여 루프 명령을 제거한 것입니다.</p>
<p>간단한 소개 이후에, 텐서 라이브러리로 돌아갈 수 있어요.</p>
<p>첫 번째 단계는 CPU에서 GPU로 텐서 데이터를 보내는 함수를 만드는 것입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/tensor.cpp</span>

<span class="hljs-keyword">void</span> <span class="hljs-title function_">to_device</span>(<span class="hljs-params">Tensor* tensor, char* target_device</span>) {
    <span class="hljs-keyword">if</span> ((<span class="hljs-title function_">strcmp</span>(target_device, <span class="hljs-string">"cuda"</span>) == <span class="hljs-number">0</span>) &#x26;&#x26; (<span class="hljs-title function_">strcmp</span>(tensor->device, <span class="hljs-string">"cpu"</span>) == <span class="hljs-number">0</span>)) {
        <span class="hljs-title function_">cpu_to_cuda</span>(tensor);
    }

    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> ((<span class="hljs-title function_">strcmp</span>(target_device, <span class="hljs-string">"cpu"</span>) == <span class="hljs-number">0</span>) &#x26;&#x26; (<span class="hljs-title function_">strcmp</span>(tensor->device, <span class="hljs-string">"cuda"</span>) == <span class="hljs-number">0</span>)) {
        <span class="hljs-title function_">cuda_to_cpu</span>(tensor);
    }
}
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/cuda.cu</span>

__host__ <span class="hljs-keyword">void</span> <span class="hljs-title function_">cpu_to_cuda</span>(<span class="hljs-params">Tensor* tensor</span>) {
    
    float* data_tmp;
    <span class="hljs-title function_">cudaMalloc</span>((<span class="hljs-keyword">void</span> **)&#x26;data_tmp, tensor->size * <span class="hljs-title function_">sizeof</span>(float));
    <span class="hljs-title function_">cudaMemcpy</span>(data_tmp, tensor->data, tensor->size * <span class="hljs-title function_">sizeof</span>(float), cudaMemcpyHostToDevice);

    tensor->data = data_tmp;

    <span class="hljs-keyword">const</span> char* device_str = <span class="hljs-string">"cuda"</span>;
    tensor->device = (char*)<span class="hljs-title function_">malloc</span>(<span class="hljs-title function_">strlen</span>(device_str) + <span class="hljs-number">1</span>);
    <span class="hljs-title function_">strcpy</span>(tensor->device, device_str); 

    <span class="hljs-title function_">printf</span>(<span class="hljs-string">"텐서가 성공적으로 %s로 전송되었습니다.\n"</span>, tensor->device);
}

__host__ <span class="hljs-keyword">void</span> <span class="hljs-title function_">cuda_to_cpu</span>(<span class="hljs-params">Tensor* tensor</span>) {
    float* data_tmp = (float*)<span class="hljs-title function_">malloc</span>(tensor->size * <span class="hljs-title function_">sizeof</span>(float));

    <span class="hljs-title function_">cudaMemcpy</span>(data_tmp, tensor->data, tensor->size * <span class="hljs-title function_">sizeof</span>(float), cudaMemcpyDeviceToHost);
    <span class="hljs-title function_">cudaFree</span>(tensor->data);

    tensor->data = data_tmp;

    <span class="hljs-keyword">const</span> char* device_str = <span class="hljs-string">"cpu"</span>;
    tensor->device = (char*)<span class="hljs-title function_">malloc</span>(<span class="hljs-title function_">strlen</span>(device_str) + <span class="hljs-number">1</span>);
    <span class="hljs-title function_">strcpy</span>(tensor->device, device_str); 

    <span class="hljs-title function_">printf</span>(<span class="hljs-string">"텐서가 성공적으로 %s로 전송되었습니다.\n"</span>, tensor->device);
}
</code></pre>
<p>파이썬으로 구현된 래퍼:</p>
<pre><code class="hljs language-js"># norch/tensor.<span class="hljs-property">py</span>

def <span class="hljs-title function_">to</span>(self, device):
    self.<span class="hljs-property">device</span> = device
    self.<span class="hljs-property">device_ctype</span> = self.<span class="hljs-property">device</span>.<span class="hljs-title function_">encode</span>(<span class="hljs-string">'utf-8'</span>)
  
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">to_device</span>.<span class="hljs-property">argtypes</span> = [ctypes.<span class="hljs-title function_">POINTER</span>(<span class="hljs-title class_">CTensor</span>), ctypes.<span class="hljs-property">c_char_p</span>]
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-property">to_device</span>.<span class="hljs-property">restype</span> = <span class="hljs-title class_">None</span>
    <span class="hljs-title class_">Tensor</span>.<span class="hljs-property">_C</span>.<span class="hljs-title function_">to_device</span>(self.<span class="hljs-property">tensor</span>, self.<span class="hljs-property">device_ctype</span>)
  
    <span class="hljs-keyword">return</span> self
</code></pre>
<p>다음으로, 모든 텐서 연산에 대해 GPU 버전을 생성합니다. 덧셈과 뺄셈에 대한 예제를 작성하겠습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/cuda.cu</span>

#define <span class="hljs-variable constant_">THREADS_PER_BLOCK</span> <span class="hljs-number">128</span>

__global__ <span class="hljs-keyword">void</span> <span class="hljs-title function_">add_tensor_cuda_kernel</span>(<span class="hljs-params">float* data1, float* data2, float* result_data, int size</span>) {
    
    int i = blockIdx.<span class="hljs-property">x</span> * blockDim.<span class="hljs-property">x</span> + threadIdx.<span class="hljs-property">x</span>;
    <span class="hljs-keyword">if</span> (i &#x3C; size) {
        result_data[i] = data1[i] + data2[i];
    }
}

__host__ <span class="hljs-keyword">void</span> <span class="hljs-title function_">add_tensor_cuda</span>(<span class="hljs-params">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {
    
    int number_of_blocks = (tensor1->size + <span class="hljs-variable constant_">THREADS_PER_BLOCK</span> - <span class="hljs-number">1</span>) / <span class="hljs-variable constant_">THREADS_PER_BLOCK</span>;
    add_tensor_cuda_kernel&#x3C;&#x3C;&#x3C;number_of_blocks, <span class="hljs-variable constant_">THREADS_PER_BLOCK</span>>>>(tensor1->data, tensor2->data, result_data, tensor1->size);

    cudaError_t error = <span class="hljs-title function_">cudaGetLastError</span>();
    <span class="hljs-keyword">if</span> (error != cudaSuccess) {
        <span class="hljs-title function_">printf</span>(<span class="hljs-string">"CUDA error: %s\n"</span>, <span class="hljs-title function_">cudaGetErrorString</span>(error));
        <span class="hljs-title function_">exit</span>(-<span class="hljs-number">1</span>);
    }

    <span class="hljs-title function_">cudaDeviceSynchronize</span>();
}

__global__ <span class="hljs-keyword">void</span> <span class="hljs-title function_">sub_tensor_cuda_kernel</span>(<span class="hljs-params">float* data1, float* data2, float* result_data, int size</span>) {
   
    int i = blockIdx.<span class="hljs-property">x</span> * blockDim.<span class="hljs-property">x</span> + threadIdx.<span class="hljs-property">x</span>;
    <span class="hljs-keyword">if</span> (i &#x3C; size) {
        result_data[i] = data1[i] - data2[i];
    }
}

__host__ <span class="hljs-keyword">void</span> <span class="hljs-title function_">sub_tensor_cuda</span>(<span class="hljs-params">Tensor* tensor1, Tensor* tensor2, float* result_data</span>) {
    
    int number_of_blocks = (tensor1->size + <span class="hljs-variable constant_">THREADS_PER_BLOCK</span> - <span class="hljs-number">1</span>) / <span class="hljs-variable constant_">THREADS_PER_BLOCK</span>;
    sub_tensor_cuda_kernel&#x3C;&#x3C;&#x3C;number_of_blocks, <span class="hljs-variable constant_">THREADS_PER_BLOCK</span>>>>(tensor1->data, tensor2->data, result_data, tensor1->size);

    cudaError_t error = <span class="hljs-title function_">cudaGetLastError</span>();
    <span class="hljs-keyword">if</span> (error != cudaSuccess) {
        <span class="hljs-title function_">printf</span>(<span class="hljs-string">"CUDA error: %s\n"</span>, <span class="hljs-title function_">cudaGetErrorString</span>(error));
        <span class="hljs-title function_">exit</span>(-<span class="hljs-number">1</span>);
    }

    <span class="hljs-title function_">cudaDeviceSynchronize</span>();
}

...
</code></pre>
<p>그런 다음, 텐서.cpp에 새로운 텐서 속성 char* device를 추가하고 작업을 실행할 위치(CPU 또는 GPU)를 선택하는 데 사용할 수 있습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-comment">//norch/csrc/tensor.cpp</span>

<span class="hljs-title class_">Tensor</span>* <span class="hljs-title function_">add_tensor</span>(<span class="hljs-params">Tensor* tensor1, Tensor* tensor2</span>) {
    <span class="hljs-keyword">if</span> (tensor1->ndim != tensor2->ndim) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"덧셈을 위해 텐서가 동일한 차원 수여야 합니다 %d and %d\n"</span>, tensor1->ndim, tensor2->ndim);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }

    <span class="hljs-keyword">if</span> (<span class="hljs-title function_">strcmp</span>(tensor1->device, tensor2->device) != <span class="hljs-number">0</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"텐서는 동일한 장치에 있어야 합니다: %s and %s\n"</span>, tensor1->device, tensor2->device);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }

    char* device = (char*)<span class="hljs-title function_">malloc</span>(<span class="hljs-title function_">strlen</span>(tensor1->device) + <span class="hljs-number">1</span>);
    <span class="hljs-keyword">if</span> (device != <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">strcpy</span>(device, tensor1->device);
    } <span class="hljs-keyword">else</span> {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(-<span class="hljs-number">1</span>);
    }
    int ndim = tensor1->ndim;
    int* shape = (int*)<span class="hljs-title function_">malloc</span>(ndim * <span class="hljs-title function_">sizeof</span>(int));
    <span class="hljs-keyword">if</span> (shape == <span class="hljs-variable constant_">NULL</span>) {
        <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
        <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
    }

    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; ndim; i++) {
        <span class="hljs-keyword">if</span> (tensor1->shape[i] != tensor2->shape[i]) {
            <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"덧셈을 위해 텐서들은 색인 %d에서 동일한 형태여야 합니다 %d and %d\n"</span>, i, tensor1->shape[i], tensor2->shape[i]);
            <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
        }
        shape[i] = tensor1->shape[i];
    }        

    <span class="hljs-keyword">if</span> (<span class="hljs-title function_">strcmp</span>(tensor1->device, <span class="hljs-string">"cuda"</span>) == <span class="hljs-number">0</span>) {

        float* result_data;
        <span class="hljs-title function_">cudaMalloc</span>((<span class="hljs-keyword">void</span> **)&#x26;result_data, tensor1->size * <span class="hljs-title function_">sizeof</span>(float));
        <span class="hljs-title function_">add_tensor_cuda</span>(tensor1, tensor2, result_data);
        <span class="hljs-keyword">return</span> <span class="hljs-title function_">create_tensor</span>(result_data, shape, ndim, device);
    } 
    <span class="hljs-keyword">else</span> {
        float* result_data = (float*)<span class="hljs-title function_">malloc</span>(tensor1->size * <span class="hljs-title function_">sizeof</span>(float));
        <span class="hljs-keyword">if</span> (result_data == <span class="hljs-variable constant_">NULL</span>) {
            <span class="hljs-title function_">fprintf</span>(stderr, <span class="hljs-string">"메모리 할당 실패\n"</span>);
            <span class="hljs-title function_">exit</span>(<span class="hljs-number">1</span>);
        }
        <span class="hljs-title function_">add_tensor_cpu</span>(tensor1, tensor2, result_data);
        <span class="hljs-keyword">return</span> <span class="hljs-title function_">create_tensor</span>(result_data, shape, ndim, device);
    }     
}
</code></pre>
<p>이제 라이브러리가 GPU 지원을 제공합니다!</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> norch

tensor1 = norch.<span class="hljs-title class_">Tensor</span>([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]]).<span class="hljs-title function_">to</span>(<span class="hljs-string">"cuda"</span>)
tensor2 = norch.<span class="hljs-title class_">Tensor</span>([[<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]).<span class="hljs-title function_">to</span>(<span class="hljs-string">"cuda"</span>)

result = tensor1 + tensor2
</code></pre>
<h1>#3 — Automatic Differentiation (Autograd)</h1>
<p>파이토치가 인기를 얻게 된 주요 이유 중 하나는 Autograd 모듈 때문입니다. Autograd 모듈은 자동 미분을 수행하여 기울기를 계산할 수 있게 해주는 핵심 구성 요소입니다 (경사 하강법과 같은 최적화 알고리즘을 사용하여 모델을 훈련하는 데 중요합니다). .backward()라는 단일 메서드 호출로 이전 텐서 연산에서 모든 기울기를 계산합니다:</p>
<pre><code class="hljs language-js">x = torch.<span class="hljs-title function_">tensor</span>([[<span class="hljs-number">1.</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">3.</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]], requires_grad=<span class="hljs-title class_">True</span>)
# [[<span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],
#  [<span class="hljs-number">3</span>,  <span class="hljs-number">2.</span>, <span class="hljs-number">1</span>]]

y = torch.<span class="hljs-title function_">tensor</span>([[<span class="hljs-number">3.</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1.</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]], requires_grad=<span class="hljs-title class_">True</span>)
# [[<span class="hljs-number">3</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],
#  [<span class="hljs-number">1</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]

L = ((x - y) ** <span class="hljs-number">3</span>).<span class="hljs-title function_">sum</span>()

L.<span class="hljs-title function_">backward</span>()

# x와 y의 기울기에 접근할 수 있습니다
<span class="hljs-title function_">print</span>(x.<span class="hljs-property">grad</span>)
# [[<span class="hljs-number">12</span>, <span class="hljs-number">0</span>, <span class="hljs-number">12</span>],
#  [<span class="hljs-number">12</span>, <span class="hljs-number">0</span>, <span class="hljs-number">12</span>]]

<span class="hljs-title function_">print</span>(y.<span class="hljs-property">grad</span>)
# [[-<span class="hljs-number">12</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">12</span>],
#  [-<span class="hljs-number">12</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">12</span>]]

# z를 최소화하기 위해서는 경사 하강법에 사용할 수 있습니다:
# x = x - 학습률 * x.<span class="hljs-property">grad</span>
# y = y - 학습률 * y.<span class="hljs-property">grad</span>
</code></pre>
<p>무슨 일이 일어나고 있는지 이해하기 위해 동일한 절차를 수동으로 복제해보겠습니다:</p>
<p>우선 계산해 봅시다:</p>
<p>x가 행렬이라는 것에 유의해야 합니다. 따라서 각 요소에 대한 L의 미분을 개별적으로 계산해야 합니다. 게다가, L은 모든 요소에 대한 합이지만 각 요소에 대한 미분에서 다른 요소들은 중요한 영향을 미치지 않는다는 것을 기억하는 것이 중요합니다. 따라서 우리는 다음과 같은 항을 얻습니다:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_12.png" alt="이미지"></p>
<p>각 항에 대해 연쇄 법칙을 적용하여 외부 함수를 미분하고 내부 함수를 미분한 값을 곱합니다:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_13.png" alt="이미지"></p>
<p>Where:</p>
<p>마침내:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_14.png" alt="이미지"></p>
<p>그러므로, x에 관한 L의 미분을 계산하는 최종 방정식은 다음과 같습니다:</p>
<p>아래는 Markdown 형식으로 변경된 내용입니다.</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_16.png" alt="Image 1"></p>
<p>Substituting the values into the equation:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_17.png" alt="Image 2"></p>
<p>Calculating the result, we get the same values we obtained with PyTorch:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_18.png" alt="image"></p>
<p>Now, let’s analyze what we just did:</p>
<p>Basically, we observed all the operations involved in reverse order: a summation, a power of 3, and a subtraction. Then, we applied the chain rule, calculating the derivative of each operation and recursively calculated the derivative for the next operation. So, first we need an implementation of the derivative for different math operations:</p>
<p>For addition:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_19.png" alt="Image"></p>
<pre><code class="hljs language-js"># norch/autograd/functions.<span class="hljs-property">py</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">AddBackward</span>:
    def <span class="hljs-title function_">__init__</span>(self, x, y):
        self.<span class="hljs-property">input</span> = [x, y]

    def <span class="hljs-title function_">backward</span>(self, gradient):
        <span class="hljs-keyword">return</span> [gradient, gradient]
</code></pre>
<p>For sin:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_20.png" alt="Image"></p>
<pre><code class="hljs language-js"># norch/autograd/functions.<span class="hljs-property">py</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">SinBackward</span>:
    def <span class="hljs-title function_">__init__</span>(self, x):
        self.<span class="hljs-property">input</span> = [x]

    def <span class="hljs-title function_">backward</span>(self, gradient):
        x = self.<span class="hljs-property">input</span>[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">return</span> [x.<span class="hljs-title function_">cos</span>() * gradient]
</code></pre>
<p>코사인에 대해:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21.png" alt="2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21"></p>
<pre><code class="hljs language-js"># norch/autograd/functions.<span class="hljs-property">py</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">CosBackward</span>:
    def <span class="hljs-title function_">__init__</span>(self, x):
        self.<span class="hljs-property">input</span> = [x]

    def <span class="hljs-title function_">backward</span>(self, gradient):
        x = self.<span class="hljs-property">input</span>[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">return</span> [- x.<span class="hljs-title function_">sin</span>() * gradient]
</code></pre>
<p>요소별 곱셈에 대한 자세한 내용을 확인해보세요:</p>
<p><img src="/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_22.png" alt="element-wise multiplication"></p>
<pre><code class="hljs language-python"><span class="hljs-comment"># norch/autograd/functions.py</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">ElementwiseMulBackward</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y</span>):
        self.<span class="hljs-built_in">input</span> = [x, y]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, gradient</span>):
        x = self.<span class="hljs-built_in">input</span>[<span class="hljs-number">0</span>]
        y = self.<span class="hljs-built_in">input</span>[<span class="hljs-number">1</span>]
        <span class="hljs-keyword">return</span> [y * gradient, x * gradient]
</code></pre>
<p>합산에 대해서:</p>
<h1>norch/autograd/functions.py</h1>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SumBackward</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x</span>):
        self.<span class="hljs-built_in">input</span> = [x]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, gradient</span>):
        <span class="hljs-comment"># sum 함수는 텐서를 스칼라로 줄이므로 기울기를 일치시키기 위해 브로드캐스트됩니다.</span>
        <span class="hljs-keyword">return</span> [<span class="hljs-built_in">float</span>(gradient.tensor.contents.data[<span class="hljs-number">0</span>]) * self.<span class="hljs-built_in">input</span>[<span class="hljs-number">0</span>].ones_like()]
</code></pre>
<p>다른 연산을 살펴볼 수 있는 GitHub 저장소 링크도 확인할 수 있습니다.</p>
<p>이제 각 작업에 대한 도함수 식을 가졌으니, 재귀적으로 역전파 체인 규칙을 구현할 수 있습니다. 텐서에 requires_grad 인자를 설정하여 이 텐서의 기울기를 저장하려는 것을 나타낼 수 있습니다. True이면 각 텐서 작업의 기울기를 저장합니다. 예를 들어:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># norch/tensor.py</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">__add__</span>(<span class="hljs-params">self, other</span>):

  <span class="hljs-keyword">if</span> self.shape != other.shape:
      <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"덧셈을 위해 텐서는 동일한 모양이어야 합니다."</span>)
  
  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]
  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)
  
  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)
  
  result_data = Tensor()
  result_data.tensor = result_tensor_ptr
  result_data.shape = self.shape.copy()
  result_data.ndim = self.ndim
  result_data.device = self.device
  
  result_data.requires_grad = self.requires_grad <span class="hljs-keyword">or</span> other.requires_grad
  <span class="hljs-keyword">if</span> result_data.requires_grad:
      result_data.grad_fn = AddBackward(self, other)
</code></pre>
<p>그럼, <code>.backward()</code> 메서드를 구현해보세요:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># norch/tensor.py</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, gradient=<span class="hljs-literal">None</span></span>):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.requires_grad:
        <span class="hljs-keyword">return</span>
    
    <span class="hljs-keyword">if</span> gradient <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">if</span> self.shape == [<span class="hljs-number">1</span>]:
            gradient = Tensor([<span class="hljs-number">1</span>]) <span class="hljs-comment"># dx/dx = 1 case</span>
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">"Gradient argument must be specified for non-scalar tensors."</span>)

    <span class="hljs-keyword">if</span> self.grad <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        self.grad = gradient

    <span class="hljs-keyword">else</span>:
        self.grad += gradient

    <span class="hljs-keyword">if</span> self.grad_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>: <span class="hljs-comment"># not a leaf</span>
        grads = self.grad_fn.backward(gradient) <span class="hljs-comment"># call the operation backward</span>
        <span class="hljs-keyword">for</span> tensor, grad <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.grad_fn.<span class="hljs-built_in">input</span>, grads):
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(tensor, Tensor):
                tensor.backward(grad) <span class="hljs-comment"># recursively call the backward again for the gradient expression (chain rule)</span>
</code></pre>
<p>마지막으로, 텐서의 그래디언트를 제로화하는 <code>.zero_grad()</code>와 텐서의 오토그래드 히스토리를 제거하는 <code>.detach()</code>를 구현해주세요:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># norch/tensor.py</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">zero_grad</span>(<span class="hljs-params">self</span>):
    self.grad = <span class="hljs-literal">None</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">detach</span>(<span class="hljs-params">self</span>):
    self.grad = <span class="hljs-literal">None</span>
    self.grad_fn = <span class="hljs-literal">None</span>
</code></pre>
<p>축하합니다! GPU 지원 및 자동 미분 기능이 있는 완전한 텐서 라이브러리를 만드셨군요! 이제 nn 및 optim 모듈을 만들어 몇 가지 딥 러닝 모델을 더 쉽게 훈련시킬 수 있습니다.</p>
<h2>#4 — nn 및 optim 모듈</h2>
<p>nn은 신경망 및 딥 러닝 모델을 구축하기 위한 모듈이며, optim은 이러한 모델을 훈련시키기 위한 최적화 알고리즘과 관련이 있습니다. 이들을 재현하기 위한 첫 번째 단계는 Parameter를 구현하는 것입니다. Parameter는 간단히 말해 항상 True로 설정된 requires_grad 속성을 갖는 훈련 가능한 텐서로, 일부 임의의 초기화 기법을 사용해 같은 연산을 수행합니다.</p>
<pre><code class="hljs language-js"># norch/nn/parameter.<span class="hljs-property">py</span>

<span class="hljs-keyword">from</span> norch.<span class="hljs-property">tensor</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Tensor</span>
<span class="hljs-keyword">from</span> norch.<span class="hljs-property">utils</span> <span class="hljs-keyword">import</span> utils
<span class="hljs-keyword">import</span> random

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Parameter</span>(<span class="hljs-title class_">Tensor</span>):
    <span class="hljs-string">""</span><span class="hljs-string">"
    A parameter is a trainable tensor.
    "</span><span class="hljs-string">""</span>
    def <span class="hljs-title function_">__init__</span>(self, shape):
        data = utils.<span class="hljs-title function_">generate_random_list</span>(shape=shape)
        <span class="hljs-variable language_">super</span>().<span class="hljs-title function_">__init__</span>(data, requires_grad=<span class="hljs-title class_">True</span>)
</code></pre>
<pre><code class="hljs language-js"># norch/utisl/utils.<span class="hljs-property">py</span>

def <span class="hljs-title function_">generate_random_list</span>(shape):
    <span class="hljs-string">""</span><span class="hljs-string">"
    랜덤한 숫자로 이루어진 'shape' 형태의 리스트를 생성합니다
    [4, 2] --> [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]
    "</span><span class="hljs-string">""</span>
    <span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(shape) == <span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> []
    <span class="hljs-attr">else</span>:
        inner_shape = shape[<span class="hljs-number">1</span>:]
        <span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(inner_shape) == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">return</span> [random.<span class="hljs-title function_">uniform</span>(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(shape[<span class="hljs-number">0</span>])]
        <span class="hljs-attr">else</span>:
            <span class="hljs-keyword">return</span> [<span class="hljs-title function_">generate_random_list</span>(inner_shape) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(shape[<span class="hljs-number">0</span>])]
</code></pre>
<p>파라미터를 활용하면 모듈을 구성할 수 있습니다:</p>
<pre><code class="hljs language-js"># norch/nn/<span class="hljs-variable language_">module</span>.<span class="hljs-property">py</span>

<span class="hljs-keyword">from</span> .<span class="hljs-property">parameter</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Parameter</span>
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> <span class="hljs-title class_">OrderedDict</span>
<span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">ABC</span>
<span class="hljs-keyword">import</span> inspect

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>(<span class="hljs-variable constant_">ABC</span>):
    <span class="hljs-string">""</span><span class="hljs-string">"
    모듈을 위한 추상 클래스
    "</span><span class="hljs-string">""</span>
    def <span class="hljs-title function_">__init__</span>(self):
        self.<span class="hljs-property">_modules</span> = <span class="hljs-title class_">OrderedDict</span>()
        self.<span class="hljs-property">_params</span> = <span class="hljs-title class_">OrderedDict</span>()
        self.<span class="hljs-property">_grads</span> = <span class="hljs-title class_">OrderedDict</span>()
        self.<span class="hljs-property">training</span> = <span class="hljs-title class_">True</span>

    def <span class="hljs-title function_">forward</span>(self, *inputs, **kwargs):
        raise <span class="hljs-title class_">NotImplementedError</span>

    def <span class="hljs-title function_">__call__</span>(self, *inputs, **kwargs):
        <span class="hljs-keyword">return</span> self.<span class="hljs-title function_">forward</span>(*inputs, **kwargs)

    def <span class="hljs-title function_">train</span>(self):
        self.<span class="hljs-property">training</span> = <span class="hljs-title class_">True</span>
        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.<span class="hljs-title function_">parameters</span>():
            param.<span class="hljs-property">requires_grad</span> = <span class="hljs-title class_">True</span>

    def <span class="hljs-built_in">eval</span>(self):
        self.<span class="hljs-property">training</span> = <span class="hljs-title class_">False</span>
        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.<span class="hljs-title function_">parameters</span>():
            param.<span class="hljs-property">requires_grad</span> = <span class="hljs-title class_">False</span>

    def <span class="hljs-title function_">parameters</span>(self):
        <span class="hljs-keyword">for</span> name, value <span class="hljs-keyword">in</span> inspect.<span class="hljs-title function_">getmembers</span>(self):
            <span class="hljs-keyword">if</span> <span class="hljs-title function_">isinstance</span>(value, <span class="hljs-title class_">Parameter</span>):
                <span class="hljs-keyword">yield</span> self, name, value
            elif <span class="hljs-title function_">isinstance</span>(value, <span class="hljs-title class_">Module</span>):
                <span class="hljs-keyword">yield</span> <span class="hljs-keyword">from</span> value.<span class="hljs-title function_">parameters</span>()

    def <span class="hljs-title function_">modules</span>(self):
        <span class="hljs-keyword">yield</span> <span class="hljs-keyword">from</span> self.<span class="hljs-property">_modules</span>.<span class="hljs-title function_">values</span>()

    def <span class="hljs-title function_">gradients</span>(self):
        <span class="hljs-keyword">for</span> <span class="hljs-variable language_">module</span> <span class="hljs-keyword">in</span> self.<span class="hljs-title function_">modules</span>():
            <span class="hljs-keyword">yield</span> <span class="hljs-variable language_">module</span>.<span class="hljs-property">_grads</span>

    def <span class="hljs-title function_">zero_grad</span>(self):
        <span class="hljs-keyword">for</span> _, _, parameter <span class="hljs-keyword">in</span> self.<span class="hljs-title function_">parameters</span>():
            parameter.<span class="hljs-title function_">zero_grad</span>()

    def <span class="hljs-title function_">to</span>(self, device):
        <span class="hljs-keyword">for</span> _, _, parameter <span class="hljs-keyword">in</span> self.<span class="hljs-title function_">parameters</span>():
            parameter.<span class="hljs-title function_">to</span>(device)

        <span class="hljs-keyword">return</span> self
    
    def <span class="hljs-title function_">inner_repr</span>(self):
        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>

    def <span class="hljs-title function_">__repr__</span>(self):
        string = f<span class="hljs-string">"{self.get_name()}("</span>
        tab = <span class="hljs-string">"   "</span>
        modules = self.<span class="hljs-property">_modules</span>
        <span class="hljs-keyword">if</span> modules == {}:
            string += f<span class="hljs-string">'\n{tab}(parameters): {self.inner_repr()}'</span>
        <span class="hljs-attr">else</span>:
            <span class="hljs-keyword">for</span> key, <span class="hljs-variable language_">module</span> <span class="hljs-keyword">in</span> modules.<span class="hljs-title function_">items</span>():
                string += f<span class="hljs-string">"\n{tab}({key}): {module.get_name()}({module.inner_repr()})"</span>
        <span class="hljs-keyword">return</span> f<span class="hljs-string">'{string}\n)'</span>
    
    def <span class="hljs-title function_">get_name</span>(self):
        <span class="hljs-keyword">return</span> self.<span class="hljs-property">__class__</span>.<span class="hljs-property">__name__</span>
    
    def <span class="hljs-title function_">__setattr__</span>(self, key, value):
        self.<span class="hljs-property">__dict__</span>[key] = value

        <span class="hljs-keyword">if</span> <span class="hljs-title function_">isinstance</span>(value, <span class="hljs-title class_">Module</span>):
            self.<span class="hljs-property">_modules</span>[key] = value
        elif <span class="hljs-title function_">isinstance</span>(value, <span class="hljs-title class_">Parameter</span>):
            self.<span class="hljs-property">_params</span>[key] = value
</code></pre>
<p>예를 들어, nn.Module을 상속하여 사용자 정의 모듈을 만들거나, 이전에 생성된 모듈 중 하나인 선형 모듈을 사용하여 y = Wx + b 작업을 구현할 수 있습니다.</p>
<pre><code class="hljs language-js"># norch/nn/modules/linear.<span class="hljs-property">py</span>

<span class="hljs-keyword">from</span> ..<span class="hljs-property">module</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Module</span>
<span class="hljs-keyword">from</span> ..<span class="hljs-property">parameter</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Parameter</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self, input_dim, output_dim):
        <span class="hljs-variable language_">super</span>().<span class="hljs-title function_">__init__</span>()
        self.<span class="hljs-property">input_dim</span> = input_dim
        self.<span class="hljs-property">output_dim</span> = output_dim
        self.<span class="hljs-property">weight</span> = <span class="hljs-title class_">Parameter</span>(shape=[self.<span class="hljs-property">output_dim</span>, self.<span class="hljs-property">input_dim</span>])
        self.<span class="hljs-property">bias</span> = <span class="hljs-title class_">Parameter</span>(shape=[self.<span class="hljs-property">output_dim</span>, <span class="hljs-number">1</span>])

    def <span class="hljs-title function_">forward</span>(self, x):
        z = self.<span class="hljs-property">weight</span> @ x + self.<span class="hljs-property">bias</span>
        <span class="hljs-keyword">return</span> z

    def <span class="hljs-title function_">inner_repr</span>(self):
        <span class="hljs-keyword">return</span> f<span class="hljs-string">"input_dim={self.input_dim}, output_dim={self.output_dim}, "</span> \
               f<span class="hljs-string">"bias={True if self.bias is not None else False}"</span>
</code></pre>
<p>이제 몇 가지 손실 및 활성화 함수를 구현할 수 있습니다. 예를 들어, 평균 제곱 오차 손실 및 시그모이드 함수:</p>
<pre><code class="hljs language-js"># norch/nn/loss.<span class="hljs-property">py</span>

<span class="hljs-keyword">from</span> .<span class="hljs-property">module</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Module</span>
 
<span class="hljs-keyword">class</span> <span class="hljs-title class_">MSELoss</span>(<span class="hljs-title class_">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self):
      pass

    def <span class="hljs-title function_">forward</span>(self, predictions, labels):
        assert labels.<span class="hljs-property">shape</span> == predictions.<span class="hljs-property">shape</span>, \
            <span class="hljs-string">"Labels and predictions shape does not match: {} and {}"</span>.<span class="hljs-title function_">format</span>(labels.<span class="hljs-property">shape</span>, predictions.<span class="hljs-property">shape</span>)
        
        <span class="hljs-keyword">return</span> ((predictions - labels) ** <span class="hljs-number">2</span>).<span class="hljs-title function_">sum</span>() / predictions.<span class="hljs-property">numel</span>

    def <span class="hljs-title function_">__call__</span>(self, *inputs):
        <span class="hljs-keyword">return</span> self.<span class="hljs-title function_">forward</span>(*inputs)
</code></pre>
<pre><code class="hljs language-js"># norch/nn/activation.<span class="hljs-property">py</span>

<span class="hljs-keyword">from</span> .<span class="hljs-property">module</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Module</span>
<span class="hljs-keyword">import</span> math

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Sigmoid</span>(<span class="hljs-title class_">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self):
        <span class="hljs-variable language_">super</span>().<span class="hljs-title function_">__init__</span>()

    def <span class="hljs-title function_">forward</span>(self, x):
        <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + (math.<span class="hljs-property">e</span>) ** (-x)) 
</code></pre>
<p>마지막으로 옵티마이저를 만들어봅시다. 예시로 확률적 경사 하강법(Stochastic Gradient Descent) 알고리즘을 구현하겠습니다:</p>
<pre><code class="hljs language-js"># norch/optim/optimizer.<span class="hljs-property">py</span>

<span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">ABC</span>
<span class="hljs-keyword">from</span> norch.<span class="hljs-property">tensor</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Tensor</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Optimizer</span>(<span class="hljs-variable constant_">ABC</span>):
    <span class="hljs-string">""</span><span class="hljs-string">"
    옵티마이저를 위한 추상 클래스
    "</span><span class="hljs-string">""</span>

    def <span class="hljs-title function_">__init__</span>(self, parameters):
        <span class="hljs-keyword">if</span> <span class="hljs-title function_">isinstance</span>(parameters, <span class="hljs-title class_">Tensor</span>):
            raise <span class="hljs-title class_">TypeError</span>(<span class="hljs-string">"parameters는 반복 가능한 객체이어야 하지만 {} 타입이 입력되었습니다"</span>.<span class="hljs-title function_">format</span>(<span class="hljs-title function_">type</span>(parameters)))
        elif <span class="hljs-title function_">isinstance</span>(parameters, dict):
            parameters = parameters.<span class="hljs-title function_">values</span>()

        self.<span class="hljs-property">parameters</span> = <span class="hljs-title function_">list</span>(parameters)

    def <span class="hljs-title function_">step</span>(self):
        raise <span class="hljs-title class_">NotImplementedError</span>
    
    def <span class="hljs-title function_">zero_grad</span>(self):
        <span class="hljs-keyword">for</span> <span class="hljs-variable language_">module</span>, name, parameter <span class="hljs-keyword">in</span> self.<span class="hljs-property">parameters</span>:
            parameter.<span class="hljs-title function_">zero_grad</span>()


<span class="hljs-keyword">class</span> <span class="hljs-title class_">SGD</span>(<span class="hljs-title class_">Optimizer</span>):
    def <span class="hljs-title function_">__init__</span>(self, parameters, lr=<span class="hljs-number">1e-1</span>, momentum=<span class="hljs-number">0</span>):
        <span class="hljs-variable language_">super</span>().<span class="hljs-title function_">__init__</span>(parameters)
        self.<span class="hljs-property">lr</span> = lr
        self.<span class="hljs-property">momentum</span> = momentum
        self.<span class="hljs-property">_cache</span> = {<span class="hljs-string">'velocity'</span>: [p.<span class="hljs-title function_">zeros_like</span>() <span class="hljs-keyword">for</span> (_, _, p) <span class="hljs-keyword">in</span> self.<span class="hljs-property">parameters</span>]}

    def <span class="hljs-title function_">step</span>(self):
        <span class="hljs-keyword">for</span> i, (<span class="hljs-variable language_">module</span>, name, _) <span class="hljs-keyword">in</span> <span class="hljs-title function_">enumerate</span>(self.<span class="hljs-property">parameters</span>):
            parameter = <span class="hljs-title function_">getattr</span>(<span class="hljs-variable language_">module</span>, name)

            velocity = self.<span class="hljs-property">_cache</span>[<span class="hljs-string">'velocity'</span>][i]

            velocity = self.<span class="hljs-property">momentum</span> * velocity - self.<span class="hljs-property">lr</span> * parameter.<span class="hljs-property">grad</span>

            updated_parameter = parameter + velocity

            <span class="hljs-title function_">setattr</span>(<span class="hljs-variable language_">module</span>, name, updated_parameter)

            self.<span class="hljs-property">_cache</span>[<span class="hljs-string">'velocity'</span>][i] = velocity

            parameter.<span class="hljs-title function_">detach</span>()
            velocity.<span class="hljs-title function_">detach</span>()
</code></pre>
<p>그리고 여기까지입니다! 이제 우리만의 딥러닝 프레임워크를 만들었어요! 🥳</p>
<p>이제 학습을 시작해봅시다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> norch
<span class="hljs-keyword">import</span> norch.<span class="hljs-property">nn</span> <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> norch.<span class="hljs-property">optim</span> <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> math

random.<span class="hljs-title function_">seed</span>(<span class="hljs-number">1</span>)

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.<span class="hljs-property">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self):
        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">MyModel</span>, self).<span class="hljs-title function_">__init__</span>()
        self.<span class="hljs-property">fc1</span> = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>)
        self.<span class="hljs-property">sigmoid</span> = nn.<span class="hljs-title class_">Sigmoid</span>()
        self.<span class="hljs-property">fc2</span> = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">10</span>, <span class="hljs-number">1</span>)

    def <span class="hljs-title function_">forward</span>(self, x):
        out = self.<span class="hljs-title function_">fc1</span>(x)
        out = self.<span class="hljs-title function_">sigmoid</span>(out)
        out = self.<span class="hljs-title function_">fc2</span>(out)
        
        <span class="hljs-keyword">return</span> out

device = <span class="hljs-string">"cuda"</span>
epochs = <span class="hljs-number">10</span>

model = <span class="hljs-title class_">MyModel</span>().<span class="hljs-title function_">to</span>(device)
criterion = nn.<span class="hljs-title class_">MSELoss</span>()
optimizer = optim.<span class="hljs-title function_">SGD</span>(model.<span class="hljs-title function_">parameters</span>(), lr=<span class="hljs-number">0.001</span>)
loss_list = []

x_values = [<span class="hljs-number">0.</span> ,  <span class="hljs-number">0.4</span>,  <span class="hljs-number">0.8</span>,  <span class="hljs-number">1.2</span>,  <span class="hljs-number">1.6</span>,  <span class="hljs-number">2.</span> ,  <span class="hljs-number">2.4</span>,  <span class="hljs-number">2.8</span>,  <span class="hljs-number">3.2</span>,  <span class="hljs-number">3.6</span>,  <span class="hljs-number">4.</span> ,
        <span class="hljs-number">4.4</span>,  <span class="hljs-number">4.8</span>,  <span class="hljs-number">5.2</span>,  <span class="hljs-number">5.6</span>,  <span class="hljs-number">6.</span> ,  <span class="hljs-number">6.4</span>,  <span class="hljs-number">6.8</span>,  <span class="hljs-number">7.2</span>,  <span class="hljs-number">7.6</span>,  <span class="hljs-number">8.</span> ,  <span class="hljs-number">8.4</span>,
        <span class="hljs-number">8.8</span>,  <span class="hljs-number">9.2</span>,  <span class="hljs-number">9.6</span>, <span class="hljs-number">10.</span> , <span class="hljs-number">10.4</span>, <span class="hljs-number">10.8</span>, <span class="hljs-number">11.2</span>, <span class="hljs-number">11.6</span>, <span class="hljs-number">12.</span> , <span class="hljs-number">12.4</span>, <span class="hljs-number">12.8</span>,
       <span class="hljs-number">13.2</span>, <span class="hljs-number">13.6</span>, <span class="hljs-number">14.</span> , <span class="hljs-number">14.4</span>, <span class="hljs-number">14.8</span>, <span class="hljs-number">15.2</span>, <span class="hljs-number">15.6</span>, <span class="hljs-number">16.</span> , <span class="hljs-number">16.4</span>, <span class="hljs-number">16.8</span>, <span class="hljs-number">17.2</span>,
       <span class="hljs-number">17.6</span>, <span class="hljs-number">18.</span> , <span class="hljs-number">18.4</span>, <span class="hljs-number">18.8</span>, <span class="hljs-number">19.2</span>, <span class="hljs-number">19.6</span>, <span class="hljs-number">20.</span>]

y_true = []
<span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-attr">x_values</span>:
    y_true.<span class="hljs-title function_">append</span>(math.<span class="hljs-title function_">pow</span>(math.<span class="hljs-title function_">sin</span>(x), <span class="hljs-number">2</span>))


<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(epochs):
    <span class="hljs-keyword">for</span> x, target <span class="hljs-keyword">in</span> <span class="hljs-title function_">zip</span>(x_values, y_true):
        x = norch.<span class="hljs-title class_">Tensor</span>([[x]]).<span class="hljs-property">T</span>
        target = norch.<span class="hljs-title class_">Tensor</span>([[target]]).<span class="hljs-property">T</span>

        x = x.<span class="hljs-title function_">to</span>(device)
        target = target.<span class="hljs-title function_">to</span>(device)

        outputs = <span class="hljs-title function_">model</span>(x)
        loss = <span class="hljs-title function_">criterion</span>(outputs, target)
        
        optimizer.<span class="hljs-title function_">zero_grad</span>()
        loss.<span class="hljs-title function_">backward</span>()
        optimizer.<span class="hljs-title function_">step</span>()

    <span class="hljs-title function_">print</span>(f<span class="hljs-string">'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}'</span>)
    loss_list.<span class="hljs-title function_">append</span>(loss[<span class="hljs-number">0</span>])

# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">1</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">1.7035</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">2</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.7193</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">3</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.3068</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">4</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.1742</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">5</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.1342</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">6</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.1232</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">7</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.1220</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">8</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.1241</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">9</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.1270</span>
# <span class="hljs-title class_">Epoch</span> [<span class="hljs-number">10</span>/<span class="hljs-number">10</span>], <span class="hljs-title class_">Loss</span>: <span class="hljs-number">0.1297</span>
</code></pre>
<p>성공적으로 모델이 생성되고 사용자 정의 딥러닝 프레임워크를 사용하여 훈련되었습니다!</p>
<p>전체 코드는 여기에서 확인할 수 있습니다.</p>
<h1>결론</h1>
<p>이 게시물에서는 텐서와 같은 기본 개념, 어떻게 모델링되는지, CUDA 및 Autograd와 같은 고급 주제 등을 다루었습니다. 우리는 GPU 지원 및 자동 미분이 가능한 딥 러닝 프레임워크를 성공적으로 만들었습니다. 이 게시물이 여러분이 PyTorch가 어떻게 작동하는지 간략히 이해하는 데 도움이 되었으면 좋겠습니다.</p>
<p>앞으로의 게시물에서는 분산 훈련(다중 노드/다중 GPU) 및 메모리 관리와 같은 고급 주제를 다루려고 할 것입니다. 의견이 있거나 다음에 어떤 내용을 다루길 원하시는지 댓글로 알려주세요! 읽어 주셔서 정말 감사합니다! 😊</p>
<p>또한 최신 기사를 받아보기 위해 여기와 제 LinkedIn 프로필에서 팔로우해 주세요!</p>
<h1>참고 자료</h1>
<ul>
<li><a href="https://github.com" rel="nofollow" target="_blank">PyNorch</a> - 이 프로젝트의 GitHub 저장소</li>
<li><a href="https://www.example.com/tutorial-cuda" rel="nofollow" target="_blank">CUDA 튜토리얼</a> - CUDA 작동 방식에 대한 간단한 소개</li>
<li><a href="https://pytorch.org/docs" rel="nofollow" target="_blank">PyTorch</a> - PyTorch 문서</li>
</ul>
<h1>MartinLwx's 블로그 - 스트라이드에 관한 튜토리얼.</h1>
<h1>스트라이드 튜토리얼 - 스트라이드에 관한 또 다른 튜토리얼.</h1>
<h1>PyTorch 내부 구조 - PyTorch 구조에 대한 가이드.</h1>
<h1>네츠 - NumPy를 사용한 PyTorch 재구현.</h1>
<p>Markdown으로 표 태그를 변경하십시오.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"파이토치를 처음부터 다시 만들어보기 GPU 지원 및 자동 미분 기능 포함","description":"","date":"2024-05-15 10:33","slug":"2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation","content":"\n\n## C/C++, CUDA, 및 Python을 기반으로 한 고유의 딥 러닝 프레임워크를 구축해 보세요. GPU 지원과 자동 미분을 제공합니다\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png)\n\n# 소개\n\n여러 해 동안 PyTorch를 사용하여 딥 러닝 모델을 구축하고 훈련해 왔습니다. 그럼에도 불구하고, 그 문법과 규칙을 익히고도, 제 궁금증을 자극하던 것이 있었습니다: 이러한 작업 중에 내부에서 어떤 일이 일어나고 있는 걸까요? 이 모든 것이 어떻게 작동할까요?\n\n\n\n여기까지 오셨다면, 아마도 비슷한 질문을 가지고 계실 것입니다. 파이토치(PyTorch)에서 모델을 생성하고 훈련하는 방법을 물어본다면 아마도 아래 코드와 비슷한 것을 생각해볼 것입니다:\n\n```js\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(1, 10)\n        self.sigmoid = nn.Sigmoid()\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        \n        return out\n\n...\n\nmodel = MyModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(epochs):\n    for x, y in ...\n        \n        x = x.to(device)\n        y = y.to(device)\n\n        outputs = model(x)\n        loss = criterion(outputs, y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\n하지만 이번에 역전파(backward) 단계가 어떻게 작동하는지 물어본다면 어떨까요? 또는 예를 들어, 텐서를 재구성할 때 무슨 일이 일어나는지 궁금하시다면요? 내부에서 데이터가 재배치되나요? 그런 일이 어떻게 일어나나요? 왜 PyTorch는 빠른가요? PyTorch가 GPU 연산을 어떻게 처리하는지요? 이런 질문들이 항상 저를 호기심 가득하게 만들었고, 여러분도 마찬가지로 호기심이 드실 것이라고 상상합니다. 그래서 이러한 개념을 더 잘 이해하기 위해 스스로 텐서 라이브러리를 처음부터 구축해보는 것이 무엇보다 좋을까요? 이 글에서 여러분이 배우게 될 것이 바로 그겁니다!\n\n## #1 — 텐서\n\n\n\n텐서 라이브러리를 구축하기 위해 가장 먼저 알아야 할 개념은 무엇이 텐서인지에 대한 명백한 개념입니다.\n\n텐서는 몇 가지 숫자를 포함하는 n차원 데이터 구조의 수학적 개념이라는 직관적인 생각을 가지고 있을 수 있습니다. 그러나 여기서는 이 데이터 구조를 계산적 관점에서 어떻게 모델링할지 이해해야 합니다. 텐서는 데이터 자체뿐만 아니라 모양이나 텐서가 있는 장치(예: CPU 메모리, GPU 메모리)와 같은 측면을 설명하는 메타데이터로 구성된다고 생각할 수 있습니다.\n\n텐서의 내부를 이해하는 데 매우 중요한 개념인 stride라는 잘 알려지지 않은 메타데이터도 있습니다. 따라서 텐서 데이터 재배열의 내부를 이해하기 위해 약간 더 이에 대해 논의해야 합니다.\n\n\n\n2-D 텐서의 모양이 [4, 8]인 경우를 상상해보세요.\n\n![텐서](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_2.png)\n\n텐서의 데이터(즉, 부동 소수점 수)는 실제로 메모리에 1차원 배열로 저장됩니다.\n\n![데이터](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_3.png)\n\n\n\n그러면 이 1차원 배열을 N차원 텐서로 나타내려면 스트라이드를 사용합니다. 기본 아이디어는 다음과 같습니다:\n\n4행 8열의 행렬이 있습니다. 그 행렬의 모든 원소가 1차원 배열의 행에 의해 구성되어 있다고 가정할 때, 위치 [2, 3]의 값을 액세스하려면 2행(각 행에 8개의 요소)을 횡단해야 하며 추가로 3개의 위치를 지나야 합니다. 수학적으로 표현하면 1차원 배열에서 3 + 2 * 8 요소를 횡단해야 합니다.\n\n따라서, '8'은 두 번째 차원의 스트라이드입니다. 이 경우, 배열에서 다른 위치로 \"점프\"하기 위해 몇 개의 요소를 횡단해야 하는지를 나타내는 정보입니다.\n\n\n\n따라서, 모양이 [shape_0, shape_1]인 2차원 텐서의 요소 [i, j]에 액세스하려면, 기본적으로 j + i * shape_1 위치에 있는 요소에 액세스해야 합니다.\n\n이제 3차원 텐서를 상상해보겠습니다:\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_5.png)\n\n이 3차원 텐서를 행렬의 시퀀스로 생각할 수 있습니다. 예를 들어, 이 [5, 4, 8] 텐서를 [4, 8] 모양의 5개 행렬로 생각할 수 있습니다.\n\n\n\n이제 [1, 3, 7] 위치에 있는 요소에 액세스하기 위해 [4,8] 형태의 행렬을 1개 완전히 횡단하고, [8] 형태의 행을 2개, [1] 형태의 열을 7개 횡단해야 합니다. 따라서 1차원 배열에서 (1 * 4 * 8) + (2 * 8) + (7 * 1) 위치를 횡단해야 합니다.\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_6.png)\n\n따라서, [shape_0, shape_1, shape_2] 모양의 3차원 텐서에서 1차원 데이터 배열에서 [i][j][k] 요소에 액세스하는 방법은 다음과 같습니다:\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_7.png)\n\n\n\n이 shape_1 * shape_2가 첫 번째 차원의 stride이고, shape_2는 두 번째 차원의 stride이며 1은 세 번째 차원의 stride입니다.\n\n그런 다음, 일반화하기 위해서는:\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_8.png)\n\n각 차원의 stride는 다음 차원 텐서 모양의 곱을 사용하여 계산할 수 있습니다:\n\n\n\n\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_9.png\" /\u003e\n\n그런 다음 stride[n-1] = 1로 설정합니다.\n\n우리의 형태의 텐서 예제 [5, 4, 8]에서 strides = [4*8, 8, 1] = [32, 8, 1]일 것입니다.\n\n여러분들도 직접 테스트할 수 있어요:\n\n\n\n```js\nimport torch\n\ntorch.rand([5, 4, 8]).stride()\n#(32, 8, 1)\n```\n\n알겠어요, 그런데 왜 모양과 스트라이드가 필요한 건가요? N차원 텐서의 요소에 접근하는 것을 넘어, 이 개념은 텐서 배열을 매우 쉽게 조작하는 데 사용될 수 있어요.\n\n예를 들어, 텐서를 재구성하려면 새로운 모양을 설정하고 새로운 스트라이드를 계산하면 됩니다! (새로운 모양은 동일한 요소 수를 보장하므로)\n\n```js\nimport torch\n\nt = torch.rand([5, 4, 8])\n\nprint(t.shape)\n# [5, 4, 8]\n\nprint(t.stride())\n# [32, 8, 1]\n\nnew_t = t.reshape([4, 5, 2, 2, 2])\n\nprint(new_t.shape)\n# [4, 5, 2, 2, 2]\n\nprint(new_t.stride())\n# [40, 8, 4, 2, 1]\n``` \n\n\n\n\n텐서 내부에서는 여전히 동일한 1차원 배열로 저장됩니다. reshape 메서드는 배열 내 요소의 순서를 변경하지 않았습니다! 대단하지 않나요? 😁\n\n다음 함수를 사용하여 PyTorch에서 내부 1차원 배열에 액세스하는 함수를 사용하여 직접 확인할 수 있습니다:\n\n```js\nimport ctypes\n\ndef print_internal(t: torch.Tensor):\n    print(\n        torch.frombuffer(\n            ctypes.string_at(t.data_ptr(), t.storage().nbytes()), dtype=t.dtype\n        )\n    )\n\nprint_internal(t)\n# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...\n\nprint_internal(new_t)\n# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...\n```\n\n예를 들어 두 축을 전치하려면 내부적으로 해당 스트라이드를 단순히 바꾸어 주면 됩니다!\n\n\n\n```js\nt = torch.arange(0, 24).reshape(2, 3, 4)\nprint(t)\n# [[[ 0,  1,  2,  3],\n#   [ 4,  5,  6,  7],\n#   [ 8,  9, 10, 11]],\n \n#  [[12, 13, 14, 15],\n#   [16, 17, 18, 19],\n#   [20, 21, 22, 23]]]\n\nprint(t.shape)\n# [2, 3, 4]\n\nprint(t.stride())\n# [12, 4, 1]\n\nnew_t = t.transpose(0, 1)\nprint(new_t)\n# [[[ 0,  1,  2,  3],\n#   [12, 13, 14, 15]],\n\n#  [[ 4,  5,  6,  7],\n#   [16, 17, 18, 19]],\n\n#  [[ 8,  9, 10, 11],\n#   [20, 21, 22, 23]]]\n\nprint(new_t.shape)\n# [3, 2, 4]\n\nprint(new_t.stride())\n# [4, 12, 1]\n```\n\n내부 배열을 출력하면 두 값 모두 동일합니다:\n\n```js\nprint_internal(t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n\nprint_internal(new_t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n```\n\n그러나 new_t의 스트라이드는 이제 위의 식과 일치하지 않습니다. 이것은 텐서가 이제 연속적이지 않기 때문에 발생합니다. 즉, 내부 배열은 동일하지만 메모리 내의 값의 순서가 텐서의 실제 순서와 일치하지 않는다는 것을 의미합니다.\n\n\n\n```js\nt.is_contiguous()\n# True\n\nnew_t.is_contiguous()\n# False\n```\n\n이는 연속되지 않는 요소에 연속적으로 액세스하는 것이 효율적이지 않다는 것을 의미합니다 (실제 텐서 요소는 메모리 상에서 순서대로 정렬되어 있지 않기 때문입니다). 이를 해결하기 위해 다음을 수행할 수 있습니다:\n\n```js\nnew_t_contiguous = new_t.contiguous()\n\nprint(new_t_contiguous.is_contiguous())\n# True\n```\n\n내부 배열을 분석하면 이제 순서가 실제 텐서 순서와 일치하여 더 나은 메모리 액세스 효율을 제공할 수 있습니다:\n\n\n\n```js\nprint(new_t)\n# [[[ 0,  1,  2,  3],\n#   [12, 13, 14, 15]],\n\n#  [[ 4,  5,  6,  7],\n#   [16, 17, 18, 19]],\n\n#  [[ 8,  9, 10, 11],\n#   [20, 21, 22, 23]]]\n\nprint_internal(new_t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n\nprint_internal(new_t_contiguous)\n# [ 0,  1,  2,  3, 12, 13, 14, 15,  4,  5,  6,  7, 16, 17, 18, 19,  8,  9, 10, 11, 20, 21, 22, 23]\n```\n\n이제 우리는 텐서가 어떻게 모델링되는지 이해했으니, 라이브러리 생성을 시작해 봅시다!\n\n내가 만들 라이브러리 이름은 Norch입니다. PyTorch가 아닌 (NOT PyTorch)을 의미하며, 성(Nogueira)을 암시하기도 합니다. 😁\n\n첫 번째로 알아야 할 것은 PyTorch가 Python을 통해 사용되지만 내부적으로는 C/C++로 실행된다는 것입니다. 그래서 먼저 내부 C/C++ 함수를 만들 것입니다.\n\n\n\n\n먼저 텐서를 데이터와 메타데이터를 저장하는 구조체로 정의하고 이를 만들기 위한 함수를 생성할 수 있습니다:\n\n```js\n//norch/csrc/tensor.cpp\n\n#include \u003cstdio.h\u003e\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n#include \u003cmath.h\u003e\n\ntypedef struct {\n    float* data;\n    int* strides;\n    int* shape;\n    int ndim;\n    int size;\n    char* device;\n} Tensor;\n\nTensor* create_tensor(float* data, int* shape, int ndim) {\n    \n    Tensor* tensor = (Tensor*)malloc(sizeof(Tensor));\n    if (tensor == NULL) {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(1);\n    }\n    tensor-\u003edata = data;\n    tensor-\u003eshape = shape;\n    tensor-\u003endim = ndim;\n\n    tensor-\u003esize = 1;\n    for (int i = 0; i \u003c ndim; i++) {\n        tensor-\u003esize *= shape[i];\n    }\n\n    tensor-\u003estrides = (int*)malloc(ndim * sizeof(int));\n    if (tensor-\u003estrides == NULL) {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(1);\n    }\n    int stride = 1;\n    for (int i = ndim - 1; i \u003e= 0; i--) {\n        tensor-\u003estrides[i] = stride;\n        stride *= shape[i];\n    }\n    \n    return tensor;\n}\n```\n\n일부 요소에 접근하기 위해서는 앞서 배웠던 스트라이드(strides)를 활용할 수 있습니다:\n\n```js\n//norch/csrc/tensor.cpp\n\nfloat get_item(Tensor* tensor, int* indices) {\n    int index = 0;\n    for (int i = 0; i \u003c tensor-\u003endim; i++) {\n        index += indices[i] * tensor-\u003estrides[i];\n    }\n\n    float result;\n    result = tensor-\u003edata[index];\n\n    return result;\n}\n```\n\n\n\n이제 텐서 작업을 만들 수 있습니다. 몇 가지 예제를 보여드리겠고, 이 글 끝에 링크된 저장소에서 완전한 버전을 찾을 수 있습니다.\n\n```js\n//norch/csrc/cpu.cpp\n\nvoid add_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    for (int i = 0; i \u003c tensor1-\u003esize; i++) {\n        result_data[i] = tensor1-\u003edata[i] + tensor2-\u003edata[i];\n    }\n}\n\nvoid sub_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    for (int i = 0; i \u003c tensor1-\u003esize; i++) {\n        result_data[i] = tensor1-\u003edata[i] - tensor2-\u003edata[i];\n    }\n}\n\nvoid elementwise_mul_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    for (int i = 0; i \u003c tensor1-\u003esize; i++) {\n        result_data[i] = tensor1-\u003edata[i] * tensor2-\u003edata[i];\n    }\n}\n\nvoid assign_tensor_cpu(Tensor* tensor, float* result_data) {\n\n    for (int i = 0; i \u003c tensor-\u003esize; i++) {\n        result_data[i] = tensor-\u003edata[i];\n    }\n}\n\n...\n```\n\n그 다음에, 이러한 작업들을 호출할 텐서 다른 함수를 만들 수 있습니다.\n\n```js\n//norch/csrc/tensor.cpp\n\nTensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {\n    if (tensor1-\u003endim != tensor2-\u003endim) {\n        fprintf(stderr, \"덧셈을 위해서 텐서는 동일한 차원 수여야 합니다 %d 와 %d\\n\", tensor1-\u003endim, tensor2-\u003endim);\n        exit(1);\n    }\n\n    int ndim = tensor1-\u003endim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i \u003c ndim; i++) {\n        if (tensor1-\u003eshape[i] != tensor2-\u003eshape[i]) {\n            fprintf(stderr, \"덧셈을 위해서 텐서는 동일한 모양이어야 합니다 %d 와 %d 인덱스 %d에서\\n\", tensor1-\u003eshape[i], tensor2-\u003eshape[i], i);\n            exit(1);\n        }\n        shape[i] = tensor1-\u003eshape[i];\n    }        \n    float* result_data = (float*)malloc(tensor1-\u003esize * sizeof(float));\n    if (result_data == NULL) {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(1);\n    }\n    add_tensor_cpu(tensor1, tensor2, result_data);\n    \n    return create_tensor(result_data, shape, ndim, device);\n}\n```\n\n\n\n이전에 언급한 대로, 텐서 재구성은 내부 데이터 배열을 수정하지 않습니다.\n\n```js\n//norch/csrc/tensor.cpp\n\nTensor* reshape_tensor(Tensor* tensor, int* new_shape, int new_ndim) {\n\n    int ndim = new_ndim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i \u003c ndim; i++) {\n        shape[i] = new_shape[i];\n    }\n\n    // 새 모양의 요소 총 수 계산\n    int size = 1;\n    for (int i = 0; i \u003c new_ndim; i++) {\n        size *= shape[i];\n    }\n\n    // 총 요소 수가 현재 텐서의 크기와 일치하는지 확인\n    if (size != tensor-\u003esize) {\n        fprintf(stderr, \"텐서를 재구성할 수 없습니다. 새 모양의 요소 총 수가 현재 텐서의 크기와 일치하지 않습니다.\\n\");\n        exit(1);\n    }\n\n    float* result_data = (float*)malloc(tensor-\u003esize * sizeof(float));\n    if (result_data == NULL) {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(1);\n    }\n    assign_tensor_cpu(tensor, result_data);\n    return create_tensor(result_data, shape, ndim, device);\n}\n```\n\n이제 일부 텐서 작업을 수행할 수 있지만, 누구나 C/C++을 사용하여 실행해야 하는 것은 아닙니다. 이제 Python 래퍼를 만들어 봅시다!\n\nPython을 사용하여 C/C++ 코드를 실행할 수 있는 다양한 옵션이 있습니다. Pybind11과 Cython 등이 있습니다. 이 예시에서는 ctypes를 사용할 것입니다.\n\n\n\n아래는 ctypes의 기본적인 구조입니다:\n\n```js\n//C 코드\n#include \u003cstdio.h\u003e\n\nfloat add_floats(float a, float b) {\n    return a + b;\n}\n```\n\n```js\n# 컴파일\ngcc -shared -o add_floats.so -fPIC add_floats.c\n```\n\n```js\n# Python 코드\nimport ctypes\n\n# 공유 라이브러리 로드\nlib = ctypes.CDLL('./add_floats.so')\n\n# 함수의 인자와 반환 유형 정의\nlib.add_floats.argtypes = [ctypes.c_float, ctypes.c_float]\nlib.add_floats.restype = ctypes.c_float\n\n# 파이썬 float 값을 c_float 유형으로 변환\na = ctypes.c_float(3.5)\nb = ctypes.c_float(2.2)\n\n# C 함수 호출\nresult = lib.add_floats(a, b)\nprint(result)\n# 5.7\n```\n\n\n\n보시다시피 매우 직관적입니다. C/C++ 코드를 컴파일한 후 Python에서 ctypes를 사용하면 매우 쉽게 사용할 수 있습니다. 함수의 매개변수 및 반환 c_types를 정의하고, 변수를 해당 c_types로 변환하고 함수를 호출하기만 하면 됩니다. 배열(부동 소수점 목록)과 같은 보다 복잡한 유형의 경우 포인터를 사용할 수 있습니다.\n\n```js\ndata = [1.0, 2.0, 3.0]\ndata_ctype = (ctypes.c_float * len(data))(*data)\n\nlib.some_array_func.argstypes = [ctypes.POINTER(ctypes.c_float)]\n\n...\n\nlib.some_array_func(data)\n```\n\n그리고 구조체 유형의 경우 직접 c_type을 만들 수 있습니다.\n\n```js\nclass CustomType(ctypes.Structure):\n    _fields_ = [\n        ('field1', ctypes.POINTER(ctypes.c_float)),\n        ('field2', ctypes.POINTER(ctypes.c_int)),\n        ('field3', ctypes.c_int),\n    ]\n\n# ctypes.POINTER(CustomType)로 사용할 수 있습니다.\n```\n\n\n\n간단히 설명하고, 텐서 C/C++ 라이브러리를 위한 Python 래퍼를 만들어 보겠습니다!\n\n```js\n# norch/tensor.py\n\nimport ctypes\n\nclass CTensor(ctypes.Structure):\n    _fields_ = [\n        ('data', ctypes.POINTER(ctypes.c_float)),\n        ('strides', ctypes.POINTER(ctypes.c_int)),\n        ('shape', ctypes.POINTER(ctypes.c_int)),\n        ('ndim', ctypes.c_int),\n        ('size', ctypes.c_int),\n    ]\n\nclass Tensor:\n    os.path.abspath(os.curdir)\n    _C = ctypes.CDLL(\"COMPILED_LIB.so\")\n\n    def __init__(self):\n        \n        data, shape = self.flatten(data)\n        self.data_ctype = (ctypes.c_float * len(data))(*data)\n        self.shape_ctype = (ctypes.c_int * len(shape))(*shape)\n        self.ndim_ctype = ctypes.c_int(len(shape))\n       \n        self.shape = shape\n        self.ndim = len(shape)\n\n        Tensor._C.create_tensor.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n        Tensor._C.create_tensor.restype = ctypes.POINTER(CTensor)\n\n        self.tensor = Tensor._C.create_tensor(\n            self.data_ctype,\n            self.shape_ctype,\n            self.ndim_ctype,\n        )\n        \n    def flatten(self, nested_list):\n        \"\"\"\n        This method simply convert a list type tensor to a flatten tensor with its shape\n        \n        Example:\n        \n        Arguments:  \n            nested_list: [[1, 2, 3], [-5, 2, 0]]\n        Return:\n            flat_data: [1, 2, 3, -5, 2, 0]\n            shape: [2, 3]\n        \"\"\"\n        def flatten_recursively(nested_list):\n            flat_data = []\n            shape = []\n            if isinstance(nested_list, list):\n                for sublist in nested_list:\n                    inner_data, inner_shape = flatten_recursively(sublist)\n                    flat_data.extend(inner_data)\n                shape.append(len(nested_list))\n                shape.extend(inner_shape)\n            else:\n                flat_data.append(nested_list)\n            return flat_data, shape\n        \n        flat_data, shape = flatten_recursively(nested_list)\n        return flat_data, shape\n```\n\n이제 Python 텐서 작업을 포함하여 C/C++ 작업을 호출할 수 있습니다.\n\n```js\n# norch/tensor.py\n\ndef __getitem__(self, indices):\n    \"\"\"\n    index 텐서를 사용하여 텐서에 액세스 tensor[i, j, k...]\n    \"\"\"\n\n    if len(indices) != self.ndim:\n        raise ValueError(\"인덱스 수가 차원 수와 일치해야 함\")\n    \n    Tensor._C.get_item.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int)]\n    Tensor._C.get_item.restype = ctypes.c_float\n                                       \n    indices = (ctypes.c_int * len(indices))(*indices)\n    value = Tensor._C.get_item(self.tensor, indices)  \n    \n    return value\n\ndef reshape(self, new_shape):\n    \"\"\"\n    텐서를 재구성합니다\n    result = tensor.reshape([1,2])\n    \"\"\"\n    new_shape_ctype = (ctypes.c_int * len(new_shape))(*new_shape)\n    new_ndim_ctype = ctypes.c_int(len(new_shape))\n    \n    Tensor._C.reshape_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n    Tensor._C.reshape_tensor.restype = ctypes.POINTER(CTensor)\n    result_tensor_ptr = Tensor._C.reshape_tensor(self.tensor, new_shape_ctype, new_ndim_ctype)   \n\n    result_data = Tensor()\n    result_data.tensor = result_tensor_ptr\n    result_data.shape = new_shape.copy()\n    result_data.ndim = len(new_shape)\n    result_data.device = self.device\n\n    return result_data\n\ndef __add__(self, other):\n    \"\"\"\n    텐서를 더합니다\n    result = tensor1 + tensor2\n    \"\"\"\n  \n    if self.shape != other.shape:\n        raise ValueError(\"덧셈을 위해서 텐서들은 동일한 모양이어야 함\")\n    \n    Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n    Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n\n    result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n\n    result_data = Tensor()\n    result_data.tensor = result_tensor_ptr\n    result_data.shape = self.shape.copy()\n    result_data.ndim = self.ndim\n    result_data.device = self.device\n\n    return result_data\n\n# 기타 연산 포함:\n# __str__\n# __sub__ (-)\n# __mul__ (*)\n# __matmul__ (@)\n# __pow__ (**)\n# __truediv__ (/)\n# log\n# ...\n```\n\n\n\n여기까지 오신 것을 환영합니다! 이제 코드를 실행하고 텐서 작업을 시작할 수 있는 능력이 생겼습니다!\n\n```js\nimport norch\n\ntensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]])\ntensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]])\n\nresult = tensor1 + tensor2\nprint(result[0, 0])\n# 4 \n```\n\n# #2 — GPU 지원\n\n우리 라이브러리의 기본 구조를 만든 후, 이제 새로운 수준으로 끌어올릴 것입니다. 데이터를 GPU로 전송하고 수학 연산을 빠르게 실행하기 위해 `.to(\"cuda\")`를 호출할 수 있다는 것은 잘 알려져 있습니다. CUDA가 어떻게 작동하는지 기본 지식이 있을 것으로 가정하겠습니다만, 그렇지 않은 경우 다른 기사인 'CUDA 튜토리얼'을 읽어볼 수 있습니다. 여기서 기다릴게요. 😊\n\n\n\n...\n\n급한 사람들을 위해, 간단한 소개가 여기 있어요:\n\n기본적으로, 지금까지의 모든 코드는 CPU 메모리에서 실행되고 있어요. 하나의 작업에 대해서는 CPU가 빠르지만, GPU의 장점은 병렬화 능력에 있어요. CPU 디자인은 연산(스레드)을 빠르게 실행하도록 목표를 한 반면, GPU 디자인은 수백만 개의 연산을 병렬로 실행하도록 목표를 해요 (개별 스레드의 성능을 희생하며).\n\n그래서 우리는 이 능력을 활용하여 병렬 연산을 수행할 수 있어요. 예를 들어, 백만 개의 요소로 구성된 텐서를 추가할 때, 반복문 내에서 각 색인의 요소를 순차적으로 추가하는 대신, GPU를 사용하여 한꺼번에 모두를 병렬로 추가할 수 있어요. 이를 위해 NVIDIA에서 개발한 개발자들이 GPU 지원을 소프트웨어 애플리케이션에 통합할 수 있게 하는 플랫폼인 CUDA를 사용할 수 있어요.\n\n\n\n그걸 하려면, 특정 GPU 작업(예: CPU 메모리에서 GPU 메모리로 데이터 복사)을 실행하기 위해 설계된 간단한 C/C++ 기반 인터페이스 인 CUDA C/C++를 사용할 수 있습니다.\n\n아래 코드는 기본적으로 CPU에서 GPU로 데이터를 복사하고 배열의 각 요소를 추가하는 AddTwoArrays 함수(커널이라고도 함)를 N개의 GPU 스레드에서 병렬로 실행하는 몇 가지 CUDA C/C++ 함수를 사용합니다.\n\n```c\n#include \u003cstdio.h\u003e\n\n// CPU 버전(비교용)\nvoid AddTwoArrays_CPU(flaot A[], float B[], float C[]) {\n    for (int i = 0; i \u003c N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n\n// 커널 정의\n__global__ void AddTwoArrays_GPU(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // 배열 크기\n    float A[N], B[N], C[N]; // 배열 A, B, C\n\n    ...\n\n    float *d_A, *d_B, *d_C; // 배열 A, B, C의 장치 포인터\n\n    // 배열 A, B, C에 대한 장치에서의 메모리 할당\n    cudaMalloc((void **)\u0026d_A, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_B, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_C, N * sizeof(float));\n\n    // 호스트에서 장치로 배열 A 및 B 복사\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // N개의 스레드를 사용하여 커널 호출\n    AddTwoArrays_GPU\u003c\u003c\u003c1, N\u003e\u003e\u003e(d_A, d_B, d_C);\n    \n    // 장치에서 호스트로 벡터 C 복사\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n}\n```\n\n주목할 점은 각 요소 쌍을 각각 추가하는 대신 모든 덧셈 작업을 병렬로 실행하여 루프 명령을 제거한 것입니다.\n\n\n\n간단한 소개 이후에, 텐서 라이브러리로 돌아갈 수 있어요.\n\n첫 번째 단계는 CPU에서 GPU로 텐서 데이터를 보내는 함수를 만드는 것입니다.\n\n```js\n//norch/csrc/tensor.cpp\n\nvoid to_device(Tensor* tensor, char* target_device) {\n    if ((strcmp(target_device, \"cuda\") == 0) \u0026\u0026 (strcmp(tensor-\u003edevice, \"cpu\") == 0)) {\n        cpu_to_cuda(tensor);\n    }\n\n    else if ((strcmp(target_device, \"cpu\") == 0) \u0026\u0026 (strcmp(tensor-\u003edevice, \"cuda\") == 0)) {\n        cuda_to_cpu(tensor);\n    }\n}\n```\n\n```js\n//norch/csrc/cuda.cu\n\n__host__ void cpu_to_cuda(Tensor* tensor) {\n    \n    float* data_tmp;\n    cudaMalloc((void **)\u0026data_tmp, tensor-\u003esize * sizeof(float));\n    cudaMemcpy(data_tmp, tensor-\u003edata, tensor-\u003esize * sizeof(float), cudaMemcpyHostToDevice);\n\n    tensor-\u003edata = data_tmp;\n\n    const char* device_str = \"cuda\";\n    tensor-\u003edevice = (char*)malloc(strlen(device_str) + 1);\n    strcpy(tensor-\u003edevice, device_str); \n\n    printf(\"텐서가 성공적으로 %s로 전송되었습니다.\\n\", tensor-\u003edevice);\n}\n\n__host__ void cuda_to_cpu(Tensor* tensor) {\n    float* data_tmp = (float*)malloc(tensor-\u003esize * sizeof(float));\n\n    cudaMemcpy(data_tmp, tensor-\u003edata, tensor-\u003esize * sizeof(float), cudaMemcpyDeviceToHost);\n    cudaFree(tensor-\u003edata);\n\n    tensor-\u003edata = data_tmp;\n\n    const char* device_str = \"cpu\";\n    tensor-\u003edevice = (char*)malloc(strlen(device_str) + 1);\n    strcpy(tensor-\u003edevice, device_str); \n\n    printf(\"텐서가 성공적으로 %s로 전송되었습니다.\\n\", tensor-\u003edevice);\n}\n```\n\n\n\n파이썬으로 구현된 래퍼:\n\n```js\n# norch/tensor.py\n\ndef to(self, device):\n    self.device = device\n    self.device_ctype = self.device.encode('utf-8')\n  \n    Tensor._C.to_device.argtypes = [ctypes.POINTER(CTensor), ctypes.c_char_p]\n    Tensor._C.to_device.restype = None\n    Tensor._C.to_device(self.tensor, self.device_ctype)\n  \n    return self\n```\n\n다음으로, 모든 텐서 연산에 대해 GPU 버전을 생성합니다. 덧셈과 뺄셈에 대한 예제를 작성하겠습니다:\n\n```js\n//norch/csrc/cuda.cu\n\n#define THREADS_PER_BLOCK 128\n\n__global__ void add_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {\n    \n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i \u003c size) {\n        result_data[i] = data1[i] + data2[i];\n    }\n}\n\n__host__ void add_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    int number_of_blocks = (tensor1-\u003esize + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n    add_tensor_cuda_kernel\u003c\u003c\u003cnumber_of_blocks, THREADS_PER_BLOCK\u003e\u003e\u003e(tensor1-\u003edata, tensor2-\u003edata, result_data, tensor1-\u003esize);\n\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    cudaDeviceSynchronize();\n}\n\n__global__ void sub_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {\n   \n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i \u003c size) {\n        result_data[i] = data1[i] - data2[i];\n    }\n}\n\n__host__ void sub_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n    \n    int number_of_blocks = (tensor1-\u003esize + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n    sub_tensor_cuda_kernel\u003c\u003c\u003cnumber_of_blocks, THREADS_PER_BLOCK\u003e\u003e\u003e(tensor1-\u003edata, tensor2-\u003edata, result_data, tensor1-\u003esize);\n\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    cudaDeviceSynchronize();\n}\n\n...\n```\n\n\n\n그런 다음, 텐서.cpp에 새로운 텐서 속성 char* device를 추가하고 작업을 실행할 위치(CPU 또는 GPU)를 선택하는 데 사용할 수 있습니다:\n\n```js\n//norch/csrc/tensor.cpp\n\nTensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {\n    if (tensor1-\u003endim != tensor2-\u003endim) {\n        fprintf(stderr, \"덧셈을 위해 텐서가 동일한 차원 수여야 합니다 %d and %d\\n\", tensor1-\u003endim, tensor2-\u003endim);\n        exit(1);\n    }\n\n    if (strcmp(tensor1-\u003edevice, tensor2-\u003edevice) != 0) {\n        fprintf(stderr, \"텐서는 동일한 장치에 있어야 합니다: %s and %s\\n\", tensor1-\u003edevice, tensor2-\u003edevice);\n        exit(1);\n    }\n\n    char* device = (char*)malloc(strlen(tensor1-\u003edevice) + 1);\n    if (device != NULL) {\n        strcpy(device, tensor1-\u003edevice);\n    } else {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(-1);\n    }\n    int ndim = tensor1-\u003endim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"메모리 할당 실패\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i \u003c ndim; i++) {\n        if (tensor1-\u003eshape[i] != tensor2-\u003eshape[i]) {\n            fprintf(stderr, \"덧셈을 위해 텐서들은 색인 %d에서 동일한 형태여야 합니다 %d and %d\\n\", i, tensor1-\u003eshape[i], tensor2-\u003eshape[i]);\n            exit(1);\n        }\n        shape[i] = tensor1-\u003eshape[i];\n    }        \n\n    if (strcmp(tensor1-\u003edevice, \"cuda\") == 0) {\n\n        float* result_data;\n        cudaMalloc((void **)\u0026result_data, tensor1-\u003esize * sizeof(float));\n        add_tensor_cuda(tensor1, tensor2, result_data);\n        return create_tensor(result_data, shape, ndim, device);\n    } \n    else {\n        float* result_data = (float*)malloc(tensor1-\u003esize * sizeof(float));\n        if (result_data == NULL) {\n            fprintf(stderr, \"메모리 할당 실패\\n\");\n            exit(1);\n        }\n        add_tensor_cpu(tensor1, tensor2, result_data);\n        return create_tensor(result_data, shape, ndim, device);\n    }     \n}\n```\n\n이제 라이브러리가 GPU 지원을 제공합니다!\n\n```js\nimport norch\n\ntensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]]).to(\"cuda\")\ntensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]]).to(\"cuda\")\n\nresult = tensor1 + tensor2\n```\n\n\n\n# #3 — Automatic Differentiation (Autograd)\n\n파이토치가 인기를 얻게 된 주요 이유 중 하나는 Autograd 모듈 때문입니다. Autograd 모듈은 자동 미분을 수행하여 기울기를 계산할 수 있게 해주는 핵심 구성 요소입니다 (경사 하강법과 같은 최적화 알고리즘을 사용하여 모델을 훈련하는 데 중요합니다). .backward()라는 단일 메서드 호출로 이전 텐서 연산에서 모든 기울기를 계산합니다:\n\n```js\nx = torch.tensor([[1., 2, 3], [3., 2, 1]], requires_grad=True)\n# [[1,  2,  3],\n#  [3,  2., 1]]\n\ny = torch.tensor([[3., 2, 1], [1., 2, 3]], requires_grad=True)\n# [[3,  2, 1],\n#  [1,  2, 3]]\n\nL = ((x - y) ** 3).sum()\n\nL.backward()\n\n# x와 y의 기울기에 접근할 수 있습니다\nprint(x.grad)\n# [[12, 0, 12],\n#  [12, 0, 12]]\n\nprint(y.grad)\n# [[-12, 0, -12],\n#  [-12, 0, -12]]\n\n# z를 최소화하기 위해서는 경사 하강법에 사용할 수 있습니다:\n# x = x - 학습률 * x.grad\n# y = y - 학습률 * y.grad\n```\n\n무슨 일이 일어나고 있는지 이해하기 위해 동일한 절차를 수동으로 복제해보겠습니다:\n\n\n\n\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_10.png\" /\u003e\n\n우선 계산해 봅시다:\n\n\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_11.png\" /\u003e\n\nx가 행렬이라는 것에 유의해야 합니다. 따라서 각 요소에 대한 L의 미분을 개별적으로 계산해야 합니다. 게다가, L은 모든 요소에 대한 합이지만 각 요소에 대한 미분에서 다른 요소들은 중요한 영향을 미치지 않는다는 것을 기억하는 것이 중요합니다. 따라서 우리는 다음과 같은 항을 얻습니다:\n\n\n\n\n![이미지](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_12.png)\n\n각 항에 대해 연쇄 법칙을 적용하여 외부 함수를 미분하고 내부 함수를 미분한 값을 곱합니다:\n\n![이미지](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_13.png)\n\nWhere:\n\n\n\n\n마침내:\n\n![이미지](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_14.png)\n\n그러므로, x에 관한 L의 미분을 계산하는 최종 방정식은 다음과 같습니다:\n\n\n\n아래는 Markdown 형식으로 변경된 내용입니다.\n\n\n![Image 1](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_16.png)\n\nSubstituting the values into the equation:\n\n![Image 2](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_17.png)\n\nCalculating the result, we get the same values we obtained with PyTorch:\n\n\n\n\n\n![image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_18.png)\n\nNow, let’s analyze what we just did:\n\nBasically, we observed all the operations involved in reverse order: a summation, a power of 3, and a subtraction. Then, we applied the chain rule, calculating the derivative of each operation and recursively calculated the derivative for the next operation. So, first we need an implementation of the derivative for different math operations:\n\nFor addition:\n\n\n\n\n\n![Image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_19.png)\n\n```js\n# norch/autograd/functions.py\n\nclass AddBackward:\n    def __init__(self, x, y):\n        self.input = [x, y]\n\n    def backward(self, gradient):\n        return [gradient, gradient]\n```\n\nFor sin:\n\n![Image](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_20.png)\n\n\n\n\n```js\n# norch/autograd/functions.py\n\nclass SinBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        return [x.cos() * gradient]\n```\n\n코사인에 대해:\n\n![2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21.png)\n\n```js\n# norch/autograd/functions.py\n\nclass CosBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        return [- x.sin() * gradient]\n```\n\n\n\n요소별 곱셈에 대한 자세한 내용을 확인해보세요:\n\n![element-wise multiplication](/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_22.png)\n\n```python\n# norch/autograd/functions.py\n\nclass ElementwiseMulBackward:\n    def __init__(self, x, y):\n        self.input = [x, y]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        y = self.input[1]\n        return [y * gradient, x * gradient]\n```\n\n합산에 대해서:\n\n\n\n\n# norch/autograd/functions.py\n\n```python\nclass SumBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        # sum 함수는 텐서를 스칼라로 줄이므로 기울기를 일치시키기 위해 브로드캐스트됩니다.\n        return [float(gradient.tensor.contents.data[0]) * self.input[0].ones_like()]\n```\n\n다른 연산을 살펴볼 수 있는 GitHub 저장소 링크도 확인할 수 있습니다.\n\n이제 각 작업에 대한 도함수 식을 가졌으니, 재귀적으로 역전파 체인 규칙을 구현할 수 있습니다. 텐서에 requires_grad 인자를 설정하여 이 텐서의 기울기를 저장하려는 것을 나타낼 수 있습니다. True이면 각 텐서 작업의 기울기를 저장합니다. 예를 들어:\n\n```python\n# norch/tensor.py\n\ndef __add__(self, other):\n\n  if self.shape != other.shape:\n      raise ValueError(\"덧셈을 위해 텐서는 동일한 모양이어야 합니다.\")\n  \n  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n  \n  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n  \n  result_data = Tensor()\n  result_data.tensor = result_tensor_ptr\n  result_data.shape = self.shape.copy()\n  result_data.ndim = self.ndim\n  result_data.device = self.device\n  \n  result_data.requires_grad = self.requires_grad or other.requires_grad\n  if result_data.requires_grad:\n      result_data.grad_fn = AddBackward(self, other)\n```\n\n\n\n그럼, `.backward()` 메서드를 구현해보세요:\n\n```python\n# norch/tensor.py\n\ndef backward(self, gradient=None):\n    if not self.requires_grad:\n        return\n    \n    if gradient is None:\n        if self.shape == [1]:\n            gradient = Tensor([1]) # dx/dx = 1 case\n        else:\n            raise RuntimeError(\"Gradient argument must be specified for non-scalar tensors.\")\n\n    if self.grad is None:\n        self.grad = gradient\n\n    else:\n        self.grad += gradient\n\n    if self.grad_fn is not None: # not a leaf\n        grads = self.grad_fn.backward(gradient) # call the operation backward\n        for tensor, grad in zip(self.grad_fn.input, grads):\n            if isinstance(tensor, Tensor):\n                tensor.backward(grad) # recursively call the backward again for the gradient expression (chain rule)\n```\n\n마지막으로, 텐서의 그래디언트를 제로화하는 `.zero_grad()`와 텐서의 오토그래드 히스토리를 제거하는 `.detach()`를 구현해주세요:\n\n```python\n# norch/tensor.py\n\ndef zero_grad(self):\n    self.grad = None\n\ndef detach(self):\n    self.grad = None\n    self.grad_fn = None\n```\n\n\n\n축하합니다! GPU 지원 및 자동 미분 기능이 있는 완전한 텐서 라이브러리를 만드셨군요! 이제 nn 및 optim 모듈을 만들어 몇 가지 딥 러닝 모델을 더 쉽게 훈련시킬 수 있습니다.\n\n## #4 — nn 및 optim 모듈\n\nnn은 신경망 및 딥 러닝 모델을 구축하기 위한 모듈이며, optim은 이러한 모델을 훈련시키기 위한 최적화 알고리즘과 관련이 있습니다. 이들을 재현하기 위한 첫 번째 단계는 Parameter를 구현하는 것입니다. Parameter는 간단히 말해 항상 True로 설정된 requires_grad 속성을 갖는 훈련 가능한 텐서로, 일부 임의의 초기화 기법을 사용해 같은 연산을 수행합니다.\n\n```js\n# norch/nn/parameter.py\n\nfrom norch.tensor import Tensor\nfrom norch.utils import utils\nimport random\n\nclass Parameter(Tensor):\n    \"\"\"\n    A parameter is a trainable tensor.\n    \"\"\"\n    def __init__(self, shape):\n        data = utils.generate_random_list(shape=shape)\n        super().__init__(data, requires_grad=True)\n```\n\n\n\n```js\n# norch/utisl/utils.py\n\ndef generate_random_list(shape):\n    \"\"\"\n    랜덤한 숫자로 이루어진 'shape' 형태의 리스트를 생성합니다\n    [4, 2] --\u003e [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]\n    \"\"\"\n    if len(shape) == 0:\n        return []\n    else:\n        inner_shape = shape[1:]\n        if len(inner_shape) == 0:\n            return [random.uniform(-1, 1) for _ in range(shape[0])]\n        else:\n            return [generate_random_list(inner_shape) for _ in range(shape[0])]\n```\n\n파라미터를 활용하면 모듈을 구성할 수 있습니다:\n\n```js\n# norch/nn/module.py\n\nfrom .parameter import Parameter\nfrom collections import OrderedDict\nfrom abc import ABC\nimport inspect\n\nclass Module(ABC):\n    \"\"\"\n    모듈을 위한 추상 클래스\n    \"\"\"\n    def __init__(self):\n        self._modules = OrderedDict()\n        self._params = OrderedDict()\n        self._grads = OrderedDict()\n        self.training = True\n\n    def forward(self, *inputs, **kwargs):\n        raise NotImplementedError\n\n    def __call__(self, *inputs, **kwargs):\n        return self.forward(*inputs, **kwargs)\n\n    def train(self):\n        self.training = True\n        for param in self.parameters():\n            param.requires_grad = True\n\n    def eval(self):\n        self.training = False\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def parameters(self):\n        for name, value in inspect.getmembers(self):\n            if isinstance(value, Parameter):\n                yield self, name, value\n            elif isinstance(value, Module):\n                yield from value.parameters()\n\n    def modules(self):\n        yield from self._modules.values()\n\n    def gradients(self):\n        for module in self.modules():\n            yield module._grads\n\n    def zero_grad(self):\n        for _, _, parameter in self.parameters():\n            parameter.zero_grad()\n\n    def to(self, device):\n        for _, _, parameter in self.parameters():\n            parameter.to(device)\n\n        return self\n    \n    def inner_repr(self):\n        return \"\"\n\n    def __repr__(self):\n        string = f\"{self.get_name()}(\"\n        tab = \"   \"\n        modules = self._modules\n        if modules == {}:\n            string += f'\\n{tab}(parameters): {self.inner_repr()}'\n        else:\n            for key, module in modules.items():\n                string += f\"\\n{tab}({key}): {module.get_name()}({module.inner_repr()})\"\n        return f'{string}\\n)'\n    \n    def get_name(self):\n        return self.__class__.__name__\n    \n    def __setattr__(self, key, value):\n        self.__dict__[key] = value\n\n        if isinstance(value, Module):\n            self._modules[key] = value\n        elif isinstance(value, Parameter):\n            self._params[key] = value\n```\n\n예를 들어, nn.Module을 상속하여 사용자 정의 모듈을 만들거나, 이전에 생성된 모듈 중 하나인 선형 모듈을 사용하여 y = Wx + b 작업을 구현할 수 있습니다.\n\n\n\n\n```js\n# norch/nn/modules/linear.py\n\nfrom ..module import Module\nfrom ..parameter import Parameter\n\nclass Linear(Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.weight = Parameter(shape=[self.output_dim, self.input_dim])\n        self.bias = Parameter(shape=[self.output_dim, 1])\n\n    def forward(self, x):\n        z = self.weight @ x + self.bias\n        return z\n\n    def inner_repr(self):\n        return f\"input_dim={self.input_dim}, output_dim={self.output_dim}, \" \\\n               f\"bias={True if self.bias is not None else False}\"\n```\n\n이제 몇 가지 손실 및 활성화 함수를 구현할 수 있습니다. 예를 들어, 평균 제곱 오차 손실 및 시그모이드 함수:\n\n```js\n# norch/nn/loss.py\n\nfrom .module import Module\n \nclass MSELoss(Module):\n    def __init__(self):\n      pass\n\n    def forward(self, predictions, labels):\n        assert labels.shape == predictions.shape, \\\n            \"Labels and predictions shape does not match: {} and {}\".format(labels.shape, predictions.shape)\n        \n        return ((predictions - labels) ** 2).sum() / predictions.numel\n\n    def __call__(self, *inputs):\n        return self.forward(*inputs)\n```\n\n```js\n# norch/nn/activation.py\n\nfrom .module import Module\nimport math\n\nclass Sigmoid(Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 1.0 / (1.0 + (math.e) ** (-x)) \n```\n\n\n\n마지막으로 옵티마이저를 만들어봅시다. 예시로 확률적 경사 하강법(Stochastic Gradient Descent) 알고리즘을 구현하겠습니다:\n\n```js\n# norch/optim/optimizer.py\n\nfrom abc import ABC\nfrom norch.tensor import Tensor\n\nclass Optimizer(ABC):\n    \"\"\"\n    옵티마이저를 위한 추상 클래스\n    \"\"\"\n\n    def __init__(self, parameters):\n        if isinstance(parameters, Tensor):\n            raise TypeError(\"parameters는 반복 가능한 객체이어야 하지만 {} 타입이 입력되었습니다\".format(type(parameters)))\n        elif isinstance(parameters, dict):\n            parameters = parameters.values()\n\n        self.parameters = list(parameters)\n\n    def step(self):\n        raise NotImplementedError\n    \n    def zero_grad(self):\n        for module, name, parameter in self.parameters:\n            parameter.zero_grad()\n\n\nclass SGD(Optimizer):\n    def __init__(self, parameters, lr=1e-1, momentum=0):\n        super().__init__(parameters)\n        self.lr = lr\n        self.momentum = momentum\n        self._cache = {'velocity': [p.zeros_like() for (_, _, p) in self.parameters]}\n\n    def step(self):\n        for i, (module, name, _) in enumerate(self.parameters):\n            parameter = getattr(module, name)\n\n            velocity = self._cache['velocity'][i]\n\n            velocity = self.momentum * velocity - self.lr * parameter.grad\n\n            updated_parameter = parameter + velocity\n\n            setattr(module, name, updated_parameter)\n\n            self._cache['velocity'][i] = velocity\n\n            parameter.detach()\n            velocity.detach()\n```\n\n그리고 여기까지입니다! 이제 우리만의 딥러닝 프레임워크를 만들었어요! 🥳\n\n이제 학습을 시작해봅시다:\n\n\n\n```js\nimport norch\nimport norch.nn as nn\nimport norch.optim as optim\nimport random\nimport math\n\nrandom.seed(1)\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(1, 10)\n        self.sigmoid = nn.Sigmoid()\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        \n        return out\n\ndevice = \"cuda\"\nepochs = 10\n\nmodel = MyModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\nloss_list = []\n\nx_values = [0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ,\n        4.4,  4.8,  5.2,  5.6,  6. ,  6.4,  6.8,  7.2,  7.6,  8. ,  8.4,\n        8.8,  9.2,  9.6, 10. , 10.4, 10.8, 11.2, 11.6, 12. , 12.4, 12.8,\n       13.2, 13.6, 14. , 14.4, 14.8, 15.2, 15.6, 16. , 16.4, 16.8, 17.2,\n       17.6, 18. , 18.4, 18.8, 19.2, 19.6, 20.]\n\ny_true = []\nfor x in x_values:\n    y_true.append(math.pow(math.sin(x), 2))\n\n\nfor epoch in range(epochs):\n    for x, target in zip(x_values, y_true):\n        x = norch.Tensor([[x]]).T\n        target = norch.Tensor([[target]]).T\n\n        x = x.to(device)\n        target = target.to(device)\n\n        outputs = model(x)\n        loss = criterion(outputs, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}')\n    loss_list.append(loss[0])\n\n# Epoch [1/10], Loss: 1.7035\n# Epoch [2/10], Loss: 0.7193\n# Epoch [3/10], Loss: 0.3068\n# Epoch [4/10], Loss: 0.1742\n# Epoch [5/10], Loss: 0.1342\n# Epoch [6/10], Loss: 0.1232\n# Epoch [7/10], Loss: 0.1220\n# Epoch [8/10], Loss: 0.1241\n# Epoch [9/10], Loss: 0.1270\n# Epoch [10/10], Loss: 0.1297\n```\n\n\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_23.png\" /\u003e\n\n성공적으로 모델이 생성되고 사용자 정의 딥러닝 프레임워크를 사용하여 훈련되었습니다!\n\n전체 코드는 여기에서 확인할 수 있습니다.\n\n\n\n# 결론\n\n이 게시물에서는 텐서와 같은 기본 개념, 어떻게 모델링되는지, CUDA 및 Autograd와 같은 고급 주제 등을 다루었습니다. 우리는 GPU 지원 및 자동 미분이 가능한 딥 러닝 프레임워크를 성공적으로 만들었습니다. 이 게시물이 여러분이 PyTorch가 어떻게 작동하는지 간략히 이해하는 데 도움이 되었으면 좋겠습니다.\n\n앞으로의 게시물에서는 분산 훈련(다중 노드/다중 GPU) 및 메모리 관리와 같은 고급 주제를 다루려고 할 것입니다. 의견이 있거나 다음에 어떤 내용을 다루길 원하시는지 댓글로 알려주세요! 읽어 주셔서 정말 감사합니다! 😊\n\n또한 최신 기사를 받아보기 위해 여기와 제 LinkedIn 프로필에서 팔로우해 주세요!\n\n\n\n# 참고 자료\n\n- [PyNorch](https://github.com) - 이 프로젝트의 GitHub 저장소 \n- [CUDA 튜토리얼](https://www.example.com/tutorial-cuda) - CUDA 작동 방식에 대한 간단한 소개\n- [PyTorch](https://pytorch.org/docs) - PyTorch 문서\n\n\n\n# MartinLwx's 블로그 - 스트라이드에 관한 튜토리얼.\n\n# 스트라이드 튜토리얼 - 스트라이드에 관한 또 다른 튜토리얼.\n\n# PyTorch 내부 구조 - PyTorch 구조에 대한 가이드.\n\n# 네츠 - NumPy를 사용한 PyTorch 재구현.\n\n\n\nMarkdown으로 표 태그를 변경하십시오.","ogImage":{"url":"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png"},"coverImage":"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png","tag":["Tech"],"readingTime":40},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003eC/C++, CUDA, 및 Python을 기반으로 한 고유의 딥 러닝 프레임워크를 구축해 보세요. GPU 지원과 자동 미분을 제공합니다\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e여러 해 동안 PyTorch를 사용하여 딥 러닝 모델을 구축하고 훈련해 왔습니다. 그럼에도 불구하고, 그 문법과 규칙을 익히고도, 제 궁금증을 자극하던 것이 있었습니다: 이러한 작업 중에 내부에서 어떤 일이 일어나고 있는 걸까요? 이 모든 것이 어떻게 작동할까요?\u003c/p\u003e\n\u003cp\u003e여기까지 오셨다면, 아마도 비슷한 질문을 가지고 계실 것입니다. 파이토치(PyTorch)에서 모델을 생성하고 훈련하는 방법을 물어본다면 아마도 아래 코드와 비슷한 것을 생각해볼 것입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch.\u003cspan class=\"hljs-property\"\u003enn\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e nn\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch.\u003cspan class=\"hljs-property\"\u003eoptim\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e optim\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMyModel\u003c/span\u003e(nn.\u003cspan class=\"hljs-property\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eMyModel\u003c/span\u003e, self).\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003efc1\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003esigmoid\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eSigmoid\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003efc2\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, x):\n        out = self.\u003cspan class=\"hljs-title function_\"\u003efc1\u003c/span\u003e(x)\n        out = self.\u003cspan class=\"hljs-title function_\"\u003esigmoid\u003c/span\u003e(out)\n        out = self.\u003cspan class=\"hljs-title function_\"\u003efc2\u003c/span\u003e(out)\n        \n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e out\n\n...\n\nmodel = \u003cspan class=\"hljs-title class_\"\u003eMyModel\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(device)\ncriterion = nn.\u003cspan class=\"hljs-title class_\"\u003eMSELoss\u003c/span\u003e()\noptimizer = optim.\u003cspan class=\"hljs-title function_\"\u003eSGD\u003c/span\u003e(model.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(), lr=\u003cspan class=\"hljs-number\"\u003e0.001\u003c/span\u003e)\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(epochs):\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x, y \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e ...\n        \n        x = x.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(device)\n        y = y.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(device)\n\n        outputs = \u003cspan class=\"hljs-title function_\"\u003emodel\u003c/span\u003e(x)\n        loss = \u003cspan class=\"hljs-title function_\"\u003ecriterion\u003c/span\u003e(outputs, y)\n        \n        optimizer.\u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e()\n        loss.\u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e()\n        optimizer.\u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e하지만 이번에 역전파(backward) 단계가 어떻게 작동하는지 물어본다면 어떨까요? 또는 예를 들어, 텐서를 재구성할 때 무슨 일이 일어나는지 궁금하시다면요? 내부에서 데이터가 재배치되나요? 그런 일이 어떻게 일어나나요? 왜 PyTorch는 빠른가요? PyTorch가 GPU 연산을 어떻게 처리하는지요? 이런 질문들이 항상 저를 호기심 가득하게 만들었고, 여러분도 마찬가지로 호기심이 드실 것이라고 상상합니다. 그래서 이러한 개념을 더 잘 이해하기 위해 스스로 텐서 라이브러리를 처음부터 구축해보는 것이 무엇보다 좋을까요? 이 글에서 여러분이 배우게 될 것이 바로 그겁니다!\u003c/p\u003e\n\u003ch2\u003e#1 — 텐서\u003c/h2\u003e\n\u003cp\u003e텐서 라이브러리를 구축하기 위해 가장 먼저 알아야 할 개념은 무엇이 텐서인지에 대한 명백한 개념입니다.\u003c/p\u003e\n\u003cp\u003e텐서는 몇 가지 숫자를 포함하는 n차원 데이터 구조의 수학적 개념이라는 직관적인 생각을 가지고 있을 수 있습니다. 그러나 여기서는 이 데이터 구조를 계산적 관점에서 어떻게 모델링할지 이해해야 합니다. 텐서는 데이터 자체뿐만 아니라 모양이나 텐서가 있는 장치(예: CPU 메모리, GPU 메모리)와 같은 측면을 설명하는 메타데이터로 구성된다고 생각할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e텐서의 내부를 이해하는 데 매우 중요한 개념인 stride라는 잘 알려지지 않은 메타데이터도 있습니다. 따라서 텐서 데이터 재배열의 내부를 이해하기 위해 약간 더 이에 대해 논의해야 합니다.\u003c/p\u003e\n\u003cp\u003e2-D 텐서의 모양이 [4, 8]인 경우를 상상해보세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_2.png\" alt=\"텐서\"\u003e\u003c/p\u003e\n\u003cp\u003e텐서의 데이터(즉, 부동 소수점 수)는 실제로 메모리에 1차원 배열로 저장됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_3.png\" alt=\"데이터\"\u003e\u003c/p\u003e\n\u003cp\u003e그러면 이 1차원 배열을 N차원 텐서로 나타내려면 스트라이드를 사용합니다. 기본 아이디어는 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e4행 8열의 행렬이 있습니다. 그 행렬의 모든 원소가 1차원 배열의 행에 의해 구성되어 있다고 가정할 때, 위치 [2, 3]의 값을 액세스하려면 2행(각 행에 8개의 요소)을 횡단해야 하며 추가로 3개의 위치를 지나야 합니다. 수학적으로 표현하면 1차원 배열에서 3 + 2 * 8 요소를 횡단해야 합니다.\u003c/p\u003e\n\u003cp\u003e따라서, '8'은 두 번째 차원의 스트라이드입니다. 이 경우, 배열에서 다른 위치로 \"점프\"하기 위해 몇 개의 요소를 횡단해야 하는지를 나타내는 정보입니다.\u003c/p\u003e\n\u003cp\u003e따라서, 모양이 [shape_0, shape_1]인 2차원 텐서의 요소 [i, j]에 액세스하려면, 기본적으로 j + i * shape_1 위치에 있는 요소에 액세스해야 합니다.\u003c/p\u003e\n\u003cp\u003e이제 3차원 텐서를 상상해보겠습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 3차원 텐서를 행렬의 시퀀스로 생각할 수 있습니다. 예를 들어, 이 [5, 4, 8] 텐서를 [4, 8] 모양의 5개 행렬로 생각할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이제 [1, 3, 7] 위치에 있는 요소에 액세스하기 위해 [4,8] 형태의 행렬을 1개 완전히 횡단하고, [8] 형태의 행을 2개, [1] 형태의 열을 7개 횡단해야 합니다. 따라서 1차원 배열에서 (1 * 4 * 8) + (2 * 8) + (7 * 1) 위치를 횡단해야 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_6.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e따라서, [shape_0, shape_1, shape_2] 모양의 3차원 텐서에서 1차원 데이터 배열에서 [i][j][k] 요소에 액세스하는 방법은 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_7.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 shape_1 * shape_2가 첫 번째 차원의 stride이고, shape_2는 두 번째 차원의 stride이며 1은 세 번째 차원의 stride입니다.\u003c/p\u003e\n\u003cp\u003e그런 다음, 일반화하기 위해서는:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_8.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e각 차원의 stride는 다음 차원 텐서 모양의 곱을 사용하여 계산할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e그런 다음 stride[n-1] = 1로 설정합니다.\u003c/p\u003e\n\u003cp\u003e우리의 형태의 텐서 예제 [5, 4, 8]에서 strides = [4*8, 8, 1] = [32, 8, 1]일 것입니다.\u003c/p\u003e\n\u003cp\u003e여러분들도 직접 테스트할 수 있어요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\ntorch.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e]).\u003cspan class=\"hljs-title function_\"\u003estride\u003c/span\u003e()\n#(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e알겠어요, 그런데 왜 모양과 스트라이드가 필요한 건가요? N차원 텐서의 요소에 접근하는 것을 넘어, 이 개념은 텐서 배열을 매우 쉽게 조작하는 데 사용될 수 있어요.\u003c/p\u003e\n\u003cp\u003e예를 들어, 텐서를 재구성하려면 새로운 모양을 설정하고 새로운 스트라이드를 계산하면 됩니다! (새로운 모양은 동일한 요소 수를 보장하므로)\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\nt = torch.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e])\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(t.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)\n# [\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(t.\u003cspan class=\"hljs-title function_\"\u003estride\u003c/span\u003e())\n# [\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\nnew_t = t.\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e])\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(new_t.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)\n# [\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(new_t.\u003cspan class=\"hljs-title function_\"\u003estride\u003c/span\u003e())\n# [\u003cspan class=\"hljs-number\"\u003e40\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e텐서 내부에서는 여전히 동일한 1차원 배열로 저장됩니다. reshape 메서드는 배열 내 요소의 순서를 변경하지 않았습니다! 대단하지 않나요? 😁\u003c/p\u003e\n\u003cp\u003e다음 함수를 사용하여 PyTorch에서 내부 1차원 배열에 액세스하는 함수를 사용하여 직접 확인할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ctypes\n\ndef \u003cspan class=\"hljs-title function_\"\u003eprint_internal\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003et\u003c/span\u003e: torch.\u003cspan class=\"hljs-property\"\u003eTensor\u003c/span\u003e):\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\n        torch.\u003cspan class=\"hljs-title function_\"\u003efrombuffer\u003c/span\u003e(\n            ctypes.\u003cspan class=\"hljs-title function_\"\u003estring_at\u003c/span\u003e(t.\u003cspan class=\"hljs-title function_\"\u003edata_ptr\u003c/span\u003e(), t.\u003cspan class=\"hljs-title function_\"\u003estorage\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003enbytes\u003c/span\u003e()), dtype=t.\u003cspan class=\"hljs-property\"\u003edtype\u003c/span\u003e\n        )\n    )\n\n\u003cspan class=\"hljs-title function_\"\u003eprint_internal\u003c/span\u003e(t)\n# [\u003cspan class=\"hljs-number\"\u003e0.0752\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5898\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.3930\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9577\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.2276\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9786\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1009\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.138\u003c/span\u003e, ...\n\n\u003cspan class=\"hljs-title function_\"\u003eprint_internal\u003c/span\u003e(new_t)\n# [\u003cspan class=\"hljs-number\"\u003e0.0752\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5898\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.3930\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9577\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.2276\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9786\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1009\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.138\u003c/span\u003e, ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e예를 들어 두 축을 전치하려면 내부적으로 해당 스트라이드를 단순히 바꾸어 주면 됩니다!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003et = torch.\u003cspan class=\"hljs-title function_\"\u003earange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e24\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(t)\n# [[[ \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e],\n#   [ \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e],\n#   [ \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e]],\n \n#  [[\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e22\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e]]]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(t.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)\n# [\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(t.\u003cspan class=\"hljs-title function_\"\u003estride\u003c/span\u003e())\n# [\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\nnew_t = t.\u003cspan class=\"hljs-title function_\"\u003etranspose\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(new_t)\n# [[[ \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e]],\n\n#  [[ \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e]],\n\n#  [[ \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e22\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e]]]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(new_t.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)\n# [\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(new_t.\u003cspan class=\"hljs-title function_\"\u003estride\u003c/span\u003e())\n# [\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e내부 배열을 출력하면 두 값 모두 동일합니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title function_\"\u003eprint_internal\u003c/span\u003e(t)\n# [ \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e22\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint_internal\u003c/span\u003e(new_t)\n# [ \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e22\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그러나 new_t의 스트라이드는 이제 위의 식과 일치하지 않습니다. 이것은 텐서가 이제 연속적이지 않기 때문에 발생합니다. 즉, 내부 배열은 동일하지만 메모리 내의 값의 순서가 텐서의 실제 순서와 일치하지 않는다는 것을 의미합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003et.\u003cspan class=\"hljs-title function_\"\u003eis_contiguous\u003c/span\u003e()\n# \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n\nnew_t.\u003cspan class=\"hljs-title function_\"\u003eis_contiguous\u003c/span\u003e()\n# \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이는 연속되지 않는 요소에 연속적으로 액세스하는 것이 효율적이지 않다는 것을 의미합니다 (실제 텐서 요소는 메모리 상에서 순서대로 정렬되어 있지 않기 때문입니다). 이를 해결하기 위해 다음을 수행할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003enew_t_contiguous = new_t.\u003cspan class=\"hljs-title function_\"\u003econtiguous\u003c/span\u003e()\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(new_t_contiguous.\u003cspan class=\"hljs-title function_\"\u003eis_contiguous\u003c/span\u003e())\n# \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e내부 배열을 분석하면 이제 순서가 실제 텐서 순서와 일치하여 더 나은 메모리 액세스 효율을 제공할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(new_t)\n# [[[ \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e]],\n\n#  [[ \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e]],\n\n#  [[ \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e],\n#   [\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e22\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e]]]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint_internal\u003c/span\u003e(new_t)\n# [ \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e22\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint_internal\u003c/span\u003e(new_t_contiguous)\n# [ \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e22\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e23\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 우리는 텐서가 어떻게 모델링되는지 이해했으니, 라이브러리 생성을 시작해 봅시다!\u003c/p\u003e\n\u003cp\u003e내가 만들 라이브러리 이름은 Norch입니다. PyTorch가 아닌 (NOT PyTorch)을 의미하며, 성(Nogueira)을 암시하기도 합니다. 😁\u003c/p\u003e\n\u003cp\u003e첫 번째로 알아야 할 것은 PyTorch가 Python을 통해 사용되지만 내부적으로는 C/C++로 실행된다는 것입니다. 그래서 먼저 내부 C/C++ 함수를 만들 것입니다.\u003c/p\u003e\n\u003cp\u003e먼저 텐서를 데이터와 메타데이터를 저장하는 구조체로 정의하고 이를 만들기 위한 함수를 생성할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/tensor.cpp\u003c/span\u003e\n\n#include \u0026#x3C;stdio.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n#include \u0026#x3C;stdlib.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n#include \u0026#x3C;string.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n#include \u0026#x3C;math.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n\ntypedef struct {\n    float* data;\n    int* strides;\n    int* shape;\n    int ndim;\n    int size;\n    char* device;\n} \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e;\n\n\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e* \u003cspan class=\"hljs-title function_\"\u003ecreate_tensor\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003efloat* data, int* shape, int ndim\u003c/span\u003e) {\n    \n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e* tensor = (\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e));\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (tensor == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n    tensor-\u003edata = data;\n    tensor-\u003eshape = shape;\n    tensor-\u003endim = ndim;\n\n    tensor-\u003esize = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; ndim; i++) {\n        tensor-\u003esize *= shape[i];\n    }\n\n    tensor-\u003estrides = (int*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(ndim * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(int));\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (tensor-\u003estrides == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n    int stride = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = ndim - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e; i \u003e= \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i--) {\n        tensor-\u003estrides[i] = stride;\n        stride *= shape[i];\n    }\n    \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e tensor;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e일부 요소에 접근하기 위해서는 앞서 배웠던 스트라이드(strides)를 활용할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/tensor.cpp\u003c/span\u003e\n\nfloat \u003cspan class=\"hljs-title function_\"\u003eget_item\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor, int* indices\u003c/span\u003e) {\n    int index = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; tensor-\u003endim; i++) {\n        index += indices[i] * tensor-\u003estrides[i];\n    }\n\n    float result;\n    result = tensor-\u003edata[index];\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e result;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 텐서 작업을 만들 수 있습니다. 몇 가지 예제를 보여드리겠고, 이 글 끝에 링크된 저장소에서 완전한 버전을 찾을 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/cpu.cpp\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eadd_tensor_cpu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor1, Tensor* tensor2, float* result_data\u003c/span\u003e) {\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; tensor1-\u003esize; i++) {\n        result_data[i] = tensor1-\u003edata[i] + tensor2-\u003edata[i];\n    }\n}\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esub_tensor_cpu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor1, Tensor* tensor2, float* result_data\u003c/span\u003e) {\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; tensor1-\u003esize; i++) {\n        result_data[i] = tensor1-\u003edata[i] - tensor2-\u003edata[i];\n    }\n}\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eelementwise_mul_tensor_cpu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor1, Tensor* tensor2, float* result_data\u003c/span\u003e) {\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; tensor1-\u003esize; i++) {\n        result_data[i] = tensor1-\u003edata[i] * tensor2-\u003edata[i];\n    }\n}\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eassign_tensor_cpu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor, float* result_data\u003c/span\u003e) {\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; tensor-\u003esize; i++) {\n        result_data[i] = tensor-\u003edata[i];\n    }\n}\n\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그 다음에, 이러한 작업들을 호출할 텐서 다른 함수를 만들 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/tensor.cpp\u003c/span\u003e\n\n\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e* \u003cspan class=\"hljs-title function_\"\u003eadd_tensor\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor1, Tensor* tensor2\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (tensor1-\u003endim != tensor2-\u003endim) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"덧셈을 위해서 텐서는 동일한 차원 수여야 합니다 %d 와 %d\\n\"\u003c/span\u003e, tensor1-\u003endim, tensor2-\u003endim);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    int ndim = tensor1-\u003endim;\n    int* shape = (int*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(ndim * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(int));\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (shape == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; ndim; i++) {\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (tensor1-\u003eshape[i] != tensor2-\u003eshape[i]) {\n            \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"덧셈을 위해서 텐서는 동일한 모양이어야 합니다 %d 와 %d 인덱스 %d에서\\n\"\u003c/span\u003e, tensor1-\u003eshape[i], tensor2-\u003eshape[i], i);\n            \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n        }\n        shape[i] = tensor1-\u003eshape[i];\n    }        \n    float* result_data = (float*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(tensor1-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float));\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (result_data == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n    \u003cspan class=\"hljs-title function_\"\u003eadd_tensor_cpu\u003c/span\u003e(tensor1, tensor2, result_data);\n    \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecreate_tensor\u003c/span\u003e(result_data, shape, ndim, device);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이전에 언급한 대로, 텐서 재구성은 내부 데이터 배열을 수정하지 않습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/tensor.cpp\u003c/span\u003e\n\n\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e* \u003cspan class=\"hljs-title function_\"\u003ereshape_tensor\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor, int* new_shape, int new_ndim\u003c/span\u003e) {\n\n    int ndim = new_ndim;\n    int* shape = (int*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(ndim * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(int));\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (shape == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; ndim; i++) {\n        shape[i] = new_shape[i];\n    }\n\n    \u003cspan class=\"hljs-comment\"\u003e// 새 모양의 요소 총 수 계산\u003c/span\u003e\n    int size = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; new_ndim; i++) {\n        size *= shape[i];\n    }\n\n    \u003cspan class=\"hljs-comment\"\u003e// 총 요소 수가 현재 텐서의 크기와 일치하는지 확인\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (size != tensor-\u003esize) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"텐서를 재구성할 수 없습니다. 새 모양의 요소 총 수가 현재 텐서의 크기와 일치하지 않습니다.\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    float* result_data = (float*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(tensor-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float));\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (result_data == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n    \u003cspan class=\"hljs-title function_\"\u003eassign_tensor_cpu\u003c/span\u003e(tensor, result_data);\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecreate_tensor\u003c/span\u003e(result_data, shape, ndim, device);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 일부 텐서 작업을 수행할 수 있지만, 누구나 C/C++을 사용하여 실행해야 하는 것은 아닙니다. 이제 Python 래퍼를 만들어 봅시다!\u003c/p\u003e\n\u003cp\u003ePython을 사용하여 C/C++ 코드를 실행할 수 있는 다양한 옵션이 있습니다. Pybind11과 Cython 등이 있습니다. 이 예시에서는 ctypes를 사용할 것입니다.\u003c/p\u003e\n\u003cp\u003e아래는 ctypes의 기본적인 구조입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//C 코드\u003c/span\u003e\n#include \u0026#x3C;stdio.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n\nfloat \u003cspan class=\"hljs-title function_\"\u003eadd_floats\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003efloat a, float b\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e a + b;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 컴파일\ngcc -shared -o add_floats.\u003cspan class=\"hljs-property\"\u003eso\u003c/span\u003e -fPIC add_floats.\u003cspan class=\"hljs-property\"\u003ec\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 코드\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ctypes\n\n# 공유 라이브러리 로드\nlib = ctypes.\u003cspan class=\"hljs-title function_\"\u003eCDLL\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'./add_floats.so'\u003c/span\u003e)\n\n# 함수의 인자와 반환 유형 정의\nlib.\u003cspan class=\"hljs-property\"\u003eadd_floats\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eargtypes\u003c/span\u003e = [ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e]\nlib.\u003cspan class=\"hljs-property\"\u003eadd_floats\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003erestype\u003c/span\u003e = ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e\n\n# 파이썬 float 값을 c_float 유형으로 변환\na = ctypes.\u003cspan class=\"hljs-title function_\"\u003ec_float\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3.5\u003c/span\u003e)\nb = ctypes.\u003cspan class=\"hljs-title function_\"\u003ec_float\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2.2\u003c/span\u003e)\n\n# C 함수 호출\nresult = lib.\u003cspan class=\"hljs-title function_\"\u003eadd_floats\u003c/span\u003e(a, b)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(result)\n# \u003cspan class=\"hljs-number\"\u003e5.7\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e보시다시피 매우 직관적입니다. C/C++ 코드를 컴파일한 후 Python에서 ctypes를 사용하면 매우 쉽게 사용할 수 있습니다. 함수의 매개변수 및 반환 c_types를 정의하고, 변수를 해당 c_types로 변환하고 함수를 호출하기만 하면 됩니다. 배열(부동 소수점 목록)과 같은 보다 복잡한 유형의 경우 포인터를 사용할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edata = [\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3.0\u003c/span\u003e]\ndata_ctype = (ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(data))(*data)\n\nlib.\u003cspan class=\"hljs-property\"\u003esome_array_func\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eargstypes\u003c/span\u003e = [ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e)]\n\n...\n\nlib.\u003cspan class=\"hljs-title function_\"\u003esome_array_func\u003c/span\u003e(data)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그리고 구조체 유형의 경우 직접 c_type을 만들 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCustomType\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003eStructure\u003c/span\u003e):\n    _fields_ = [\n        (\u003cspan class=\"hljs-string\"\u003e'field1'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e)),\n        (\u003cspan class=\"hljs-string\"\u003e'field2'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e)),\n        (\u003cspan class=\"hljs-string\"\u003e'field3'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e),\n    ]\n\n# ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCustomType\u003c/span\u003e)로 사용할 수 있습니다.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e간단히 설명하고, 텐서 C/C++ 라이브러리를 위한 Python 래퍼를 만들어 보겠습니다!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/tensor.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ctypes\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003eStructure\u003c/span\u003e):\n    _fields_ = [\n        (\u003cspan class=\"hljs-string\"\u003e'data'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e)),\n        (\u003cspan class=\"hljs-string\"\u003e'strides'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e)),\n        (\u003cspan class=\"hljs-string\"\u003e'shape'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e)),\n        (\u003cspan class=\"hljs-string\"\u003e'ndim'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e),\n        (\u003cspan class=\"hljs-string\"\u003e'size'\u003c/span\u003e, ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e),\n    ]\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e:\n    os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eabspath\u003c/span\u003e(os.\u003cspan class=\"hljs-property\"\u003ecurdir\u003c/span\u003e)\n    _C = ctypes.\u003cspan class=\"hljs-title function_\"\u003eCDLL\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"COMPILED_LIB.so\"\u003c/span\u003e)\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        \n        data, shape = self.\u003cspan class=\"hljs-title function_\"\u003eflatten\u003c/span\u003e(data)\n        self.\u003cspan class=\"hljs-property\"\u003edata_ctype\u003c/span\u003e = (ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(data))(*data)\n        self.\u003cspan class=\"hljs-property\"\u003eshape_ctype\u003c/span\u003e = (ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(shape))(*shape)\n        self.\u003cspan class=\"hljs-property\"\u003endim_ctype\u003c/span\u003e = ctypes.\u003cspan class=\"hljs-title function_\"\u003ec_int\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(shape))\n       \n        self.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e = shape\n        self.\u003cspan class=\"hljs-property\"\u003endim\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(shape)\n\n        \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ecreate_tensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eargtypes\u003c/span\u003e = [ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e), ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e), ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e]\n        \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ecreate_tensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003erestype\u003c/span\u003e = ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e)\n\n        self.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ecreate_tensor\u003c/span\u003e(\n            self.\u003cspan class=\"hljs-property\"\u003edata_ctype\u003c/span\u003e,\n            self.\u003cspan class=\"hljs-property\"\u003eshape_ctype\u003c/span\u003e,\n            self.\u003cspan class=\"hljs-property\"\u003endim_ctype\u003c/span\u003e,\n        )\n        \n    def \u003cspan class=\"hljs-title function_\"\u003eflatten\u003c/span\u003e(self, nested_list):\n        \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n        This method simply convert a list type tensor to a flatten tensor with its shape\n        \n        Example:\n        \n        Arguments:  \n            nested_list: [[1, 2, 3], [-5, 2, 0]]\n        Return:\n            flat_data: [1, 2, 3, -5, 2, 0]\n            shape: [2, 3]\n        \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n        def \u003cspan class=\"hljs-title function_\"\u003eflatten_recursively\u003c/span\u003e(nested_list):\n            flat_data = []\n            shape = []\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eisinstance\u003c/span\u003e(nested_list, list):\n                \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e sublist \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003enested_list\u003c/span\u003e:\n                    inner_data, inner_shape = \u003cspan class=\"hljs-title function_\"\u003eflatten_recursively\u003c/span\u003e(sublist)\n                    flat_data.\u003cspan class=\"hljs-title function_\"\u003eextend\u003c/span\u003e(inner_data)\n                shape.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(nested_list))\n                shape.\u003cspan class=\"hljs-title function_\"\u003eextend\u003c/span\u003e(inner_shape)\n            \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n                flat_data.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(nested_list)\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e flat_data, shape\n        \n        flat_data, shape = \u003cspan class=\"hljs-title function_\"\u003eflatten_recursively\u003c/span\u003e(nested_list)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e flat_data, shape\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 Python 텐서 작업을 포함하여 C/C++ 작업을 호출할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/tensor.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\ndef \u003cspan class=\"hljs-title function_\"\u003e__getitem__\u003c/span\u003e(self, indices):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    index 텐서를 사용하여 텐서에 액세스 tensor[i, j, k...]\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(indices) != self.\u003cspan class=\"hljs-property\"\u003endim\u003c/span\u003e:\n        raise \u003cspan class=\"hljs-title class_\"\u003eValueError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"인덱스 수가 차원 수와 일치해야 함\"\u003c/span\u003e)\n    \n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eget_item\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eargtypes\u003c/span\u003e = [ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e), ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e)]\n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eget_item\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003erestype\u003c/span\u003e = ctypes.\u003cspan class=\"hljs-property\"\u003ec_float\u003c/span\u003e\n                                       \n    indices = (ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(indices))(*indices)\n    value = \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eget_item\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e, indices)  \n    \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e value\n\ndef \u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(self, new_shape):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    텐서를 재구성합니다\n    result = tensor.reshape([1,2])\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    new_shape_ctype = (ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(new_shape))(*new_shape)\n    new_ndim_ctype = ctypes.\u003cspan class=\"hljs-title function_\"\u003ec_int\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(new_shape))\n    \n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ereshape_tensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eargtypes\u003c/span\u003e = [ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e), ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e), ctypes.\u003cspan class=\"hljs-property\"\u003ec_int\u003c/span\u003e]\n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ereshape_tensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003erestype\u003c/span\u003e = ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e)\n    result_tensor_ptr = \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ereshape_tensor\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e, new_shape_ctype, new_ndim_ctype)   \n\n    result_data = \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e()\n    result_data.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e = result_tensor_ptr\n    result_data.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e = new_shape.\u003cspan class=\"hljs-title function_\"\u003ecopy\u003c/span\u003e()\n    result_data.\u003cspan class=\"hljs-property\"\u003endim\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(new_shape)\n    result_data.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e result_data\n\ndef \u003cspan class=\"hljs-title function_\"\u003e__add__\u003c/span\u003e(self, other):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    텐서를 더합니다\n    result = tensor1 + tensor2\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n  \n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e != other.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e:\n        raise \u003cspan class=\"hljs-title class_\"\u003eValueError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"덧셈을 위해서 텐서들은 동일한 모양이어야 함\"\u003c/span\u003e)\n    \n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eadd_tensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eargtypes\u003c/span\u003e = [ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e), ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e)]\n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eadd_tensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003erestype\u003c/span\u003e = ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e)\n\n    result_tensor_ptr = \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eadd_tensor\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e, other.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e)\n\n    result_data = \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e()\n    result_data.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e = result_tensor_ptr\n    result_data.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ecopy\u003c/span\u003e()\n    result_data.\u003cspan class=\"hljs-property\"\u003endim\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003endim\u003c/span\u003e\n    result_data.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e result_data\n\n# 기타 연산 포함:\n# __str__\n# __sub__ (-)\n# __mul__ (*)\n# __matmul__ (@)\n# __pow__ (**)\n# __truediv__ (/)\n# log\n# ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기까지 오신 것을 환영합니다! 이제 코드를 실행하고 텐서 작업을 시작할 수 있는 능력이 생겼습니다!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e norch\n\ntensor1 = norch.\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]])\ntensor2 = norch.\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]])\n\nresult = tensor1 + tensor2\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(result[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n# \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e#2 — GPU 지원\u003c/h1\u003e\n\u003cp\u003e우리 라이브러리의 기본 구조를 만든 후, 이제 새로운 수준으로 끌어올릴 것입니다. 데이터를 GPU로 전송하고 수학 연산을 빠르게 실행하기 위해 \u003ccode\u003e.to(\"cuda\")\u003c/code\u003e를 호출할 수 있다는 것은 잘 알려져 있습니다. CUDA가 어떻게 작동하는지 기본 지식이 있을 것으로 가정하겠습니다만, 그렇지 않은 경우 다른 기사인 'CUDA 튜토리얼'을 읽어볼 수 있습니다. 여기서 기다릴게요. 😊\u003c/p\u003e\n\u003cp\u003e...\u003c/p\u003e\n\u003cp\u003e급한 사람들을 위해, 간단한 소개가 여기 있어요:\u003c/p\u003e\n\u003cp\u003e기본적으로, 지금까지의 모든 코드는 CPU 메모리에서 실행되고 있어요. 하나의 작업에 대해서는 CPU가 빠르지만, GPU의 장점은 병렬화 능력에 있어요. CPU 디자인은 연산(스레드)을 빠르게 실행하도록 목표를 한 반면, GPU 디자인은 수백만 개의 연산을 병렬로 실행하도록 목표를 해요 (개별 스레드의 성능을 희생하며).\u003c/p\u003e\n\u003cp\u003e그래서 우리는 이 능력을 활용하여 병렬 연산을 수행할 수 있어요. 예를 들어, 백만 개의 요소로 구성된 텐서를 추가할 때, 반복문 내에서 각 색인의 요소를 순차적으로 추가하는 대신, GPU를 사용하여 한꺼번에 모두를 병렬로 추가할 수 있어요. 이를 위해 NVIDIA에서 개발한 개발자들이 GPU 지원을 소프트웨어 애플리케이션에 통합할 수 있게 하는 플랫폼인 CUDA를 사용할 수 있어요.\u003c/p\u003e\n\u003cp\u003e그걸 하려면, 특정 GPU 작업(예: CPU 메모리에서 GPU 메모리로 데이터 복사)을 실행하기 위해 설계된 간단한 C/C++ 기반 인터페이스 인 CUDA C/C++를 사용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e아래 코드는 기본적으로 CPU에서 GPU로 데이터를 복사하고 배열의 각 요소를 추가하는 AddTwoArrays 함수(커널이라고도 함)를 N개의 GPU 스레드에서 병렬로 실행하는 몇 가지 CUDA C/C++ 함수를 사용합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-meta\"\u003e#\u003cspan class=\"hljs-keyword\"\u003einclude\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\u0026#x3C;stdio.h\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e// CPU 버전(비교용)\u003c/span\u003e\n\u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eAddTwoArrays_CPU\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(flaot A[], \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e B[], \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e C[])\u003c/span\u003e {\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n\n\u003cspan class=\"hljs-comment\"\u003e// 커널 정의\u003c/span\u003e\n__global__ \u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eAddTwoArrays_GPU\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e A[], \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e B[], \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e C[])\u003c/span\u003e {\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\n\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003emain\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e()\u003c/span\u003e {\n\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e N = \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e; \u003cspan class=\"hljs-comment\"\u003e// 배열 크기\u003c/span\u003e\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e A[N], B[N], C[N]; \u003cspan class=\"hljs-comment\"\u003e// 배열 A, B, C\u003c/span\u003e\n\n    ...\n\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e *d_A, *d_B, *d_C; \u003cspan class=\"hljs-comment\"\u003e// 배열 A, B, C의 장치 포인터\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e// 배열 A, B, C에 대한 장치에서의 메모리 할당\u003c/span\u003e\n    cudaMalloc((\u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e **)\u0026#x26;d_A, N * \u003cspan class=\"hljs-keyword\"\u003esizeof\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e));\n    cudaMalloc((\u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e **)\u0026#x26;d_B, N * \u003cspan class=\"hljs-keyword\"\u003esizeof\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e));\n    cudaMalloc((\u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e **)\u0026#x26;d_C, N * \u003cspan class=\"hljs-keyword\"\u003esizeof\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e));\n\n    \u003cspan class=\"hljs-comment\"\u003e// 호스트에서 장치로 배열 A 및 B 복사\u003c/span\u003e\n    cudaMemcpy(d_A, A, N * \u003cspan class=\"hljs-keyword\"\u003esizeof\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * \u003cspan class=\"hljs-keyword\"\u003esizeof\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e), cudaMemcpyHostToDevice);\n\n    \u003cspan class=\"hljs-comment\"\u003e// N개의 스레드를 사용하여 커널 호출\u003c/span\u003e\n    AddTwoArrays_GPU\u0026#x3C;\u0026#x3C;\u0026#x3C;\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, N\u003e\u003e\u003e(d_A, d_B, d_C);\n    \n    \u003cspan class=\"hljs-comment\"\u003e// 장치에서 호스트로 벡터 C 복사\u003c/span\u003e\n    cudaMemcpy(C, d_C, N * \u003cspan class=\"hljs-keyword\"\u003esizeof\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e), cudaMemcpyDeviceToHost);\n\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e주목할 점은 각 요소 쌍을 각각 추가하는 대신 모든 덧셈 작업을 병렬로 실행하여 루프 명령을 제거한 것입니다.\u003c/p\u003e\n\u003cp\u003e간단한 소개 이후에, 텐서 라이브러리로 돌아갈 수 있어요.\u003c/p\u003e\n\u003cp\u003e첫 번째 단계는 CPU에서 GPU로 텐서 데이터를 보내는 함수를 만드는 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/tensor.cpp\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eto_device\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor, char* target_device\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e ((\u003cspan class=\"hljs-title function_\"\u003estrcmp\u003c/span\u003e(target_device, \u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) \u0026#x26;\u0026#x26; (\u003cspan class=\"hljs-title function_\"\u003estrcmp\u003c/span\u003e(tensor-\u003edevice, \u003cspan class=\"hljs-string\"\u003e\"cpu\"\u003c/span\u003e) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)) {\n        \u003cspan class=\"hljs-title function_\"\u003ecpu_to_cuda\u003c/span\u003e(tensor);\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e ((\u003cspan class=\"hljs-title function_\"\u003estrcmp\u003c/span\u003e(target_device, \u003cspan class=\"hljs-string\"\u003e\"cpu\"\u003c/span\u003e) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) \u0026#x26;\u0026#x26; (\u003cspan class=\"hljs-title function_\"\u003estrcmp\u003c/span\u003e(tensor-\u003edevice, \u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)) {\n        \u003cspan class=\"hljs-title function_\"\u003ecuda_to_cpu\u003c/span\u003e(tensor);\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/cuda.cu\u003c/span\u003e\n\n__host__ \u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecpu_to_cuda\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor\u003c/span\u003e) {\n    \n    float* data_tmp;\n    \u003cspan class=\"hljs-title function_\"\u003ecudaMalloc\u003c/span\u003e((\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e **)\u0026#x26;data_tmp, tensor-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float));\n    \u003cspan class=\"hljs-title function_\"\u003ecudaMemcpy\u003c/span\u003e(data_tmp, tensor-\u003edata, tensor-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float), cudaMemcpyHostToDevice);\n\n    tensor-\u003edata = data_tmp;\n\n    \u003cspan class=\"hljs-keyword\"\u003econst\u003c/span\u003e char* device_str = \u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e;\n    tensor-\u003edevice = (char*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003estrlen\u003c/span\u003e(device_str) + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    \u003cspan class=\"hljs-title function_\"\u003estrcpy\u003c/span\u003e(tensor-\u003edevice, device_str); \n\n    \u003cspan class=\"hljs-title function_\"\u003eprintf\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"텐서가 성공적으로 %s로 전송되었습니다.\\n\"\u003c/span\u003e, tensor-\u003edevice);\n}\n\n__host__ \u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecuda_to_cpu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor\u003c/span\u003e) {\n    float* data_tmp = (float*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(tensor-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float));\n\n    \u003cspan class=\"hljs-title function_\"\u003ecudaMemcpy\u003c/span\u003e(data_tmp, tensor-\u003edata, tensor-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float), cudaMemcpyDeviceToHost);\n    \u003cspan class=\"hljs-title function_\"\u003ecudaFree\u003c/span\u003e(tensor-\u003edata);\n\n    tensor-\u003edata = data_tmp;\n\n    \u003cspan class=\"hljs-keyword\"\u003econst\u003c/span\u003e char* device_str = \u003cspan class=\"hljs-string\"\u003e\"cpu\"\u003c/span\u003e;\n    tensor-\u003edevice = (char*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003estrlen\u003c/span\u003e(device_str) + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    \u003cspan class=\"hljs-title function_\"\u003estrcpy\u003c/span\u003e(tensor-\u003edevice, device_str); \n\n    \u003cspan class=\"hljs-title function_\"\u003eprintf\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"텐서가 성공적으로 %s로 전송되었습니다.\\n\"\u003c/span\u003e, tensor-\u003edevice);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e파이썬으로 구현된 래퍼:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/tensor.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\ndef \u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(self, device):\n    self.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e = device\n    self.\u003cspan class=\"hljs-property\"\u003edevice_ctype\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003edevice\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eencode\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'utf-8'\u003c/span\u003e)\n  \n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eto_device\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eargtypes\u003c/span\u003e = [ctypes.\u003cspan class=\"hljs-title function_\"\u003ePOINTER\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eCTensor\u003c/span\u003e), ctypes.\u003cspan class=\"hljs-property\"\u003ec_char_p\u003c/span\u003e]\n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eto_device\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003erestype\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n    \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_C\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eto_device\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003edevice_ctype\u003c/span\u003e)\n  \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음으로, 모든 텐서 연산에 대해 GPU 버전을 생성합니다. 덧셈과 뺄셈에 대한 예제를 작성하겠습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/cuda.cu\u003c/span\u003e\n\n#define \u003cspan class=\"hljs-variable constant_\"\u003eTHREADS_PER_BLOCK\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e\n\n__global__ \u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eadd_tensor_cuda_kernel\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003efloat* data1, float* data2, float* result_data, int size\u003c/span\u003e) {\n    \n    int i = blockIdx.\u003cspan class=\"hljs-property\"\u003ex\u003c/span\u003e * blockDim.\u003cspan class=\"hljs-property\"\u003ex\u003c/span\u003e + threadIdx.\u003cspan class=\"hljs-property\"\u003ex\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (i \u0026#x3C; size) {\n        result_data[i] = data1[i] + data2[i];\n    }\n}\n\n__host__ \u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eadd_tensor_cuda\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor1, Tensor* tensor2, float* result_data\u003c/span\u003e) {\n    \n    int number_of_blocks = (tensor1-\u003esize + \u003cspan class=\"hljs-variable constant_\"\u003eTHREADS_PER_BLOCK\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) / \u003cspan class=\"hljs-variable constant_\"\u003eTHREADS_PER_BLOCK\u003c/span\u003e;\n    add_tensor_cuda_kernel\u0026#x3C;\u0026#x3C;\u0026#x3C;number_of_blocks, \u003cspan class=\"hljs-variable constant_\"\u003eTHREADS_PER_BLOCK\u003c/span\u003e\u003e\u003e\u003e(tensor1-\u003edata, tensor2-\u003edata, result_data, tensor1-\u003esize);\n\n    cudaError_t error = \u003cspan class=\"hljs-title function_\"\u003ecudaGetLastError\u003c/span\u003e();\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (error != cudaSuccess) {\n        \u003cspan class=\"hljs-title function_\"\u003eprintf\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"CUDA error: %s\\n\"\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003ecudaGetErrorString\u003c/span\u003e(error));\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    \u003cspan class=\"hljs-title function_\"\u003ecudaDeviceSynchronize\u003c/span\u003e();\n}\n\n__global__ \u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esub_tensor_cuda_kernel\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003efloat* data1, float* data2, float* result_data, int size\u003c/span\u003e) {\n   \n    int i = blockIdx.\u003cspan class=\"hljs-property\"\u003ex\u003c/span\u003e * blockDim.\u003cspan class=\"hljs-property\"\u003ex\u003c/span\u003e + threadIdx.\u003cspan class=\"hljs-property\"\u003ex\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (i \u0026#x3C; size) {\n        result_data[i] = data1[i] - data2[i];\n    }\n}\n\n__host__ \u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esub_tensor_cuda\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor1, Tensor* tensor2, float* result_data\u003c/span\u003e) {\n    \n    int number_of_blocks = (tensor1-\u003esize + \u003cspan class=\"hljs-variable constant_\"\u003eTHREADS_PER_BLOCK\u003c/span\u003e - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) / \u003cspan class=\"hljs-variable constant_\"\u003eTHREADS_PER_BLOCK\u003c/span\u003e;\n    sub_tensor_cuda_kernel\u0026#x3C;\u0026#x3C;\u0026#x3C;number_of_blocks, \u003cspan class=\"hljs-variable constant_\"\u003eTHREADS_PER_BLOCK\u003c/span\u003e\u003e\u003e\u003e(tensor1-\u003edata, tensor2-\u003edata, result_data, tensor1-\u003esize);\n\n    cudaError_t error = \u003cspan class=\"hljs-title function_\"\u003ecudaGetLastError\u003c/span\u003e();\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (error != cudaSuccess) {\n        \u003cspan class=\"hljs-title function_\"\u003eprintf\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"CUDA error: %s\\n\"\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003ecudaGetErrorString\u003c/span\u003e(error));\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    \u003cspan class=\"hljs-title function_\"\u003ecudaDeviceSynchronize\u003c/span\u003e();\n}\n\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그런 다음, 텐서.cpp에 새로운 텐서 속성 char* device를 추가하고 작업을 실행할 위치(CPU 또는 GPU)를 선택하는 데 사용할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-comment\"\u003e//norch/csrc/tensor.cpp\u003c/span\u003e\n\n\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e* \u003cspan class=\"hljs-title function_\"\u003eadd_tensor\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eTensor* tensor1, Tensor* tensor2\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (tensor1-\u003endim != tensor2-\u003endim) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"덧셈을 위해 텐서가 동일한 차원 수여야 합니다 %d and %d\\n\"\u003c/span\u003e, tensor1-\u003endim, tensor2-\u003endim);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (\u003cspan class=\"hljs-title function_\"\u003estrcmp\u003c/span\u003e(tensor1-\u003edevice, tensor2-\u003edevice) != \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"텐서는 동일한 장치에 있어야 합니다: %s and %s\\n\"\u003c/span\u003e, tensor1-\u003edevice, tensor2-\u003edevice);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    char* device = (char*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003estrlen\u003c/span\u003e(tensor1-\u003edevice) + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (device != \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003estrcpy\u003c/span\u003e(device, tensor1-\u003edevice);\n    } \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n    int ndim = tensor1-\u003endim;\n    int* shape = (int*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(ndim * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(int));\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (shape == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n        \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; ndim; i++) {\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (tensor1-\u003eshape[i] != tensor2-\u003eshape[i]) {\n            \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"덧셈을 위해 텐서들은 색인 %d에서 동일한 형태여야 합니다 %d and %d\\n\"\u003c/span\u003e, i, tensor1-\u003eshape[i], tensor2-\u003eshape[i]);\n            \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n        }\n        shape[i] = tensor1-\u003eshape[i];\n    }        \n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (\u003cspan class=\"hljs-title function_\"\u003estrcmp\u003c/span\u003e(tensor1-\u003edevice, \u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) {\n\n        float* result_data;\n        \u003cspan class=\"hljs-title function_\"\u003ecudaMalloc\u003c/span\u003e((\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e **)\u0026#x26;result_data, tensor1-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float));\n        \u003cspan class=\"hljs-title function_\"\u003eadd_tensor_cuda\u003c/span\u003e(tensor1, tensor2, result_data);\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecreate_tensor\u003c/span\u003e(result_data, shape, ndim, device);\n    } \n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e {\n        float* result_data = (float*)\u003cspan class=\"hljs-title function_\"\u003emalloc\u003c/span\u003e(tensor1-\u003esize * \u003cspan class=\"hljs-title function_\"\u003esizeof\u003c/span\u003e(float));\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e (result_data == \u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e) {\n            \u003cspan class=\"hljs-title function_\"\u003efprintf\u003c/span\u003e(stderr, \u003cspan class=\"hljs-string\"\u003e\"메모리 할당 실패\\n\"\u003c/span\u003e);\n            \u003cspan class=\"hljs-title function_\"\u003eexit\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e);\n        }\n        \u003cspan class=\"hljs-title function_\"\u003eadd_tensor_cpu\u003c/span\u003e(tensor1, tensor2, result_data);\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ecreate_tensor\u003c/span\u003e(result_data, shape, ndim, device);\n    }     \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 라이브러리가 GPU 지원을 제공합니다!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e norch\n\ntensor1 = norch.\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]]).\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e)\ntensor2 = norch.\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]]).\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e)\n\nresult = tensor1 + tensor2\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e#3 — Automatic Differentiation (Autograd)\u003c/h1\u003e\n\u003cp\u003e파이토치가 인기를 얻게 된 주요 이유 중 하나는 Autograd 모듈 때문입니다. Autograd 모듈은 자동 미분을 수행하여 기울기를 계산할 수 있게 해주는 핵심 구성 요소입니다 (경사 하강법과 같은 최적화 알고리즘을 사용하여 모델을 훈련하는 데 중요합니다). .backward()라는 단일 메서드 호출로 이전 텐서 연산에서 모든 기울기를 계산합니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ex = torch.\u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e1.\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e3.\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]], requires_grad=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n# [[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e],\n#  [\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]]\n\ny = torch.\u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e([[\u003cspan class=\"hljs-number\"\u003e3.\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e1.\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]], requires_grad=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n# [[\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n#  [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]]\n\nL = ((x - y) ** \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e()\n\nL.\u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e()\n\n# x와 y의 기울기에 접근할 수 있습니다\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(x.\u003cspan class=\"hljs-property\"\u003egrad\u003c/span\u003e)\n# [[\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e],\n#  [\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e]]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(y.\u003cspan class=\"hljs-property\"\u003egrad\u003c/span\u003e)\n# [[-\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e],\n#  [-\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e]]\n\n# z를 최소화하기 위해서는 경사 하강법에 사용할 수 있습니다:\n# x = x - 학습률 * x.\u003cspan class=\"hljs-property\"\u003egrad\u003c/span\u003e\n# y = y - 학습률 * y.\u003cspan class=\"hljs-property\"\u003egrad\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e무슨 일이 일어나고 있는지 이해하기 위해 동일한 절차를 수동으로 복제해보겠습니다:\u003c/p\u003e\n\u003cp\u003e우선 계산해 봅시다:\u003c/p\u003e\n\u003cp\u003ex가 행렬이라는 것에 유의해야 합니다. 따라서 각 요소에 대한 L의 미분을 개별적으로 계산해야 합니다. 게다가, L은 모든 요소에 대한 합이지만 각 요소에 대한 미분에서 다른 요소들은 중요한 영향을 미치지 않는다는 것을 기억하는 것이 중요합니다. 따라서 우리는 다음과 같은 항을 얻습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_12.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e각 항에 대해 연쇄 법칙을 적용하여 외부 함수를 미분하고 내부 함수를 미분한 값을 곱합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_13.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cp\u003e마침내:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_14.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e그러므로, x에 관한 L의 미분을 계산하는 최종 방정식은 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e아래는 Markdown 형식으로 변경된 내용입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_16.png\" alt=\"Image 1\"\u003e\u003c/p\u003e\n\u003cp\u003eSubstituting the values into the equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_17.png\" alt=\"Image 2\"\u003e\u003c/p\u003e\n\u003cp\u003eCalculating the result, we get the same values we obtained with PyTorch:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_18.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, let’s analyze what we just did:\u003c/p\u003e\n\u003cp\u003eBasically, we observed all the operations involved in reverse order: a summation, a power of 3, and a subtraction. Then, we applied the chain rule, calculating the derivative of each operation and recursively calculated the derivative for the next operation. So, first we need an implementation of the derivative for different math operations:\u003c/p\u003e\n\u003cp\u003eFor addition:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_19.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/autograd/functions.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAddBackward\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, x, y):\n        self.\u003cspan class=\"hljs-property\"\u003einput\u003c/span\u003e = [x, y]\n\n    def \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(self, gradient):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [gradient, gradient]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor sin:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_20.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/autograd/functions.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSinBackward\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, x):\n        self.\u003cspan class=\"hljs-property\"\u003einput\u003c/span\u003e = [x]\n\n    def \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(self, gradient):\n        x = self.\u003cspan class=\"hljs-property\"\u003einput\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [x.\u003cspan class=\"hljs-title function_\"\u003ecos\u003c/span\u003e() * gradient]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e코사인에 대해:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21.png\" alt=\"2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_21\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/autograd/functions.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCosBackward\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, x):\n        self.\u003cspan class=\"hljs-property\"\u003einput\u003c/span\u003e = [x]\n\n    def \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(self, gradient):\n        x = self.\u003cspan class=\"hljs-property\"\u003einput\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [- x.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e() * gradient]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e요소별 곱셈에 대한 자세한 내용을 확인해보세요:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation_22.png\" alt=\"element-wise multiplication\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# norch/autograd/functions.py\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eElementwiseMulBackward\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x, y\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e = [x, y]\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, gradient\u003c/span\u003e):\n        x = self.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        y = self.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [y * gradient, x * gradient]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e합산에 대해서:\u003c/p\u003e\n\u003ch1\u003enorch/autograd/functions.py\u003c/h1\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSumBackward\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e = [x]\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, gradient\u003c/span\u003e):\n        \u003cspan class=\"hljs-comment\"\u003e# sum 함수는 텐서를 스칼라로 줄이므로 기울기를 일치시키기 위해 브로드캐스트됩니다.\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(gradient.tensor.contents.data[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]) * self.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].ones_like()]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다른 연산을 살펴볼 수 있는 GitHub 저장소 링크도 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이제 각 작업에 대한 도함수 식을 가졌으니, 재귀적으로 역전파 체인 규칙을 구현할 수 있습니다. 텐서에 requires_grad 인자를 설정하여 이 텐서의 기울기를 저장하려는 것을 나타낼 수 있습니다. True이면 각 텐서 작업의 기울기를 저장합니다. 예를 들어:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# norch/tensor.py\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__add__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, other\u003c/span\u003e):\n\n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.shape != other.shape:\n      \u003cspan class=\"hljs-keyword\"\u003eraise\u003c/span\u003e ValueError(\u003cspan class=\"hljs-string\"\u003e\"덧셈을 위해 텐서는 동일한 모양이어야 합니다.\"\u003c/span\u003e)\n  \n  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n  \n  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n  \n  result_data = Tensor()\n  result_data.tensor = result_tensor_ptr\n  result_data.shape = self.shape.copy()\n  result_data.ndim = self.ndim\n  result_data.device = self.device\n  \n  result_data.requires_grad = self.requires_grad \u003cspan class=\"hljs-keyword\"\u003eor\u003c/span\u003e other.requires_grad\n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e result_data.requires_grad:\n      result_data.grad_fn = AddBackward(self, other)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그럼, \u003ccode\u003e.backward()\u003c/code\u003e 메서드를 구현해보세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# norch/tensor.py\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, gradient=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e self.requires_grad:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e\n    \n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e gradient \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.shape == [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]:\n            gradient = Tensor([\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]) \u003cspan class=\"hljs-comment\"\u003e# dx/dx = 1 case\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003eraise\u003c/span\u003e RuntimeError(\u003cspan class=\"hljs-string\"\u003e\"Gradient argument must be specified for non-scalar tensors.\"\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.grad \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        self.grad = gradient\n\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        self.grad += gradient\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.grad_fn \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e: \u003cspan class=\"hljs-comment\"\u003e# not a leaf\u003c/span\u003e\n        grads = self.grad_fn.backward(gradient) \u003cspan class=\"hljs-comment\"\u003e# call the operation backward\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e tensor, grad \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(self.grad_fn.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e, grads):\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eisinstance\u003c/span\u003e(tensor, Tensor):\n                tensor.backward(grad) \u003cspan class=\"hljs-comment\"\u003e# recursively call the backward again for the gradient expression (chain rule)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로, 텐서의 그래디언트를 제로화하는 \u003ccode\u003e.zero_grad()\u003c/code\u003e와 텐서의 오토그래드 히스토리를 제거하는 \u003ccode\u003e.detach()\u003c/code\u003e를 구현해주세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# norch/tensor.py\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n    self.grad = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003edetach\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n    self.grad = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n    self.grad_fn = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e축하합니다! GPU 지원 및 자동 미분 기능이 있는 완전한 텐서 라이브러리를 만드셨군요! 이제 nn 및 optim 모듈을 만들어 몇 가지 딥 러닝 모델을 더 쉽게 훈련시킬 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e#4 — nn 및 optim 모듈\u003c/h2\u003e\n\u003cp\u003enn은 신경망 및 딥 러닝 모델을 구축하기 위한 모듈이며, optim은 이러한 모델을 훈련시키기 위한 최적화 알고리즘과 관련이 있습니다. 이들을 재현하기 위한 첫 번째 단계는 Parameter를 구현하는 것입니다. Parameter는 간단히 말해 항상 True로 설정된 requires_grad 속성을 갖는 훈련 가능한 텐서로, 일부 임의의 초기화 기법을 사용해 같은 연산을 수행합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/nn/parameter.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e norch.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e norch.\u003cspan class=\"hljs-property\"\u003eutils\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e utils\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e random\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eParameter\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    A parameter is a trainable tensor.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, shape):\n        data = utils.\u003cspan class=\"hljs-title function_\"\u003egenerate_random_list\u003c/span\u003e(shape=shape)\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(data, requires_grad=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/utisl/utils.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\ndef \u003cspan class=\"hljs-title function_\"\u003egenerate_random_list\u003c/span\u003e(shape):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    랜덤한 숫자로 이루어진 'shape' 형태의 리스트를 생성합니다\n    [4, 2] --\u003e [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(shape) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e []\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        inner_shape = shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e:]\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(inner_shape) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [random.\u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _ \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(shape[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])]\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [\u003cspan class=\"hljs-title function_\"\u003egenerate_random_list\u003c/span\u003e(inner_shape) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _ \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(shape[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e파라미터를 활용하면 모듈을 구성할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/nn/\u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e .\u003cspan class=\"hljs-property\"\u003eparameter\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eParameter\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e collections \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOrderedDict\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e abc \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eABC\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e inspect\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eABC\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    모듈을 위한 추상 클래스\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        self.\u003cspan class=\"hljs-property\"\u003e_modules\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eOrderedDict\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003e_params\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eOrderedDict\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003e_grads\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eOrderedDict\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003etraining\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, *inputs, **kwargs):\n        raise \u003cspan class=\"hljs-title class_\"\u003eNotImplementedError\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__call__\u003c/span\u003e(self, *inputs, **kwargs):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(*inputs, **kwargs)\n\n    def \u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(self):\n        self.\u003cspan class=\"hljs-property\"\u003etraining\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e param \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e():\n            param.\u003cspan class=\"hljs-property\"\u003erequires_grad\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-built_in\"\u003eeval\u003c/span\u003e(self):\n        self.\u003cspan class=\"hljs-property\"\u003etraining\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e param \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e():\n            param.\u003cspan class=\"hljs-property\"\u003erequires_grad\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e name, value \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e inspect.\u003cspan class=\"hljs-title function_\"\u003egetmembers\u003c/span\u003e(self):\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eisinstance\u003c/span\u003e(value, \u003cspan class=\"hljs-title class_\"\u003eParameter\u003c/span\u003e):\n                \u003cspan class=\"hljs-keyword\"\u003eyield\u003c/span\u003e self, name, value\n            elif \u003cspan class=\"hljs-title function_\"\u003eisinstance\u003c/span\u003e(value, \u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e):\n                \u003cspan class=\"hljs-keyword\"\u003eyield\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e value.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e()\n\n    def \u003cspan class=\"hljs-title function_\"\u003emodules\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003eyield\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003e_modules\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003evalues\u003c/span\u003e()\n\n    def \u003cspan class=\"hljs-title function_\"\u003egradients\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e \u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003emodules\u003c/span\u003e():\n            \u003cspan class=\"hljs-keyword\"\u003eyield\u003c/span\u003e \u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e_grads\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _, _, parameter \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e():\n            parameter.\u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e()\n\n    def \u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(self, device):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _, _, parameter \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e():\n            parameter.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(device)\n\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self\n    \n    def \u003cspan class=\"hljs-title function_\"\u003einner_repr\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__repr__\u003c/span\u003e(self):\n        string = f\u003cspan class=\"hljs-string\"\u003e\"{self.get_name()}(\"\u003c/span\u003e\n        tab = \u003cspan class=\"hljs-string\"\u003e\"   \"\u003c/span\u003e\n        modules = self.\u003cspan class=\"hljs-property\"\u003e_modules\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e modules == {}:\n            string += f\u003cspan class=\"hljs-string\"\u003e'\\n{tab}(parameters): {self.inner_repr()}'\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e key, \u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e modules.\u003cspan class=\"hljs-title function_\"\u003eitems\u003c/span\u003e():\n                string += f\u003cspan class=\"hljs-string\"\u003e\"\\n{tab}({key}): {module.get_name()}({module.inner_repr()})\"\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e f\u003cspan class=\"hljs-string\"\u003e'{string}\\n)'\u003c/span\u003e\n    \n    def \u003cspan class=\"hljs-title function_\"\u003eget_name\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003e__class__\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003e__name__\u003c/span\u003e\n    \n    def \u003cspan class=\"hljs-title function_\"\u003e__setattr__\u003c/span\u003e(self, key, value):\n        self.\u003cspan class=\"hljs-property\"\u003e__dict__\u003c/span\u003e[key] = value\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eisinstance\u003c/span\u003e(value, \u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e):\n            self.\u003cspan class=\"hljs-property\"\u003e_modules\u003c/span\u003e[key] = value\n        elif \u003cspan class=\"hljs-title function_\"\u003eisinstance\u003c/span\u003e(value, \u003cspan class=\"hljs-title class_\"\u003eParameter\u003c/span\u003e):\n            self.\u003cspan class=\"hljs-property\"\u003e_params\u003c/span\u003e[key] = value\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e예를 들어, nn.Module을 상속하여 사용자 정의 모듈을 만들거나, 이전에 생성된 모듈 중 하나인 선형 모듈을 사용하여 y = Wx + b 작업을 구현할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/nn/modules/linear.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e ..\u003cspan class=\"hljs-property\"\u003emodule\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e ..\u003cspan class=\"hljs-property\"\u003eparameter\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eParameter\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, input_dim, output_dim):\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003einput_dim\u003c/span\u003e = input_dim\n        self.\u003cspan class=\"hljs-property\"\u003eoutput_dim\u003c/span\u003e = output_dim\n        self.\u003cspan class=\"hljs-property\"\u003eweight\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eParameter\u003c/span\u003e(shape=[self.\u003cspan class=\"hljs-property\"\u003eoutput_dim\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003einput_dim\u003c/span\u003e])\n        self.\u003cspan class=\"hljs-property\"\u003ebias\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eParameter\u003c/span\u003e(shape=[self.\u003cspan class=\"hljs-property\"\u003eoutput_dim\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, x):\n        z = self.\u003cspan class=\"hljs-property\"\u003eweight\u003c/span\u003e @ x + self.\u003cspan class=\"hljs-property\"\u003ebias\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e z\n\n    def \u003cspan class=\"hljs-title function_\"\u003einner_repr\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e f\u003cspan class=\"hljs-string\"\u003e\"input_dim={self.input_dim}, output_dim={self.output_dim}, \"\u003c/span\u003e \\\n               f\u003cspan class=\"hljs-string\"\u003e\"bias={True if self.bias is not None else False}\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 몇 가지 손실 및 활성화 함수를 구현할 수 있습니다. 예를 들어, 평균 제곱 오차 손실 및 시그모이드 함수:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/nn/loss.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e .\u003cspan class=\"hljs-property\"\u003emodule\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e\n \n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMSELoss\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n      pass\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, predictions, labels):\n        assert labels.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e == predictions.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e, \\\n            \u003cspan class=\"hljs-string\"\u003e\"Labels and predictions shape does not match: {} and {}\"\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(labels.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e, predictions.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)\n        \n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e ((predictions - labels) ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e() / predictions.\u003cspan class=\"hljs-property\"\u003enumel\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__call__\u003c/span\u003e(self, *inputs):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(*inputs)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/nn/activation.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e .\u003cspan class=\"hljs-property\"\u003emodule\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e math\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSigmoid\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, x):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e / (\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e + (math.\u003cspan class=\"hljs-property\"\u003ee\u003c/span\u003e) ** (-x)) \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로 옵티마이저를 만들어봅시다. 예시로 확률적 경사 하강법(Stochastic Gradient Descent) 알고리즘을 구현하겠습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# norch/optim/optimizer.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e abc \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eABC\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e norch.\u003cspan class=\"hljs-property\"\u003etensor\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOptimizer\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eABC\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    옵티마이저를 위한 추상 클래스\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, parameters):\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eisinstance\u003c/span\u003e(parameters, \u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e):\n            raise \u003cspan class=\"hljs-title class_\"\u003eTypeError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"parameters는 반복 가능한 객체이어야 하지만 {} 타입이 입력되었습니다\"\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003etype\u003c/span\u003e(parameters)))\n        elif \u003cspan class=\"hljs-title function_\"\u003eisinstance\u003c/span\u003e(parameters, dict):\n            parameters = parameters.\u003cspan class=\"hljs-title function_\"\u003evalues\u003c/span\u003e()\n\n        self.\u003cspan class=\"hljs-property\"\u003eparameters\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(parameters)\n\n    def \u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e(self):\n        raise \u003cspan class=\"hljs-title class_\"\u003eNotImplementedError\u003c/span\u003e\n    \n    def \u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e \u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e, name, parameter \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eparameters\u003c/span\u003e:\n            parameter.\u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e()\n\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSGD\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eOptimizer\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, parameters, lr=\u003cspan class=\"hljs-number\"\u003e1e-1\u003c/span\u003e, momentum=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e):\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(parameters)\n        self.\u003cspan class=\"hljs-property\"\u003elr\u003c/span\u003e = lr\n        self.\u003cspan class=\"hljs-property\"\u003emomentum\u003c/span\u003e = momentum\n        self.\u003cspan class=\"hljs-property\"\u003e_cache\u003c/span\u003e = {\u003cspan class=\"hljs-string\"\u003e'velocity'\u003c/span\u003e: [p.\u003cspan class=\"hljs-title function_\"\u003ezeros_like\u003c/span\u003e() \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (_, _, p) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eparameters\u003c/span\u003e]}\n\n    def \u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i, (\u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e, name, _) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eenumerate\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003eparameters\u003c/span\u003e):\n            parameter = \u003cspan class=\"hljs-title function_\"\u003egetattr\u003c/span\u003e(\u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e, name)\n\n            velocity = self.\u003cspan class=\"hljs-property\"\u003e_cache\u003c/span\u003e[\u003cspan class=\"hljs-string\"\u003e'velocity'\u003c/span\u003e][i]\n\n            velocity = self.\u003cspan class=\"hljs-property\"\u003emomentum\u003c/span\u003e * velocity - self.\u003cspan class=\"hljs-property\"\u003elr\u003c/span\u003e * parameter.\u003cspan class=\"hljs-property\"\u003egrad\u003c/span\u003e\n\n            updated_parameter = parameter + velocity\n\n            \u003cspan class=\"hljs-title function_\"\u003esetattr\u003c/span\u003e(\u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e, name, updated_parameter)\n\n            self.\u003cspan class=\"hljs-property\"\u003e_cache\u003c/span\u003e[\u003cspan class=\"hljs-string\"\u003e'velocity'\u003c/span\u003e][i] = velocity\n\n            parameter.\u003cspan class=\"hljs-title function_\"\u003edetach\u003c/span\u003e()\n            velocity.\u003cspan class=\"hljs-title function_\"\u003edetach\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그리고 여기까지입니다! 이제 우리만의 딥러닝 프레임워크를 만들었어요! 🥳\u003c/p\u003e\n\u003cp\u003e이제 학습을 시작해봅시다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e norch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e norch.\u003cspan class=\"hljs-property\"\u003enn\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e nn\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e norch.\u003cspan class=\"hljs-property\"\u003eoptim\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e optim\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e random\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e math\n\nrandom.\u003cspan class=\"hljs-title function_\"\u003eseed\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMyModel\u003c/span\u003e(nn.\u003cspan class=\"hljs-property\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eMyModel\u003c/span\u003e, self).\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003efc1\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003esigmoid\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eSigmoid\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003efc2\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, x):\n        out = self.\u003cspan class=\"hljs-title function_\"\u003efc1\u003c/span\u003e(x)\n        out = self.\u003cspan class=\"hljs-title function_\"\u003esigmoid\u003c/span\u003e(out)\n        out = self.\u003cspan class=\"hljs-title function_\"\u003efc2\u003c/span\u003e(out)\n        \n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e out\n\ndevice = \u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e\nepochs = \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e\n\nmodel = \u003cspan class=\"hljs-title class_\"\u003eMyModel\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(device)\ncriterion = nn.\u003cspan class=\"hljs-title class_\"\u003eMSELoss\u003c/span\u003e()\noptimizer = optim.\u003cspan class=\"hljs-title function_\"\u003eSGD\u003c/span\u003e(model.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(), lr=\u003cspan class=\"hljs-number\"\u003e0.001\u003c/span\u003e)\nloss_list = []\n\nx_values = [\u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e ,  \u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e0.8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1.2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e1.6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e ,  \u003cspan class=\"hljs-number\"\u003e2.4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e2.8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3.2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e3.6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e4.\u003c/span\u003e ,\n        \u003cspan class=\"hljs-number\"\u003e4.4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e4.8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5.2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e5.6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6.\u003c/span\u003e ,  \u003cspan class=\"hljs-number\"\u003e6.4\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e6.8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7.2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e7.6\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e8.\u003c/span\u003e ,  \u003cspan class=\"hljs-number\"\u003e8.4\u003c/span\u003e,\n        \u003cspan class=\"hljs-number\"\u003e8.8\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9.2\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e9.6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10.\u003c/span\u003e , \u003cspan class=\"hljs-number\"\u003e10.4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10.8\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11.2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e11.6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12.\u003c/span\u003e , \u003cspan class=\"hljs-number\"\u003e12.4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e12.8\u003c/span\u003e,\n       \u003cspan class=\"hljs-number\"\u003e13.2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e13.6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14.\u003c/span\u003e , \u003cspan class=\"hljs-number\"\u003e14.4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e14.8\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15.2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15.6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e16.\u003c/span\u003e , \u003cspan class=\"hljs-number\"\u003e16.4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e16.8\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e17.2\u003c/span\u003e,\n       \u003cspan class=\"hljs-number\"\u003e17.6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18.\u003c/span\u003e , \u003cspan class=\"hljs-number\"\u003e18.4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e18.8\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19.2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e19.6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20.\u003c/span\u003e]\n\ny_true = []\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ex_values\u003c/span\u003e:\n    y_true.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(math.\u003cspan class=\"hljs-title function_\"\u003epow\u003c/span\u003e(math.\u003cspan class=\"hljs-title function_\"\u003esin\u003c/span\u003e(x), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e))\n\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(epochs):\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x, target \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ezip\u003c/span\u003e(x_values, y_true):\n        x = norch.\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e([[x]]).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e\n        target = norch.\u003cspan class=\"hljs-title class_\"\u003eTensor\u003c/span\u003e([[target]]).\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e\n\n        x = x.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(device)\n        target = target.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(device)\n\n        outputs = \u003cspan class=\"hljs-title function_\"\u003emodel\u003c/span\u003e(x)\n        loss = \u003cspan class=\"hljs-title function_\"\u003ecriterion\u003c/span\u003e(outputs, target)\n        \n        optimizer.\u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e()\n        loss.\u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e()\n        optimizer.\u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e()\n\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}'\u003c/span\u003e)\n    loss_list.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(loss[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1.7035\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.7193\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.3068\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.1742\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.1342\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.1232\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.1220\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.1241\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e9\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.1270\u003c/span\u003e\n# \u003cspan class=\"hljs-title class_\"\u003eEpoch\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-title class_\"\u003eLoss\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e0.1297\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e성공적으로 모델이 생성되고 사용자 정의 딥러닝 프레임워크를 사용하여 훈련되었습니다!\u003c/p\u003e\n\u003cp\u003e전체 코드는 여기에서 확인할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e이 게시물에서는 텐서와 같은 기본 개념, 어떻게 모델링되는지, CUDA 및 Autograd와 같은 고급 주제 등을 다루었습니다. 우리는 GPU 지원 및 자동 미분이 가능한 딥 러닝 프레임워크를 성공적으로 만들었습니다. 이 게시물이 여러분이 PyTorch가 어떻게 작동하는지 간략히 이해하는 데 도움이 되었으면 좋겠습니다.\u003c/p\u003e\n\u003cp\u003e앞으로의 게시물에서는 분산 훈련(다중 노드/다중 GPU) 및 메모리 관리와 같은 고급 주제를 다루려고 할 것입니다. 의견이 있거나 다음에 어떤 내용을 다루길 원하시는지 댓글로 알려주세요! 읽어 주셔서 정말 감사합니다! 😊\u003c/p\u003e\n\u003cp\u003e또한 최신 기사를 받아보기 위해 여기와 제 LinkedIn 프로필에서 팔로우해 주세요!\u003c/p\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com\" rel=\"nofollow\" target=\"_blank\"\u003ePyNorch\u003c/a\u003e - 이 프로젝트의 GitHub 저장소\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.example.com/tutorial-cuda\" rel=\"nofollow\" target=\"_blank\"\u003eCUDA 튜토리얼\u003c/a\u003e - CUDA 작동 방식에 대한 간단한 소개\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pytorch.org/docs\" rel=\"nofollow\" target=\"_blank\"\u003ePyTorch\u003c/a\u003e - PyTorch 문서\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eMartinLwx's 블로그 - 스트라이드에 관한 튜토리얼.\u003c/h1\u003e\n\u003ch1\u003e스트라이드 튜토리얼 - 스트라이드에 관한 또 다른 튜토리얼.\u003c/h1\u003e\n\u003ch1\u003ePyTorch 내부 구조 - PyTorch 구조에 대한 가이드.\u003c/h1\u003e\n\u003ch1\u003e네츠 - NumPy를 사용한 PyTorch 재구현.\u003c/h1\u003e\n\u003cp\u003eMarkdown으로 표 태그를 변경하십시오.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-RecreatingPyTorchfromScratchwithGPUSupportandAutomaticDifferentiation"},"buildId":"t9N7vwmpvBMQnO2PSctoH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>