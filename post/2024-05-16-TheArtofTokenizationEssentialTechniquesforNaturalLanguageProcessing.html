<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²• | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²• | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²• | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing" data-gatsby-head="true"/><meta name="twitter:title" content="í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²• | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 04:22" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-985df180e46efe53.js" defer=""></script><script src="/_next/static/837W-BjvPVBgft6aM4api/_buildManifest.js" defer=""></script><script src="/_next/static/837W-BjvPVBgft6aM4api/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²•</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²•" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/assets/profile.jpg"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>í† í°í™”ê°€ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? í˜„ì¬ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Models)ì€ ì–´ë–¤ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ìˆ˜í–‰í• ê¹Œìš”? í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•´ìš”!</p>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" alt="ì´ë¯¸ì§€"/></p>
<p>ìì—°ì–´ ì²˜ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°œë°œ ì´í›„ ë§ì€ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•œ í›„ NLP ì‘ì—…ê³¼ ê´€ë ¨ëœ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” í† í°í™”ì…ë‹ˆë‹¤. ì²˜ìŒì˜ í™”ì´íŠ¸ìŠ¤í˜ì´ìŠ¤(whitespace) ë° êµ¬ë‘ì (tokenizer)ì„ êµ¬ì¶•í•œ ì´í›„ í˜„ì¬ì˜ ë¬¸ë§¥ì (contextual) ë° êµ¬ì¡°ì (tokenizers) í† í¬ë‚˜ì´ì €ë“¤ê¹Œì§€ ë§ì€ ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìš”ì¦˜ì—ëŠ” BERT ë° ê·¸ ë³€í˜•, ChatGPT, Claudeì™€ ê°™ì€ ìƒì„± ëª¨ë¸ì´ íŠ¹íˆ NLP ë¶„ì•¼ì—ì„œ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ì—ì„œëŠ” í…ìŠ¤íŠ¸ í† í°í™” ê³¼ì •ì´ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ë° ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.</p>
<h1>í† í°í™” ê¸°ìˆ  ë°œì „ì˜ ì—¬ì •</h1>
<p>í† í°í™”ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ê¸°ë³¸ í† í°í™” ê¸°ìˆ ì—ëŠ” ê³µë°±, ë‹¨ì–´ ë° ë¬¸ì¥ í† í°í™”ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´íœ˜ í¬ê¸° ë° ì •ë³´ ì†ì‹¤, ë¬¸ë§¥ ë¶€ì¡± ë“±ê³¼ ê°™ì€ ì¼ë¶€ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ n-gram, BPE (Byte Pair Encoding), SentencePiece í† í°í™”ì™€ ê°™ì€ ê¸°ìˆ ì´ ì†Œê°œë˜ì—ˆìœ¼ë©° ê±°ì˜ ëª¨ë“  í•œê³„ë¥¼ í•´ì†Œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ í˜„ì¬ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ë©° ì„ë² ë”©ì—ì„œ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì´í•´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì´ì œ ê° ê¸°ìˆ ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤!</p>
<h2>ê¸°ë³¸ í† í°í™” ê¸°ìˆ </h2>
<p>ì´ëŸ¬í•œ ê¸°ìˆ ì€ ë°ì´í„°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë° ì£¼ë¡œ ì´ˆì ì„ ë§ì¶”ë©° ì–´ë–¤ ì²­í¬ê°€ ë‹¤ë¥¸ ì²­í¬ì™€ ì–´ë–»ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ í¬ê²Œ ì‹ ê²½ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ê¸°ìˆ ì´ ì‘ë™í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>
<ol>
<li>ê³µë°± í† í°í™” - íƒ­, ê³µë°±, ìƒˆ ì¤„ ë“±ì˜ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë“  ë‹¨ì–´ê°€ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.</li>
</ol>
<p>:warning: í•œê³„</p>
<ul>
<li>ë¬¸ë§¥ì  ì˜ë¯¸ ì†ì‹¤: ë‹¨ì–´ë¥¼ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ì¢…ì¢… ë¬¸ì¥ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤.</li>
<li>ì–´íœ˜ í­ë°œ: ê° ê³ ìœ í•œ ë‹¨ì–´ê°€ í† í°ì´ ë˜ë¯€ë¡œ, ì–´ë– í•œ ì–¸ì–´ë„ ìˆ˜ì‹­ì–µ ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¢…ì¢… ë§¤ìš° í° í›ˆë ¨ ì–´íœ˜ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.</li>
<li>ì¡ìŒì´ ë§ì€ ë°ì´í„° ì²˜ë¦¬ ì–´ë ¤ì›€: ì´ëª¨ì§€, ê³¼ë„í•œ ë¬¸ì¥ ë¶€í˜¸ ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ì—¬ í† í°í™”ê°€ ë¶€ì •í™•í•´ì§‘ë‹ˆë‹¤.</li>
</ul>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png" alt="word tokenization"/></p>
<ol start="2">
<li>ë‹¨ì–´ í† í°í™” - ê³µë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ëœ ë¬¸ì¥ í† í°í™”ì—ì„œ ë¬¸ì¥ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ë‹¨ì–´ê°€ ë”°ë¡œ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
âš ï¸ í•œê³„</li>
</ol>
<ul>
<li>ë‹¨ì–´ ì‚¬ì´ì˜ ìƒí™©ì  ì˜ë¯¸ ì†ì‹¤</li>
<li>ì–´íœ˜í­ë°œ</li>
</ul>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png" alt="sentence tokenization"/></p>
<ol start="3">
<li>ë¬¸ì¥ í† í°í™” - ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ ë“±ì˜ êµ¬ë‘ì  ë° ë‹¤ë¥¸ ì–¸ì–´ë³„ ê·œì¹™ì„ ì´í•´í•˜ì—¬ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
âš ï¸ í•œê³„ - ê¸°ê³„ ë²ˆì—­ ë“±ì˜ ì‘ì—…ì— ìœ ìš©í•˜ì§€ë§Œ ì—¬ì „íˆ ë‹¨ì–´ ìˆ˜ì¤€ í† í°í™”ì— ì˜ì¡´í•˜ë©° ì´ë¡œ ì¸í•œ í•œê³„ë¥¼ ë¬¼ë ¤ë°›ìŠµë‹ˆë‹¤.</li>
</ol>
<p>ğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤:</p>
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">NLTK</span> ì‚¬ìš©
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">from</span> nltk.<span class="hljs-property">tokenize</span> <span class="hljs-keyword">import</span> word_tokenize, sent_tokenize

nltk.<span class="hljs-title function_">download</span>(<span class="hljs-string">&#x27;punkt&#x27;</span>)

# ì…ë ¥ ë¬¸ì¥
text = <span class="hljs-string">&quot;When I left the place, I didn&#x27;t take the left turn.&quot;</span>

# ê³µë°± ê¸°ì¤€ í† í°í™”
whitespace_tokens = text.<span class="hljs-title function_">split</span>()

# ë‹¨ì–´ í† í°í™”
word_tokens = <span class="hljs-title function_">word_tokenize</span>(text)

# ë¬¸ì¥ í† í°í™”
sentence_tokens = <span class="hljs-title function_">sent_tokenize</span>(text)

<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;Whitespace Tokenization:&quot;</span>, whitespace_tokens)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;Word Tokenization:&quot;</span>, word_tokens)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;Sentence Tokenization:&quot;</span>, sentence_tokens)
</code></pre>
<p>ë˜í•œ SpaCy, Scikit-learn, Stanza ë“±ì˜ ë‹¤ë¥¸ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì´ëŸ¬í•œ í† í°í™” ê¸°ìˆ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<h1>ê³ ê¸‰ í† í°í™” ê¸°ìˆ </h1>
<p>ê³ ê¸‰ ê¸°ìˆ ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ í•œê³„ë¥¼ ì™„í™”í•˜ë ¤ê³  ì‹œë„í•˜ê³ , ë‹¨ì–´ ê°„ ìƒí˜¸ ê´€ê³„ ë° ë¬¸ì¥ ë‚´ ë§¥ë½ì— ì´ˆì ì„ ë§ì¶”ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤:</p>
<p>ï¸1. N-ê·¸ë¨-
â–ª í…ìŠ¤íŠ¸ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì§€ì •ëœ N ê¸¸ì´ì˜ í† í°ì„ ë§Œë“­ë‹ˆë‹¤.
â–ª ì´ ë°©ë²•ì€ ì„œë¡œ ê°€ê¹ê²Œ ë°œìƒí•˜ëŠ” ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì¡ì•„ëƒ…ë‹ˆë‹¤.
ğŸ’¡ì´ ê¸°ìˆ ì€ ìŒì„± ì¸ì‹, í…ìŠ¤íŠ¸ ì™„ì„± ë“±ê³¼ ê°™ì€ ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ ê¸°ë³¸ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.
âš ï¸ í•œê³„ â€” ì—°ì†ëœ ë‹¨ì–´ì™€ì˜ ê´€ê³„ë§Œ íŒŒì•…í•©ë‹ˆë‹¤. ë” ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„  ë‹¤ì‹œ ë§¥ë½ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤.</p>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png" alt="image"/></p>
<ol start="2">
<li>ë°”ì´íŠ¸ ìŒ ë¶€í˜¸í™”-
â–ª ì—¬ê¸°ì„œëŠ” í•™ìŠµ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ë¬¸ì/ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ì–´íœ˜ì§‘ì„ ë§Œë“­ë‹ˆë‹¤.
â–ª ì—°ì† ë°œìƒ ë¬¸ìì˜ ë¹ˆë„ìˆ˜ì— ê¸°ë°˜í•˜ì—¬ ì–´íœ˜ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
â–ª ì¤‘ì§€ ì¡°ê±´(ë˜ëŠ” ìµœëŒ€ ë³‘í•© ìˆ˜)ì´ ì¶©ì¡±ë˜ë©´ ì…ë ¥ í…ìŠ¤íŠ¸(í…ŒìŠ¤íŠ¸ ì…ë ¥)ëŠ” ì´ ìƒì„±ëœ ì–´íœ˜ì§‘ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ë©ë‹ˆë‹¤.
â–ª ì–´íœ˜ ì™¸ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ì–´íœ˜ í¬ê¸°ê°€ ë¬´ë„ˆì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
ğŸ’¡RoBERTa, GPT2ëŠ” ì´ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
âš ï¸ í•œê³„-
â–ª í›ˆë ¨ ë‹¨ê³„ì—ì„œ ê°œë°œëœ ê³ ì •ëœ ì–´íœ˜ í¬ê¸°ë¡œ ì¸í•´ ë•Œë¡œëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ì— ë¬¸ì œê°€ ìƒê¸°ê¸°ë„ í•©ë‹ˆë‹¤.
â–ª ì´ ì•Œê³ ë¦¬ì¦˜ì€ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ì„ ëª¨ì•„ ì‚¬ìš©í•˜ë©°, ë¬¸ì¥ì˜ í˜•íƒœí•™ì  ë° ë¬¸ë§¥ì  ë³µì¡ì„±ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.</li>
</ol>
<img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png"/>
<ol start="3">
<li>SentencePiece-</li>
</ol>
<ul>
<li>SentencePieceëŠ” Unigramê³¼ Dynamic Programming ë˜ëŠ” BPE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì„œë¸Œì›Œë“œ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.</li>
<li>ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ Unicode ë¬¸ìë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ˆê¸° ë‹¨ì–´ í† í°í™”ê°€ í•„ìš”ì—†ìŠµë‹ˆë‹¤.</li>
<li>ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li>ì²˜ìŒì— Unicode ë¬¸ì ìˆ˜ì¤€ í† í°ì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ì˜ í† í°í™” ë° ë””í† í°í™”ë¥¼ ëª¨ë‘ ë„ì™€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
ğŸ’¡BERT, XLNet, T5 ë“± ë§ì€ HuggingFace íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì˜ ìœ ì§€ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
âš ï¸ ì œí•œ ì‚¬í•­-</li>
<li>ì–¸ì–´ì— ë…ë¦½ì ì´ì§€ë§Œ ë‹¤ì–‘í•œ ì–¸ì–´ì— ëŒ€í•´ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li>ë¬¸ë‹¨ì´ë‚˜ ì„¹ì…˜ê³¼ ê°™ì€ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì„¸ë¶€ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  í•˜ìœ„ ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ì „íˆ ì·¨ê¸‰í•©ë‹ˆë‹¤.</li>
</ul>
<p>ğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ìˆ ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ:</p>
<pre><code class="hljs language-js"># í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°
<span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm
<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> <span class="hljs-title class_">ByteLevelBPETokenizer</span>
model_path = <span class="hljs-string">&quot;ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ&quot;</span>
train_text = <span class="hljs-string">&quot;í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ&quot;</span>

###############################
# <span class="hljs-variable constant_">BPE</span> êµ¬í˜„
###############################

BPE_tokenizer = <span class="hljs-title class_">ByteLevelBPETokenizer</span>()

# utf-<span class="hljs-number">8</span> ì¸ì½”ë”©ëœ ì½”í¼ìŠ¤ë¡œ í† í¬ë‚˜ì´ì € í›ˆë ¨ì‹œí‚¤ê¸°
BPE_tokenizer.<span class="hljs-title function_">train</span>(files=[<span class="hljs-string">&#x27;í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ&#x27;</span>], vocab_size=<span class="hljs-number">1000</span>, min_frequency=<span class="hljs-number">2</span>)

# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ì €ì¥
model_path = <span class="hljs-string">&#x27;ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ&#x27;</span>
BPE_tokenizer.<span class="hljs-title function_">save_model</span>(model_path)

# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
BPE_tokenizer = <span class="hljs-title class_">ByteLevelBPETokenizer</span>.<span class="hljs-title function_">from_file</span>(f<span class="hljs-string">&quot;{model_path}/vocab.json&quot;</span>, f<span class="hljs-string">&quot;{model_path}/merges.txt&quot;</span>)

# í…ìŠ¤íŠ¸ í† í°í™”
text = <span class="hljs-string">&quot;I would love to see a lion!&quot;</span>
BPE_encoded_tokens = BPE_tokenizer.<span class="hljs-title function_">encode</span>(text)

<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;ì›ë³¸ í…ìŠ¤íŠ¸:&quot;</span>, text)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;ì¸ì½”ë”©ëœ í† í°:&quot;</span>, BPE_encoded_tokens.<span class="hljs-property">tokens</span>)


###############################
# <span class="hljs-title class_">SentencePiece</span> êµ¬í˜„
###############################

spm.<span class="hljs-property">SentencePieceTrainer</span>.<span class="hljs-title function_">train</span>(input=train_text, model_prefix=model_path, vocab_size=<span class="hljs-number">1000</span>, num_threads=<span class="hljs-number">4</span>)

# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
sp_model = model_path + <span class="hljs-string">&quot;.model&quot;</span>
sp = spm.<span class="hljs-title class_">SentencePieceProcessor</span>(model_file=sp_model)

text = <span class="hljs-string">&quot;I would love to see a lion when we reach the zoo!&quot;</span>

# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° ë°˜í™˜
tokens_subword = sp.<span class="hljs-title function_">encode_as_pieces</span>(text)
# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° <span class="hljs-variable constant_">ID</span> ë°˜í™˜
tokens_ids = sp.<span class="hljs-title function_">encode_as_ids</span>(text)
# ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í°í™” ë° ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í° <span class="hljs-variable constant_">ID</span> ë°˜í™˜
tokens_byte = sp.<span class="hljs-title function_">encode</span>(text)

# í† í°ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
decoded_text = sp.<span class="hljs-title function_">decode_pieces</span>(tokens_subword)

<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;ì›ë³¸ í…ìŠ¤íŠ¸:&quot;</span>, text)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;í† í°í™”ëœ í…ìŠ¤íŠ¸:&quot;</span>, tokens_subword)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸:&quot;</span>, decoded_text)
</code></pre>
<p>ì´ëŸ¬í•œ ê³ ê¸‰ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œí•œ í† í°ë“¤ì€ BERT, GPT ë“±ê³¼ ê°™ì€ ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì‘ì—…ì— í•„ìš”í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í°ë“¤ì€ ëª¨ë¸ë¡œ ì „ì†¡ë˜ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥ì  ë° êµ¬ì¡°ì  ì˜ë¯¸ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"í† í°í™”ì˜ ê¸°ìˆ  ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ê¸°ë²•","description":"","date":"2024-05-16 04:22","slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing","content":"\n\ní† í°í™”ê°€ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? í˜„ì¬ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Models)ì€ ì–´ë–¤ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ìˆ˜í–‰í• ê¹Œìš”? í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•´ìš”!\n\n![ì´ë¯¸ì§€](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png)\n\nìì—°ì–´ ì²˜ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°œë°œ ì´í›„ ë§ì€ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•œ í›„ NLP ì‘ì—…ê³¼ ê´€ë ¨ëœ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” í† í°í™”ì…ë‹ˆë‹¤. ì²˜ìŒì˜ í™”ì´íŠ¸ìŠ¤í˜ì´ìŠ¤(whitespace) ë° êµ¬ë‘ì (tokenizer)ì„ êµ¬ì¶•í•œ ì´í›„ í˜„ì¬ì˜ ë¬¸ë§¥ì (contextual) ë° êµ¬ì¡°ì (tokenizers) í† í¬ë‚˜ì´ì €ë“¤ê¹Œì§€ ë§ì€ ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìš”ì¦˜ì—ëŠ” BERT ë° ê·¸ ë³€í˜•, ChatGPT, Claudeì™€ ê°™ì€ ìƒì„± ëª¨ë¸ì´ íŠ¹íˆ NLP ë¶„ì•¼ì—ì„œ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ì—ì„œëŠ” í…ìŠ¤íŠ¸ í† í°í™” ê³¼ì •ì´ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ë° ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.\n\n# í† í°í™” ê¸°ìˆ  ë°œì „ì˜ ì—¬ì •\n\n\n\ní† í°í™”ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ê¸°ë³¸ í† í°í™” ê¸°ìˆ ì—ëŠ” ê³µë°±, ë‹¨ì–´ ë° ë¬¸ì¥ í† í°í™”ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´íœ˜ í¬ê¸° ë° ì •ë³´ ì†ì‹¤, ë¬¸ë§¥ ë¶€ì¡± ë“±ê³¼ ê°™ì€ ì¼ë¶€ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ n-gram, BPE (Byte Pair Encoding), SentencePiece í† í°í™”ì™€ ê°™ì€ ê¸°ìˆ ì´ ì†Œê°œë˜ì—ˆìœ¼ë©° ê±°ì˜ ëª¨ë“  í•œê³„ë¥¼ í•´ì†Œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ í˜„ì¬ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ë©° ì„ë² ë”©ì—ì„œ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì´í•´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì´ì œ ê° ê¸°ìˆ ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤!\n\n## ê¸°ë³¸ í† í°í™” ê¸°ìˆ \n\nì´ëŸ¬í•œ ê¸°ìˆ ì€ ë°ì´í„°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë° ì£¼ë¡œ ì´ˆì ì„ ë§ì¶”ë©° ì–´ë–¤ ì²­í¬ê°€ ë‹¤ë¥¸ ì²­í¬ì™€ ì–´ë–»ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ í¬ê²Œ ì‹ ê²½ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ê¸°ìˆ ì´ ì‘ë™í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n1. ê³µë°± í† í°í™” - íƒ­, ê³µë°±, ìƒˆ ì¤„ ë“±ì˜ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë“  ë‹¨ì–´ê°€ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n   \n:warning: í•œê³„\n- ë¬¸ë§¥ì  ì˜ë¯¸ ì†ì‹¤: ë‹¨ì–´ë¥¼ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ì¢…ì¢… ë¬¸ì¥ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤.\n- ì–´íœ˜ í­ë°œ: ê° ê³ ìœ í•œ ë‹¨ì–´ê°€ í† í°ì´ ë˜ë¯€ë¡œ, ì–´ë– í•œ ì–¸ì–´ë„ ìˆ˜ì‹­ì–µ ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¢…ì¢… ë§¤ìš° í° í›ˆë ¨ ì–´íœ˜ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.\n- ì¡ìŒì´ ë§ì€ ë°ì´í„° ì²˜ë¦¬ ì–´ë ¤ì›€: ì´ëª¨ì§€, ê³¼ë„í•œ ë¬¸ì¥ ë¶€í˜¸ ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ì—¬ í† í°í™”ê°€ ë¶€ì •í™•í•´ì§‘ë‹ˆë‹¤.\n\n\n\n\n![word tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png)\n\n2. ë‹¨ì–´ í† í°í™” - ê³µë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ëœ ë¬¸ì¥ í† í°í™”ì—ì„œ ë¬¸ì¥ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ë‹¨ì–´ê°€ ë”°ë¡œ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„\n- ë‹¨ì–´ ì‚¬ì´ì˜ ìƒí™©ì  ì˜ë¯¸ ì†ì‹¤\n- ì–´íœ˜í­ë°œ\n\n![sentence tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png)\n\n3. ë¬¸ì¥ í† í°í™” - ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ ë“±ì˜ êµ¬ë‘ì  ë° ë‹¤ë¥¸ ì–¸ì–´ë³„ ê·œì¹™ì„ ì´í•´í•˜ì—¬ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„ - ê¸°ê³„ ë²ˆì—­ ë“±ì˜ ì‘ì—…ì— ìœ ìš©í•˜ì§€ë§Œ ì—¬ì „íˆ ë‹¨ì–´ ìˆ˜ì¤€ í† í°í™”ì— ì˜ì¡´í•˜ë©° ì´ë¡œ ì¸í•œ í•œê³„ë¥¼ ë¬¼ë ¤ë°›ìŠµë‹ˆë‹¤.\n\n\n\n\nğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤:\n\n```js\n# NLTK ì‚¬ìš©\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nnltk.download('punkt')\n\n# ì…ë ¥ ë¬¸ì¥\ntext = \"When I left the place, I didn't take the left turn.\"\n\n# ê³µë°± ê¸°ì¤€ í† í°í™”\nwhitespace_tokens = text.split()\n\n# ë‹¨ì–´ í† í°í™”\nword_tokens = word_tokenize(text)\n\n# ë¬¸ì¥ í† í°í™”\nsentence_tokens = sent_tokenize(text)\n\nprint(\"Whitespace Tokenization:\", whitespace_tokens)\nprint(\"Word Tokenization:\", word_tokens)\nprint(\"Sentence Tokenization:\", sentence_tokens)\n```\n\në˜í•œ SpaCy, Scikit-learn, Stanza ë“±ì˜ ë‹¤ë¥¸ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì´ëŸ¬í•œ í† í°í™” ê¸°ìˆ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n# ê³ ê¸‰ í† í°í™” ê¸°ìˆ \n\n\n\nê³ ê¸‰ ê¸°ìˆ ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ í•œê³„ë¥¼ ì™„í™”í•˜ë ¤ê³  ì‹œë„í•˜ê³ , ë‹¨ì–´ ê°„ ìƒí˜¸ ê´€ê³„ ë° ë¬¸ì¥ ë‚´ ë§¥ë½ì— ì´ˆì ì„ ë§ì¶”ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤:\n\nï¸1. N-ê·¸ë¨-\nâ–ª í…ìŠ¤íŠ¸ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì§€ì •ëœ N ê¸¸ì´ì˜ í† í°ì„ ë§Œë“­ë‹ˆë‹¤.\nâ–ª ì´ ë°©ë²•ì€ ì„œë¡œ ê°€ê¹ê²Œ ë°œìƒí•˜ëŠ” ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì¡ì•„ëƒ…ë‹ˆë‹¤.\nğŸ’¡ì´ ê¸°ìˆ ì€ ìŒì„± ì¸ì‹, í…ìŠ¤íŠ¸ ì™„ì„± ë“±ê³¼ ê°™ì€ ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ ê¸°ë³¸ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„ â€” ì—°ì†ëœ ë‹¨ì–´ì™€ì˜ ê´€ê³„ë§Œ íŒŒì•…í•©ë‹ˆë‹¤. ë” ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„  ë‹¤ì‹œ ë§¥ë½ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\n\n![image](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png)\n\n2. ë°”ì´íŠ¸ ìŒ ë¶€í˜¸í™”-\nâ–ª ì—¬ê¸°ì„œëŠ” í•™ìŠµ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ë¬¸ì/ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ì–´íœ˜ì§‘ì„ ë§Œë“­ë‹ˆë‹¤.\nâ–ª ì—°ì† ë°œìƒ ë¬¸ìì˜ ë¹ˆë„ìˆ˜ì— ê¸°ë°˜í•˜ì—¬ ì–´íœ˜ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\nâ–ª ì¤‘ì§€ ì¡°ê±´(ë˜ëŠ” ìµœëŒ€ ë³‘í•© ìˆ˜)ì´ ì¶©ì¡±ë˜ë©´ ì…ë ¥ í…ìŠ¤íŠ¸(í…ŒìŠ¤íŠ¸ ì…ë ¥)ëŠ” ì´ ìƒì„±ëœ ì–´íœ˜ì§‘ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ë©ë‹ˆë‹¤.\nâ–ª ì–´íœ˜ ì™¸ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ì–´íœ˜ í¬ê¸°ê°€ ë¬´ë„ˆì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\nğŸ’¡RoBERTa, GPT2ëŠ” ì´ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\nâš ï¸ í•œê³„-\nâ–ª í›ˆë ¨ ë‹¨ê³„ì—ì„œ ê°œë°œëœ ê³ ì •ëœ ì–´íœ˜ í¬ê¸°ë¡œ ì¸í•´ ë•Œë¡œëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ì— ë¬¸ì œê°€ ìƒê¸°ê¸°ë„ í•©ë‹ˆë‹¤.\nâ–ª ì´ ì•Œê³ ë¦¬ì¦˜ì€ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ì„ ëª¨ì•„ ì‚¬ìš©í•˜ë©°, ë¬¸ì¥ì˜ í˜•íƒœí•™ì  ë° ë¬¸ë§¥ì  ë³µì¡ì„±ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\" /\u003e\n\n3. SentencePiece-  \n- SentencePieceëŠ” Unigramê³¼ Dynamic Programming ë˜ëŠ” BPE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì„œë¸Œì›Œë“œ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n- ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ Unicode ë¬¸ìë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ˆê¸° ë‹¨ì–´ í† í°í™”ê°€ í•„ìš”ì—†ìŠµë‹ˆë‹¤.\n- ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- ì²˜ìŒì— Unicode ë¬¸ì ìˆ˜ì¤€ í† í°ì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ì˜ í† í°í™” ë° ë””í† í°í™”ë¥¼ ëª¨ë‘ ë„ì™€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\nğŸ’¡BERT, XLNet, T5 ë“± ë§ì€ HuggingFace íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì˜ ìœ ì§€ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\nâš ï¸ ì œí•œ ì‚¬í•­-  \n- ì–¸ì–´ì— ë…ë¦½ì ì´ì§€ë§Œ ë‹¤ì–‘í•œ ì–¸ì–´ì— ëŒ€í•´ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- ë¬¸ë‹¨ì´ë‚˜ ì„¹ì…˜ê³¼ ê°™ì€ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì„¸ë¶€ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  í•˜ìœ„ ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ì „íˆ ì·¨ê¸‰í•©ë‹ˆë‹¤.\n\nğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ìˆ ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ:\n\n```js\n# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°\nimport sentencepiece as spm\nfrom tokenizers import ByteLevelBPETokenizer\nmodel_path = \"ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ\"\ntrain_text = \"í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ\"\n\n###############################\n# BPE êµ¬í˜„\n###############################\n\nBPE_tokenizer = ByteLevelBPETokenizer()\n\n# utf-8 ì¸ì½”ë”©ëœ ì½”í¼ìŠ¤ë¡œ í† í¬ë‚˜ì´ì € í›ˆë ¨ì‹œí‚¤ê¸°\nBPE_tokenizer.train(files=['í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ'], vocab_size=1000, min_frequency=2)\n\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ì €ì¥\nmodel_path = 'ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ'\nBPE_tokenizer.save_model(model_path)\n\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\nBPE_tokenizer = ByteLevelBPETokenizer.from_file(f\"{model_path}/vocab.json\", f\"{model_path}/merges.txt\")\n\n# í…ìŠ¤íŠ¸ í† í°í™”\ntext = \"I would love to see a lion!\"\nBPE_encoded_tokens = BPE_tokenizer.encode(text)\n\nprint(\"ì›ë³¸ í…ìŠ¤íŠ¸:\", text)\nprint(\"ì¸ì½”ë”©ëœ í† í°:\", BPE_encoded_tokens.tokens)\n\n\n###############################\n# SentencePiece êµ¬í˜„\n###############################\n\nspm.SentencePieceTrainer.train(input=train_text, model_prefix=model_path, vocab_size=1000, num_threads=4)\n\n# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\nsp_model = model_path + \".model\"\nsp = spm.SentencePieceProcessor(model_file=sp_model)\n\ntext = \"I would love to see a lion when we reach the zoo!\"\n\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° ë°˜í™˜\ntokens_subword = sp.encode_as_pieces(text)\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° ID ë°˜í™˜\ntokens_ids = sp.encode_as_ids(text)\n# ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í°í™” ë° ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í° ID ë°˜í™˜\ntokens_byte = sp.encode(text)\n\n# í† í°ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\ndecoded_text = sp.decode_pieces(tokens_subword)\n\nprint(\"ì›ë³¸ í…ìŠ¤íŠ¸:\", text)\nprint(\"í† í°í™”ëœ í…ìŠ¤íŠ¸:\", tokens_subword)\nprint(\"ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸:\", decoded_text)\n```\n\n\n\nì´ëŸ¬í•œ ê³ ê¸‰ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œí•œ í† í°ë“¤ì€ BERT, GPT ë“±ê³¼ ê°™ì€ ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì‘ì—…ì— í•„ìš”í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í°ë“¤ì€ ëª¨ë¸ë¡œ ì „ì†¡ë˜ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥ì  ë° êµ¬ì¡°ì  ì˜ë¯¸ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.","ogImage":{"url":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png"},"coverImage":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png","tag":["Tech"],"readingTime":6},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h1: \"h1\",\n    h2: \"h2\",\n    ol: \"ol\",\n    li: \"li\",\n    ul: \"ul\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"í† í°í™”ê°€ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? í˜„ì¬ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Models)ì€ ì–´ë–¤ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ìˆ˜í–‰í• ê¹Œìš”? í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•´ìš”!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png\",\n        alt: \"ì´ë¯¸ì§€\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ìì—°ì–´ ì²˜ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ê°œë°œ ì´í›„ ë§ì€ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•œ í›„ NLP ì‘ì—…ê³¼ ê´€ë ¨ëœ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” í† í°í™”ì…ë‹ˆë‹¤. ì²˜ìŒì˜ í™”ì´íŠ¸ìŠ¤í˜ì´ìŠ¤(whitespace) ë° êµ¬ë‘ì (tokenizer)ì„ êµ¬ì¶•í•œ ì´í›„ í˜„ì¬ì˜ ë¬¸ë§¥ì (contextual) ë° êµ¬ì¡°ì (tokenizers) í† í¬ë‚˜ì´ì €ë“¤ê¹Œì§€ ë§ì€ ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìš”ì¦˜ì—ëŠ” BERT ë° ê·¸ ë³€í˜•, ChatGPT, Claudeì™€ ê°™ì€ ìƒì„± ëª¨ë¸ì´ íŠ¹íˆ NLP ë¶„ì•¼ì—ì„œ í™”ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ì—ì„œëŠ” í…ìŠ¤íŠ¸ í† í°í™” ê³¼ì •ì´ ì–´ë–»ê²Œ ë°œì „í•´ ì™”ëŠ”ì§€ ë° ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"í† í°í™” ê¸°ìˆ  ë°œì „ì˜ ì—¬ì •\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"í† í°í™”ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ê¸°ë³¸ í† í°í™” ê¸°ìˆ ì—ëŠ” ê³µë°±, ë‹¨ì–´ ë° ë¬¸ì¥ í† í°í™”ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´íœ˜ í¬ê¸° ë° ì •ë³´ ì†ì‹¤, ë¬¸ë§¥ ë¶€ì¡± ë“±ê³¼ ê°™ì€ ì¼ë¶€ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ n-gram, BPE (Byte Pair Encoding), SentencePiece í† í°í™”ì™€ ê°™ì€ ê¸°ìˆ ì´ ì†Œê°œë˜ì—ˆìœ¼ë©° ê±°ì˜ ëª¨ë“  í•œê³„ë¥¼ í•´ì†Œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì€ í˜„ì¬ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ë©° ì„ë² ë”©ì—ì„œ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì´í•´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì´ì œ ê° ê¸°ìˆ ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤!\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"ê¸°ë³¸ í† í°í™” ê¸°ìˆ \"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ì´ëŸ¬í•œ ê¸°ìˆ ì€ ë°ì´í„°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë° ì£¼ë¡œ ì´ˆì ì„ ë§ì¶”ë©° ì–´ë–¤ ì²­í¬ê°€ ë‹¤ë¥¸ ì²­í¬ì™€ ì–´ë–»ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ í¬ê²Œ ì‹ ê²½ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ê¸°ìˆ ì´ ì‘ë™í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ê³µë°± í† í°í™” - íƒ­, ê³µë°±, ìƒˆ ì¤„ ë“±ì˜ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë“  ë‹¨ì–´ê°€ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \":warning: í•œê³„\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë¬¸ë§¥ì  ì˜ë¯¸ ì†ì‹¤: ë‹¨ì–´ë¥¼ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ì¢…ì¢… ë¬¸ì¥ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ê°„ê³¼í•©ë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì–´íœ˜ í­ë°œ: ê° ê³ ìœ í•œ ë‹¨ì–´ê°€ í† í°ì´ ë˜ë¯€ë¡œ, ì–´ë– í•œ ì–¸ì–´ë„ ìˆ˜ì‹­ì–µ ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¢…ì¢… ë§¤ìš° í° í›ˆë ¨ ì–´íœ˜ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì¡ìŒì´ ë§ì€ ë°ì´í„° ì²˜ë¦¬ ì–´ë ¤ì›€: ì´ëª¨ì§€, ê³¼ë„í•œ ë¬¸ì¥ ë¶€í˜¸ ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ì—¬ í† í°í™”ê°€ ë¶€ì •í™•í•´ì§‘ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png\",\n        alt: \"word tokenization\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë‹¨ì–´ í† í°í™” - ê³µë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ëœ ë¬¸ì¥ í† í°í™”ì—ì„œ ë¬¸ì¥ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ë‹¨ì–´ê°€ ë”°ë¡œ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë‹¨ì–´ ì‚¬ì´ì˜ ìƒí™©ì  ì˜ë¯¸ ì†ì‹¤\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì–´íœ˜í­ë°œ\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png\",\n        alt: \"sentence tokenization\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë¬¸ì¥ í† í°í™” - ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ ë“±ì˜ êµ¬ë‘ì  ë° ë‹¤ë¥¸ ì–¸ì–´ë³„ ê·œì¹™ì„ ì´í•´í•˜ì—¬ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„ - ê¸°ê³„ ë²ˆì—­ ë“±ì˜ ì‘ì—…ì— ìœ ìš©í•˜ì§€ë§Œ ì—¬ì „íˆ ë‹¨ì–´ ìˆ˜ì¤€ í† í°í™”ì— ì˜ì¡´í•˜ë©° ì´ë¡œ ì¸í•œ í•œê³„ë¥¼ ë¬¼ë ¤ë°›ìŠµë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NLTK\"\n        }), \" ì‚¬ìš©\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" nltk\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" nltk.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"tokenize\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" word_tokenize, sent_tokenize\\n\\nnltk.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"download\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'punkt'\"\n        }), \")\\n\\n# ì…ë ¥ ë¬¸ì¥\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"When I left the place, I didn't take the left turn.\\\"\"\n        }), \"\\n\\n# ê³µë°± ê¸°ì¤€ í† í°í™”\\nwhitespace_tokens = text.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"split\"\n        }), \"()\\n\\n# ë‹¨ì–´ í† í°í™”\\nword_tokens = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"word_tokenize\"\n        }), \"(text)\\n\\n# ë¬¸ì¥ í† í°í™”\\nsentence_tokens = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"sent_tokenize\"\n        }), \"(text)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Whitespace Tokenization:\\\"\"\n        }), \", whitespace_tokens)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Word Tokenization:\\\"\"\n        }), \", word_tokens)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Sentence Tokenization:\\\"\"\n        }), \", sentence_tokens)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ë˜í•œ SpaCy, Scikit-learn, Stanza ë“±ì˜ ë‹¤ë¥¸ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì´ëŸ¬í•œ í† í°í™” ê¸°ìˆ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"ê³ ê¸‰ í† í°í™” ê¸°ìˆ \"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ê³ ê¸‰ ê¸°ìˆ ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ í•œê³„ë¥¼ ì™„í™”í•˜ë ¤ê³  ì‹œë„í•˜ê³ , ë‹¨ì–´ ê°„ ìƒí˜¸ ê´€ê³„ ë° ë¬¸ì¥ ë‚´ ë§¥ë½ì— ì´ˆì ì„ ë§ì¶”ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ï¸1. N-ê·¸ë¨-\\nâ–ª í…ìŠ¤íŠ¸ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì§€ì •ëœ N ê¸¸ì´ì˜ í† í°ì„ ë§Œë“­ë‹ˆë‹¤.\\nâ–ª ì´ ë°©ë²•ì€ ì„œë¡œ ê°€ê¹ê²Œ ë°œìƒí•˜ëŠ” ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì¡ì•„ëƒ…ë‹ˆë‹¤.\\nğŸ’¡ì´ ê¸°ìˆ ì€ ìŒì„± ì¸ì‹, í…ìŠ¤íŠ¸ ì™„ì„± ë“±ê³¼ ê°™ì€ ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ ê¸°ë³¸ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„ â€” ì—°ì†ëœ ë‹¨ì–´ì™€ì˜ ê´€ê³„ë§Œ íŒŒì•…í•©ë‹ˆë‹¤. ë” ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„  ë‹¤ì‹œ ë§¥ë½ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ë°”ì´íŠ¸ ìŒ ë¶€í˜¸í™”-\\nâ–ª ì—¬ê¸°ì„œëŠ” í•™ìŠµ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ë¬¸ì/ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ì–´íœ˜ì§‘ì„ ë§Œë“­ë‹ˆë‹¤.\\nâ–ª ì—°ì† ë°œìƒ ë¬¸ìì˜ ë¹ˆë„ìˆ˜ì— ê¸°ë°˜í•˜ì—¬ ì–´íœ˜ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\\nâ–ª ì¤‘ì§€ ì¡°ê±´(ë˜ëŠ” ìµœëŒ€ ë³‘í•© ìˆ˜)ì´ ì¶©ì¡±ë˜ë©´ ì…ë ¥ í…ìŠ¤íŠ¸(í…ŒìŠ¤íŠ¸ ì…ë ¥)ëŠ” ì´ ìƒì„±ëœ ì–´íœ˜ì§‘ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ë©ë‹ˆë‹¤.\\nâ–ª ì–´íœ˜ ì™¸ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ì–´íœ˜ í¬ê¸°ê°€ ë¬´ë„ˆì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\\nğŸ’¡RoBERTa, GPT2ëŠ” ì´ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\\nâš ï¸ í•œê³„-\\nâ–ª í›ˆë ¨ ë‹¨ê³„ì—ì„œ ê°œë°œëœ ê³ ì •ëœ ì–´íœ˜ í¬ê¸°ë¡œ ì¸í•´ ë•Œë¡œëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ì— ë¬¸ì œê°€ ìƒê¸°ê¸°ë„ í•©ë‹ˆë‹¤.\\nâ–ª ì´ ì•Œê³ ë¦¬ì¦˜ì€ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ì„ ëª¨ì•„ ì‚¬ìš©í•˜ë©°, ë¬¸ì¥ì˜ í˜•íƒœí•™ì  ë° ë¬¸ë§¥ì  ë³µì¡ì„±ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"SentencePiece-\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"SentencePieceëŠ” Unigramê³¼ Dynamic Programming ë˜ëŠ” BPE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì„œë¸Œì›Œë“œ í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ Unicode ë¬¸ìë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ˆê¸° ë‹¨ì–´ í† í°í™”ê°€ í•„ìš”ì—†ìŠµë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì²˜ìŒì— Unicode ë¬¸ì ìˆ˜ì¤€ í† í°ì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ì˜ í† í°í™” ë° ë””í† í°í™”ë¥¼ ëª¨ë‘ ë„ì™€ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\\nğŸ’¡BERT, XLNet, T5 ë“± ë§ì€ HuggingFace íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ì´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì˜ ìœ ì§€ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\\nâš ï¸ ì œí•œ ì‚¬í•­-\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ì–¸ì–´ì— ë…ë¦½ì ì´ì§€ë§Œ ë‹¤ì–‘í•œ ì–¸ì–´ì— ëŒ€í•´ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ë¬¸ë‹¨ì´ë‚˜ ì„¹ì…˜ê³¼ ê°™ì€ ë¬¸ë§¥ ë° êµ¬ì¡°ì  ì„¸ë¶€ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  í•˜ìœ„ ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ì „íˆ ì·¨ê¸‰í•©ë‹ˆë‹¤.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ğŸ’» ìœ„ì˜ ì„¸ ê°€ì§€ í† í°í™” ê¸°ìˆ ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" sentencepiece \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" spm\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \"\\nmodel_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ\\\"\"\n        }), \"\\ntrain_text = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ\\\"\"\n        }), \"\\n\\n###############################\\n# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BPE\"\n        }), \" êµ¬í˜„\\n###############################\\n\\nBPE_tokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \"()\\n\\n# utf-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8\"\n        }), \" ì¸ì½”ë”©ëœ ì½”í¼ìŠ¤ë¡œ í† í¬ë‚˜ì´ì € í›ˆë ¨ì‹œí‚¤ê¸°\\nBPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(files=[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'í›ˆë ¨ì„ ìœ„í•œ txt íŒŒì¼ ê²½ë¡œ'\"\n        }), \"], vocab_size=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1000\"\n        }), \", min_frequency=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \")\\n\\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ì €ì¥\\nmodel_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ'\"\n        }), \"\\nBPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"save_model\"\n        }), \"(model_path)\\n\\n# í›ˆë ¨ëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\\nBPE_tokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_file\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"{model_path}/vocab.json\\\"\"\n        }), \", f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"{model_path}/merges.txt\\\"\"\n        }), \")\\n\\n# í…ìŠ¤íŠ¸ í† í°í™”\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"I would love to see a lion!\\\"\"\n        }), \"\\nBPE_encoded_tokens = BPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode\"\n        }), \"(text)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ì›ë³¸ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", text)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ì¸ì½”ë”©ëœ í† í°:\\\"\"\n        }), \", BPE_encoded_tokens.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"tokens\"\n        }), \")\\n\\n\\n###############################\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentencePiece\"\n        }), \" êµ¬í˜„\\n###############################\\n\\nspm.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"SentencePieceTrainer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(input=train_text, model_prefix=model_path, vocab_size=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1000\"\n        }), \", num_threads=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4\"\n        }), \")\\n\\n# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\\nsp_model = model_path + \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\".model\\\"\"\n        }), \"\\nsp = spm.\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentencePieceProcessor\"\n        }), \"(model_file=sp_model)\\n\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"I would love to see a lion when we reach the zoo!\\\"\"\n        }), \"\\n\\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° ë°˜í™˜\\ntokens_subword = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode_as_pieces\"\n        }), \"(text)\\n# ì„œë¸Œì›Œë“œ í† í°í™” ë° í† í° \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"ID\"\n        }), \" ë°˜í™˜\\ntokens_ids = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode_as_ids\"\n        }), \"(text)\\n# ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í°í™” ë° ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í° \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"ID\"\n        }), \" ë°˜í™˜\\ntokens_byte = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode\"\n        }), \"(text)\\n\\n# í† í°ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\\ndecoded_text = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"decode_pieces\"\n        }), \"(tokens_subword)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ì›ë³¸ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", text)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"í† í°í™”ëœ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", tokens_subword)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸:\\\"\"\n        }), \", decoded_text)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ì´ëŸ¬í•œ ê³ ê¸‰ í† í°í™” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œí•œ í† í°ë“¤ì€ BERT, GPT ë“±ê³¼ ê°™ì€ ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì‘ì—…ì— í•„ìš”í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í°ë“¤ì€ ëª¨ë¸ë¡œ ì „ì†¡ë˜ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥ì  ë° êµ¬ì¡°ì  ì˜ë¯¸ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing"},"buildId":"837W-BjvPVBgft6aM4api","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>