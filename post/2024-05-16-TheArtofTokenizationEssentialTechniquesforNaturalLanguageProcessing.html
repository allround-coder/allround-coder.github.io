<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing" data-gatsby-head="true"/><meta name="twitter:title" content="토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 04:22" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-985df180e46efe53.js" defer=""></script><script src="/_next/static/837W-BjvPVBgft6aM4api/_buildManifest.js" defer=""></script><script src="/_next/static/837W-BjvPVBgft6aM4api/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">토큰화의 기술 자연어 처리를 위한 필수 기법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="토큰화의 기술 자연어 처리를 위한 필수 기법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/assets/profile.jpg"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!</p>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" alt="이미지"/></p>
<p>자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.</p>
<h1>토큰화 기술 발전의 여정</h1>
<p>토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!</p>
<h2>기본 토큰화 기술</h2>
<p>이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:</p>
<ol>
<li>공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.</li>
</ol>
<p>:warning: 한계</p>
<ul>
<li>문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.</li>
<li>어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.</li>
<li>잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.</li>
</ul>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png" alt="word tokenization"/></p>
<ol start="2">
<li>단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.
⚠️ 한계</li>
</ol>
<ul>
<li>단어 사이의 상황적 의미 손실</li>
<li>어휘폭발</li>
</ul>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png" alt="sentence tokenization"/></p>
<ol start="3">
<li>문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.
⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.</li>
</ol>
<p>💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:</p>
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">NLTK</span> 사용
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">from</span> nltk.<span class="hljs-property">tokenize</span> <span class="hljs-keyword">import</span> word_tokenize, sent_tokenize

nltk.<span class="hljs-title function_">download</span>(<span class="hljs-string">&#x27;punkt&#x27;</span>)

# 입력 문장
text = <span class="hljs-string">&quot;When I left the place, I didn&#x27;t take the left turn.&quot;</span>

# 공백 기준 토큰화
whitespace_tokens = text.<span class="hljs-title function_">split</span>()

# 단어 토큰화
word_tokens = <span class="hljs-title function_">word_tokenize</span>(text)

# 문장 토큰화
sentence_tokens = <span class="hljs-title function_">sent_tokenize</span>(text)

<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;Whitespace Tokenization:&quot;</span>, whitespace_tokens)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;Word Tokenization:&quot;</span>, word_tokens)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;Sentence Tokenization:&quot;</span>, sentence_tokens)
</code></pre>
<p>또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.</p>
<h1>고급 토큰화 기술</h1>
<p>고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:</p>
<p>️1. N-그램-
▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.
▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.
💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.
⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.</p>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png" alt="image"/></p>
<ol start="2">
<li>바이트 쌍 부호화-
▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.
▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.
▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.
▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.
💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.
⚠️ 한계-
▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.
▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.</li>
</ol>
<img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png"/>
<ol start="3">
<li>SentencePiece-</li>
</ol>
<ul>
<li>SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.</li>
<li>입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.</li>
<li>단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.</li>
<li>처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.
💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.
⚠️ 제한 사항-</li>
<li>언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.</li>
<li>문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.</li>
</ul>
<p>💻 위의 세 가지 토큰화 기술을 보여주는 코드:</p>
<pre><code class="hljs language-js"># 필요한 라이브러리 가져오기
<span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm
<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> <span class="hljs-title class_">ByteLevelBPETokenizer</span>
model_path = <span class="hljs-string">&quot;모델을 저장할 경로&quot;</span>
train_text = <span class="hljs-string">&quot;훈련을 위한 txt 파일 경로&quot;</span>

###############################
# <span class="hljs-variable constant_">BPE</span> 구현
###############################

BPE_tokenizer = <span class="hljs-title class_">ByteLevelBPETokenizer</span>()

# utf-<span class="hljs-number">8</span> 인코딩된 코퍼스로 토크나이저 훈련시키기
BPE_tokenizer.<span class="hljs-title function_">train</span>(files=[<span class="hljs-string">&#x27;훈련을 위한 txt 파일 경로&#x27;</span>], vocab_size=<span class="hljs-number">1000</span>, min_frequency=<span class="hljs-number">2</span>)

# 훈련된 토크나이저 저장
model_path = <span class="hljs-string">&#x27;모델을 저장할 경로&#x27;</span>
BPE_tokenizer.<span class="hljs-title function_">save_model</span>(model_path)

# 훈련된 토크나이저 불러오기
BPE_tokenizer = <span class="hljs-title class_">ByteLevelBPETokenizer</span>.<span class="hljs-title function_">from_file</span>(f<span class="hljs-string">&quot;{model_path}/vocab.json&quot;</span>, f<span class="hljs-string">&quot;{model_path}/merges.txt&quot;</span>)

# 텍스트 토큰화
text = <span class="hljs-string">&quot;I would love to see a lion!&quot;</span>
BPE_encoded_tokens = BPE_tokenizer.<span class="hljs-title function_">encode</span>(text)

<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;원본 텍스트:&quot;</span>, text)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;인코딩된 토큰:&quot;</span>, BPE_encoded_tokens.<span class="hljs-property">tokens</span>)


###############################
# <span class="hljs-title class_">SentencePiece</span> 구현
###############################

spm.<span class="hljs-property">SentencePieceTrainer</span>.<span class="hljs-title function_">train</span>(input=train_text, model_prefix=model_path, vocab_size=<span class="hljs-number">1000</span>, num_threads=<span class="hljs-number">4</span>)

# 사전 훈련된 모델 불러오기
sp_model = model_path + <span class="hljs-string">&quot;.model&quot;</span>
sp = spm.<span class="hljs-title class_">SentencePieceProcessor</span>(model_file=sp_model)

text = <span class="hljs-string">&quot;I would love to see a lion when we reach the zoo!&quot;</span>

# 서브워드 토큰화 및 토큰 반환
tokens_subword = sp.<span class="hljs-title function_">encode_as_pieces</span>(text)
# 서브워드 토큰화 및 토큰 <span class="hljs-variable constant_">ID</span> 반환
tokens_ids = sp.<span class="hljs-title function_">encode_as_ids</span>(text)
# 바이트 수준 토큰화 및 바이트 수준 토큰 <span class="hljs-variable constant_">ID</span> 반환
tokens_byte = sp.<span class="hljs-title function_">encode</span>(text)

# 토큰을 다시 텍스트로 디코딩
decoded_text = sp.<span class="hljs-title function_">decode_pieces</span>(tokens_subword)

<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;원본 텍스트:&quot;</span>, text)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;토큰화된 텍스트:&quot;</span>, tokens_subword)
<span class="hljs-title function_">print</span>(<span class="hljs-string">&quot;디코딩된 텍스트:&quot;</span>, decoded_text)
</code></pre>
<p>이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"토큰화의 기술 자연어 처리를 위한 필수 기법","description":"","date":"2024-05-16 04:22","slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing","content":"\n\n토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!\n\n![이미지](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png)\n\n자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.\n\n# 토큰화 기술 발전의 여정\n\n\n\n토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!\n\n## 기본 토큰화 기술\n\n이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:\n\n1. 공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.\n   \n:warning: 한계\n- 문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.\n- 어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.\n- 잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.\n\n\n\n\n![word tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png)\n\n2. 단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.\n⚠️ 한계\n- 단어 사이의 상황적 의미 손실\n- 어휘폭발\n\n![sentence tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png)\n\n3. 문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.\n⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.\n\n\n\n\n💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:\n\n```js\n# NLTK 사용\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nnltk.download('punkt')\n\n# 입력 문장\ntext = \"When I left the place, I didn't take the left turn.\"\n\n# 공백 기준 토큰화\nwhitespace_tokens = text.split()\n\n# 단어 토큰화\nword_tokens = word_tokenize(text)\n\n# 문장 토큰화\nsentence_tokens = sent_tokenize(text)\n\nprint(\"Whitespace Tokenization:\", whitespace_tokens)\nprint(\"Word Tokenization:\", word_tokens)\nprint(\"Sentence Tokenization:\", sentence_tokens)\n```\n\n또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.\n\n# 고급 토큰화 기술\n\n\n\n고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:\n\n️1. N-그램-\n▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.\n▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.\n💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.\n⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.\n\n![image](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png)\n\n2. 바이트 쌍 부호화-\n▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.\n▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.\n▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.\n▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.\n💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.\n⚠️ 한계-\n▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.\n▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\" /\u003e\n\n3. SentencePiece-  \n- SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.\n- 입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.\n- 단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.\n- 처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.\n💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.\n⚠️ 제한 사항-  \n- 언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.\n- 문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.\n\n💻 위의 세 가지 토큰화 기술을 보여주는 코드:\n\n```js\n# 필요한 라이브러리 가져오기\nimport sentencepiece as spm\nfrom tokenizers import ByteLevelBPETokenizer\nmodel_path = \"모델을 저장할 경로\"\ntrain_text = \"훈련을 위한 txt 파일 경로\"\n\n###############################\n# BPE 구현\n###############################\n\nBPE_tokenizer = ByteLevelBPETokenizer()\n\n# utf-8 인코딩된 코퍼스로 토크나이저 훈련시키기\nBPE_tokenizer.train(files=['훈련을 위한 txt 파일 경로'], vocab_size=1000, min_frequency=2)\n\n# 훈련된 토크나이저 저장\nmodel_path = '모델을 저장할 경로'\nBPE_tokenizer.save_model(model_path)\n\n# 훈련된 토크나이저 불러오기\nBPE_tokenizer = ByteLevelBPETokenizer.from_file(f\"{model_path}/vocab.json\", f\"{model_path}/merges.txt\")\n\n# 텍스트 토큰화\ntext = \"I would love to see a lion!\"\nBPE_encoded_tokens = BPE_tokenizer.encode(text)\n\nprint(\"원본 텍스트:\", text)\nprint(\"인코딩된 토큰:\", BPE_encoded_tokens.tokens)\n\n\n###############################\n# SentencePiece 구현\n###############################\n\nspm.SentencePieceTrainer.train(input=train_text, model_prefix=model_path, vocab_size=1000, num_threads=4)\n\n# 사전 훈련된 모델 불러오기\nsp_model = model_path + \".model\"\nsp = spm.SentencePieceProcessor(model_file=sp_model)\n\ntext = \"I would love to see a lion when we reach the zoo!\"\n\n# 서브워드 토큰화 및 토큰 반환\ntokens_subword = sp.encode_as_pieces(text)\n# 서브워드 토큰화 및 토큰 ID 반환\ntokens_ids = sp.encode_as_ids(text)\n# 바이트 수준 토큰화 및 바이트 수준 토큰 ID 반환\ntokens_byte = sp.encode(text)\n\n# 토큰을 다시 텍스트로 디코딩\ndecoded_text = sp.decode_pieces(tokens_subword)\n\nprint(\"원본 텍스트:\", text)\nprint(\"토큰화된 텍스트:\", tokens_subword)\nprint(\"디코딩된 텍스트:\", decoded_text)\n```\n\n\n\n이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.","ogImage":{"url":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png"},"coverImage":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png","tag":["Tech"],"readingTime":6},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h1: \"h1\",\n    h2: \"h2\",\n    ol: \"ol\",\n    li: \"li\",\n    ul: \"ul\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"토큰화 기술 발전의 여정\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"기본 토큰화 기술\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \":warning: 한계\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png\",\n        alt: \"word tokenization\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.\\n⚠️ 한계\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단어 사이의 상황적 의미 손실\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"어휘폭발\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png\",\n        alt: \"sentence tokenization\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.\\n⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NLTK\"\n        }), \" 사용\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" nltk\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" nltk.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"tokenize\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" word_tokenize, sent_tokenize\\n\\nnltk.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"download\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'punkt'\"\n        }), \")\\n\\n# 입력 문장\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"When I left the place, I didn't take the left turn.\\\"\"\n        }), \"\\n\\n# 공백 기준 토큰화\\nwhitespace_tokens = text.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"split\"\n        }), \"()\\n\\n# 단어 토큰화\\nword_tokens = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"word_tokenize\"\n        }), \"(text)\\n\\n# 문장 토큰화\\nsentence_tokens = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"sent_tokenize\"\n        }), \"(text)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Whitespace Tokenization:\\\"\"\n        }), \", whitespace_tokens)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Word Tokenization:\\\"\"\n        }), \", word_tokens)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Sentence Tokenization:\\\"\"\n        }), \", sentence_tokens)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"고급 토큰화 기술\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"️1. N-그램-\\n▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.\\n▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.\\n💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.\\n⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"바이트 쌍 부호화-\\n▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.\\n▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.\\n▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.\\n▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.\\n💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.\\n⚠️ 한계-\\n▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.\\n▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"SentencePiece-\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.\\n💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.\\n⚠️ 제한 사항-\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"💻 위의 세 가지 토큰화 기술을 보여주는 코드:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# 필요한 라이브러리 가져오기\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" sentencepiece \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" spm\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \"\\nmodel_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"모델을 저장할 경로\\\"\"\n        }), \"\\ntrain_text = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"훈련을 위한 txt 파일 경로\\\"\"\n        }), \"\\n\\n###############################\\n# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BPE\"\n        }), \" 구현\\n###############################\\n\\nBPE_tokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \"()\\n\\n# utf-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8\"\n        }), \" 인코딩된 코퍼스로 토크나이저 훈련시키기\\nBPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(files=[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'훈련을 위한 txt 파일 경로'\"\n        }), \"], vocab_size=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1000\"\n        }), \", min_frequency=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \")\\n\\n# 훈련된 토크나이저 저장\\nmodel_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'모델을 저장할 경로'\"\n        }), \"\\nBPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"save_model\"\n        }), \"(model_path)\\n\\n# 훈련된 토크나이저 불러오기\\nBPE_tokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_file\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"{model_path}/vocab.json\\\"\"\n        }), \", f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"{model_path}/merges.txt\\\"\"\n        }), \")\\n\\n# 텍스트 토큰화\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"I would love to see a lion!\\\"\"\n        }), \"\\nBPE_encoded_tokens = BPE_tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode\"\n        }), \"(text)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"원본 텍스트:\\\"\"\n        }), \", text)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"인코딩된 토큰:\\\"\"\n        }), \", BPE_encoded_tokens.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"tokens\"\n        }), \")\\n\\n\\n###############################\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentencePiece\"\n        }), \" 구현\\n###############################\\n\\nspm.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"SentencePieceTrainer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(input=train_text, model_prefix=model_path, vocab_size=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1000\"\n        }), \", num_threads=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4\"\n        }), \")\\n\\n# 사전 훈련된 모델 불러오기\\nsp_model = model_path + \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\".model\\\"\"\n        }), \"\\nsp = spm.\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentencePieceProcessor\"\n        }), \"(model_file=sp_model)\\n\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"I would love to see a lion when we reach the zoo!\\\"\"\n        }), \"\\n\\n# 서브워드 토큰화 및 토큰 반환\\ntokens_subword = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode_as_pieces\"\n        }), \"(text)\\n# 서브워드 토큰화 및 토큰 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"ID\"\n        }), \" 반환\\ntokens_ids = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode_as_ids\"\n        }), \"(text)\\n# 바이트 수준 토큰화 및 바이트 수준 토큰 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"ID\"\n        }), \" 반환\\ntokens_byte = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode\"\n        }), \"(text)\\n\\n# 토큰을 다시 텍스트로 디코딩\\ndecoded_text = sp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"decode_pieces\"\n        }), \"(tokens_subword)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"원본 텍스트:\\\"\"\n        }), \", text)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"토큰화된 텍스트:\\\"\"\n        }), \", tokens_subword)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"디코딩된 텍스트:\\\"\"\n        }), \", decoded_text)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing"},"buildId":"837W-BjvPVBgft6aM4api","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>