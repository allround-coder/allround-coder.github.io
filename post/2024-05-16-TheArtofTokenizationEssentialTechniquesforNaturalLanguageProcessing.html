<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing" data-gatsby-head="true"/><meta name="twitter:title" content="토큰화의 기술 자연어 처리를 위한 필수 기법 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-16 04:22" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_buildManifest.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">토큰화의 기술 자연어 처리를 위한 필수 기법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="토큰화의 기술 자연어 처리를 위한 필수 기법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 16, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!</p>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png" alt="이미지"></p>
<p>자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.</p>
<h1>토큰화 기술 발전의 여정</h1>
<p>토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!</p>
<h2>기본 토큰화 기술</h2>
<p>이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:</p>
<ol>
<li>공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.</li>
</ol>
<p>:warning: 한계</p>
<ul>
<li>문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.</li>
<li>어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.</li>
<li>잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.</li>
</ul>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png" alt="word tokenization"></p>
<ol start="2">
<li>단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.
⚠️ 한계</li>
</ol>
<ul>
<li>단어 사이의 상황적 의미 손실</li>
<li>어휘폭발</li>
</ul>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png" alt="sentence tokenization"></p>
<ol start="3">
<li>문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.
⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.</li>
</ol>
<p>💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:</p>
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">NLTK</span> 사용
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">from</span> nltk.<span class="hljs-property">tokenize</span> <span class="hljs-keyword">import</span> word_tokenize, sent_tokenize

nltk.<span class="hljs-title function_">download</span>(<span class="hljs-string">'punkt'</span>)

# 입력 문장
text = <span class="hljs-string">"When I left the place, I didn't take the left turn."</span>

# 공백 기준 토큰화
whitespace_tokens = text.<span class="hljs-title function_">split</span>()

# 단어 토큰화
word_tokens = <span class="hljs-title function_">word_tokenize</span>(text)

# 문장 토큰화
sentence_tokens = <span class="hljs-title function_">sent_tokenize</span>(text)

<span class="hljs-title function_">print</span>(<span class="hljs-string">"Whitespace Tokenization:"</span>, whitespace_tokens)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"Word Tokenization:"</span>, word_tokens)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"Sentence Tokenization:"</span>, sentence_tokens)
</code></pre>
<p>또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.</p>
<h1>고급 토큰화 기술</h1>
<p>고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:</p>
<p>️1. N-그램-
▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.
▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.
💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.
⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.</p>
<p><img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png" alt="image"></p>
<ol start="2">
<li>바이트 쌍 부호화-
▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.
▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.
▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.
▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.
💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.
⚠️ 한계-
▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.
▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.</li>
</ol>
<img src="/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png">
<ol start="3">
<li>SentencePiece-</li>
</ol>
<ul>
<li>SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.</li>
<li>입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.</li>
<li>단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.</li>
<li>처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.
💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.
⚠️ 제한 사항-</li>
<li>언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.</li>
<li>문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.</li>
</ul>
<p>💻 위의 세 가지 토큰화 기술을 보여주는 코드:</p>
<pre><code class="hljs language-js"># 필요한 라이브러리 가져오기
<span class="hljs-keyword">import</span> sentencepiece <span class="hljs-keyword">as</span> spm
<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> <span class="hljs-title class_">ByteLevelBPETokenizer</span>
model_path = <span class="hljs-string">"모델을 저장할 경로"</span>
train_text = <span class="hljs-string">"훈련을 위한 txt 파일 경로"</span>

###############################
# <span class="hljs-variable constant_">BPE</span> 구현
###############################

BPE_tokenizer = <span class="hljs-title class_">ByteLevelBPETokenizer</span>()

# utf-<span class="hljs-number">8</span> 인코딩된 코퍼스로 토크나이저 훈련시키기
BPE_tokenizer.<span class="hljs-title function_">train</span>(files=[<span class="hljs-string">'훈련을 위한 txt 파일 경로'</span>], vocab_size=<span class="hljs-number">1000</span>, min_frequency=<span class="hljs-number">2</span>)

# 훈련된 토크나이저 저장
model_path = <span class="hljs-string">'모델을 저장할 경로'</span>
BPE_tokenizer.<span class="hljs-title function_">save_model</span>(model_path)

# 훈련된 토크나이저 불러오기
BPE_tokenizer = <span class="hljs-title class_">ByteLevelBPETokenizer</span>.<span class="hljs-title function_">from_file</span>(f<span class="hljs-string">"{model_path}/vocab.json"</span>, f<span class="hljs-string">"{model_path}/merges.txt"</span>)

# 텍스트 토큰화
text = <span class="hljs-string">"I would love to see a lion!"</span>
BPE_encoded_tokens = BPE_tokenizer.<span class="hljs-title function_">encode</span>(text)

<span class="hljs-title function_">print</span>(<span class="hljs-string">"원본 텍스트:"</span>, text)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"인코딩된 토큰:"</span>, BPE_encoded_tokens.<span class="hljs-property">tokens</span>)


###############################
# <span class="hljs-title class_">SentencePiece</span> 구현
###############################

spm.<span class="hljs-property">SentencePieceTrainer</span>.<span class="hljs-title function_">train</span>(input=train_text, model_prefix=model_path, vocab_size=<span class="hljs-number">1000</span>, num_threads=<span class="hljs-number">4</span>)

# 사전 훈련된 모델 불러오기
sp_model = model_path + <span class="hljs-string">".model"</span>
sp = spm.<span class="hljs-title class_">SentencePieceProcessor</span>(model_file=sp_model)

text = <span class="hljs-string">"I would love to see a lion when we reach the zoo!"</span>

# 서브워드 토큰화 및 토큰 반환
tokens_subword = sp.<span class="hljs-title function_">encode_as_pieces</span>(text)
# 서브워드 토큰화 및 토큰 <span class="hljs-variable constant_">ID</span> 반환
tokens_ids = sp.<span class="hljs-title function_">encode_as_ids</span>(text)
# 바이트 수준 토큰화 및 바이트 수준 토큰 <span class="hljs-variable constant_">ID</span> 반환
tokens_byte = sp.<span class="hljs-title function_">encode</span>(text)

# 토큰을 다시 텍스트로 디코딩
decoded_text = sp.<span class="hljs-title function_">decode_pieces</span>(tokens_subword)

<span class="hljs-title function_">print</span>(<span class="hljs-string">"원본 텍스트:"</span>, text)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"토큰화된 텍스트:"</span>, tokens_subword)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"디코딩된 텍스트:"</span>, decoded_text)
</code></pre>
<p>이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"토큰화의 기술 자연어 처리를 위한 필수 기법","description":"","date":"2024-05-16 04:22","slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing","content":"\n\n토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!\n\n![이미지](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png)\n\n자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.\n\n# 토큰화 기술 발전의 여정\n\n\n\n토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!\n\n## 기본 토큰화 기술\n\n이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:\n\n1. 공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.\n   \n:warning: 한계\n- 문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.\n- 어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.\n- 잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.\n\n\n\n\n![word tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png)\n\n2. 단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.\n⚠️ 한계\n- 단어 사이의 상황적 의미 손실\n- 어휘폭발\n\n![sentence tokenization](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png)\n\n3. 문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.\n⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.\n\n\n\n\n💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:\n\n```js\n# NLTK 사용\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nnltk.download('punkt')\n\n# 입력 문장\ntext = \"When I left the place, I didn't take the left turn.\"\n\n# 공백 기준 토큰화\nwhitespace_tokens = text.split()\n\n# 단어 토큰화\nword_tokens = word_tokenize(text)\n\n# 문장 토큰화\nsentence_tokens = sent_tokenize(text)\n\nprint(\"Whitespace Tokenization:\", whitespace_tokens)\nprint(\"Word Tokenization:\", word_tokens)\nprint(\"Sentence Tokenization:\", sentence_tokens)\n```\n\n또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.\n\n# 고급 토큰화 기술\n\n\n\n고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:\n\n️1. N-그램-\n▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.\n▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.\n💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.\n⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.\n\n![image](/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png)\n\n2. 바이트 쌍 부호화-\n▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.\n▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.\n▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.\n▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.\n💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.\n⚠️ 한계-\n▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.\n▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.\n\n\n\n\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\" /\u003e\n\n3. SentencePiece-  \n- SentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.\n- 입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.\n- 단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.\n- 처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.\n💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.\n⚠️ 제한 사항-  \n- 언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.\n- 문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.\n\n💻 위의 세 가지 토큰화 기술을 보여주는 코드:\n\n```js\n# 필요한 라이브러리 가져오기\nimport sentencepiece as spm\nfrom tokenizers import ByteLevelBPETokenizer\nmodel_path = \"모델을 저장할 경로\"\ntrain_text = \"훈련을 위한 txt 파일 경로\"\n\n###############################\n# BPE 구현\n###############################\n\nBPE_tokenizer = ByteLevelBPETokenizer()\n\n# utf-8 인코딩된 코퍼스로 토크나이저 훈련시키기\nBPE_tokenizer.train(files=['훈련을 위한 txt 파일 경로'], vocab_size=1000, min_frequency=2)\n\n# 훈련된 토크나이저 저장\nmodel_path = '모델을 저장할 경로'\nBPE_tokenizer.save_model(model_path)\n\n# 훈련된 토크나이저 불러오기\nBPE_tokenizer = ByteLevelBPETokenizer.from_file(f\"{model_path}/vocab.json\", f\"{model_path}/merges.txt\")\n\n# 텍스트 토큰화\ntext = \"I would love to see a lion!\"\nBPE_encoded_tokens = BPE_tokenizer.encode(text)\n\nprint(\"원본 텍스트:\", text)\nprint(\"인코딩된 토큰:\", BPE_encoded_tokens.tokens)\n\n\n###############################\n# SentencePiece 구현\n###############################\n\nspm.SentencePieceTrainer.train(input=train_text, model_prefix=model_path, vocab_size=1000, num_threads=4)\n\n# 사전 훈련된 모델 불러오기\nsp_model = model_path + \".model\"\nsp = spm.SentencePieceProcessor(model_file=sp_model)\n\ntext = \"I would love to see a lion when we reach the zoo!\"\n\n# 서브워드 토큰화 및 토큰 반환\ntokens_subword = sp.encode_as_pieces(text)\n# 서브워드 토큰화 및 토큰 ID 반환\ntokens_ids = sp.encode_as_ids(text)\n# 바이트 수준 토큰화 및 바이트 수준 토큰 ID 반환\ntokens_byte = sp.encode(text)\n\n# 토큰을 다시 텍스트로 디코딩\ndecoded_text = sp.decode_pieces(tokens_subword)\n\nprint(\"원본 텍스트:\", text)\nprint(\"토큰화된 텍스트:\", tokens_subword)\nprint(\"디코딩된 텍스트:\", decoded_text)\n```\n\n\n\n이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.","ogImage":{"url":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png"},"coverImage":"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e토큰화가 어떻게 발전해 왔는지 궁금하신가요? 현재의 대형 언어 모델(Large Language Models)은 어떤 기술을 사용하여 토큰화를 수행할까요? 함께 알아보도록 해요!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e자연어 처리는 트랜스포머 모델 개발 이후 많은 발전을 이루었습니다. 텍스트를 정제한 후 NLP 작업과 관련된 첫 번째 단계는 토큰화입니다. 처음의 화이트스페이스(whitespace) 및 구두점(tokenizer)을 구축한 이후 현재의 문맥적(contextual) 및 구조적(tokenizers) 토크나이저들까지 많은 변화가 있었습니다. 요즘에는 BERT 및 그 변형, ChatGPT, Claude와 같은 생성 모델이 특히 NLP 분야에서 화제가 되고 있습니다. 이 블로그에서는 텍스트 토큰화 과정이 어떻게 발전해 왔는지 및 최신 대형 언어 모델에서 어떻게 사용되고 있는지 알아볼 것입니다.\u003c/p\u003e\n\u003ch1\u003e토큰화 기술 발전의 여정\u003c/h1\u003e\n\u003cp\u003e토큰화는 다양한 기술을 사용하여 텍스트 데이터를 작은 조각으로 나누는 것을 말합니다. 모델이 데이터를 더 잘 처리하고 분석할 수 있도록 합니다. 기본 토큰화 기술에는 공백, 단어 및 문장 토큰화가 포함되어 있습니다. 이러한 기술은 어휘 크기 및 정보 손실, 문맥 부족 등과 같은 일부 한계가 있었습니다. 따라서 n-gram, BPE (Byte Pair Encoding), SentencePiece 토큰화와 같은 기술이 소개되었으며 거의 모든 한계를 해소할 수 있었습니다. 이러한 기술은 현재 언어 모델에서 사용되며 임베딩에서 문맥 및 구조적 이해를 캡처하는 데 도움이 됩니다. 이제 각 기술을 자세히 살펴보겠습니다!\u003c/p\u003e\n\u003ch2\u003e기본 토큰화 기술\u003c/h2\u003e\n\u003cp\u003e이러한 기술은 데이터를 직관적으로 작은 조각으로 나누는 데 주로 초점을 맞추며 어떤 청크가 다른 청크와 어떻게 관련되어 있는지에 대해 크게 신경쓰지 않습니다. 각 기술이 작동하는 방식에 대한 자세한 설명은 다음과 같습니다:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e공백 토큰화 - 탭, 공백, 새 줄 등의 공백을 기준으로 텍스트를 분할합니다. 이 기술은 모든 단어가 공백으로 분리되어 있다고 가정합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e:warning: 한계\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e문맥적 의미 손실: 단어를 별도의 토큰으로 취급하여 종종 문장 내에서의 관계를 간과합니다.\u003c/li\u003e\n\u003cli\u003e어휘 폭발: 각 고유한 단어가 토큰이 되므로, 어떠한 언어도 수십억 개의 단어를 가질 수 있기 때문에 종종 매우 큰 훈련 어휘로 이어집니다.\u003c/li\u003e\n\u003cli\u003e잡음이 많은 데이터 처리 어려움: 이모지, 과도한 문장 부호 또는 특수 문자를 처리하지 못하여 토큰화가 부정확해집니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_1.png\" alt=\"word tokenization\"\u003e\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e단어 토큰화 - 공백을 기반으로 분할된 문장 토큰화에서 문장의 기본 단위로 단어가 따로 있다고 가정합니다.\n⚠️ 한계\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e단어 사이의 상황적 의미 손실\u003c/li\u003e\n\u003cli\u003e어휘폭발\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_2.png\" alt=\"sentence tokenization\"\u003e\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e문장 토큰화 - 마침표, 물음표 등의 구두점 및 다른 언어별 규칙을 이해하여 문장을 기준으로 텍스트를 분할합니다.\n⚠️ 한계 - 기계 번역 등의 작업에 유용하지만 여전히 단어 수준 토큰화에 의존하며 이로 인한 한계를 물려받습니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e💻 위의 세 가지 토큰화 기법을 보여주는 코드입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-variable constant_\"\u003eNLTK\u003c/span\u003e 사용\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e nltk\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e nltk.\u003cspan class=\"hljs-property\"\u003etokenize\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e word_tokenize, sent_tokenize\n\nnltk.\u003cspan class=\"hljs-title function_\"\u003edownload\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'punkt'\u003c/span\u003e)\n\n# 입력 문장\ntext = \u003cspan class=\"hljs-string\"\u003e\"When I left the place, I didn't take the left turn.\"\u003c/span\u003e\n\n# 공백 기준 토큰화\nwhitespace_tokens = text.\u003cspan class=\"hljs-title function_\"\u003esplit\u003c/span\u003e()\n\n# 단어 토큰화\nword_tokens = \u003cspan class=\"hljs-title function_\"\u003eword_tokenize\u003c/span\u003e(text)\n\n# 문장 토큰화\nsentence_tokens = \u003cspan class=\"hljs-title function_\"\u003esent_tokenize\u003c/span\u003e(text)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Whitespace Tokenization:\"\u003c/span\u003e, whitespace_tokens)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Word Tokenization:\"\u003c/span\u003e, word_tokens)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Sentence Tokenization:\"\u003c/span\u003e, sentence_tokens)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e또한 SpaCy, Scikit-learn, Stanza 등의 다른 파이썬 라이브러리도 이러한 토큰화 기술을 수행할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e고급 토큰화 기술\u003c/h1\u003e\n\u003cp\u003e고급 기술은 위에서 언급한 한계를 완화하려고 시도하고, 단어 간 상호 관계 및 문장 내 맥락에 초점을 맞추려고 노력합니다. 이 기술이 어떻게 작동하는지 살펴봅시다:\u003c/p\u003e\n\u003cp\u003e️1. N-그램-\n▪ 텍스트를 슬라이딩 윈도우 방식으로 분할하여 지정된 N 길이의 토큰을 만듭니다.\n▪ 이 방법은 서로 가깝게 발생하는 단어 간의 관계를 잡아냅니다.\n💡이 기술은 음성 인식, 텍스트 완성 등과 같은 새로운 작업에서 기본적인 역할을 합니다.\n⚠️ 한계 — 연속된 단어와의 관계만 파악합니다. 더 긴 문장에 대해선 다시 맥락이 사라집니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e바이트 쌍 부호화-\n▪ 여기서는 학습 텍스트에 포함된 모든 문자/바이트를 사용하여 먼저 어휘집을 만듭니다.\n▪ 연속 발생 문자의 빈도수에 기반하여 어휘집을 반복적으로 업데이트합니다.\n▪ 중지 조건(또는 최대 병합 수)이 충족되면 입력 텍스트(테스트 입력)는 이 생성된 어휘집을 기반으로 분할됩니다.\n▪ 어휘 외 단어를 처리할 수 있으며 어휘 크기가 무너지지 않습니다.\n💡RoBERTa, GPT2는 이 토큰화 기술을 사용합니다.\n⚠️ 한계-\n▪ 훈련 단계에서 개발된 고정된 어휘 크기로 인해 때로는 새로운 단어에 문제가 생기기도 합니다.\n▪ 이 알고리즘은 가장 빈도가 높은 단어들을 모아 사용하며, 문장의 형태학적 및 문맥적 복잡성을 무시합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cimg src=\"/assets/img/2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing_4.png\"\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eSentencePiece-\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSentencePiece는 Unigram과 Dynamic Programming 또는 BPE 알고리즘을 사용하는 서브워드 토큰화 라이브러리입니다.\u003c/li\u003e\n\u003cli\u003e입력 텍스트를 Unicode 문자로 사용하므로 초기 단어 토큰화가 필요없습니다.\u003c/li\u003e\n\u003cli\u003e단일 모델을 사용하여 여러 언어를 처리할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e처음에 Unicode 문자 수준 토큰을 생성하기 때문에 텍스트의 토큰화 및 디토큰화를 모두 도와 전처리 및 후처리를 쉽게 만들어 줍니다.\n💡BERT, XLNet, T5 등 많은 HuggingFace 트랜스포머 모델이 이 토크나이저를 사용하고 있습니다. 이는 오픈 소스로 잘 유지되는 라이브러리입니다.\n⚠️ 제한 사항-\u003c/li\u003e\n\u003cli\u003e언어에 독립적이지만 다양한 언어에 대해 사용할 때 성능이 달라질 수 있습니다.\u003c/li\u003e\n\u003cli\u003e문단이나 섹션과 같은 문맥 및 구조적 세부 정보를 고려하지 않고 하위 단어의 시퀀스로 텍스트를 여전히 취급합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e💻 위의 세 가지 토큰화 기술을 보여주는 코드:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 필요한 라이브러리 가져오기\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sentencepiece \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e spm\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tokenizers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eByteLevelBPETokenizer\u003c/span\u003e\nmodel_path = \u003cspan class=\"hljs-string\"\u003e\"모델을 저장할 경로\"\u003c/span\u003e\ntrain_text = \u003cspan class=\"hljs-string\"\u003e\"훈련을 위한 txt 파일 경로\"\u003c/span\u003e\n\n###############################\n# \u003cspan class=\"hljs-variable constant_\"\u003eBPE\u003c/span\u003e 구현\n###############################\n\nBPE_tokenizer = \u003cspan class=\"hljs-title class_\"\u003eByteLevelBPETokenizer\u003c/span\u003e()\n\n# utf-\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e 인코딩된 코퍼스로 토크나이저 훈련시키기\nBPE_tokenizer.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(files=[\u003cspan class=\"hljs-string\"\u003e'훈련을 위한 txt 파일 경로'\u003c/span\u003e], vocab_size=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, min_frequency=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n\n# 훈련된 토크나이저 저장\nmodel_path = \u003cspan class=\"hljs-string\"\u003e'모델을 저장할 경로'\u003c/span\u003e\nBPE_tokenizer.\u003cspan class=\"hljs-title function_\"\u003esave_model\u003c/span\u003e(model_path)\n\n# 훈련된 토크나이저 불러오기\nBPE_tokenizer = \u003cspan class=\"hljs-title class_\"\u003eByteLevelBPETokenizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_file\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"{model_path}/vocab.json\"\u003c/span\u003e, f\u003cspan class=\"hljs-string\"\u003e\"{model_path}/merges.txt\"\u003c/span\u003e)\n\n# 텍스트 토큰화\ntext = \u003cspan class=\"hljs-string\"\u003e\"I would love to see a lion!\"\u003c/span\u003e\nBPE_encoded_tokens = BPE_tokenizer.\u003cspan class=\"hljs-title function_\"\u003eencode\u003c/span\u003e(text)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"원본 텍스트:\"\u003c/span\u003e, text)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"인코딩된 토큰:\"\u003c/span\u003e, BPE_encoded_tokens.\u003cspan class=\"hljs-property\"\u003etokens\u003c/span\u003e)\n\n\n###############################\n# \u003cspan class=\"hljs-title class_\"\u003eSentencePiece\u003c/span\u003e 구현\n###############################\n\nspm.\u003cspan class=\"hljs-property\"\u003eSentencePieceTrainer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(input=train_text, model_prefix=model_path, vocab_size=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, num_threads=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e)\n\n# 사전 훈련된 모델 불러오기\nsp_model = model_path + \u003cspan class=\"hljs-string\"\u003e\".model\"\u003c/span\u003e\nsp = spm.\u003cspan class=\"hljs-title class_\"\u003eSentencePieceProcessor\u003c/span\u003e(model_file=sp_model)\n\ntext = \u003cspan class=\"hljs-string\"\u003e\"I would love to see a lion when we reach the zoo!\"\u003c/span\u003e\n\n# 서브워드 토큰화 및 토큰 반환\ntokens_subword = sp.\u003cspan class=\"hljs-title function_\"\u003eencode_as_pieces\u003c/span\u003e(text)\n# 서브워드 토큰화 및 토큰 \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e 반환\ntokens_ids = sp.\u003cspan class=\"hljs-title function_\"\u003eencode_as_ids\u003c/span\u003e(text)\n# 바이트 수준 토큰화 및 바이트 수준 토큰 \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e 반환\ntokens_byte = sp.\u003cspan class=\"hljs-title function_\"\u003eencode\u003c/span\u003e(text)\n\n# 토큰을 다시 텍스트로 디코딩\ndecoded_text = sp.\u003cspan class=\"hljs-title function_\"\u003edecode_pieces\u003c/span\u003e(tokens_subword)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"원본 텍스트:\"\u003c/span\u003e, text)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"토큰화된 텍스트:\"\u003c/span\u003e, tokens_subword)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"디코딩된 텍스트:\"\u003c/span\u003e, decoded_text)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이러한 고급 토큰화 기술을 사용하여 추출한 토큰들은 BERT, GPT 등과 같은 고급 언어 모델을 사용하는 작업에 필요한 첫 번째 단계입니다. 이러한 토큰들은 모델로 전송되어 임베딩으로 변환되어 전체 텍스트의 문맥적 및 구조적 의미를 포착합니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-16-TheArtofTokenizationEssentialTechniquesforNaturalLanguageProcessing"},"buildId":"aCCUs-qPrLLLWRnkN0AOd","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>