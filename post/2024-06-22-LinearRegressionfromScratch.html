<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-06-22-LinearRegressionfromScratch" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-22-LinearRegressionfromScratch_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-06-22-LinearRegressionfromScratch" data-gatsby-head="true"/><meta name="twitter:title" content="파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-22-LinearRegressionfromScratch_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-22 04:46" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/Rv-NbbtWUaja2joH5WkO_/_buildManifest.js" defer=""></script><script src="/_next/static/Rv-NbbtWUaja2joH5WkO_/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 22, 2024</span><span class="posts_reading_time__f7YPP">16<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-22-LinearRegressionfromScratch&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_0.png" alt="Linear Regression"></p>
<p>안녕하세요! 가장 간단한 머신 러닝 기술 중 하나인 선형 회귀로 시작합니다. 이 게시물의 수학적 부분은 선형 대수와 미적분의 좋은 이해력이 필요할 것입니다. 이 부분은 다음 시리즈에도 해당될 것이죠. 이는 머신 러닝의 많은 부분을 뒷받침하고 있고, 깊은 이해를 위한 선행 요건입니다. 그렇다면 함께 알아보도록 하죠!</p>
<h1>단순 선형 회귀</h1>
<p>선형 회귀는 여러 점들을 고려하여 최적의 선을 찾는 과제입니다. 최적의 선을 찾는 방법을 알아내기 전에, 이것이 실제로 무엇을 의미하는지를 이해해야 합니다.</p>
<div class="content-ad"></div>
<p>그림 1의 선이 이 선보다 더 데이터에 잘 맞는 것을 직관적으로 알 수 있어요:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_1.png" alt="Figure 1"></p>
<p>왜냐하면 그림 1의 점들이 그림 2의 점들보다 선으로부터 더 멀리 떨어져 있어요. 이 직관을 수학적으로 어떻게 형식화할지 알아보도록 할게요. 이렇게 하면 “가장 잘 맞는”이 무엇을 의미하는지 명확하게 정의할 수 있을 거예요.</p>
<p>간단하고 시각화하기 쉽게 하기 위해, 2차원에서 시작할게요. 이 경우 데이터 포인트는 (x, y) 쌍이고 위의 그림처럼 그래프에 표시할 수 있어요. 우리는 데이터를 가장 잘 나타내는 f(x) = kx와 같은 선형 함수를 찾고 싶어해요. 이 모델은 원점을 통과하는 선을 가정해요. 우리는 아직 원점 이외의 교차점의 가능성에 대해 고려하지 않을 거예요.</p>
<div class="content-ad"></div>
<p>n개의 데이터 포인트가 있는 컬렉션이 있다고 가정해 보겠습니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_2.png" alt="Linear Regression from Scratch 2"></p>
<p>각 x값에 대해 모델을 사용하여 예측된 y값을 얻을 수 있습니다. 이러한 상황은 하나의 독립 변수(x)와 하나의 종속 변수(y)만 있는 단순 선형 회귀로 알려져 있습니다. 예측된 y값과 실제 y값을 구분하기 위해 프라임 기호를 사용할 것입니다. 따라서 특정 x값에 대한 모델의 예측된 y값은 다음과 같은 공식으로 표시됩니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_3.png" alt="Linear Regression from Scratch 3"></p>
<div class="content-ad"></div>
<p>이제 x 값들을 하나의 벡터에, y 값들을 다른 벡터에 넣어봅시다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_4.png" alt="image"></p>
<p>이를 벡터화(Vectorization)라고 합니다. 데이터 과학 문제에 대한 많은 이점이 있습니다. 여러 개별 값을 벡터로 결합하면 수학 공식이 훨씬 더 간결하고 이해하기 쉬워집니다. 코드에서의 벡터화도 성능을 향상시킵니다. 큰 값 배열에 대해 벡터 산술을 수행하는 것이 각각 하나씩 처리하는 루프를 거쳐 동작하는 것보다 훨씬 빠릅니다. Numpy와 같은 많은 숫자 계산 라이브러리가 빠른 벡터 산술을 위해 설계되었습니다. 벡터화는 또한 GPU와 같은 하드웨어를 사용한 병렬화도 가능하게 합니다. 여러 개의 배열 요소에 동시에 연산을 수행합니다. 한 번 더 언급하자면, 각 연산이 하나씩 차례로 이루어지는 루프를 사용하지 않고는 이것이 불가능할 것입니다.</p>
<p>또한 우리가 예측한 y 값들을 모두 담은 벡터를 생성할 수 있습니다:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-22-LinearRegressionfromScratch_5.png">
<img src="/assets/img/2024-06-22-LinearRegressionfromScratch_6.png">
<p>최적 적합 선을 찾기 위해서는 y'와 실제 값인 y의 벡터 사이의 거리를 알아야 합니다. 이 두 벡터의 차이를 살펴볼 수 있습니다: y' - y. 그러나 이것은 벡터 자체이며, 모델의 오류를 나타내는 단일 숫자를 원합니다. 우리는 제곱합 오류(SSE)를 사용할 것입니다. SSE는 ||y' - y||²로, 차이 벡터의 제곱 크기와 같습니다. 이것은 "제곱합"으로 불립니다. 왜냐하면 y' - y의 제곱된 항목들의 합과 같기 때문입니다:</p>
<img src="/assets/img/2024-06-22-LinearRegressionfromScratch_7.png">
<div class="content-ad"></div>
<p>왜 ||y’ — y||²을 사용하는지 궁금하다면 단순히 ||y’ — y||만 사용하는 것보다 제곱 크기를 사용하는 게 훨씬 간단하다는 점이 하나의 답일 수 있습니다. ||y’ — y||는 합계 외부에 하나의 추가 제곱근 기호가 있습니다:</p>
<img src="/assets/img/2024-06-22-LinearRegressionfromScratch_8.png">
<p>이로 인해 미분을 할 때 공식을 처리하는 것이 훨씬 더 까다로워집니다.</p>
<p>이제 선형 회귀 모델의 오차를 정의했으니, 이를 최소화하는 방법을 찾아야 합니다. SSE에 대한 표현을 확장해 보겠습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_9.png" alt="이미지"></p>
<p>방정식 1에 따라 y'에 kx를 대입하면,</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_10.png" alt="이미지"></p>
<p>x와 y를 일정한 값으로 유지할 때 오차를 최소화하는 k의 값을 찾아야 합니다. 이를 위해서는 식 1의 k에 대한 미분값을 0으로 설정하고 해를 구할 수 있습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_11.png" alt="Linear Regression 1"></p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_12.png" alt="Linear Regression 2"></p>
<p>이를 통해 제곱 오차의 합을 최소화하는 k 값을 알 수 있습니다. 이 지식을 바탕으로 SimpleLinearRegressor를 코딩할 수 있습니다. 이것은 단 하나의 인스턴스 변수를 갖게 될 것입니다 — 기울기, k.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleLinearRegressor</span>:
    <span class="hljs-string">""</span><span class="hljs-string">"단순 선형 회귀를 수행합니다."</span><span class="hljs-string">""</span>

    def <span class="hljs-title function_">__init__</span>(self):
        self.<span class="hljs-property">k</span> = <span class="hljs-title class_">None</span>
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):
    <span class="hljs-string">"""
    입력값 x 또는 x 값 벡터를 사용하여 예측된 y값을 제공합니다.
    :param x: 입력 값(들).
    :return: 예측된 y값(들).
    """</span>

    <span class="hljs-keyword">if</span> self.k <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">raise</span> RegressionModelNotFitError(<span class="hljs-string">'앗! 모델이 아직 피팅되지 않았어요!'</span>)

    <span class="hljs-keyword">return</span> self.k * x
</code></pre>
<p>또한 x와 y 벡터를 사용하여 Equation 2를 기반으로 k의 적절한 값을 찾는 fit 메서드가 필요합니다. 이것이 클래스의 본질입니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, x, y</span>):
    <span class="hljs-string">"""
    주어진 x 값과 y 값 벡터를 기반으로 모델을 맞춥니다.
    :param x: x값 벡터.
    :param y: y값 벡터.
    :return: 적합된 모델의 제곱 오차 합.
    """</span>

    self.k = x @ y / (x @ x)
    diff = self.predict(x) - y
    <span class="hljs-keyword">return</span> diff @ diff
</code></pre>
<div class="content-ad"></div>
<p>모델을 테스트하기 위해 데이터를 생성해야 합니다. 범위 내에서 임의의 x 값을 생성하고 선형 모델을 사용하여 해당하는 y 값을 계산한 다음 이 y 값에 가우시안 노이즈를 추가하는 함수를 만들겠습니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">generate_noisy_data</span>(n_points, slope, x_range, noise_stddev):
    <span class="hljs-string">""</span><span class="hljs-string">"
    추가된 가우시안 노이즈를 이용해 선형 관계에 기반한 데이터 점을 생성합니다.
    :param n_points: 생성할 데이터 점의 수.
    :param slope: 직선의 기울기.
    :param x_range: x 값을 추출할 범위.
    :param noise_stddev: 각 y 값에 추가할 가우시안 노이즈의 표준 편차.
    :return: x 값과 y 값의 벡터.
    "</span><span class="hljs-string">""</span>

    x = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">uniform</span>(*x_range, n_points)
    y = slope * x + np.<span class="hljs-property">random</span>.<span class="hljs-title function_">normal</span>(scale=noise_stddev, size=n_points)
    <span class="hljs-keyword">return</span> x, y
</code></pre>
<p>SimpleLinearRegressor가 무작위로 생성된 데이터에서 원래의 기울기를 얼마나 잘 복원하는지 살펴봅시다. 시각화를 위해 matplotlib를 사용하겠습니다.</p>
<pre><code class="hljs language-js">x_range = np.<span class="hljs-title function_">array</span>([<span class="hljs-number">0</span>, <span class="hljs-number">5</span>])
x, y = <span class="hljs-title function_">generate_noisy_data</span>(n_points=<span class="hljs-number">20</span>, slope=<span class="hljs-number">0.42</span>, x_range=x_range, noise_stddev=<span class="hljs-number">0.5</span>)
plt.<span class="hljs-title function_">scatter</span>(x, y)

regressor = <span class="hljs-title class_">SimpleLinearRegressor</span>()
fit = regressor.<span class="hljs-title function_">fit</span>(x, y)
slope = regressor.<span class="hljs-property">k</span>
plt.<span class="hljs-title function_">plot</span>(x_range, [<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * x_range[<span class="hljs-number">1</span>]], color=<span class="hljs-string">'red'</span>)
plt.<span class="hljs-title function_">text</span>(<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, f<span class="hljs-string">'오차: {"{:.2f}".format(fit)}\n예측된 기울기: {"{:.2f}".format(slope)}'</span>)
plt.<span class="hljs-title function_">show</span>()
</code></pre>
<div class="content-ad"></div>
<p>아래는 한 번의 실행 결과입니다:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_13.png" alt="Linear Regression"></p>
<p>보이시다시피, 이 모델은 훌륭한 작업을 합니다! 회귀 모델에 의해 예측된 기울기는 <code>generate_noisy_data</code>에 입력된 기울기와 소수점 셋째 자리까지 일치합니다.</p>
<h1>다중 선형 회귀</h1>
<div class="content-ad"></div>
<p>하나의 독립 변수 x와 하나의 종속 변수 y로 선형 회귀를 수행하는 방법을 배웠습니다. 이제 y가 m개의 독립 변수에 의존한다고 가정해 보겠습니다. 따라서 우리는 (m + 1)차원 데이터를 다루게 됩니다. 우리가 가진 데이터가 다음과 같은 n개의 데이터 포인트일 수 있습니다:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_14.png" alt="image"></p>
<p>여기서 x_ij는 i번째 데이터 포인트에서 j번째 독립 변수의 값을 나타냅니다.</p>
<p>데이터를 벡터화하여 정리하는 것은 항상 좋은 첫 번째 단계입니다.</p>
<div class="content-ad"></div>
<p>우리는 이전과 마찬가지로 모든 y 값들을 벡터로 모아낼 수 있어요:</p>
<p>이제는 x 데이터가 두 개의 인덱스를 가지고 있기 때문에 xs에 대해 벡터를 사용하는 것이 더 이상이 아니에요. 대신, 각 행이 하나의 데이터 포인트인 행렬로 모아낼 수 있어요:</p>
<div class="content-ad"></div>
<p>이제부터 이 행렬의 항목을 나타내는 변수로 대문자 X_ij와 소문자 x_ij를 서로 바꿔 사용할 거에요.</p>
<p>지금 데이터에 맞추려고 하는 선형 모델은 조금 더 복잡해 보여요:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_17.png" alt="image"></p>
<p>독립 변수 각각에 대한 계수 또는 "기울기"인 βs가 있는 m개의 계수가 있어요.</p>
<div class="content-ad"></div>
<p>각 데이터 포인트의 벡터를 만들 수 있어요.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_18.png" alt="Vector"></p>
<p>행렬 X는 이러한 벡터를 각각의 행으로 갖고 있다고 생각할 수 있어요.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_19.png" alt="Matrix X"></p>
<div class="content-ad"></div>
<p>β 계수들의 벡터를 만들어보세요.</p>
<p>Equation 3은 매우 간결하게 다음과 같이 표현될 수 있습니다.</p>
<div class="content-ad"></div>
<p>하지만 각 예측 값 y'_i에 대한 방정식을 모두 예측 값의 벡터로 결합하여보다 간결하게 할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_22.png" alt="image"></p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_23.png" alt="image"></p>
<p>단순 선형 회귀와 마찬가지로, 우리는 제곱 오차의 합 ||y' - y||²를 최소화하려고 합니다.</p>
<div class="content-ad"></div>
<p>등식 4를 사용하여 SSE 공식을 X, y, 그리고 β를 사용하여 확장할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_24.png" alt="Equation-24"></p>
<p>익숙하게 느껴지나요? 이는 단순 선형 회귀에서의 오차 공식과 매우 비슷합니다. 우리는 그것을 최소화하는 β의 값을 찾아야 합니다. 먼저 ||y||² 항은 β에 영향을 미치지 않으므로 무시됩니다. 따라서 실제로 최소화해야 할 값은 다음과 같습니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_25.png" alt="Equation-25"></p>
<div class="content-ad"></div>
<p>우리는 여기서 멈출 수 있습니다. Numpy에는 X와 y만 입력으로 사용하여 β의 적절한 값을 찾을 수 있는 numpy.linalg.lstsq 메서드가 있습니다. 기술적으로는 Python과 Numpy만 사용해야 한다는 내 규칙을 위반하는 것은 아니지만, 이것은 "처음부터 선형 회귀"에 대한 포스트에서 속임수 같아 보입니다. 대신, 수학적인 부분으로 들어가겠습니다.</p>
<p>Expression 2를 최소화하기 위해 그래디언트를 β에 대해 제로로 설정하고 해결해야 합니다. 이를 위해 Expression 2를 구성별 형식으로 변환한 다음, β의 각 구성 요소에 대해 개별적으로 미분을 수행할 것입니다.</p>
<p>점곱의 구성별 공식을 사용하여,</p>
<div class="content-ad"></div>
<p>행렬-벡터 곱셈의 경우,</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_27.png" alt="matrix-vector multiplication"></p>
<p>식 2를 요소별 형태로 변환할 수 있습니다:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_28.png" alt="componentwise form"></p>
<div class="content-ad"></div>
<p>이제 특정  β_l  컴포넌트에 대한 식 3의 미분을 취해 봅시다:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_30.png" alt="image"></p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_31.png" alt="image"></p>
<div class="content-ad"></div>
<p>식 4를 단순화하려면 두 합의 미분을 취해야 합니다:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_32.png" alt="식1"></p>
<p>그리고</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_33.png" alt="식2"></p>
<div class="content-ad"></div>
<p>각각을 개별적으로 다루어 봅시다.</p>
<p>식 5 미분</p>
<p>식 5는 다음과 같이 확장할 수 있습니다:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_34.png" alt="image"></p>
<div class="content-ad"></div>
<p>위의 내용을 번역해 드리겠습니다.</p>
<p>영어로 된 내용은 "j나 k 둘 중 하나가 l과 같지 않은 부분, 그리고 k는 l과 같지만 j는 아닌 부분, 그리고 j가 l과 같지만 k는 아닌 부분, 그리고 j와 k가 모두 l과 같은 부분"을 뜻합니다. j와 k 둘 다 l과 같거나 같지 않아야 하기 때문에, 이 네 항목은 모든 가능성을 포함합니다. 이 모든 부분이 결합하여 Expression 5의 원래 합계와 동일합니다.</p>
<p>Expression 7의 두 가운데 항목은 인덱스 변수의 이름이 다를 뿐 동일합니다 (j vs k). 이름이 임의적이므로 우리는 세 번째 합에 있는 인덱스 변수의 이름을 j로 변경할 수 있으며, 따라서 두 항목은 같은 값을 갖습니다. 따라서 식은 다음과 같이 다시 쓸 수 있습니다.</p>
<img src="/assets/img/2024-06-22-LinearRegressionfromScratch_35.png">
<p>이제 미분을 할 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_36.png" alt="image"></p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_37.png" alt="image"></p>
<p>첫 번째 항이 베타_l을 포함하지 않기 때문에 0이 됩니다.</p>
<p>식 6 미분</p>
<div class="content-ad"></div>
<p>식 6의 도함수를 찾는 것은 훨씬 간단합니다:</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_38.png" alt=""></p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_39" alt=""></p>
<p>여기에서 두 번째 합계를 다시 β_l을 포함하는 부분과 β_l을 포함하지 않는 부분으로 분할하였습니다. 후자는 미분 중에 0으로 만들어집니다.</p>
<div class="content-ad"></div>
<p>모든 것을 합해 봅시다</p>
<p>이제 방금 발견한 미분식, Expression 8과 9를 식 4에 대입하고 간단히 정리해 보겠습니다. 그런 다음, 구성 요소 형식에서 벡터 형식으로 다시 변환할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_40.png" alt="image"></p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_41.png" alt="image"></p>
<div class="content-ad"></div>
<p>이 시점에서 우리는 다음 항등식을 사용할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_42.png" alt="식1"></p>
<p>이를 통해 방정식 5를 더 변형할 수 있습니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_43.png" alt="식2"></p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-22-LinearRegressionfromScratch_44.png">
<p>그러면 끝났어요! 에러의 기울기가 영인 경우 β는 반드시 방정식 6을 따라야 합니다.</p>
<p>기울기를 영으로 설정하면 실제로 최적의 해결책을 보장하는지 궁금할 수 있습니다. 결국, 이는 단지 전역 최소값이 아닌 지역 최소값을 찾을 수도 있습니다. 다행히 선형 회귀는 볼록 최적화 문제입니다. 이 수학 스택 익스체인지 답변에서 증명이 제공됩니다. 볼록 최적화 문제의 중요한 특성 중 하나는 어떤 지역 최소값도 전역 최소값이 될 수 있다는 것이기 때문에 걱정할 것이 없습니다.</p>
<p>솔루션이 올바름을 확인했으므로, 이제 방정식 6을 β에 대해 해결해야 합니다. Numpy에는 numpy.linalg.solve 함수가 제공되지만, 이 방정식이 하나의 솔루션만 있는 경우에만 작동합니다. 다른 옵션으로는 행렬을 축소된 행 사다리꼴 형태로 변환하는 것이 있지만, 놀랍게도 Numpy에는 이를 위한 유틸리티가 없습니다. 일부 조사를 한 결과, numpy.linalg.qr이라는 것을 발견했는데, 이 함수는 입력 행렬의 QR 분해를 수행합니다. 수학 스택 익스체인지의 답변 및 그 댓글이 방정식 풀이에 QR 분해를 사용하는 방법을 배우는 데 도움이 되었습니다.</p>
<div class="content-ad"></div>
<p>만약 A가 정사각 행렬이고(X^TX도 정사각이어야 함), 선형 방정식 Ax = b를 해결하려면 먼저 직교하는 정사각 행렬 Q와 상삼각 행렬 R을 찾아야 합니다. 여기서 QR = A가 성립합니다. Ax = b 방정식은 QRx = b로 변환됩니다. Q가 역행렬이어야 하므로 방정식은 Rx = Q^-1b로 더욱 단순화될 수 있습니다. R은 상삼각 행렬이고, 오른쪽 부분은 단순히 벡터이므로 Uv = w를 해결할 수 있는 능력이 있으면 충분합니다.</p>
<p>저는 작업 수행을 위해 solve_upper_triangular 함수를 만들었습니다. 선형 방정식을 해결하는 방법에 대해서는 자세히 설명하지 않겠습니다. 단순히 행의 마지막에서 시작하여 역방향으로 작업하며, 각 행에서 이전에 설정된 변수 값들을 대체하고, 남은 변수 중 계수가 0이 아닌 변수에 대해 하나를 제외한 모든 변수에 값을 1로 할당하고, 다른 변수들에 대한 마지막 변수를 나머지 변수들을 이용하여 구합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">solve_upper_triangular</span>(a, b):
    <span class="hljs-string">""</span><span class="hljs-string">"
    선형 방정식 ax = b를 x에 대해 해결합니다.
    :param a: 크기가 n x n인 상삼각 행렬.
    :param b: n 차원 벡터.
    :return: ax = b를 만족하는 x 벡터.
    "</span><span class="hljs-string">""</span>

    tracker = np.<span class="hljs-title function_">zeros</span>(a.<span class="hljs-property">shape</span>[<span class="hljs-number">1</span>])
    result = np.<span class="hljs-title function_">zeros</span>(a.<span class="hljs-property">shape</span>[<span class="hljs-number">1</span>])

    <span class="hljs-keyword">for</span> row, val <span class="hljs-keyword">in</span> <span class="hljs-title function_">zip</span>(a[::-<span class="hljs-number">1</span>], b[::-<span class="hljs-number">1</span>]):
        unset_var_indices = np.<span class="hljs-title function_">where</span>((tracker == <span class="hljs-number">0</span>) &#x26; (row != <span class="hljs-number">0</span>))[<span class="hljs-number">0</span>]

        <span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(unset_var_indices) == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">if</span> np.<span class="hljs-title function_">isclose</span>(result @ row, val):
                <span class="hljs-keyword">continue</span>
            <span class="hljs-attr">else</span>:
                raise <span class="hljs-title class_">UnsolvableError</span>(<span class="hljs-string">'주어진 a와 b 값으로 인해 해결할 수 없는 방정식입니다.'</span>)

        tracker[unset_var_indices] = <span class="hljs-number">1</span>
        result[unset_var_indices[<span class="hljs-number">1</span>:]] = <span class="hljs-number">1</span>
        i = unset_var_indices[<span class="hljs-number">0</span>]
        result[i] = (val - result @ row) / row[i]

    <span class="hljs-keyword">return</span> result
</code></pre>
<p>이제 MultipleLinearRegressor를 생성할 준비가 되었습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultipleLinearRegressor</span>:
    <span class="hljs-string">""</span><span class="hljs-string">"다중 선형 회귀를 수행합니다."</span><span class="hljs-string">""</span>

    def <span class="hljs-title function_">__init__</span>(self):
        self.<span class="hljs-property">beta</span> = <span class="hljs-title class_">None</span>
</code></pre>
<p>단순 선형 회귀와 마찬가지로 predict 메서드와 fit 메서드를 갖게 될 것입니다.</p>
<p>predict 메서드는 간단히 행렬 X 또는 벡터 x와 β 사이의 행렬 곱을 계산합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">predict</span>(self, x):
    <span class="hljs-string">""</span><span class="hljs-string">"
    주어진 x값 배열로부터 예측된 y값을 제공합니다.
    :param x: x값의 벡터 또는 행렬.
    :return: 예측된 y값의 벡터.
    "</span><span class="hljs-string">""</span>

    <span class="hljs-keyword">if</span> self.<span class="hljs-property">beta</span> is <span class="hljs-title class_">None</span>:
        raise <span class="hljs-title class_">RegressionModelNotFitError</span>(<span class="hljs-string">'앗! 모델이 적합되지 않았습니다!'</span>)

    <span class="hljs-keyword">return</span> x @ self.<span class="hljs-property">beta</span>
</code></pre>
<div class="content-ad"></div>
<p>fit 메소드는 방정식 6을 해결하기 위해 X^TX를 QR 분해한 다음 solve_upper_triangular을 사용하여 Rβ = Q^-1X^Ty의 해를 찾습니다. 또한 적합한 모델의 제곱 오차의 합을 반환합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">fit</span>(self, x, y):
    <span class="hljs-string">""</span><span class="hljs-string">"
    x-값 행렬과 해당 y-값 벡터를 기반으로 모델을 적합합니다.
    :param x: x-값 행렬.
    :param y: y-값 벡터.
    :return: 적합된 모델의 제곱 오차의 합.
    "</span><span class="hljs-string">""</span>

    x_t = x.<span class="hljs-title function_">transpose</span>()
    q, r = np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">qr</span>(x_t @ x)
    vec = np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">inv</span>(q) @ x_t @ y
    self.<span class="hljs-property">beta</span> = <span class="hljs-title function_">solve_upper_triangular</span>(r, vec)
    diff = self.<span class="hljs-title function_">predict</span>(x) - y
    <span class="hljs-keyword">return</span> diff @ diff
</code></pre>
<p>다중 선형 회귀기의 성능을 살펴봅시다. 이전과 매우 유사한 generate_noisy_data 함수를 만들겠습니다. 이 함수는 매개변수 벡터 β를 받아들이고 X 행렬과 데이터 포인트의 y-값 벡터를 생성한 다음 이전과 같이 각 y-값에 가우시안 노이즈를 추가합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">generate_noisy_data</span>(n_data_points, n_independent_variables, beta, x_range, noise_stddev):
    <span class="hljs-string">""</span><span class="hljs-string">"
    가우시안 노이즈가 추가된 선형 관계를 기반으로 데이터 포인트를 생성합니다.
    :param n_data_points: 생성할 데이터 포인트 수.
    :param n_independent_variables: 각 데이터 포인트에서의 독립 변수 수.
    :param beta: 독립 변수의 계수 벡터.
    :param x_range: x-값을 추출할 범위.
    :param noise_stddev: 각 y-값에 추가할 가우시안 노이즈의 표준 편차.
    :return: x-값 행렬과 y-값 벡터.
    "</span><span class="hljs-string">""</span>

    x = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">uniform</span>(*x_range, (n_data_points, n_independent_variables))
    y = x @ beta + np.<span class="hljs-property">random</span>.<span class="hljs-title function_">normal</span>(scale=noise_stddev, size=n_data_points)
    <span class="hljs-keyword">return</span> x, y
</code></pre>
<div class="content-ad"></div>
<p>이제 데이터를 생성하고 회귀자가 원래 β를 얼마나 잘 복원하는지 살펴보는 시간입니다.</p>
<pre><code class="hljs language-js">regressor = <span class="hljs-title class_">MultipleLinearRegressor</span>()
x, y = <span class="hljs-title function_">generate_noisy_data</span>(n_data_points=<span class="hljs-number">500</span>,
                           n_independent_variables=<span class="hljs-number">10</span>,
                           beta=np.<span class="hljs-title function_">array</span>([-<span class="hljs-number">10</span>, <span class="hljs-number">5</span>, -<span class="hljs-number">8</span>, -<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">5</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">3</span>]),
                           x_range=np.<span class="hljs-title function_">array</span>([-<span class="hljs-number">100</span>, <span class="hljs-number">100</span>]),
                           noise_stddev=<span class="hljs-number">50</span>)
sse = regressor.<span class="hljs-title function_">fit</span>(x, y)
<span class="hljs-title function_">print</span>(f<span class="hljs-string">'Sum Squared Error: {sse}'</span>)
<span class="hljs-title function_">print</span>(f<span class="hljs-string">'Beta: {regressor.beta}'</span>)
</code></pre>
<p>한 번 실행한 결과는 다음과 같습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-title class_">Sum</span> <span class="hljs-title class_">Squared</span> <span class="hljs-title class_">Error</span>: <span class="hljs-number">1259196.6403705715</span>
<span class="hljs-title class_">Beta</span>: [-<span class="hljs-number">9.95436533</span>  <span class="hljs-number">5.02469925</span> -<span class="hljs-number">7.95431319</span> -<span class="hljs-number">1.97266714</span>  <span class="hljs-number">1.03726794</span> -<span class="hljs-number">2.95935233</span>
  <span class="hljs-number">4.03854255</span> -<span class="hljs-number">4.98106051</span> -<span class="hljs-number">1.01840914</span>  <span class="hljs-number">3.0410695</span>]
</code></pre>
<div class="content-ad"></div>
<p>위에서 확인할 수 있듯이, 원래 매개변수 값에 꽤 가까운 결과를 얻는 데 잘 작동하는 것 같습니다.</p>
<h2>편향(bias)에 대하여</h2>
<p>지금까지 y절편이 0인 회귀 모델에 대해만 논의했습니다. 그러나 이는 모든 데이터에 적합한 것은 아닙니다. 만약 x⋅β + b와 같은 모델인 f(x) = x⋅β + b를 원한다면 어떻게 해야 할까요? 여기서 b는 0이 아닌 상수입니다. 기계 학습의 맥락에서 이 값 b를 편향(bias)이라고 부르며, 모든 모델 입력이 0일 때에도 데이터를 특정 값으로 '편향'시킨다는 의미입니다.</p>
<p>이러한 고려 사항은 회귀 모델에 편향을 추가하는 데 많은 노력이 필요하지 않다는 것으로 추가 사항으로 남겨두었습니다: 회귀 모델에 편향을 추가하는 것은 데이터에 항상 1로 설정된 추가 독립 변수를 추가하는 것과 동일합니다. 예를 들어, 우리가 2차원 데이터를 가지고 있고 회귀자에 편향을 추가하려는 경우, f(x) = kx 형태의 모델을 적합시키는 대신에</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_45.png" alt="image"></p>
<p>변수 x_1은 원본 데이터를 나타내고, x_2는 모든 데이터 포인트에서 1로 설정됩니다. 데이터 포인트 (x, y)는 이렇게 (x_1, x_2, y) = (x, 1, y)가 됩니다. x_1 = x이고 x_2 = 1을 대입하면, 방정식 7은 다음과 같이 단순화됩니다.</p>
<p><img src="/assets/img/2024-06-22-LinearRegressionfromScratch_46.png" alt="image"></p>
<p>여기서 β_1은 기울기이고, β_2는 바이어스입니다. 우리는 다중 선형 회귀를 사용하여 이 모델을 적합시킬 수 있습니다. 고차원 데이터의 경우, 이 과정은 비슷하게 작동합니다.</p>
<div class="content-ad"></div>
<h1>결론</h1>
<p>여기서 한 가지! 이것이 바로 처음부터 선형 회귀입니다. 즐겁게 즐겼고 무언가를 배웠으면 좋겠어요. 어떤 피드백과 건설적인 비평도 환영합니다. 다음 포스트에서는 선형 분류에 대해 다룰 예정이니 기대해주세요.</p>
<p>모든 코드는 github에서 확인하실 수 있습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"파이썬으로 선형 회귀 직접 구현하기 기초부터 완성까지","description":"","date":"2024-06-22 04:46","slug":"2024-06-22-LinearRegressionfromScratch","content":"\n\n\n![Linear Regression](/assets/img/2024-06-22-LinearRegressionfromScratch_0.png)\n\n안녕하세요! 가장 간단한 머신 러닝 기술 중 하나인 선형 회귀로 시작합니다. 이 게시물의 수학적 부분은 선형 대수와 미적분의 좋은 이해력이 필요할 것입니다. 이 부분은 다음 시리즈에도 해당될 것이죠. 이는 머신 러닝의 많은 부분을 뒷받침하고 있고, 깊은 이해를 위한 선행 요건입니다. 그렇다면 함께 알아보도록 하죠!\n\n# 단순 선형 회귀\n\n선형 회귀는 여러 점들을 고려하여 최적의 선을 찾는 과제입니다. 최적의 선을 찾는 방법을 알아내기 전에, 이것이 실제로 무엇을 의미하는지를 이해해야 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그림 1의 선이 이 선보다 더 데이터에 잘 맞는 것을 직관적으로 알 수 있어요:\n\n![Figure 1](/assets/img/2024-06-22-LinearRegressionfromScratch_1.png)\n\n왜냐하면 그림 1의 점들이 그림 2의 점들보다 선으로부터 더 멀리 떨어져 있어요. 이 직관을 수학적으로 어떻게 형식화할지 알아보도록 할게요. 이렇게 하면 “가장 잘 맞는”이 무엇을 의미하는지 명확하게 정의할 수 있을 거예요.\n\n간단하고 시각화하기 쉽게 하기 위해, 2차원에서 시작할게요. 이 경우 데이터 포인트는 (x, y) 쌍이고 위의 그림처럼 그래프에 표시할 수 있어요. 우리는 데이터를 가장 잘 나타내는 f(x) = kx와 같은 선형 함수를 찾고 싶어해요. 이 모델은 원점을 통과하는 선을 가정해요. 우리는 아직 원점 이외의 교차점의 가능성에 대해 고려하지 않을 거예요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nn개의 데이터 포인트가 있는 컬렉션이 있다고 가정해 보겠습니다.\n\n![Linear Regression from Scratch 2](/assets/img/2024-06-22-LinearRegressionfromScratch_2.png)\n\n각 x값에 대해 모델을 사용하여 예측된 y값을 얻을 수 있습니다. 이러한 상황은 하나의 독립 변수(x)와 하나의 종속 변수(y)만 있는 단순 선형 회귀로 알려져 있습니다. 예측된 y값과 실제 y값을 구분하기 위해 프라임 기호를 사용할 것입니다. 따라서 특정 x값에 대한 모델의 예측된 y값은 다음과 같은 공식으로 표시됩니다.\n\n![Linear Regression from Scratch 3](/assets/img/2024-06-22-LinearRegressionfromScratch_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 x 값들을 하나의 벡터에, y 값들을 다른 벡터에 넣어봅시다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_4.png)\n\n이를 벡터화(Vectorization)라고 합니다. 데이터 과학 문제에 대한 많은 이점이 있습니다. 여러 개별 값을 벡터로 결합하면 수학 공식이 훨씬 더 간결하고 이해하기 쉬워집니다. 코드에서의 벡터화도 성능을 향상시킵니다. 큰 값 배열에 대해 벡터 산술을 수행하는 것이 각각 하나씩 처리하는 루프를 거쳐 동작하는 것보다 훨씬 빠릅니다. Numpy와 같은 많은 숫자 계산 라이브러리가 빠른 벡터 산술을 위해 설계되었습니다. 벡터화는 또한 GPU와 같은 하드웨어를 사용한 병렬화도 가능하게 합니다. 여러 개의 배열 요소에 동시에 연산을 수행합니다. 한 번 더 언급하자면, 각 연산이 하나씩 차례로 이루어지는 루프를 사용하지 않고는 이것이 불가능할 것입니다.\n\n또한 우리가 예측한 y 값들을 모두 담은 벡터를 생성할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_5.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_6.png\" /\u003e\n\n최적 적합 선을 찾기 위해서는 y'와 실제 값인 y의 벡터 사이의 거리를 알아야 합니다. 이 두 벡터의 차이를 살펴볼 수 있습니다: y' - y. 그러나 이것은 벡터 자체이며, 모델의 오류를 나타내는 단일 숫자를 원합니다. 우리는 제곱합 오류(SSE)를 사용할 것입니다. SSE는 ||y' - y||²로, 차이 벡터의 제곱 크기와 같습니다. 이것은 \"제곱합\"으로 불립니다. 왜냐하면 y' - y의 제곱된 항목들의 합과 같기 때문입니다:\n\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_7.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n왜 ||y’ — y||²을 사용하는지 궁금하다면 단순히 ||y’ — y||만 사용하는 것보다 제곱 크기를 사용하는 게 훨씬 간단하다는 점이 하나의 답일 수 있습니다. ||y’ — y||는 합계 외부에 하나의 추가 제곱근 기호가 있습니다:\n\n\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_8.png\" /\u003e\n\n\n이로 인해 미분을 할 때 공식을 처리하는 것이 훨씬 더 까다로워집니다.\n\n이제 선형 회귀 모델의 오차를 정의했으니, 이를 최소화하는 방법을 찾아야 합니다. SSE에 대한 표현을 확장해 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-06-22-LinearRegressionfromScratch_9.png)\n\n방정식 1에 따라 y'에 kx를 대입하면,\n\n![이미지](/assets/img/2024-06-22-LinearRegressionfromScratch_10.png)\n\nx와 y를 일정한 값으로 유지할 때 오차를 최소화하는 k의 값을 찾아야 합니다. 이를 위해서는 식 1의 k에 대한 미분값을 0으로 설정하고 해를 구할 수 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Linear Regression 1](/assets/img/2024-06-22-LinearRegressionfromScratch_11.png)\n\n![Linear Regression 2](/assets/img/2024-06-22-LinearRegressionfromScratch_12.png)\n\n이를 통해 제곱 오차의 합을 최소화하는 k 값을 알 수 있습니다. 이 지식을 바탕으로 SimpleLinearRegressor를 코딩할 수 있습니다. 이것은 단 하나의 인스턴스 변수를 갖게 될 것입니다 — 기울기, k.\n\n```js\nclass SimpleLinearRegressor:\n    \"\"\"단순 선형 회귀를 수행합니다.\"\"\"\n\n    def __init__(self):\n        self.k = None\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef predict(self, x):\n    \"\"\"\n    입력값 x 또는 x 값 벡터를 사용하여 예측된 y값을 제공합니다.\n    :param x: 입력 값(들).\n    :return: 예측된 y값(들).\n    \"\"\"\n\n    if self.k is None:\n        raise RegressionModelNotFitError('앗! 모델이 아직 피팅되지 않았어요!')\n\n    return self.k * x\n```\n\n또한 x와 y 벡터를 사용하여 Equation 2를 기반으로 k의 적절한 값을 찾는 fit 메서드가 필요합니다. 이것이 클래스의 본질입니다.\n\n```python\ndef fit(self, x, y):\n    \"\"\"\n    주어진 x 값과 y 값 벡터를 기반으로 모델을 맞춥니다.\n    :param x: x값 벡터.\n    :param y: y값 벡터.\n    :return: 적합된 모델의 제곱 오차 합.\n    \"\"\"\n\n    self.k = x @ y / (x @ x)\n    diff = self.predict(x) - y\n    return diff @ diff\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델을 테스트하기 위해 데이터를 생성해야 합니다. 범위 내에서 임의의 x 값을 생성하고 선형 모델을 사용하여 해당하는 y 값을 계산한 다음 이 y 값에 가우시안 노이즈를 추가하는 함수를 만들겠습니다.\n\n```js\ndef generate_noisy_data(n_points, slope, x_range, noise_stddev):\n    \"\"\"\n    추가된 가우시안 노이즈를 이용해 선형 관계에 기반한 데이터 점을 생성합니다.\n    :param n_points: 생성할 데이터 점의 수.\n    :param slope: 직선의 기울기.\n    :param x_range: x 값을 추출할 범위.\n    :param noise_stddev: 각 y 값에 추가할 가우시안 노이즈의 표준 편차.\n    :return: x 값과 y 값의 벡터.\n    \"\"\"\n\n    x = np.random.uniform(*x_range, n_points)\n    y = slope * x + np.random.normal(scale=noise_stddev, size=n_points)\n    return x, y\n```\n\nSimpleLinearRegressor가 무작위로 생성된 데이터에서 원래의 기울기를 얼마나 잘 복원하는지 살펴봅시다. 시각화를 위해 matplotlib를 사용하겠습니다.\n\n```js\nx_range = np.array([0, 5])\nx, y = generate_noisy_data(n_points=20, slope=0.42, x_range=x_range, noise_stddev=0.5)\nplt.scatter(x, y)\n\nregressor = SimpleLinearRegressor()\nfit = regressor.fit(x, y)\nslope = regressor.k\nplt.plot(x_range, [0, 2 * x_range[1]], color='red')\nplt.text(3, 0, f'오차: {\"{:.2f}\".format(fit)}\\n예측된 기울기: {\"{:.2f}\".format(slope)}')\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 한 번의 실행 결과입니다:\n\n![Linear Regression](/assets/img/2024-06-22-LinearRegressionfromScratch_13.png)\n\n보이시다시피, 이 모델은 훌륭한 작업을 합니다! 회귀 모델에 의해 예측된 기울기는 `generate_noisy_data`에 입력된 기울기와 소수점 셋째 자리까지 일치합니다.\n\n# 다중 선형 회귀\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하나의 독립 변수 x와 하나의 종속 변수 y로 선형 회귀를 수행하는 방법을 배웠습니다. 이제 y가 m개의 독립 변수에 의존한다고 가정해 보겠습니다. 따라서 우리는 (m + 1)차원 데이터를 다루게 됩니다. 우리가 가진 데이터가 다음과 같은 n개의 데이터 포인트일 수 있습니다:\n\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_14.png)\n\n\n여기서 x_ij는 i번째 데이터 포인트에서 j번째 독립 변수의 값을 나타냅니다.\n\n데이터를 벡터화하여 정리하는 것은 항상 좋은 첫 번째 단계입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 이전과 마찬가지로 모든 y 값들을 벡터로 모아낼 수 있어요:\n\n이제는 x 데이터가 두 개의 인덱스를 가지고 있기 때문에 xs에 대해 벡터를 사용하는 것이 더 이상이 아니에요. 대신, 각 행이 하나의 데이터 포인트인 행렬로 모아낼 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제부터 이 행렬의 항목을 나타내는 변수로 대문자 X_ij와 소문자 x_ij를 서로 바꿔 사용할 거에요.\n\n지금 데이터에 맞추려고 하는 선형 모델은 조금 더 복잡해 보여요:\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_17.png)\n\n독립 변수 각각에 대한 계수 또는 \"기울기\"인 βs가 있는 m개의 계수가 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 데이터 포인트의 벡터를 만들 수 있어요.\n\n![Vector](/assets/img/2024-06-22-LinearRegressionfromScratch_18.png)\n\n행렬 X는 이러한 벡터를 각각의 행으로 갖고 있다고 생각할 수 있어요.\n\n![Matrix X](/assets/img/2024-06-22-LinearRegressionfromScratch_19.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nβ 계수들의 벡터를 만들어보세요.\n\n\nEquation 3은 매우 간결하게 다음과 같이 표현될 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 각 예측 값 y'_i에 대한 방정식을 모두 예측 값의 벡터로 결합하여보다 간결하게 할 수 있습니다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_22.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_23.png)\n\n단순 선형 회귀와 마찬가지로, 우리는 제곱 오차의 합 ||y' - y||²를 최소화하려고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n등식 4를 사용하여 SSE 공식을 X, y, 그리고 β를 사용하여 확장할 수 있습니다.\n\n![Equation-24](/assets/img/2024-06-22-LinearRegressionfromScratch_24.png)\n\n익숙하게 느껴지나요? 이는 단순 선형 회귀에서의 오차 공식과 매우 비슷합니다. 우리는 그것을 최소화하는 β의 값을 찾아야 합니다. 먼저 ||y||² 항은 β에 영향을 미치지 않으므로 무시됩니다. 따라서 실제로 최소화해야 할 값은 다음과 같습니다.\n\n![Equation-25](/assets/img/2024-06-22-LinearRegressionfromScratch_25.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 여기서 멈출 수 있습니다. Numpy에는 X와 y만 입력으로 사용하여 β의 적절한 값을 찾을 수 있는 numpy.linalg.lstsq 메서드가 있습니다. 기술적으로는 Python과 Numpy만 사용해야 한다는 내 규칙을 위반하는 것은 아니지만, 이것은 \"처음부터 선형 회귀\"에 대한 포스트에서 속임수 같아 보입니다. 대신, 수학적인 부분으로 들어가겠습니다.\n\nExpression 2를 최소화하기 위해 그래디언트를 β에 대해 제로로 설정하고 해결해야 합니다. 이를 위해 Expression 2를 구성별 형식으로 변환한 다음, β의 각 구성 요소에 대해 개별적으로 미분을 수행할 것입니다.\n\n점곱의 구성별 공식을 사용하여,\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n행렬-벡터 곱셈의 경우,\n\n![matrix-vector multiplication](/assets/img/2024-06-22-LinearRegressionfromScratch_27.png)\n\n식 2를 요소별 형태로 변환할 수 있습니다:\n\n![componentwise form](/assets/img/2024-06-22-LinearRegressionfromScratch_28.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 특정  β_l  컴포넌트에 대한 식 3의 미분을 취해 봅시다:\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_30.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_31.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n식 4를 단순화하려면 두 합의 미분을 취해야 합니다:\n\n![식1](/assets/img/2024-06-22-LinearRegressionfromScratch_32.png)\n\n그리고\n\n![식2](/assets/img/2024-06-22-LinearRegressionfromScratch_33.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각각을 개별적으로 다루어 봅시다.\n\n식 5 미분\n\n식 5는 다음과 같이 확장할 수 있습니다:\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_34.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 내용을 번역해 드리겠습니다.\n\n영어로 된 내용은 \"j나 k 둘 중 하나가 l과 같지 않은 부분, 그리고 k는 l과 같지만 j는 아닌 부분, 그리고 j가 l과 같지만 k는 아닌 부분, 그리고 j와 k가 모두 l과 같은 부분\"을 뜻합니다. j와 k 둘 다 l과 같거나 같지 않아야 하기 때문에, 이 네 항목은 모든 가능성을 포함합니다. 이 모든 부분이 결합하여 Expression 5의 원래 합계와 동일합니다.\n\nExpression 7의 두 가운데 항목은 인덱스 변수의 이름이 다를 뿐 동일합니다 (j vs k). 이름이 임의적이므로 우리는 세 번째 합에 있는 인덱스 변수의 이름을 j로 변경할 수 있으며, 따라서 두 항목은 같은 값을 갖습니다. 따라서 식은 다음과 같이 다시 쓸 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_35.png\" /\u003e\n\n이제 미분을 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_36.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_37.png)\n\n첫 번째 항이 베타_l을 포함하지 않기 때문에 0이 됩니다.\n\n식 6 미분\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n식 6의 도함수를 찾는 것은 훨씬 간단합니다:\n\n![](/assets/img/2024-06-22-LinearRegressionfromScratch_38.png)\n\n![](/assets/img/2024-06-22-LinearRegressionfromScratch_39)\n\n여기에서 두 번째 합계를 다시 β_l을 포함하는 부분과 β_l을 포함하지 않는 부분으로 분할하였습니다. 후자는 미분 중에 0으로 만들어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 것을 합해 봅시다\n\n이제 방금 발견한 미분식, Expression 8과 9를 식 4에 대입하고 간단히 정리해 보겠습니다. 그런 다음, 구성 요소 형식에서 벡터 형식으로 다시 변환할 수 있습니다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_40.png)\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_41.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 시점에서 우리는 다음 항등식을 사용할 수 있습니다.\n\n\n![식1](/assets/img/2024-06-22-LinearRegressionfromScratch_42.png)\n\n\n이를 통해 방정식 5를 더 변형할 수 있습니다.\n\n\n![식2](/assets/img/2024-06-22-LinearRegressionfromScratch_43.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_44.png\" /\u003e\n\n그러면 끝났어요! 에러의 기울기가 영인 경우 β는 반드시 방정식 6을 따라야 합니다.\n\n기울기를 영으로 설정하면 실제로 최적의 해결책을 보장하는지 궁금할 수 있습니다. 결국, 이는 단지 전역 최소값이 아닌 지역 최소값을 찾을 수도 있습니다. 다행히 선형 회귀는 볼록 최적화 문제입니다. 이 수학 스택 익스체인지 답변에서 증명이 제공됩니다. 볼록 최적화 문제의 중요한 특성 중 하나는 어떤 지역 최소값도 전역 최소값이 될 수 있다는 것이기 때문에 걱정할 것이 없습니다.\n\n솔루션이 올바름을 확인했으므로, 이제 방정식 6을 β에 대해 해결해야 합니다. Numpy에는 numpy.linalg.solve 함수가 제공되지만, 이 방정식이 하나의 솔루션만 있는 경우에만 작동합니다. 다른 옵션으로는 행렬을 축소된 행 사다리꼴 형태로 변환하는 것이 있지만, 놀랍게도 Numpy에는 이를 위한 유틸리티가 없습니다. 일부 조사를 한 결과, numpy.linalg.qr이라는 것을 발견했는데, 이 함수는 입력 행렬의 QR 분해를 수행합니다. 수학 스택 익스체인지의 답변 및 그 댓글이 방정식 풀이에 QR 분해를 사용하는 방법을 배우는 데 도움이 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 A가 정사각 행렬이고(X^TX도 정사각이어야 함), 선형 방정식 Ax = b를 해결하려면 먼저 직교하는 정사각 행렬 Q와 상삼각 행렬 R을 찾아야 합니다. 여기서 QR = A가 성립합니다. Ax = b 방정식은 QRx = b로 변환됩니다. Q가 역행렬이어야 하므로 방정식은 Rx = Q^-1b로 더욱 단순화될 수 있습니다. R은 상삼각 행렬이고, 오른쪽 부분은 단순히 벡터이므로 Uv = w를 해결할 수 있는 능력이 있으면 충분합니다.\n\n저는 작업 수행을 위해 solve_upper_triangular 함수를 만들었습니다. 선형 방정식을 해결하는 방법에 대해서는 자세히 설명하지 않겠습니다. 단순히 행의 마지막에서 시작하여 역방향으로 작업하며, 각 행에서 이전에 설정된 변수 값들을 대체하고, 남은 변수 중 계수가 0이 아닌 변수에 대해 하나를 제외한 모든 변수에 값을 1로 할당하고, 다른 변수들에 대한 마지막 변수를 나머지 변수들을 이용하여 구합니다.\n\n```js\ndef solve_upper_triangular(a, b):\n    \"\"\"\n    선형 방정식 ax = b를 x에 대해 해결합니다.\n    :param a: 크기가 n x n인 상삼각 행렬.\n    :param b: n 차원 벡터.\n    :return: ax = b를 만족하는 x 벡터.\n    \"\"\"\n\n    tracker = np.zeros(a.shape[1])\n    result = np.zeros(a.shape[1])\n\n    for row, val in zip(a[::-1], b[::-1]):\n        unset_var_indices = np.where((tracker == 0) \u0026 (row != 0))[0]\n\n        if len(unset_var_indices) == 0:\n            if np.isclose(result @ row, val):\n                continue\n            else:\n                raise UnsolvableError('주어진 a와 b 값으로 인해 해결할 수 없는 방정식입니다.')\n\n        tracker[unset_var_indices] = 1\n        result[unset_var_indices[1:]] = 1\n        i = unset_var_indices[0]\n        result[i] = (val - result @ row) / row[i]\n\n    return result\n```\n\n이제 MultipleLinearRegressor를 생성할 준비가 되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nclass MultipleLinearRegressor:\n    \"\"\"다중 선형 회귀를 수행합니다.\"\"\"\n\n    def __init__(self):\n        self.beta = None\n```\n\n단순 선형 회귀와 마찬가지로 predict 메서드와 fit 메서드를 갖게 될 것입니다.\n\npredict 메서드는 간단히 행렬 X 또는 벡터 x와 β 사이의 행렬 곱을 계산합니다.\n\n```js\ndef predict(self, x):\n    \"\"\"\n    주어진 x값 배열로부터 예측된 y값을 제공합니다.\n    :param x: x값의 벡터 또는 행렬.\n    :return: 예측된 y값의 벡터.\n    \"\"\"\n\n    if self.beta is None:\n        raise RegressionModelNotFitError('앗! 모델이 적합되지 않았습니다!')\n\n    return x @ self.beta\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nfit 메소드는 방정식 6을 해결하기 위해 X^TX를 QR 분해한 다음 solve_upper_triangular을 사용하여 Rβ = Q^-1X^Ty의 해를 찾습니다. 또한 적합한 모델의 제곱 오차의 합을 반환합니다.\n\n```js\ndef fit(self, x, y):\n    \"\"\"\n    x-값 행렬과 해당 y-값 벡터를 기반으로 모델을 적합합니다.\n    :param x: x-값 행렬.\n    :param y: y-값 벡터.\n    :return: 적합된 모델의 제곱 오차의 합.\n    \"\"\"\n\n    x_t = x.transpose()\n    q, r = np.linalg.qr(x_t @ x)\n    vec = np.linalg.inv(q) @ x_t @ y\n    self.beta = solve_upper_triangular(r, vec)\n    diff = self.predict(x) - y\n    return diff @ diff\n```\n\n다중 선형 회귀기의 성능을 살펴봅시다. 이전과 매우 유사한 generate_noisy_data 함수를 만들겠습니다. 이 함수는 매개변수 벡터 β를 받아들이고 X 행렬과 데이터 포인트의 y-값 벡터를 생성한 다음 이전과 같이 각 y-값에 가우시안 노이즈를 추가합니다.\n\n```js\ndef generate_noisy_data(n_data_points, n_independent_variables, beta, x_range, noise_stddev):\n    \"\"\"\n    가우시안 노이즈가 추가된 선형 관계를 기반으로 데이터 포인트를 생성합니다.\n    :param n_data_points: 생성할 데이터 포인트 수.\n    :param n_independent_variables: 각 데이터 포인트에서의 독립 변수 수.\n    :param beta: 독립 변수의 계수 벡터.\n    :param x_range: x-값을 추출할 범위.\n    :param noise_stddev: 각 y-값에 추가할 가우시안 노이즈의 표준 편차.\n    :return: x-값 행렬과 y-값 벡터.\n    \"\"\"\n\n    x = np.random.uniform(*x_range, (n_data_points, n_independent_variables))\n    y = x @ beta + np.random.normal(scale=noise_stddev, size=n_data_points)\n    return x, y\n```  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 데이터를 생성하고 회귀자가 원래 β를 얼마나 잘 복원하는지 살펴보는 시간입니다.\n\n```js\nregressor = MultipleLinearRegressor()\nx, y = generate_noisy_data(n_data_points=500,\n                           n_independent_variables=10,\n                           beta=np.array([-10, 5, -8, -2, 1, -3, 4, -5, -1, 3]),\n                           x_range=np.array([-100, 100]),\n                           noise_stddev=50)\nsse = regressor.fit(x, y)\nprint(f'Sum Squared Error: {sse}')\nprint(f'Beta: {regressor.beta}')\n```\n\n한 번 실행한 결과는 다음과 같습니다.\n\n```js\nSum Squared Error: 1259196.6403705715\nBeta: [-9.95436533  5.02469925 -7.95431319 -1.97266714  1.03726794 -2.95935233\n  4.03854255 -4.98106051 -1.01840914  3.0410695]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에서 확인할 수 있듯이, 원래 매개변수 값에 꽤 가까운 결과를 얻는 데 잘 작동하는 것 같습니다.\n\n## 편향(bias)에 대하여\n\n지금까지 y절편이 0인 회귀 모델에 대해만 논의했습니다. 그러나 이는 모든 데이터에 적합한 것은 아닙니다. 만약 x⋅β + b와 같은 모델인 f(x) = x⋅β + b를 원한다면 어떻게 해야 할까요? 여기서 b는 0이 아닌 상수입니다. 기계 학습의 맥락에서 이 값 b를 편향(bias)이라고 부르며, 모든 모델 입력이 0일 때에도 데이터를 특정 값으로 '편향'시킨다는 의미입니다.\n\n이러한 고려 사항은 회귀 모델에 편향을 추가하는 데 많은 노력이 필요하지 않다는 것으로 추가 사항으로 남겨두었습니다: 회귀 모델에 편향을 추가하는 것은 데이터에 항상 1로 설정된 추가 독립 변수를 추가하는 것과 동일합니다. 예를 들어, 우리가 2차원 데이터를 가지고 있고 회귀자에 편향을 추가하려는 경우, f(x) = kx 형태의 모델을 적합시키는 대신에\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_45.png)\n\n변수 x_1은 원본 데이터를 나타내고, x_2는 모든 데이터 포인트에서 1로 설정됩니다. 데이터 포인트 (x, y)는 이렇게 (x_1, x_2, y) = (x, 1, y)가 됩니다. x_1 = x이고 x_2 = 1을 대입하면, 방정식 7은 다음과 같이 단순화됩니다.\n\n![image](/assets/img/2024-06-22-LinearRegressionfromScratch_46.png)\n\n여기서 β_1은 기울기이고, β_2는 바이어스입니다. 우리는 다중 선형 회귀를 사용하여 이 모델을 적합시킬 수 있습니다. 고차원 데이터의 경우, 이 과정은 비슷하게 작동합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n여기서 한 가지! 이것이 바로 처음부터 선형 회귀입니다. 즐겁게 즐겼고 무언가를 배웠으면 좋겠어요. 어떤 피드백과 건설적인 비평도 환영합니다. 다음 포스트에서는 선형 분류에 대해 다룰 예정이니 기대해주세요.\n\n모든 코드는 github에서 확인하실 수 있습니다.","ogImage":{"url":"/assets/img/2024-06-22-LinearRegressionfromScratch_0.png"},"coverImage":"/assets/img/2024-06-22-LinearRegressionfromScratch_0.png","tag":["Tech"],"readingTime":16},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_0.png\" alt=\"Linear Regression\"\u003e\u003c/p\u003e\n\u003cp\u003e안녕하세요! 가장 간단한 머신 러닝 기술 중 하나인 선형 회귀로 시작합니다. 이 게시물의 수학적 부분은 선형 대수와 미적분의 좋은 이해력이 필요할 것입니다. 이 부분은 다음 시리즈에도 해당될 것이죠. 이는 머신 러닝의 많은 부분을 뒷받침하고 있고, 깊은 이해를 위한 선행 요건입니다. 그렇다면 함께 알아보도록 하죠!\u003c/p\u003e\n\u003ch1\u003e단순 선형 회귀\u003c/h1\u003e\n\u003cp\u003e선형 회귀는 여러 점들을 고려하여 최적의 선을 찾는 과제입니다. 최적의 선을 찾는 방법을 알아내기 전에, 이것이 실제로 무엇을 의미하는지를 이해해야 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e그림 1의 선이 이 선보다 더 데이터에 잘 맞는 것을 직관적으로 알 수 있어요:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_1.png\" alt=\"Figure 1\"\u003e\u003c/p\u003e\n\u003cp\u003e왜냐하면 그림 1의 점들이 그림 2의 점들보다 선으로부터 더 멀리 떨어져 있어요. 이 직관을 수학적으로 어떻게 형식화할지 알아보도록 할게요. 이렇게 하면 “가장 잘 맞는”이 무엇을 의미하는지 명확하게 정의할 수 있을 거예요.\u003c/p\u003e\n\u003cp\u003e간단하고 시각화하기 쉽게 하기 위해, 2차원에서 시작할게요. 이 경우 데이터 포인트는 (x, y) 쌍이고 위의 그림처럼 그래프에 표시할 수 있어요. 우리는 데이터를 가장 잘 나타내는 f(x) = kx와 같은 선형 함수를 찾고 싶어해요. 이 모델은 원점을 통과하는 선을 가정해요. 우리는 아직 원점 이외의 교차점의 가능성에 대해 고려하지 않을 거예요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003en개의 데이터 포인트가 있는 컬렉션이 있다고 가정해 보겠습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_2.png\" alt=\"Linear Regression from Scratch 2\"\u003e\u003c/p\u003e\n\u003cp\u003e각 x값에 대해 모델을 사용하여 예측된 y값을 얻을 수 있습니다. 이러한 상황은 하나의 독립 변수(x)와 하나의 종속 변수(y)만 있는 단순 선형 회귀로 알려져 있습니다. 예측된 y값과 실제 y값을 구분하기 위해 프라임 기호를 사용할 것입니다. 따라서 특정 x값에 대한 모델의 예측된 y값은 다음과 같은 공식으로 표시됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_3.png\" alt=\"Linear Regression from Scratch 3\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 x 값들을 하나의 벡터에, y 값들을 다른 벡터에 넣어봅시다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이를 벡터화(Vectorization)라고 합니다. 데이터 과학 문제에 대한 많은 이점이 있습니다. 여러 개별 값을 벡터로 결합하면 수학 공식이 훨씬 더 간결하고 이해하기 쉬워집니다. 코드에서의 벡터화도 성능을 향상시킵니다. 큰 값 배열에 대해 벡터 산술을 수행하는 것이 각각 하나씩 처리하는 루프를 거쳐 동작하는 것보다 훨씬 빠릅니다. Numpy와 같은 많은 숫자 계산 라이브러리가 빠른 벡터 산술을 위해 설계되었습니다. 벡터화는 또한 GPU와 같은 하드웨어를 사용한 병렬화도 가능하게 합니다. 여러 개의 배열 요소에 동시에 연산을 수행합니다. 한 번 더 언급하자면, 각 연산이 하나씩 차례로 이루어지는 루프를 사용하지 않고는 이것이 불가능할 것입니다.\u003c/p\u003e\n\u003cp\u003e또한 우리가 예측한 y 값들을 모두 담은 벡터를 생성할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_5.png\"\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_6.png\"\u003e\n\u003cp\u003e최적 적합 선을 찾기 위해서는 y'와 실제 값인 y의 벡터 사이의 거리를 알아야 합니다. 이 두 벡터의 차이를 살펴볼 수 있습니다: y' - y. 그러나 이것은 벡터 자체이며, 모델의 오류를 나타내는 단일 숫자를 원합니다. 우리는 제곱합 오류(SSE)를 사용할 것입니다. SSE는 ||y' - y||²로, 차이 벡터의 제곱 크기와 같습니다. 이것은 \"제곱합\"으로 불립니다. 왜냐하면 y' - y의 제곱된 항목들의 합과 같기 때문입니다:\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_7.png\"\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e왜 ||y’ — y||²을 사용하는지 궁금하다면 단순히 ||y’ — y||만 사용하는 것보다 제곱 크기를 사용하는 게 훨씬 간단하다는 점이 하나의 답일 수 있습니다. ||y’ — y||는 합계 외부에 하나의 추가 제곱근 기호가 있습니다:\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_8.png\"\u003e\n\u003cp\u003e이로 인해 미분을 할 때 공식을 처리하는 것이 훨씬 더 까다로워집니다.\u003c/p\u003e\n\u003cp\u003e이제 선형 회귀 모델의 오차를 정의했으니, 이를 최소화하는 방법을 찾아야 합니다. SSE에 대한 표현을 확장해 보겠습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_9.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e방정식 1에 따라 y'에 kx를 대입하면,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_10.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003ex와 y를 일정한 값으로 유지할 때 오차를 최소화하는 k의 값을 찾아야 합니다. 이를 위해서는 식 1의 k에 대한 미분값을 0으로 설정하고 해를 구할 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_11.png\" alt=\"Linear Regression 1\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_12.png\" alt=\"Linear Regression 2\"\u003e\u003c/p\u003e\n\u003cp\u003e이를 통해 제곱 오차의 합을 최소화하는 k 값을 알 수 있습니다. 이 지식을 바탕으로 SimpleLinearRegressor를 코딩할 수 있습니다. 이것은 단 하나의 인스턴스 변수를 갖게 될 것입니다 — 기울기, k.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSimpleLinearRegressor\u003c/span\u003e:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"단순 선형 회귀를 수행합니다.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        self.\u003cspan class=\"hljs-property\"\u003ek\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    입력값 x 또는 x 값 벡터를 사용하여 예측된 y값을 제공합니다.\n    :param x: 입력 값(들).\n    :return: 예측된 y값(들).\n    \"\"\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.k \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003eraise\u003c/span\u003e RegressionModelNotFitError(\u003cspan class=\"hljs-string\"\u003e'앗! 모델이 아직 피팅되지 않았어요!'\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.k * x\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e또한 x와 y 벡터를 사용하여 Equation 2를 기반으로 k의 적절한 값을 찾는 fit 메서드가 필요합니다. 이것이 클래스의 본질입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x, y\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    주어진 x 값과 y 값 벡터를 기반으로 모델을 맞춥니다.\n    :param x: x값 벡터.\n    :param y: y값 벡터.\n    :return: 적합된 모델의 제곱 오차 합.\n    \"\"\"\u003c/span\u003e\n\n    self.k = x @ y / (x @ x)\n    diff = self.predict(x) - y\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e diff @ diff\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e모델을 테스트하기 위해 데이터를 생성해야 합니다. 범위 내에서 임의의 x 값을 생성하고 선형 모델을 사용하여 해당하는 y 값을 계산한 다음 이 y 값에 가우시안 노이즈를 추가하는 함수를 만들겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003egenerate_noisy_data\u003c/span\u003e(n_points, slope, x_range, noise_stddev):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    추가된 가우시안 노이즈를 이용해 선형 관계에 기반한 데이터 점을 생성합니다.\n    :param n_points: 생성할 데이터 점의 수.\n    :param slope: 직선의 기울기.\n    :param x_range: x 값을 추출할 범위.\n    :param noise_stddev: 각 y 값에 추가할 가우시안 노이즈의 표준 편차.\n    :return: x 값과 y 값의 벡터.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    x = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(*x_range, n_points)\n    y = slope * x + np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enormal\u003c/span\u003e(scale=noise_stddev, size=n_points)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e x, y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimpleLinearRegressor가 무작위로 생성된 데이터에서 원래의 기울기를 얼마나 잘 복원하는지 살펴봅시다. 시각화를 위해 matplotlib를 사용하겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ex_range = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e])\nx, y = \u003cspan class=\"hljs-title function_\"\u003egenerate_noisy_data\u003c/span\u003e(n_points=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e, slope=\u003cspan class=\"hljs-number\"\u003e0.42\u003c/span\u003e, x_range=x_range, noise_stddev=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003escatter\u003c/span\u003e(x, y)\n\nregressor = \u003cspan class=\"hljs-title class_\"\u003eSimpleLinearRegressor\u003c/span\u003e()\nfit = regressor.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(x, y)\nslope = regressor.\u003cspan class=\"hljs-property\"\u003ek\u003c/span\u003e\nplt.\u003cspan class=\"hljs-title function_\"\u003eplot\u003c/span\u003e(x_range, [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * x_range[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]], color=\u003cspan class=\"hljs-string\"\u003e'red'\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003etext\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, f\u003cspan class=\"hljs-string\"\u003e'오차: {\"{:.2f}\".format(fit)}\\n예측된 기울기: {\"{:.2f}\".format(slope)}'\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eshow\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e아래는 한 번의 실행 결과입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_13.png\" alt=\"Linear Regression\"\u003e\u003c/p\u003e\n\u003cp\u003e보이시다시피, 이 모델은 훌륭한 작업을 합니다! 회귀 모델에 의해 예측된 기울기는 \u003ccode\u003egenerate_noisy_data\u003c/code\u003e에 입력된 기울기와 소수점 셋째 자리까지 일치합니다.\u003c/p\u003e\n\u003ch1\u003e다중 선형 회귀\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e하나의 독립 변수 x와 하나의 종속 변수 y로 선형 회귀를 수행하는 방법을 배웠습니다. 이제 y가 m개의 독립 변수에 의존한다고 가정해 보겠습니다. 따라서 우리는 (m + 1)차원 데이터를 다루게 됩니다. 우리가 가진 데이터가 다음과 같은 n개의 데이터 포인트일 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_14.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 x_ij는 i번째 데이터 포인트에서 j번째 독립 변수의 값을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e데이터를 벡터화하여 정리하는 것은 항상 좋은 첫 번째 단계입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 이전과 마찬가지로 모든 y 값들을 벡터로 모아낼 수 있어요:\u003c/p\u003e\n\u003cp\u003e이제는 x 데이터가 두 개의 인덱스를 가지고 있기 때문에 xs에 대해 벡터를 사용하는 것이 더 이상이 아니에요. 대신, 각 행이 하나의 데이터 포인트인 행렬로 모아낼 수 있어요:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제부터 이 행렬의 항목을 나타내는 변수로 대문자 X_ij와 소문자 x_ij를 서로 바꿔 사용할 거에요.\u003c/p\u003e\n\u003cp\u003e지금 데이터에 맞추려고 하는 선형 모델은 조금 더 복잡해 보여요:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_17.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e독립 변수 각각에 대한 계수 또는 \"기울기\"인 βs가 있는 m개의 계수가 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e각 데이터 포인트의 벡터를 만들 수 있어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_18.png\" alt=\"Vector\"\u003e\u003c/p\u003e\n\u003cp\u003e행렬 X는 이러한 벡터를 각각의 행으로 갖고 있다고 생각할 수 있어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_19.png\" alt=\"Matrix X\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eβ 계수들의 벡터를 만들어보세요.\u003c/p\u003e\n\u003cp\u003eEquation 3은 매우 간결하게 다음과 같이 표현될 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e하지만 각 예측 값 y'_i에 대한 방정식을 모두 예측 값의 벡터로 결합하여보다 간결하게 할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_22.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_23.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e단순 선형 회귀와 마찬가지로, 우리는 제곱 오차의 합 ||y' - y||²를 최소화하려고 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e등식 4를 사용하여 SSE 공식을 X, y, 그리고 β를 사용하여 확장할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_24.png\" alt=\"Equation-24\"\u003e\u003c/p\u003e\n\u003cp\u003e익숙하게 느껴지나요? 이는 단순 선형 회귀에서의 오차 공식과 매우 비슷합니다. 우리는 그것을 최소화하는 β의 값을 찾아야 합니다. 먼저 ||y||² 항은 β에 영향을 미치지 않으므로 무시됩니다. 따라서 실제로 최소화해야 할 값은 다음과 같습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_25.png\" alt=\"Equation-25\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리는 여기서 멈출 수 있습니다. Numpy에는 X와 y만 입력으로 사용하여 β의 적절한 값을 찾을 수 있는 numpy.linalg.lstsq 메서드가 있습니다. 기술적으로는 Python과 Numpy만 사용해야 한다는 내 규칙을 위반하는 것은 아니지만, 이것은 \"처음부터 선형 회귀\"에 대한 포스트에서 속임수 같아 보입니다. 대신, 수학적인 부분으로 들어가겠습니다.\u003c/p\u003e\n\u003cp\u003eExpression 2를 최소화하기 위해 그래디언트를 β에 대해 제로로 설정하고 해결해야 합니다. 이를 위해 Expression 2를 구성별 형식으로 변환한 다음, β의 각 구성 요소에 대해 개별적으로 미분을 수행할 것입니다.\u003c/p\u003e\n\u003cp\u003e점곱의 구성별 공식을 사용하여,\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e행렬-벡터 곱셈의 경우,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_27.png\" alt=\"matrix-vector multiplication\"\u003e\u003c/p\u003e\n\u003cp\u003e식 2를 요소별 형태로 변환할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_28.png\" alt=\"componentwise form\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 특정  β_l  컴포넌트에 대한 식 3의 미분을 취해 봅시다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_30.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_31.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e식 4를 단순화하려면 두 합의 미분을 취해야 합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_32.png\" alt=\"식1\"\u003e\u003c/p\u003e\n\u003cp\u003e그리고\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_33.png\" alt=\"식2\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e각각을 개별적으로 다루어 봅시다.\u003c/p\u003e\n\u003cp\u003e식 5 미분\u003c/p\u003e\n\u003cp\u003e식 5는 다음과 같이 확장할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_34.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위의 내용을 번역해 드리겠습니다.\u003c/p\u003e\n\u003cp\u003e영어로 된 내용은 \"j나 k 둘 중 하나가 l과 같지 않은 부분, 그리고 k는 l과 같지만 j는 아닌 부분, 그리고 j가 l과 같지만 k는 아닌 부분, 그리고 j와 k가 모두 l과 같은 부분\"을 뜻합니다. j와 k 둘 다 l과 같거나 같지 않아야 하기 때문에, 이 네 항목은 모든 가능성을 포함합니다. 이 모든 부분이 결합하여 Expression 5의 원래 합계와 동일합니다.\u003c/p\u003e\n\u003cp\u003eExpression 7의 두 가운데 항목은 인덱스 변수의 이름이 다를 뿐 동일합니다 (j vs k). 이름이 임의적이므로 우리는 세 번째 합에 있는 인덱스 변수의 이름을 j로 변경할 수 있으며, 따라서 두 항목은 같은 값을 갖습니다. 따라서 식은 다음과 같이 다시 쓸 수 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_35.png\"\u003e\n\u003cp\u003e이제 미분을 할 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_36.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_37.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e첫 번째 항이 베타_l을 포함하지 않기 때문에 0이 됩니다.\u003c/p\u003e\n\u003cp\u003e식 6 미분\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e식 6의 도함수를 찾는 것은 훨씬 간단합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_38.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_39\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e여기에서 두 번째 합계를 다시 β_l을 포함하는 부분과 β_l을 포함하지 않는 부분으로 분할하였습니다. 후자는 미분 중에 0으로 만들어집니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e모든 것을 합해 봅시다\u003c/p\u003e\n\u003cp\u003e이제 방금 발견한 미분식, Expression 8과 9를 식 4에 대입하고 간단히 정리해 보겠습니다. 그런 다음, 구성 요소 형식에서 벡터 형식으로 다시 변환할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_40.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_41.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 시점에서 우리는 다음 항등식을 사용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_42.png\" alt=\"식1\"\u003e\u003c/p\u003e\n\u003cp\u003e이를 통해 방정식 5를 더 변형할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_43.png\" alt=\"식2\"\u003e\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_44.png\"\u003e\n\u003cp\u003e그러면 끝났어요! 에러의 기울기가 영인 경우 β는 반드시 방정식 6을 따라야 합니다.\u003c/p\u003e\n\u003cp\u003e기울기를 영으로 설정하면 실제로 최적의 해결책을 보장하는지 궁금할 수 있습니다. 결국, 이는 단지 전역 최소값이 아닌 지역 최소값을 찾을 수도 있습니다. 다행히 선형 회귀는 볼록 최적화 문제입니다. 이 수학 스택 익스체인지 답변에서 증명이 제공됩니다. 볼록 최적화 문제의 중요한 특성 중 하나는 어떤 지역 최소값도 전역 최소값이 될 수 있다는 것이기 때문에 걱정할 것이 없습니다.\u003c/p\u003e\n\u003cp\u003e솔루션이 올바름을 확인했으므로, 이제 방정식 6을 β에 대해 해결해야 합니다. Numpy에는 numpy.linalg.solve 함수가 제공되지만, 이 방정식이 하나의 솔루션만 있는 경우에만 작동합니다. 다른 옵션으로는 행렬을 축소된 행 사다리꼴 형태로 변환하는 것이 있지만, 놀랍게도 Numpy에는 이를 위한 유틸리티가 없습니다. 일부 조사를 한 결과, numpy.linalg.qr이라는 것을 발견했는데, 이 함수는 입력 행렬의 QR 분해를 수행합니다. 수학 스택 익스체인지의 답변 및 그 댓글이 방정식 풀이에 QR 분해를 사용하는 방법을 배우는 데 도움이 되었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e만약 A가 정사각 행렬이고(X^TX도 정사각이어야 함), 선형 방정식 Ax = b를 해결하려면 먼저 직교하는 정사각 행렬 Q와 상삼각 행렬 R을 찾아야 합니다. 여기서 QR = A가 성립합니다. Ax = b 방정식은 QRx = b로 변환됩니다. Q가 역행렬이어야 하므로 방정식은 Rx = Q^-1b로 더욱 단순화될 수 있습니다. R은 상삼각 행렬이고, 오른쪽 부분은 단순히 벡터이므로 Uv = w를 해결할 수 있는 능력이 있으면 충분합니다.\u003c/p\u003e\n\u003cp\u003e저는 작업 수행을 위해 solve_upper_triangular 함수를 만들었습니다. 선형 방정식을 해결하는 방법에 대해서는 자세히 설명하지 않겠습니다. 단순히 행의 마지막에서 시작하여 역방향으로 작업하며, 각 행에서 이전에 설정된 변수 값들을 대체하고, 남은 변수 중 계수가 0이 아닌 변수에 대해 하나를 제외한 모든 변수에 값을 1로 할당하고, 다른 변수들에 대한 마지막 변수를 나머지 변수들을 이용하여 구합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003esolve_upper_triangular\u003c/span\u003e(a, b):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    선형 방정식 ax = b를 x에 대해 해결합니다.\n    :param a: 크기가 n x n인 상삼각 행렬.\n    :param b: n 차원 벡터.\n    :return: ax = b를 만족하는 x 벡터.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    tracker = np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e(a.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n    result = np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e(a.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e row, val \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ezip\u003c/span\u003e(a[::-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], b[::-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]):\n        unset_var_indices = np.\u003cspan class=\"hljs-title function_\"\u003ewhere\u003c/span\u003e((tracker == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) \u0026#x26; (row != \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e))[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(unset_var_indices) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003eisclose\u003c/span\u003e(result @ row, val):\n                \u003cspan class=\"hljs-keyword\"\u003econtinue\u003c/span\u003e\n            \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n                raise \u003cspan class=\"hljs-title class_\"\u003eUnsolvableError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'주어진 a와 b 값으로 인해 해결할 수 없는 방정식입니다.'\u003c/span\u003e)\n\n        tracker[unset_var_indices] = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n        result[unset_var_indices[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e:]] = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n        i = unset_var_indices[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        result[i] = (val - result @ row) / row[i]\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e result\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 MultipleLinearRegressor를 생성할 준비가 되었습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMultipleLinearRegressor\u003c/span\u003e:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"다중 선형 회귀를 수행합니다.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self):\n        self.\u003cspan class=\"hljs-property\"\u003ebeta\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e단순 선형 회귀와 마찬가지로 predict 메서드와 fit 메서드를 갖게 될 것입니다.\u003c/p\u003e\n\u003cp\u003epredict 메서드는 간단히 행렬 X 또는 벡터 x와 β 사이의 행렬 곱을 계산합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(self, x):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    주어진 x값 배열로부터 예측된 y값을 제공합니다.\n    :param x: x값의 벡터 또는 행렬.\n    :return: 예측된 y값의 벡터.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003ebeta\u003c/span\u003e is \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e:\n        raise \u003cspan class=\"hljs-title class_\"\u003eRegressionModelNotFitError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'앗! 모델이 적합되지 않았습니다!'\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e x @ self.\u003cspan class=\"hljs-property\"\u003ebeta\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003efit 메소드는 방정식 6을 해결하기 위해 X^TX를 QR 분해한 다음 solve_upper_triangular을 사용하여 Rβ = Q^-1X^Ty의 해를 찾습니다. 또한 적합한 모델의 제곱 오차의 합을 반환합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(self, x, y):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    x-값 행렬과 해당 y-값 벡터를 기반으로 모델을 적합합니다.\n    :param x: x-값 행렬.\n    :param y: y-값 벡터.\n    :return: 적합된 모델의 제곱 오차의 합.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    x_t = x.\u003cspan class=\"hljs-title function_\"\u003etranspose\u003c/span\u003e()\n    q, r = np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eqr\u003c/span\u003e(x_t @ x)\n    vec = np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003einv\u003c/span\u003e(q) @ x_t @ y\n    self.\u003cspan class=\"hljs-property\"\u003ebeta\u003c/span\u003e = \u003cspan class=\"hljs-title function_\"\u003esolve_upper_triangular\u003c/span\u003e(r, vec)\n    diff = self.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(x) - y\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e diff @ diff\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다중 선형 회귀기의 성능을 살펴봅시다. 이전과 매우 유사한 generate_noisy_data 함수를 만들겠습니다. 이 함수는 매개변수 벡터 β를 받아들이고 X 행렬과 데이터 포인트의 y-값 벡터를 생성한 다음 이전과 같이 각 y-값에 가우시안 노이즈를 추가합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003egenerate_noisy_data\u003c/span\u003e(n_data_points, n_independent_variables, beta, x_range, noise_stddev):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    가우시안 노이즈가 추가된 선형 관계를 기반으로 데이터 포인트를 생성합니다.\n    :param n_data_points: 생성할 데이터 포인트 수.\n    :param n_independent_variables: 각 데이터 포인트에서의 독립 변수 수.\n    :param beta: 독립 변수의 계수 벡터.\n    :param x_range: x-값을 추출할 범위.\n    :param noise_stddev: 각 y-값에 추가할 가우시안 노이즈의 표준 편차.\n    :return: x-값 행렬과 y-값 벡터.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    x = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(*x_range, (n_data_points, n_independent_variables))\n    y = x @ beta + np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enormal\u003c/span\u003e(scale=noise_stddev, size=n_data_points)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e x, y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 데이터를 생성하고 회귀자가 원래 β를 얼마나 잘 복원하는지 살펴보는 시간입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eregressor = \u003cspan class=\"hljs-title class_\"\u003eMultipleLinearRegressor\u003c/span\u003e()\nx, y = \u003cspan class=\"hljs-title function_\"\u003egenerate_noisy_data\u003c/span\u003e(n_data_points=\u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e,\n                           n_independent_variables=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e,\n                           beta=np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([-\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n                           x_range=np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([-\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e]),\n                           noise_stddev=\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e)\nsse = regressor.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(x, y)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'Sum Squared Error: {sse}'\u003c/span\u003e)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'Beta: {regressor.beta}'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e한 번 실행한 결과는 다음과 같습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eSum\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSquared\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eError\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1259196.6403705715\u003c/span\u003e\n\u003cspan class=\"hljs-title class_\"\u003eBeta\u003c/span\u003e: [-\u003cspan class=\"hljs-number\"\u003e9.95436533\u003c/span\u003e  \u003cspan class=\"hljs-number\"\u003e5.02469925\u003c/span\u003e -\u003cspan class=\"hljs-number\"\u003e7.95431319\u003c/span\u003e -\u003cspan class=\"hljs-number\"\u003e1.97266714\u003c/span\u003e  \u003cspan class=\"hljs-number\"\u003e1.03726794\u003c/span\u003e -\u003cspan class=\"hljs-number\"\u003e2.95935233\u003c/span\u003e\n  \u003cspan class=\"hljs-number\"\u003e4.03854255\u003c/span\u003e -\u003cspan class=\"hljs-number\"\u003e4.98106051\u003c/span\u003e -\u003cspan class=\"hljs-number\"\u003e1.01840914\u003c/span\u003e  \u003cspan class=\"hljs-number\"\u003e3.0410695\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e위에서 확인할 수 있듯이, 원래 매개변수 값에 꽤 가까운 결과를 얻는 데 잘 작동하는 것 같습니다.\u003c/p\u003e\n\u003ch2\u003e편향(bias)에 대하여\u003c/h2\u003e\n\u003cp\u003e지금까지 y절편이 0인 회귀 모델에 대해만 논의했습니다. 그러나 이는 모든 데이터에 적합한 것은 아닙니다. 만약 x⋅β + b와 같은 모델인 f(x) = x⋅β + b를 원한다면 어떻게 해야 할까요? 여기서 b는 0이 아닌 상수입니다. 기계 학습의 맥락에서 이 값 b를 편향(bias)이라고 부르며, 모든 모델 입력이 0일 때에도 데이터를 특정 값으로 '편향'시킨다는 의미입니다.\u003c/p\u003e\n\u003cp\u003e이러한 고려 사항은 회귀 모델에 편향을 추가하는 데 많은 노력이 필요하지 않다는 것으로 추가 사항으로 남겨두었습니다: 회귀 모델에 편향을 추가하는 것은 데이터에 항상 1로 설정된 추가 독립 변수를 추가하는 것과 동일합니다. 예를 들어, 우리가 2차원 데이터를 가지고 있고 회귀자에 편향을 추가하려는 경우, f(x) = kx 형태의 모델을 적합시키는 대신에\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_45.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e변수 x_1은 원본 데이터를 나타내고, x_2는 모든 데이터 포인트에서 1로 설정됩니다. 데이터 포인트 (x, y)는 이렇게 (x_1, x_2, y) = (x, 1, y)가 됩니다. x_1 = x이고 x_2 = 1을 대입하면, 방정식 7은 다음과 같이 단순화됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-22-LinearRegressionfromScratch_46.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 β_1은 기울기이고, β_2는 바이어스입니다. 우리는 다중 선형 회귀를 사용하여 이 모델을 적합시킬 수 있습니다. 고차원 데이터의 경우, 이 과정은 비슷하게 작동합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e여기서 한 가지! 이것이 바로 처음부터 선형 회귀입니다. 즐겁게 즐겼고 무언가를 배웠으면 좋겠어요. 어떤 피드백과 건설적인 비평도 환영합니다. 다음 포스트에서는 선형 분류에 대해 다룰 예정이니 기대해주세요.\u003c/p\u003e\n\u003cp\u003e모든 코드는 github에서 확인하실 수 있습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-22-LinearRegressionfromScratch"},"buildId":"Rv-NbbtWUaja2joH5WkO_","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>