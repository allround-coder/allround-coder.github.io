<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>얼굴 인식을 통해 감정 해독하기 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-06-20-DecodingEmotionswithFacialRecognition" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="얼굴 인식을 통해 감정 해독하기 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="얼굴 인식을 통해 감정 해독하기 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-06-20-DecodingEmotionswithFacialRecognition" data-gatsby-head="true"/><meta name="twitter:title" content="얼굴 인식을 통해 감정 해독하기 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 05:00" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_buildManifest.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">얼굴 인식을 통해 감정 해독하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="얼굴 인식을 통해 감정 해독하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">8<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-DecodingEmotionswithFacialRecognition&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png">
<p>인간의 감정을 얼굴 표현을 통해 이해하는 것은 우리에게 자연스러운 기술이지만, 컴퓨터에게 같은 기술을 가르치는 것은 어렵게 느껴질 수 있습니다. 다행히도, 적절한 도구와 조금의 코딩을 통해 이를 실현할 수 있습니다. 이 글에서는 Python을 사용하여 감정 감지를 위한 얼굴 표현 인식을 구현하는 두 가지 방법을 탐구해 보겠습니다: DeepFace를 사용하는 방법과 Keras를 활용한 합성곱 신경망(CNN)을 사용하는 방법</p>
<h1>1.) Deepface 사용</h1>
<p>Deepface는 파이썬용 경량 얼굴 인식 및 얼굴 속성 분석(나이, 성별, 감정 및 인종) 프레임워크입니다. DeepFace를 몇 줄의 코드로 실행할 수 있지만, 그 뒤의 모든 과정에 대해 깊이 있는 지식을 습득할 필요가 없습니다. 사실, 라이브러리를 가져오고 정확한 이미지 경로를 입력으로 전달하기만 하면 됩니다; 그게 전부입니다!</p>
<div class="content-ad"></div>
<p>1.) .py 파일로 필요한 모듈 가져오기</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> deepface <span class="hljs-keyword">import</span> <span class="hljs-title class_">DeepFace</span>
<span class="hljs-keyword">import</span> cv2
</code></pre>
<p>2.) 얼굴 캐스케이드 분류기 로드</p>
<p>이 명령은 전면 얼굴 검출 모델과 함께 CascadeClassifier 객체를 초기화합니다. 그 결과인 face_cascade 객체를 사용하여 이미지에서 얼굴을 감지할 수 있습니다. Haar Cascade는 파이썬의 OpenCV 라이브러리를 사용하여 쉽게 구현할 수 있는 얼굴 검출을 위한 인기 있는 알고리즘입니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">face_cascade = cv2.<span class="hljs-title class_">CascadeClassifier</span>(cv2.<span class="hljs-property">data</span>.<span class="hljs-property">haarcascades</span> + <span class="hljs-string">'haarcascade_frontalface_default.xml'</span>)
</code></pre>
<p>3.) 비디오 스트림을 시작하고 분류기를 실행합니다.</p>
<p>0은 기본 카메라를 나타냅니다. 외부 웹캠을 연결한 경우 1을 입력하세요.</p>
<pre><code class="hljs language-js">cap = cv2.<span class="hljs-title class_">VideoCapture</span>(<span class="hljs-number">0</span>)

<span class="hljs-keyword">while</span> <span class="hljs-title class_">True</span>:
    # 성공 또는 실패 여부를 나타내는 부울 값인 ret 및 캡쳐된 프레임인 frame을 캡쳐합니다.
    ret, frame = cap.<span class="hljs-title function_">read</span>()

    # 프레임을 그레이스케일로 변환합니다.
    gray_frame = cv2.<span class="hljs-title function_">cvtColor</span>(frame, cv2.<span class="hljs-property">COLOR_BGR2GRAY</span>)

    # 그레이스케일 프레임을 <span class="hljs-variable constant_">RGB</span> 형식으로 변환합니다.
    rgb_frame = cv2.<span class="hljs-title function_">cvtColor</span>(gray_frame, cv2.<span class="hljs-property">COLOR_GRAY2RGB</span>)

    # 프레임에 얼굴을 감지합니다.
    faces = face_cascade.<span class="hljs-title function_">detectMultiScale</span>(gray_frame, scaleFactor=<span class="hljs-number">1.1</span>, minNeighbors=<span class="hljs-number">10</span>, minSize=(<span class="hljs-number">30</span>, <span class="hljs-number">30</span>))

    <span class="hljs-keyword">for</span> (x, y, w, h) <span class="hljs-keyword">in</span> <span class="hljs-attr">faces</span>:
        # <span class="hljs-variable constant_">RGB</span> 프레임에서 y에서 y+h, x에서 x+w까지의 영역에서 얼굴 <span class="hljs-title function_">ROI</span>(관심 영역)를 추출합니다.
        face_roi = rgb_frame[<span class="hljs-attr">y</span>:y + h, <span class="hljs-attr">x</span>:x + w]

        # <span class="hljs-title class_">DeepFace</span>를 사용하여 얼굴 <span class="hljs-variable constant_">ROI</span>에서 감정 분석을 수행합니다.
        result = <span class="hljs-title class_">DeepFace</span>.<span class="hljs-title function_">analyze</span>(face_roi, actions=[<span class="hljs-string">'emotion'</span>], enforce_detection=<span class="hljs-title class_">False</span>)

        # 주요 감정을 결정합니다.
        emotion = result[<span class="hljs-number">0</span>][<span class="hljs-string">'dominant_emotion'</span>]

        # 얼굴 주위에 직사각형을 그리고 예측된 감정과 함께 레이블을 붙입니다.
        cv2.<span class="hljs-title function_">rectangle</span>(frame, (x, y), (x + w, y + h), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>)
        cv2.<span class="hljs-title function_">putText</span>(frame, emotion, (x, y - <span class="hljs-number">10</span>), cv2.<span class="hljs-property">FONT_HERSHEY_SIMPLEX</span>, <span class="hljs-number">0.9</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>)

    # 결과 프레임을 표시합니다.
    cv2.<span class="hljs-title function_">imshow</span>(<span class="hljs-string">'실시간 감정 감지'</span>, frame)

    # 종료하려면 <span class="hljs-string">'q'</span>를 누르세요.
    <span class="hljs-keyword">if</span> cv2.<span class="hljs-title function_">waitKey</span>(<span class="hljs-number">1</span>) &#x26; <span class="hljs-number">0xFF</span> == <span class="hljs-title function_">ord</span>(<span class="hljs-string">'q'</span>):
        <span class="hljs-keyword">break</span>

# 캡처를 해제하고 모든 창을 닫습니다.
cap.<span class="hljs-title function_">release</span>()
cv2.<span class="hljs-title function_">destroyAllWindows</span>()
</code></pre>
<div class="content-ad"></div>
<p>다음 링크에서 haarcascade 파일을 다운로드할 수 있어요 — <a href="https://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml" rel="nofollow" target="_blank">https://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml</a></p>
<p>카스케이드 분류기에 대해 더 읽어보고 싶다면 -<a href="https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html" rel="nofollow" target="_blank">https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html</a></p>
<p>.py 파일과 haarcascade_frontalface_default.xml 파일을 동일한 폴더에 넣고 .py 파일을 실행해주세요. 모두 잘 작동되면, 카메라 스트림이 보이는 외부 창에 감정이 표시될 거예요!</p>
<h1>2.) Keras를 이용한 합성곱 신경망</h1>
<div class="content-ad"></div>
<p>합성곱 신경망(Convolutional Neural Networks)은 이미지 처리에 사용되는 피드 포워드 네트워크의 일종입니다. 이러한 네트워크는 일반적인 완전 연결 레이어에 추가적인 합성곱(Convolutional) 및 풀링(Pooling) 레이어를 특징으로 합니다. 주로 그리드(grid) 형식의 데이터(이미지, 비디오)와 함께 작동합니다.</p>
<ul>
<li><a href="https://www.kaggle.com/datasets/msambare/fer2013" rel="nofollow" target="_blank">https://www.kaggle.com/datasets/msambare/fer2013</a> 에서 FER-2013 데이터셋을 다운로드하세요. 훈련 및 테스트 디렉토리를 'data'라는 공통 폴더 아래에 넣으세요.</li>
<li>.py 파일에 필요한 모듈을 가져오세요</li>
</ul>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models, layers
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D, MaxPooling2D, Dense, Dropout, Flatten
<span class="hljs-keyword">import</span> os
</code></pre>
<ol start="3">
<li>동일한 파일에서 모델을 구축하고 훈련시키세요.</li>
</ol>
<div class="content-ad"></div>
<pre><code class="hljs language-js">train_data_dir=<span class="hljs-string">'data/train/'</span>
validation_data_dir=<span class="hljs-string">'data/test/'</span>

train_datagen = tf.<span class="hljs-property">keras</span>.<span class="hljs-property">preprocessing</span>.<span class="hljs-property">image</span>.<span class="hljs-title class_">ImageDataGenerator</span>(
    rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>,
    rotation_range=<span class="hljs-number">30</span>,
    shear_range=<span class="hljs-number">0.3</span>,
    zoom_range=<span class="hljs-number">0.3</span>,
    horizontal_flip=<span class="hljs-title class_">True</span>,
    fill_mode= <span class="hljs-string">'nearest'</span>)
validation_datagen = tf.<span class="hljs-property">keras</span>.<span class="hljs-property">preprocessing</span>.<span class="hljs-property">image</span>.<span class="hljs-title class_">ImageDataGenerator</span>(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)


train_generator = train_datagen.<span class="hljs-title function_">flow_from_directory</span>(
    train_data_dir,
    color_mode=<span class="hljs-string">'grayscale'</span>,
    target_size=(<span class="hljs-number">48</span>, <span class="hljs-number">48</span>),
    batch_size=<span class="hljs-number">32</span>,
    class_mode=<span class="hljs-string">'categorical'</span> ,
    shuffle=<span class="hljs-title class_">True</span>)

validation_generator = validation_datagen.<span class="hljs-title function_">flow_from_directory</span>(
    validation_data_dir,
    color_mode=<span class="hljs-string">'grayscale'</span>,
    target_size=(<span class="hljs-number">48</span>, <span class="hljs-number">48</span>),
    batch_size=<span class="hljs-number">32</span>,
    class_mode=<span class="hljs-string">'categorical'</span>,
    shuffle=<span class="hljs-title class_">True</span>)

class_labels=[<span class="hljs-string">'Angry'</span>, <span class="hljs-string">'Disgust'</span>, <span class="hljs-string">'Fear'</span>, <span class="hljs-string">'Happy'</span>, <span class="hljs-string">'Neutral'</span>, <span class="hljs-string">'Sad'</span>, <span class="hljs-string">'Surprise'</span>]
img, label = train_generator.<span class="hljs-title function_">__next__</span>()


model = <span class="hljs-title class_">Sequential</span>()

model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">Conv2D</span>(<span class="hljs-number">32</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, input_shape=(<span class="hljs-number">48</span>,<span class="hljs-number">48</span>,<span class="hljs-number">1</span>)))
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">Conv2D</span>(<span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">MaxPooling2D</span>(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">Dropout</span> (<span class="hljs-number">0.1</span>))
model.<span class="hljs-property">add</span> (<span class="hljs-title class_">Conv2D</span>(<span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">MaxPooling2D</span>(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
model.<span class="hljs-property">add</span> (<span class="hljs-title class_">Dropout</span>(<span class="hljs-number">0.1</span>))
model. add (<span class="hljs-title class_">Conv2D</span>(<span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">MaxPooling2D</span>(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
model. <span class="hljs-title function_">add</span>(<span class="hljs-title class_">Dropout</span>(<span class="hljs-number">0.1</span>))
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">Flatten</span>())
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">Dense</span>(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>))
model.<span class="hljs-title function_">add</span>(<span class="hljs-title class_">Dropout</span> (<span class="hljs-number">0.2</span>))
model. <span class="hljs-title function_">add</span>(<span class="hljs-title class_">Dense</span>(<span class="hljs-number">7</span>, activation=<span class="hljs-string">'softmax'</span>))

model.<span class="hljs-title function_">compile</span>(optimizer = <span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'categorical_crossentropy'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
<span class="hljs-title function_">print</span>(model.<span class="hljs-title function_">summary</span>())

train_path = <span class="hljs-string">"data/train"</span>
test_path = <span class="hljs-string">"data/test"</span>
num_train_imgs = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> root, dirs, files <span class="hljs-keyword">in</span> os.<span class="hljs-title function_">walk</span>(train_path):
    num_train_imgs += <span class="hljs-title function_">len</span>(files)
num_test_imgs = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> root, dirs, files <span class="hljs-keyword">in</span> os.<span class="hljs-title function_">walk</span>(test_path):
    num_test_imgs += <span class="hljs-title function_">len</span>(files)

<span class="hljs-title function_">print</span>(<span class="hljs-string">"Number of training images: "</span>, num_train_imgs)
<span class="hljs-title function_">print</span>(<span class="hljs-string">"Number of testing images: "</span>, num_test_imgs)

model.<span class="hljs-title function_">fit</span>(train_generator, steps_per_epoch=num_train_imgs<span class="hljs-comment">//32, epochs=50, validation_data=validation_generator, validation_steps=num_test_imgs//32)</span>

model.<span class="hljs-title function_">save</span>(<span class="hljs-string">'model.h5'</span>)  
</code></pre>
<p>모델.h5 파일이 현재 디렉토리에 저장됩니다.</p>
<ol start="4">
<li>테스트</li>
</ol>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt

model=tf.<span class="hljs-property">keras</span>.<span class="hljs-property">models</span>.<span class="hljs-title function_">load_model</span>(<span class="hljs-string">'model.h5'</span>)

faceDetect=cv2.<span class="hljs-title class_">CascadeClassifier</span>(<span class="hljs-string">'haarcascade_frontalface_default.xml'</span>)

video=cv2.<span class="hljs-title class_">VideoCapture</span>(<span class="hljs-number">0</span>)

labels_dict={<span class="hljs-number">0</span>:<span class="hljs-string">'Angry'</span>,<span class="hljs-number">1</span>:<span class="hljs-string">'Disgust'</span>, <span class="hljs-number">2</span>:<span class="hljs-string">'Fear'</span>, <span class="hljs-number">3</span>:<span class="hljs-string">'Happy'</span>,<span class="hljs-number">4</span>:<span class="hljs-string">'Neutral'</span>,<span class="hljs-number">5</span>:<span class="hljs-string">'Sad'</span>,<span class="hljs-number">6</span>:<span class="hljs-string">'Surprise'</span>}

<span class="hljs-keyword">while</span> <span class="hljs-title class_">True</span>:
    ret,frame=video.<span class="hljs-title function_">read</span>()
    gray=cv2.<span class="hljs-title function_">cvtColor</span>(frame, cv2.<span class="hljs-property">COLOR_BGR2GRAY</span>)
    faces= faceDetect.<span class="hljs-title function_">detectMultiScale</span>(gray, <span class="hljs-number">1.3</span>, <span class="hljs-number">3</span>)
    <span class="hljs-keyword">for</span> x,y,w,h <span class="hljs-keyword">in</span> <span class="hljs-attr">faces</span>:
        sub_face_img=gray[<span class="hljs-attr">y</span>:y+h, <span class="hljs-attr">x</span>:x+w]
        resized=cv2.<span class="hljs-title function_">resize</span>(sub_face_img,(<span class="hljs-number">48</span>,<span class="hljs-number">48</span>))
        normalize=resized/<span class="hljs-number">255.0</span>
        reshaped=np.<span class="hljs-title function_">reshape</span>(normalize, (<span class="hljs-number">1</span>, <span class="hljs-number">48</span>, <span class="hljs-number">48</span>, <span class="hljs-number">1</span>))
        result=model.<span class="hljs-title function_">predict</span>(reshaped)
        label=np.<span class="hljs-title function_">argmax</span>(result, axis=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
        <span class="hljs-title function_">print</span>(label)
        cv2.<span class="hljs-title function_">rectangle</span>(frame, (x,y), (x+w, y+h), (<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">255</span>), <span class="hljs-number">1</span>)
        cv2.<span class="hljs-title function_">rectangle</span>(frame,(x,y),(x+w,y+h),(<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">255</span>),<span class="hljs-number">2</span>)
        cv2.<span class="hljs-title function_">rectangle</span>(frame,(x,y-<span class="hljs-number">40</span>),(x+w,y),(<span class="hljs-number">50</span>,<span class="hljs-number">50</span>,<span class="hljs-number">255</span>),-<span class="hljs-number">1</span>)
        cv2.<span class="hljs-title function_">putText</span>(frame, labels_dict[label], (x, y-<span class="hljs-number">10</span>),cv2.<span class="hljs-property">FONT_HERSHEY_SIMPLEX</span>,<span class="hljs-number">0.8</span>,(<span class="hljs-number">255</span>,<span class="hljs-number">255</span>,<span class="hljs-number">255</span>),<span class="hljs-number">2</span>)
        
    cv2.<span class="hljs-title function_">imshow</span>(<span class="hljs-string">"실시간 감정 인식"</span>,frame)
    k=cv2.<span class="hljs-title function_">waitKey</span>(<span class="hljs-number">1</span>)
    <span class="hljs-keyword">if</span> k==<span class="hljs-title function_">ord</span>(<span class="hljs-string">'q'</span>):
        <span class="hljs-keyword">break</span>

video.<span class="hljs-title function_">release</span>()
cv2.<span class="hljs-title function_">destroyAllWindows</span>()
</code></pre>
<div class="content-ad"></div>
<p>해당 파이썬 스크립트를 실행해보세요. 코드가 동작할 것을 기대합니다!</p>
<h1>개선 사항</h1>
<p>이 코드에 주의를 집중시키기 위해 Spatial Transformer의 추가를 활용할 수도 있습니다. 해당 내용은 논문에 언급되어 있습니다.</p>
<p>Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network- Shervin Minaee, Amirali Abdolrashidi, Expedia Group
University of California, Riverside</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"얼굴 인식을 통해 감정 해독하기","description":"","date":"2024-06-20 05:00","slug":"2024-06-20-DecodingEmotionswithFacialRecognition","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png\" /\u003e\n\n인간의 감정을 얼굴 표현을 통해 이해하는 것은 우리에게 자연스러운 기술이지만, 컴퓨터에게 같은 기술을 가르치는 것은 어렵게 느껴질 수 있습니다. 다행히도, 적절한 도구와 조금의 코딩을 통해 이를 실현할 수 있습니다. 이 글에서는 Python을 사용하여 감정 감지를 위한 얼굴 표현 인식을 구현하는 두 가지 방법을 탐구해 보겠습니다: DeepFace를 사용하는 방법과 Keras를 활용한 합성곱 신경망(CNN)을 사용하는 방법\n\n# 1.) Deepface 사용\n\nDeepface는 파이썬용 경량 얼굴 인식 및 얼굴 속성 분석(나이, 성별, 감정 및 인종) 프레임워크입니다. DeepFace를 몇 줄의 코드로 실행할 수 있지만, 그 뒤의 모든 과정에 대해 깊이 있는 지식을 습득할 필요가 없습니다. 사실, 라이브러리를 가져오고 정확한 이미지 경로를 입력으로 전달하기만 하면 됩니다; 그게 전부입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1.) .py 파일로 필요한 모듈 가져오기\n\n```js\nfrom deepface import DeepFace\nimport cv2\n```\n\n2.) 얼굴 캐스케이드 분류기 로드\n\n이 명령은 전면 얼굴 검출 모델과 함께 CascadeClassifier 객체를 초기화합니다. 그 결과인 face_cascade 객체를 사용하여 이미지에서 얼굴을 감지할 수 있습니다. Haar Cascade는 파이썬의 OpenCV 라이브러리를 사용하여 쉽게 구현할 수 있는 얼굴 검출을 위한 인기 있는 알고리즘입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\r\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\r\n```\r\n\r\n3.) 비디오 스트림을 시작하고 분류기를 실행합니다.\r\n\r\n0은 기본 카메라를 나타냅니다. 외부 웹캠을 연결한 경우 1을 입력하세요.\r\n\r\n```js\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    # 성공 또는 실패 여부를 나타내는 부울 값인 ret 및 캡쳐된 프레임인 frame을 캡쳐합니다.\r\n    ret, frame = cap.read()\r\n\r\n    # 프레임을 그레이스케일로 변환합니다.\r\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\n    # 그레이스케일 프레임을 RGB 형식으로 변환합니다.\r\n    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\r\n\r\n    # 프레임에 얼굴을 감지합니다.\r\n    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=10, minSize=(30, 30))\r\n\r\n    for (x, y, w, h) in faces:\r\n        # RGB 프레임에서 y에서 y+h, x에서 x+w까지의 영역에서 얼굴 ROI(관심 영역)를 추출합니다.\r\n        face_roi = rgb_frame[y:y + h, x:x + w]\r\n\r\n        # DeepFace를 사용하여 얼굴 ROI에서 감정 분석을 수행합니다.\r\n        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\r\n\r\n        # 주요 감정을 결정합니다.\r\n        emotion = result[0]['dominant_emotion']\r\n\r\n        # 얼굴 주위에 직사각형을 그리고 예측된 감정과 함께 레이블을 붙입니다.\r\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\r\n        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\r\n\r\n    # 결과 프레임을 표시합니다.\r\n    cv2.imshow('실시간 감정 감지', frame)\r\n\r\n    # 종료하려면 'q'를 누르세요.\r\n    if cv2.waitKey(1) \u0026 0xFF == ord('q'):\r\n        break\r\n\r\n# 캡처를 해제하고 모든 창을 닫습니다.\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 링크에서 haarcascade 파일을 다운로드할 수 있어요 — https://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml\n\n카스케이드 분류기에 대해 더 읽어보고 싶다면 -https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\n\n.py 파일과 haarcascade_frontalface_default.xml 파일을 동일한 폴더에 넣고 .py 파일을 실행해주세요. 모두 잘 작동되면, 카메라 스트림이 보이는 외부 창에 감정이 표시될 거예요!\n\n# 2.) Keras를 이용한 합성곱 신경망\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n합성곱 신경망(Convolutional Neural Networks)은 이미지 처리에 사용되는 피드 포워드 네트워크의 일종입니다. 이러한 네트워크는 일반적인 완전 연결 레이어에 추가적인 합성곱(Convolutional) 및 풀링(Pooling) 레이어를 특징으로 합니다. 주로 그리드(grid) 형식의 데이터(이미지, 비디오)와 함께 작동합니다.\n\n- https://www.kaggle.com/datasets/msambare/fer2013 에서 FER-2013 데이터셋을 다운로드하세요. 훈련 및 테스트 디렉토리를 'data'라는 공통 폴더 아래에 넣으세요.\n- .py 파일에 필요한 모듈을 가져오세요\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import models, layers\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\nimport os\n```\n\n3. 동일한 파일에서 모델을 구축하고 훈련시키세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ntrain_data_dir='data/train/'\nvalidation_data_dir='data/test/'\n\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=30,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode= 'nearest')\nvalidation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    color_mode='grayscale',\n    target_size=(48, 48),\n    batch_size=32,\n    class_mode='categorical' ,\n    shuffle=True)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_data_dir,\n    color_mode='grayscale',\n    target_size=(48, 48),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle=True)\n\nclass_labels=['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\nimg, label = train_generator.__next__()\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout (0.1))\nmodel.add (Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add (Dropout(0.1))\nmodel. add (Conv2D(256, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel. add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout (0.2))\nmodel. add(Dense(7, activation='softmax'))\n\nmodel.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())\n\ntrain_path = \"data/train\"\ntest_path = \"data/test\"\nnum_train_imgs = 0\nfor root, dirs, files in os.walk(train_path):\n    num_train_imgs += len(files)\nnum_test_imgs = 0\nfor root, dirs, files in os.walk(test_path):\n    num_test_imgs += len(files)\n\nprint(\"Number of training images: \", num_train_imgs)\nprint(\"Number of testing images: \", num_test_imgs)\n\nmodel.fit(train_generator, steps_per_epoch=num_train_imgs//32, epochs=50, validation_data=validation_generator, validation_steps=num_test_imgs//32)\n\nmodel.save('model.h5')  \n```\n\n모델.h5 파일이 현재 디렉토리에 저장됩니다.\n\n4. 테스트\n\n```js\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nmodel=tf.keras.models.load_model('model.h5')\n\nfaceDetect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\nvideo=cv2.VideoCapture(0)\n\nlabels_dict={0:'Angry',1:'Disgust', 2:'Fear', 3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}\n\nwhile True:\n    ret,frame=video.read()\n    gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces= faceDetect.detectMultiScale(gray, 1.3, 3)\n    for x,y,w,h in faces:\n        sub_face_img=gray[y:y+h, x:x+w]\n        resized=cv2.resize(sub_face_img,(48,48))\n        normalize=resized/255.0\n        reshaped=np.reshape(normalize, (1, 48, 48, 1))\n        result=model.predict(reshaped)\n        label=np.argmax(result, axis=1)[0]\n        print(label)\n        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 1)\n        cv2.rectangle(frame,(x,y),(x+w,y+h),(50,50,255),2)\n        cv2.rectangle(frame,(x,y-40),(x+w,y),(50,50,255),-1)\n        cv2.putText(frame, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n        \n    cv2.imshow(\"실시간 감정 인식\",frame)\n    k=cv2.waitKey(1)\n    if k==ord('q'):\n        break\n\nvideo.release()\ncv2.destroyAllWindows()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해당 파이썬 스크립트를 실행해보세요. 코드가 동작할 것을 기대합니다!\n\n# 개선 사항\n\n이 코드에 주의를 집중시키기 위해 Spatial Transformer의 추가를 활용할 수도 있습니다. 해당 내용은 논문에 언급되어 있습니다.\n\nDeep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network- Shervin Minaee, Amirali Abdolrashidi, Expedia Group\nUniversity of California, Riverside","ogImage":{"url":"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png"},"coverImage":"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png","tag":["Tech"],"readingTime":8},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/assets/img/2024-06-20-DecodingEmotionswithFacialRecognition_0.png\"\u003e\n\u003cp\u003e인간의 감정을 얼굴 표현을 통해 이해하는 것은 우리에게 자연스러운 기술이지만, 컴퓨터에게 같은 기술을 가르치는 것은 어렵게 느껴질 수 있습니다. 다행히도, 적절한 도구와 조금의 코딩을 통해 이를 실현할 수 있습니다. 이 글에서는 Python을 사용하여 감정 감지를 위한 얼굴 표현 인식을 구현하는 두 가지 방법을 탐구해 보겠습니다: DeepFace를 사용하는 방법과 Keras를 활용한 합성곱 신경망(CNN)을 사용하는 방법\u003c/p\u003e\n\u003ch1\u003e1.) Deepface 사용\u003c/h1\u003e\n\u003cp\u003eDeepface는 파이썬용 경량 얼굴 인식 및 얼굴 속성 분석(나이, 성별, 감정 및 인종) 프레임워크입니다. DeepFace를 몇 줄의 코드로 실행할 수 있지만, 그 뒤의 모든 과정에 대해 깊이 있는 지식을 습득할 필요가 없습니다. 사실, 라이브러리를 가져오고 정확한 이미지 경로를 입력으로 전달하기만 하면 됩니다; 그게 전부입니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e1.) .py 파일로 필요한 모듈 가져오기\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e deepface \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDeepFace\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2.) 얼굴 캐스케이드 분류기 로드\u003c/p\u003e\n\u003cp\u003e이 명령은 전면 얼굴 검출 모델과 함께 CascadeClassifier 객체를 초기화합니다. 그 결과인 face_cascade 객체를 사용하여 이미지에서 얼굴을 감지할 수 있습니다. Haar Cascade는 파이썬의 OpenCV 라이브러리를 사용하여 쉽게 구현할 수 있는 얼굴 검출을 위한 인기 있는 알고리즘입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eface_cascade = cv2.\u003cspan class=\"hljs-title class_\"\u003eCascadeClassifier\u003c/span\u003e(cv2.\u003cspan class=\"hljs-property\"\u003edata\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ehaarcascades\u003c/span\u003e + \u003cspan class=\"hljs-string\"\u003e'haarcascade_frontalface_default.xml'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e3.) 비디오 스트림을 시작하고 분류기를 실행합니다.\u003c/p\u003e\n\u003cp\u003e0은 기본 카메라를 나타냅니다. 외부 웹캠을 연결한 경우 1을 입력하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecap = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\r\n\r\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e:\r\n    # 성공 또는 실패 여부를 나타내는 부울 값인 ret 및 캡쳐된 프레임인 frame을 캡쳐합니다.\r\n    ret, frame = cap.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\r\n\r\n    # 프레임을 그레이스케일로 변환합니다.\r\n    gray_frame = cv2.\u003cspan class=\"hljs-title function_\"\u003ecvtColor\u003c/span\u003e(frame, cv2.\u003cspan class=\"hljs-property\"\u003eCOLOR_BGR2GRAY\u003c/span\u003e)\r\n\r\n    # 그레이스케일 프레임을 \u003cspan class=\"hljs-variable constant_\"\u003eRGB\u003c/span\u003e 형식으로 변환합니다.\r\n    rgb_frame = cv2.\u003cspan class=\"hljs-title function_\"\u003ecvtColor\u003c/span\u003e(gray_frame, cv2.\u003cspan class=\"hljs-property\"\u003eCOLOR_GRAY2RGB\u003c/span\u003e)\r\n\r\n    # 프레임에 얼굴을 감지합니다.\r\n    faces = face_cascade.\u003cspan class=\"hljs-title function_\"\u003edetectMultiScale\u003c/span\u003e(gray_frame, scaleFactor=\u003cspan class=\"hljs-number\"\u003e1.1\u003c/span\u003e, minNeighbors=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, minSize=(\u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e))\r\n\r\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (x, y, w, h) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efaces\u003c/span\u003e:\r\n        # \u003cspan class=\"hljs-variable constant_\"\u003eRGB\u003c/span\u003e 프레임에서 y에서 y+h, x에서 x+w까지의 영역에서 얼굴 \u003cspan class=\"hljs-title function_\"\u003eROI\u003c/span\u003e(관심 영역)를 추출합니다.\r\n        face_roi = rgb_frame[\u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e:y + h, \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e:x + w]\r\n\r\n        # \u003cspan class=\"hljs-title class_\"\u003eDeepFace\u003c/span\u003e를 사용하여 얼굴 \u003cspan class=\"hljs-variable constant_\"\u003eROI\u003c/span\u003e에서 감정 분석을 수행합니다.\r\n        result = \u003cspan class=\"hljs-title class_\"\u003eDeepFace\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eanalyze\u003c/span\u003e(face_roi, actions=[\u003cspan class=\"hljs-string\"\u003e'emotion'\u003c/span\u003e], enforce_detection=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\r\n\r\n        # 주요 감정을 결정합니다.\r\n        emotion = result[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'dominant_emotion'\u003c/span\u003e]\r\n\r\n        # 얼굴 주위에 직사각형을 그리고 예측된 감정과 함께 레이블을 붙입니다.\r\n        cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(frame, (x, y), (x + w, y + h), (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\r\n        cv2.\u003cspan class=\"hljs-title function_\"\u003eputText\u003c/span\u003e(frame, emotion, (x, y - \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e), cv2.\u003cspan class=\"hljs-property\"\u003eFONT_HERSHEY_SIMPLEX\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9\u003c/span\u003e, (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\r\n\r\n    # 결과 프레임을 표시합니다.\r\n    cv2.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'실시간 감정 감지'\u003c/span\u003e, frame)\r\n\r\n    # 종료하려면 \u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e를 누르세요.\r\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e cv2.\u003cspan class=\"hljs-title function_\"\u003ewaitKey\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u0026#x26; \u003cspan class=\"hljs-number\"\u003e0xFF\u003c/span\u003e == \u003cspan class=\"hljs-title function_\"\u003eord\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e):\r\n        \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\r\n\r\n# 캡처를 해제하고 모든 창을 닫습니다.\r\ncap.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\r\ncv2.\u003cspan class=\"hljs-title function_\"\u003edestroyAllWindows\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e다음 링크에서 haarcascade 파일을 다운로드할 수 있어요 — \u003ca href=\"https://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/opencv/opencv/blob/4.x/data/haarcascades/haarcascade_frontalface_default.xml\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e카스케이드 분류기에 대해 더 읽어보고 싶다면 -\u003ca href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e.py 파일과 haarcascade_frontalface_default.xml 파일을 동일한 폴더에 넣고 .py 파일을 실행해주세요. 모두 잘 작동되면, 카메라 스트림이 보이는 외부 창에 감정이 표시될 거예요!\u003c/p\u003e\n\u003ch1\u003e2.) Keras를 이용한 합성곱 신경망\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e합성곱 신경망(Convolutional Neural Networks)은 이미지 처리에 사용되는 피드 포워드 네트워크의 일종입니다. 이러한 네트워크는 일반적인 완전 연결 레이어에 추가적인 합성곱(Convolutional) 및 풀링(Pooling) 레이어를 특징으로 합니다. 주로 그리드(grid) 형식의 데이터(이미지, 비디오)와 함께 작동합니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/datasets/msambare/fer2013\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://www.kaggle.com/datasets/msambare/fer2013\u003c/a\u003e 에서 FER-2013 데이터셋을 다운로드하세요. 훈련 및 테스트 디렉토리를 'data'라는 공통 폴더 아래에 넣으세요.\u003c/li\u003e\n\u003cli\u003e.py 파일에 필요한 모듈을 가져오세요\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tf\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e keras\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e models, layers\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.models \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Sequential\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.layers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e동일한 파일에서 모델을 구축하고 훈련시키세요.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etrain_data_dir=\u003cspan class=\"hljs-string\"\u003e'data/train/'\u003c/span\u003e\nvalidation_data_dir=\u003cspan class=\"hljs-string\"\u003e'data/test/'\u003c/span\u003e\n\ntrain_datagen = tf.\u003cspan class=\"hljs-property\"\u003ekeras\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epreprocessing\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eimage\u003c/span\u003e.\u003cspan class=\"hljs-title class_\"\u003eImageDataGenerator\u003c/span\u003e(\n    rescale=\u003cspan class=\"hljs-number\"\u003e1.\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\n    rotation_range=\u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e,\n    shear_range=\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e,\n    zoom_range=\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e,\n    horizontal_flip=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n    fill_mode= \u003cspan class=\"hljs-string\"\u003e'nearest'\u003c/span\u003e)\nvalidation_datagen = tf.\u003cspan class=\"hljs-property\"\u003ekeras\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epreprocessing\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eimage\u003c/span\u003e.\u003cspan class=\"hljs-title class_\"\u003eImageDataGenerator\u003c/span\u003e(rescale=\u003cspan class=\"hljs-number\"\u003e1.\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e)\n\n\ntrain_generator = train_datagen.\u003cspan class=\"hljs-title function_\"\u003eflow_from_directory\u003c/span\u003e(\n    train_data_dir,\n    color_mode=\u003cspan class=\"hljs-string\"\u003e'grayscale'\u003c/span\u003e,\n    target_size=(\u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e),\n    batch_size=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n    class_mode=\u003cspan class=\"hljs-string\"\u003e'categorical'\u003c/span\u003e ,\n    shuffle=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\nvalidation_generator = validation_datagen.\u003cspan class=\"hljs-title function_\"\u003eflow_from_directory\u003c/span\u003e(\n    validation_data_dir,\n    color_mode=\u003cspan class=\"hljs-string\"\u003e'grayscale'\u003c/span\u003e,\n    target_size=(\u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e),\n    batch_size=\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n    class_mode=\u003cspan class=\"hljs-string\"\u003e'categorical'\u003c/span\u003e,\n    shuffle=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\nclass_labels=[\u003cspan class=\"hljs-string\"\u003e'Angry'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Disgust'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Fear'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Happy'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Neutral'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Sad'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Surprise'\u003c/span\u003e]\nimg, label = train_generator.\u003cspan class=\"hljs-title function_\"\u003e__next__\u003c/span\u003e()\n\n\nmodel = \u003cspan class=\"hljs-title class_\"\u003eSequential\u003c/span\u003e()\n\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eConv2D\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, kernel_size=(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e), activation=\u003cspan class=\"hljs-string\"\u003e'relu'\u003c/span\u003e, input_shape=(\u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)))\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eConv2D\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, kernel_size=(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e), activation=\u003cspan class=\"hljs-string\"\u003e'relu'\u003c/span\u003e))\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eMaxPooling2D\u003c/span\u003e(pool_size=(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)))\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eDropout\u003c/span\u003e (\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e))\nmodel.\u003cspan class=\"hljs-property\"\u003eadd\u003c/span\u003e (\u003cspan class=\"hljs-title class_\"\u003eConv2D\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, kernel_size=(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e), activation=\u003cspan class=\"hljs-string\"\u003e'relu'\u003c/span\u003e))\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eMaxPooling2D\u003c/span\u003e(pool_size=(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)))\nmodel.\u003cspan class=\"hljs-property\"\u003eadd\u003c/span\u003e (\u003cspan class=\"hljs-title class_\"\u003eDropout\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e))\nmodel. add (\u003cspan class=\"hljs-title class_\"\u003eConv2D\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, kernel_size=(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e), activation=\u003cspan class=\"hljs-string\"\u003e'relu'\u003c/span\u003e))\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eMaxPooling2D\u003c/span\u003e(pool_size=(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)))\nmodel. \u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eDropout\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e))\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eFlatten\u003c/span\u003e())\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eDense\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e'relu'\u003c/span\u003e))\nmodel.\u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eDropout\u003c/span\u003e (\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e))\nmodel. \u003cspan class=\"hljs-title function_\"\u003eadd\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eDense\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e'softmax'\u003c/span\u003e))\n\nmodel.\u003cspan class=\"hljs-title function_\"\u003ecompile\u003c/span\u003e(optimizer = \u003cspan class=\"hljs-string\"\u003e'adam'\u003c/span\u003e, loss=\u003cspan class=\"hljs-string\"\u003e'categorical_crossentropy'\u003c/span\u003e, metrics=[\u003cspan class=\"hljs-string\"\u003e'accuracy'\u003c/span\u003e])\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(model.\u003cspan class=\"hljs-title function_\"\u003esummary\u003c/span\u003e())\n\ntrain_path = \u003cspan class=\"hljs-string\"\u003e\"data/train\"\u003c/span\u003e\ntest_path = \u003cspan class=\"hljs-string\"\u003e\"data/test\"\u003c/span\u003e\nnum_train_imgs = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e root, dirs, files \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e os.\u003cspan class=\"hljs-title function_\"\u003ewalk\u003c/span\u003e(train_path):\n    num_train_imgs += \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(files)\nnum_test_imgs = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e root, dirs, files \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e os.\u003cspan class=\"hljs-title function_\"\u003ewalk\u003c/span\u003e(test_path):\n    num_test_imgs += \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(files)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Number of training images: \"\u003c/span\u003e, num_train_imgs)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Number of testing images: \"\u003c/span\u003e, num_test_imgs)\n\nmodel.\u003cspan class=\"hljs-title function_\"\u003efit\u003c/span\u003e(train_generator, steps_per_epoch=num_train_imgs\u003cspan class=\"hljs-comment\"\u003e//32, epochs=50, validation_data=validation_generator, validation_steps=num_test_imgs//32)\u003c/span\u003e\n\nmodel.\u003cspan class=\"hljs-title function_\"\u003esave\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'model.h5'\u003c/span\u003e)  \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e모델.h5 파일이 현재 디렉토리에 저장됩니다.\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e테스트\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tf\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\nmodel=tf.\u003cspan class=\"hljs-property\"\u003ekeras\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003emodels\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload_model\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'model.h5'\u003c/span\u003e)\n\nfaceDetect=cv2.\u003cspan class=\"hljs-title class_\"\u003eCascadeClassifier\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'haarcascade_frontalface_default.xml'\u003c/span\u003e)\n\nvideo=cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\nlabels_dict={\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\u003cspan class=\"hljs-string\"\u003e'Angry'\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e:\u003cspan class=\"hljs-string\"\u003e'Disgust'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e:\u003cspan class=\"hljs-string\"\u003e'Fear'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e:\u003cspan class=\"hljs-string\"\u003e'Happy'\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e:\u003cspan class=\"hljs-string\"\u003e'Neutral'\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e:\u003cspan class=\"hljs-string\"\u003e'Sad'\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e:\u003cspan class=\"hljs-string\"\u003e'Surprise'\u003c/span\u003e}\n\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e:\n    ret,frame=video.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\n    gray=cv2.\u003cspan class=\"hljs-title function_\"\u003ecvtColor\u003c/span\u003e(frame, cv2.\u003cspan class=\"hljs-property\"\u003eCOLOR_BGR2GRAY\u003c/span\u003e)\n    faces= faceDetect.\u003cspan class=\"hljs-title function_\"\u003edetectMultiScale\u003c/span\u003e(gray, \u003cspan class=\"hljs-number\"\u003e1.3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x,y,w,h \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efaces\u003c/span\u003e:\n        sub_face_img=gray[\u003cspan class=\"hljs-attr\"\u003ey\u003c/span\u003e:y+h, \u003cspan class=\"hljs-attr\"\u003ex\u003c/span\u003e:x+w]\n        resized=cv2.\u003cspan class=\"hljs-title function_\"\u003eresize\u003c/span\u003e(sub_face_img,(\u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e))\n        normalize=resized/\u003cspan class=\"hljs-number\"\u003e255.0\u003c/span\u003e\n        reshaped=np.\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(normalize, (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e))\n        result=model.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(reshaped)\n        label=np.\u003cspan class=\"hljs-title function_\"\u003eargmax\u003c/span\u003e(result, axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(label)\n        cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(frame, (x,y), (x+w, y+h), (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(frame,(x,y),(x+w,y+h),(\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e),\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n        cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(frame,(x,y-\u003cspan class=\"hljs-number\"\u003e40\u003c/span\u003e),(x+w,y),(\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e),-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        cv2.\u003cspan class=\"hljs-title function_\"\u003eputText\u003c/span\u003e(frame, labels_dict[label], (x, y-\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e),cv2.\u003cspan class=\"hljs-property\"\u003eFONT_HERSHEY_SIMPLEX\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0.8\u003c/span\u003e,(\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e),\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n        \n    cv2.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"실시간 감정 인식\"\u003c/span\u003e,frame)\n    k=cv2.\u003cspan class=\"hljs-title function_\"\u003ewaitKey\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e k==\u003cspan class=\"hljs-title function_\"\u003eord\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n\nvideo.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\ncv2.\u003cspan class=\"hljs-title function_\"\u003edestroyAllWindows\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e해당 파이썬 스크립트를 실행해보세요. 코드가 동작할 것을 기대합니다!\u003c/p\u003e\n\u003ch1\u003e개선 사항\u003c/h1\u003e\n\u003cp\u003e이 코드에 주의를 집중시키기 위해 Spatial Transformer의 추가를 활용할 수도 있습니다. 해당 내용은 논문에 언급되어 있습니다.\u003c/p\u003e\n\u003cp\u003eDeep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network- Shervin Minaee, Amirali Abdolrashidi, Expedia Group\nUniversity of California, Riverside\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-DecodingEmotionswithFacialRecognition"},"buildId":"aCCUs-qPrLLLWRnkN0AOd","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>