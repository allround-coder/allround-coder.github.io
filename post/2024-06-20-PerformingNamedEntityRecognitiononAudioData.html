<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>오디오 데이터에 대한 명명된 개체 인식Named Entity Recognition 수행하기 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-06-20-PerformingNamedEntityRecognitiononAudioData" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="오디오 데이터에 대한 명명된 개체 인식Named Entity Recognition 수행하기 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="오디오 데이터에 대한 명명된 개체 인식Named Entity Recognition 수행하기 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-06-20-PerformingNamedEntityRecognitiononAudioData" data-gatsby-head="true"/><meta name="twitter:title" content="오디오 데이터에 대한 명명된 개체 인식Named Entity Recognition 수행하기 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 05:02" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/QH5Mz7n7Y6w0r4_gCGFQf/_buildManifest.js" defer=""></script><script src="/_next/static/QH5Mz7n7Y6w0r4_gCGFQf/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">오디오 데이터에 대한 명명된 개체 인식Named Entity Recognition 수행하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="오디오 데이터에 대한 명명된 개체 인식Named Entity Recognition 수행하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-PerformingNamedEntityRecognitiononAudioData&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_0.png" alt="이미지"></p>
<p>이 Deepnote Notebook에서이 문서에 대한 코드를 찾을 수 있습니다.</p>
<p>Named Entity Recognition (또는 NER)은 주어진 정보에서 실제 세계 개체를 식별하는 작업으로 정의됩니다. NER은 자연어 처리 (NLP) 기술을 사용하여 해결되는 기계 학습에서 인기있는 작업입니다. 텍스트 데이터의 경우, 목적은 주어진 텍스트를 이해하고 실제 세계 개체를 참조하는 단어를 식별하고 추출할 수 있는 모델을 교육하는 것입니다. 이러한 실제 세계 개체는 Named Entities로도 불립니다.</p>
<p>NER 시스템의 고수준 개요는 아래 이미지에 표시되어 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_1.png" alt="이미지"></p>
<p>자연어 처리(NLP) 연구 커뮤니티에서는 텍스트 데이터에 대한 NER에 대해 다양한 접근 방식을 제안해 왔습니다. 이 도메인에서 가장 많이 실험된 데이터셋은 CoNLL 2003과 OntoNotes 데이터셋입니다. 그러나 최근에는 음성 기반 상호 작용 도구의 널리 퍼진 채택으로 인해 연구자와 기관들이 음성 공간에서 NER 시스템을 탐구하고 구축하는 모습을 볼 수 있었습니다.</p>
<p>따라서 본 게시물에서는 AssemblyAI API 및 Python을 사용하여 음성 데이터에서 Named Entity Recognition 시스템을 구축하는 방법을 보여드리겠습니다. 이 통합 시스템은 엄격한 언어 이해, 요약 및 키워드 추출에 광범위하게 적용되며 특히 음성 도메인에서 해결해야 할 중요하고 가치 있는 문제로 여겨집니다. 이 게시물을 종합분석하여 얻은 결과를 해석하고 데이터로부터 적절한 통찰을 얻을 수 있도록 하겠습니다.</p>
<p>이 게시물에 대한 코드는 여기에서 찾을 수 있습니다. 게시물의 하이라이트는 다음과 같습니다:</p>
<div class="content-ad"></div>
<p>오디오 데이터의 엔터티 탐지
엔터티 탐지 결과
엔터티 탐지 인사이트</p>
<p>시작해 봅시다 🚀!</p>
<h1>오디오 데이터의 엔터티 탐지</h1>
<p>이 섹션에서는 미리 녹음된 오디오 파일에서 명명된 엔터티를 식별하고 추출하기 위해 AssemblyAI API를 사용하는 방법을 보여드리겠습니다. 더 나아가, 추출된 엔터티는 "런던"과 같이 위치로 분류되는 방식과 유사하게 개인, 위치, 조직, 날짜, 이벤트, 직업 등과 같은 해당 엔터티 클래스로 분류될 것입니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_2.png">
<h2>단계 1: 요구 사항 설치</h2>
<p>어셈블리AI API를 로컬 머신에서 호출하고 entity detection 모듈 및 entity classifier를 빌드하려면 Python의 requests 패키지가 필요합니다. 다음과 같이 설치할 수 있습니다:</p>
<pre><code class="hljs language-js">pip install requests
</code></pre>
<div class="content-ad"></div>
<h2>단계 2: API 토큰 생성하기</h2>
<p>다음 단계는 AssemblyAI의 음성 대 텍스트 모델에 액세스하기 위한 API 키를 받는 것입니다. 무료로 AssemblyAI 웹 사이트에서 계정을 만들어 이 작업을 수행할 수 있습니다.</p>
<h2>단계 3: 오디오 파일 업로드</h2>
<p>전사 및 명명된 엔티티를 추출할 오디오 파일은 URL을 통해 액세스할 수 있어야 합니다. 따라서 음성 대 텍스트 모델을 호출하기 전에 오디오 파일을 파일 호스팅 서비스에 업로드해야 합니다. 옵션으로는 AWS S3 버킷, SoundCloud와 같은 오디오 호스팅 서비스, 그리고 AssemblyAI의 자체 호스팅 서비스 등이 있습니다. 이번 튜토리얼에서는 오디오 파일을 SoundCloud에 업로드했습니다.</p>
<div class="content-ad"></div>
<h2>단계 4: Entity 감지 및 분류 수행하기</h2>
<p>이제 이 단계에서 오디오 파일에서 엔티티를 감지하기 위한 모든 필수 사전 요구사항을 충족했습니다. 이제 API를 호출하여 명명된 엔티티를 추출할 수 있습니다. 이는 아래 절에서 보다 상세히 설명한 두 단계 프로세스입니다.</p>
<h2>단계 4.1: 전사를 위해 파일 제출하기</h2>
<p>첫 번째 단계는 HTTP Post 요청을 사용하여 음성 파일을 트리거하여 텍스트 모델을 활성화하는 것입니다. POST 요청은 오디오 파일을 audio_url로 사용하고 entity_detection 플래그를 사용하여 명명된 엔티티 인식을 수행하도록 모델에 지시합니다. 오디오 파일에는 여러 화자가 포함되어 있기 때문에 speaker_labels 플래그를 True로 설정했습니다.</p>
<div class="content-ad"></div>
<p>수신된 JSON 응답에 따르면, 포스트 요청의 상태가 대기 중인 것을 나타내며, 파일이 전사 대기열에 있는 것을 의미합니다.</p>
<p>뿐만 아니라, JSON 응답에 entity_detection 플래그도 True인 것으로 나타납니다. 그러나 entities 키에 해당하는 값은 현재 대기 중인 상태이기 때문에 None입니다.</p>
<h2>단계 4.2: 전사 결과 가져오기</h2>
<p>우리의 POST 요청 상태를 확인하고 전사 결과를 보려면, 위에서 수신한 JSON 응답의 id 키를 사용하여 GET 요청을 해야 합니다. 우리는 POST 요청에서 받은 response_id를 전사 상태를 확인하기 위해 전달합니다.</p>
<div class="content-ad"></div>
<h1>Entity Detection Results</h1>
<p>상태가 완료로 변경되면 아래와 유사한 응답을 받게 됩니다.</p>
<ul>
<li>JSON 응답에서 상태를 완료로 확인합니다. 이는 오디오의 정상적인 전사를 나타냅니다.</li>
<li>텍스트 키에는 스피커 수준의 구분 없이 입력 오디오 파일의 전사가 문자열로 포함됩니다. 총 문장 수는 12개입니다.</li>
<li>오디오 파일은 여러 목소리로 구성되어 있기 때문에 단어 키 내에서 모든 스피커 키를 Not Null로 볼 수 있습니다. 스피커 키는 "A" 또는 "B"입니다.</li>
<li>확신 점수는 모델이 개별 단어와 전체 전사 텍스트를 전사하는 데 대한 확신도를 나타냅니다. 이는 0에서 1까지의 범위로, "0"이 가장 낮고 "1"이 가장 높습니다.</li>
<li>오디오의 각각의 12개 문장에서 감지된 엔터티에 액세스하려면 JSON 응답의 entities 키를 사용할 수 있습니다.</li>
<li>오디오 파일에서 식별된 엔터티 수는 17개입니다.</li>
<li>각 엔터티에 대응하여 감지된 엔터티의 카테고리를 나타내는 entity_type을 얻을 수 있습니다.</li>
</ul>
<h1>Entity Detection Insights</h1>
<div class="content-ad"></div>
<p>JSON은 일반적으로 읽고 해석하기 어려운 편이기 때문에, 위의 엔티티 감지 결과를 DataFrame으로 변환하여 데이터를 시각적으로 보기 좋게 만들 수 있습니다. 이렇게 하면 추가적인 분석을 효과적으로 수행하는 데 도움이 될 것입니다. 우리는 텍스트, 문장의 지속 시간, 화자, 그리고 문장의 엔티티 수를 저장할 것입니다. 이를 아래 코드로 구현하였습니다:</p>
<p>위의 코드 스니펫으로 생성된 DataFrame은 아래 이미지에 나와 있습니다. 여기에는 오디오 파일에서 발화된 12개의 문장이 포함되어 있고, 해당 화자 레이블("A" 및 "B"), 문장의 지속 시간(초), 그리고 문장의 엔티티 수를 나타내는 필드가 포함되어 있습니다.</p>
<p><img src="/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_3.png" alt="DataFrame"></p>
<p>다음으로, 오디오 파일에서 식별된 엔티티를 취하는 또 다른 DataFrame을 생성합니다. 이는 아래의 코드 블록을 따라 구현되었습니다:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_4.png">
<h2>#1 Speaker-Sentence distribution</h2>
<p>먼저, 오디오 파일에서 각 화자가 말한 문장의 수를 계산해 봅시다. 아래와 같이 value_counts() 메서드를 사용하여 이 작업을 수행할 수 있습니다.</p>
<p>각 화자의 분포를 백분율로 표시하려면 value_counts() 메서드에 normalize=True를 전달하면 됩니다.</p>
<div class="content-ad"></div>
<h2>#2 발화자별 시간 분포</h2>
<p>이제 개별 발화자들의 총 발화 시간을 찾아봅시다. 아래에서 확인할 수 있습니다:</p>
<p>groupby() 메서드를 사용하여 개별 문장의 지속 시간을 합산하여 총 지속 시간을 계산합니다. 발화자 "B"가 지속 시간 측면에서 우세한 발화자입니다.</p>
<h2>#3 발화자 엔터티-카운트 분포</h2>
<div class="content-ad"></div>
<p>오디오 파일에서 총 17개의 엔티티가 언급되었는데, 그 중 9개는 "A" 스피커에 의해 발화되었고 나머지는 "B" 스피커가 발화했습니다.</p>
<h2>#4 엔티티 유형 분포</h2>
<p>다음으로, 오디오 파일에서 발화된 개별 엔티티 유형의 분포를 분석해 보겠습니다. 아래에서 value_counts() 메서드를 사용하여 이를 구현했습니다:</p>
<h2>#5 스피커-엔티티 유형 분포</h2>
<div class="content-ad"></div>
<p>마지막으로, 각 화자가 말한 각 Entity 유형의 수를 평가해 봅시다. 여기서는 groupby() 메소드 대신 시각화를 위해 crosstab()을 사용할 것입니다. 아래에서 이를 보여드리겠습니다:</p>
<p>이렇게 말씀드리면, 이 게시물에서는 AssemblyAI API를 사용하여 사전 녹음된 오디오 파일에서 Named Entity Recognition 모듈을 구축했습니다. 마지막으로, 감지된 Entity들에 대한 철저한 분석을 수행했습니다. API에서 얻은 결과는 입력 오디오 파일의 12개 개별 문장 내에서 식별된 17개 Entity를 강조했습니다.</p>
<p>독서해 주셔서 감사합니다!</p>
<p>🚀 내 매일 뉴스레터를 구독하시면 550페이지 이상의 무료 데이터 과학 PDF와 320편 이상의 게시물을 받아 보실 수 있습니다:</p>
<div class="content-ad"></div>
<img src="https://miro.medium.com/v2/resize:fit:1400/0*gtA9qrsOn5ZsnBXW.gif">
<p>DataDrivenInvestor.com에서 저희를 방문해주세요.</p>
<p>DDIntel을 여기서 구독하세요.</p>
<p>주요 기사:</p>
<div class="content-ad"></div>
<p>우리의 창조자 생태계에 참여해보세요.</p>
<p>DDI 공식 텔레그램 채널: <a href="https://t.me/+tafUp6ecEys4YjQ1" rel="nofollow" target="_blank">https://t.me/+tafUp6ecEys4YjQ1</a></p>
<p>LinkedIn, Twitter, YouTube, 그리고 Facebook에서 팔로우해주세요.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"오디오 데이터에 대한 명명된 개체 인식Named Entity Recognition 수행하기","description":"","date":"2024-06-20 05:02","slug":"2024-06-20-PerformingNamedEntityRecognitiononAudioData","content":"\n\n![이미지](/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_0.png)\n\n이 Deepnote Notebook에서이 문서에 대한 코드를 찾을 수 있습니다.\n\nNamed Entity Recognition (또는 NER)은 주어진 정보에서 실제 세계 개체를 식별하는 작업으로 정의됩니다. NER은 자연어 처리 (NLP) 기술을 사용하여 해결되는 기계 학습에서 인기있는 작업입니다. 텍스트 데이터의 경우, 목적은 주어진 텍스트를 이해하고 실제 세계 개체를 참조하는 단어를 식별하고 추출할 수 있는 모델을 교육하는 것입니다. 이러한 실제 세계 개체는 Named Entities로도 불립니다.\n\nNER 시스템의 고수준 개요는 아래 이미지에 표시되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_1.png)\n\n자연어 처리(NLP) 연구 커뮤니티에서는 텍스트 데이터에 대한 NER에 대해 다양한 접근 방식을 제안해 왔습니다. 이 도메인에서 가장 많이 실험된 데이터셋은 CoNLL 2003과 OntoNotes 데이터셋입니다. 그러나 최근에는 음성 기반 상호 작용 도구의 널리 퍼진 채택으로 인해 연구자와 기관들이 음성 공간에서 NER 시스템을 탐구하고 구축하는 모습을 볼 수 있었습니다.\n\n따라서 본 게시물에서는 AssemblyAI API 및 Python을 사용하여 음성 데이터에서 Named Entity Recognition 시스템을 구축하는 방법을 보여드리겠습니다. 이 통합 시스템은 엄격한 언어 이해, 요약 및 키워드 추출에 광범위하게 적용되며 특히 음성 도메인에서 해결해야 할 중요하고 가치 있는 문제로 여겨집니다. 이 게시물을 종합분석하여 얻은 결과를 해석하고 데이터로부터 적절한 통찰을 얻을 수 있도록 하겠습니다.\n\n이 게시물에 대한 코드는 여기에서 찾을 수 있습니다. 게시물의 하이라이트는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오디오 데이터의 엔터티 탐지\n엔터티 탐지 결과\n엔터티 탐지 인사이트\n\n시작해 봅시다 🚀!\n\n# 오디오 데이터의 엔터티 탐지\n\n이 섹션에서는 미리 녹음된 오디오 파일에서 명명된 엔터티를 식별하고 추출하기 위해 AssemblyAI API를 사용하는 방법을 보여드리겠습니다. 더 나아가, 추출된 엔터티는 \"런던\"과 같이 위치로 분류되는 방식과 유사하게 개인, 위치, 조직, 날짜, 이벤트, 직업 등과 같은 해당 엔터티 클래스로 분류될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_2.png\" /\u003e\n\n## 단계 1: 요구 사항 설치\n\n어셈블리AI API를 로컬 머신에서 호출하고 entity detection 모듈 및 entity classifier를 빌드하려면 Python의 requests 패키지가 필요합니다. 다음과 같이 설치할 수 있습니다:\n\n```js\r\npip install requests\r\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 2: API 토큰 생성하기\n\n다음 단계는 AssemblyAI의 음성 대 텍스트 모델에 액세스하기 위한 API 키를 받는 것입니다. 무료로 AssemblyAI 웹 사이트에서 계정을 만들어 이 작업을 수행할 수 있습니다.\n\n## 단계 3: 오디오 파일 업로드\n\n전사 및 명명된 엔티티를 추출할 오디오 파일은 URL을 통해 액세스할 수 있어야 합니다. 따라서 음성 대 텍스트 모델을 호출하기 전에 오디오 파일을 파일 호스팅 서비스에 업로드해야 합니다. 옵션으로는 AWS S3 버킷, SoundCloud와 같은 오디오 호스팅 서비스, 그리고 AssemblyAI의 자체 호스팅 서비스 등이 있습니다. 이번 튜토리얼에서는 오디오 파일을 SoundCloud에 업로드했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 4: Entity 감지 및 분류 수행하기\n\n이제 이 단계에서 오디오 파일에서 엔티티를 감지하기 위한 모든 필수 사전 요구사항을 충족했습니다. 이제 API를 호출하여 명명된 엔티티를 추출할 수 있습니다. 이는 아래 절에서 보다 상세히 설명한 두 단계 프로세스입니다.\n\n## 단계 4.1: 전사를 위해 파일 제출하기\n\n첫 번째 단계는 HTTP Post 요청을 사용하여 음성 파일을 트리거하여 텍스트 모델을 활성화하는 것입니다. POST 요청은 오디오 파일을 audio_url로 사용하고 entity_detection 플래그를 사용하여 명명된 엔티티 인식을 수행하도록 모델에 지시합니다. 오디오 파일에는 여러 화자가 포함되어 있기 때문에 speaker_labels 플래그를 True로 설정했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수신된 JSON 응답에 따르면, 포스트 요청의 상태가 대기 중인 것을 나타내며, 파일이 전사 대기열에 있는 것을 의미합니다.\n\n뿐만 아니라, JSON 응답에 entity_detection 플래그도 True인 것으로 나타납니다. 그러나 entities 키에 해당하는 값은 현재 대기 중인 상태이기 때문에 None입니다.\n\n## 단계 4.2: 전사 결과 가져오기\n\n우리의 POST 요청 상태를 확인하고 전사 결과를 보려면, 위에서 수신한 JSON 응답의 id 키를 사용하여 GET 요청을 해야 합니다. 우리는 POST 요청에서 받은 response_id를 전사 상태를 확인하기 위해 전달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Entity Detection Results\n\n상태가 완료로 변경되면 아래와 유사한 응답을 받게 됩니다.\n\n- JSON 응답에서 상태를 완료로 확인합니다. 이는 오디오의 정상적인 전사를 나타냅니다.\n- 텍스트 키에는 스피커 수준의 구분 없이 입력 오디오 파일의 전사가 문자열로 포함됩니다. 총 문장 수는 12개입니다.\n- 오디오 파일은 여러 목소리로 구성되어 있기 때문에 단어 키 내에서 모든 스피커 키를 Not Null로 볼 수 있습니다. 스피커 키는 \"A\" 또는 \"B\"입니다.\n- 확신 점수는 모델이 개별 단어와 전체 전사 텍스트를 전사하는 데 대한 확신도를 나타냅니다. 이는 0에서 1까지의 범위로, \"0\"이 가장 낮고 \"1\"이 가장 높습니다.\n- 오디오의 각각의 12개 문장에서 감지된 엔터티에 액세스하려면 JSON 응답의 entities 키를 사용할 수 있습니다.\n- 오디오 파일에서 식별된 엔터티 수는 17개입니다.\n- 각 엔터티에 대응하여 감지된 엔터티의 카테고리를 나타내는 entity_type을 얻을 수 있습니다.\n\n# Entity Detection Insights\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nJSON은 일반적으로 읽고 해석하기 어려운 편이기 때문에, 위의 엔티티 감지 결과를 DataFrame으로 변환하여 데이터를 시각적으로 보기 좋게 만들 수 있습니다. 이렇게 하면 추가적인 분석을 효과적으로 수행하는 데 도움이 될 것입니다. 우리는 텍스트, 문장의 지속 시간, 화자, 그리고 문장의 엔티티 수를 저장할 것입니다. 이를 아래 코드로 구현하였습니다:\n\n위의 코드 스니펫으로 생성된 DataFrame은 아래 이미지에 나와 있습니다. 여기에는 오디오 파일에서 발화된 12개의 문장이 포함되어 있고, 해당 화자 레이블(\"A\" 및 \"B\"), 문장의 지속 시간(초), 그리고 문장의 엔티티 수를 나타내는 필드가 포함되어 있습니다.\n\n![DataFrame](/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_3.png)\n\n다음으로, 오디오 파일에서 식별된 엔티티를 취하는 또 다른 DataFrame을 생성합니다. 이는 아래의 코드 블록을 따라 구현되었습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_4.png\" /\u003e\n\n## #1 Speaker-Sentence distribution\n\n먼저, 오디오 파일에서 각 화자가 말한 문장의 수를 계산해 봅시다. 아래와 같이 value_counts() 메서드를 사용하여 이 작업을 수행할 수 있습니다.\n\n각 화자의 분포를 백분율로 표시하려면 value_counts() 메서드에 normalize=True를 전달하면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## #2 발화자별 시간 분포\n\n이제 개별 발화자들의 총 발화 시간을 찾아봅시다. 아래에서 확인할 수 있습니다:\n\ngroupby() 메서드를 사용하여 개별 문장의 지속 시간을 합산하여 총 지속 시간을 계산합니다. 발화자 \"B\"가 지속 시간 측면에서 우세한 발화자입니다.\n\n## #3 발화자 엔터티-카운트 분포\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오디오 파일에서 총 17개의 엔티티가 언급되었는데, 그 중 9개는 \"A\" 스피커에 의해 발화되었고 나머지는 \"B\" 스피커가 발화했습니다.\n\n## #4 엔티티 유형 분포\n\n다음으로, 오디오 파일에서 발화된 개별 엔티티 유형의 분포를 분석해 보겠습니다. 아래에서 value_counts() 메서드를 사용하여 이를 구현했습니다:\n\n## #5 스피커-엔티티 유형 분포\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 각 화자가 말한 각 Entity 유형의 수를 평가해 봅시다. 여기서는 groupby() 메소드 대신 시각화를 위해 crosstab()을 사용할 것입니다. 아래에서 이를 보여드리겠습니다:\n\n이렇게 말씀드리면, 이 게시물에서는 AssemblyAI API를 사용하여 사전 녹음된 오디오 파일에서 Named Entity Recognition 모듈을 구축했습니다. 마지막으로, 감지된 Entity들에 대한 철저한 분석을 수행했습니다. API에서 얻은 결과는 입력 오디오 파일의 12개 개별 문장 내에서 식별된 17개 Entity를 강조했습니다.\n\n독서해 주셔서 감사합니다!\n\n🚀 내 매일 뉴스레터를 구독하시면 550페이지 이상의 무료 데이터 과학 PDF와 320편 이상의 게시물을 받아 보실 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/0*gtA9qrsOn5ZsnBXW.gif\" /\u003e\n\nDataDrivenInvestor.com에서 저희를 방문해주세요.\n\nDDIntel을 여기서 구독하세요.\n\n주요 기사:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 창조자 생태계에 참여해보세요.\n\nDDI 공식 텔레그램 채널: [https://t.me/+tafUp6ecEys4YjQ1](https://t.me/+tafUp6ecEys4YjQ1)\n\nLinkedIn, Twitter, YouTube, 그리고 Facebook에서 팔로우해주세요.","ogImage":{"url":"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_0.png"},"coverImage":"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이 Deepnote Notebook에서이 문서에 대한 코드를 찾을 수 있습니다.\u003c/p\u003e\n\u003cp\u003eNamed Entity Recognition (또는 NER)은 주어진 정보에서 실제 세계 개체를 식별하는 작업으로 정의됩니다. NER은 자연어 처리 (NLP) 기술을 사용하여 해결되는 기계 학습에서 인기있는 작업입니다. 텍스트 데이터의 경우, 목적은 주어진 텍스트를 이해하고 실제 세계 개체를 참조하는 단어를 식별하고 추출할 수 있는 모델을 교육하는 것입니다. 이러한 실제 세계 개체는 Named Entities로도 불립니다.\u003c/p\u003e\n\u003cp\u003eNER 시스템의 고수준 개요는 아래 이미지에 표시되어 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e자연어 처리(NLP) 연구 커뮤니티에서는 텍스트 데이터에 대한 NER에 대해 다양한 접근 방식을 제안해 왔습니다. 이 도메인에서 가장 많이 실험된 데이터셋은 CoNLL 2003과 OntoNotes 데이터셋입니다. 그러나 최근에는 음성 기반 상호 작용 도구의 널리 퍼진 채택으로 인해 연구자와 기관들이 음성 공간에서 NER 시스템을 탐구하고 구축하는 모습을 볼 수 있었습니다.\u003c/p\u003e\n\u003cp\u003e따라서 본 게시물에서는 AssemblyAI API 및 Python을 사용하여 음성 데이터에서 Named Entity Recognition 시스템을 구축하는 방법을 보여드리겠습니다. 이 통합 시스템은 엄격한 언어 이해, 요약 및 키워드 추출에 광범위하게 적용되며 특히 음성 도메인에서 해결해야 할 중요하고 가치 있는 문제로 여겨집니다. 이 게시물을 종합분석하여 얻은 결과를 해석하고 데이터로부터 적절한 통찰을 얻을 수 있도록 하겠습니다.\u003c/p\u003e\n\u003cp\u003e이 게시물에 대한 코드는 여기에서 찾을 수 있습니다. 게시물의 하이라이트는 다음과 같습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e오디오 데이터의 엔터티 탐지\n엔터티 탐지 결과\n엔터티 탐지 인사이트\u003c/p\u003e\n\u003cp\u003e시작해 봅시다 🚀!\u003c/p\u003e\n\u003ch1\u003e오디오 데이터의 엔터티 탐지\u003c/h1\u003e\n\u003cp\u003e이 섹션에서는 미리 녹음된 오디오 파일에서 명명된 엔터티를 식별하고 추출하기 위해 AssemblyAI API를 사용하는 방법을 보여드리겠습니다. 더 나아가, 추출된 엔터티는 \"런던\"과 같이 위치로 분류되는 방식과 유사하게 개인, 위치, 조직, 날짜, 이벤트, 직업 등과 같은 해당 엔터티 클래스로 분류될 것입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_2.png\"\u003e\n\u003ch2\u003e단계 1: 요구 사항 설치\u003c/h2\u003e\n\u003cp\u003e어셈블리AI API를 로컬 머신에서 호출하고 entity detection 모듈 및 entity classifier를 빌드하려면 Python의 requests 패키지가 필요합니다. 다음과 같이 설치할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install requests\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 2: API 토큰 생성하기\u003c/h2\u003e\n\u003cp\u003e다음 단계는 AssemblyAI의 음성 대 텍스트 모델에 액세스하기 위한 API 키를 받는 것입니다. 무료로 AssemblyAI 웹 사이트에서 계정을 만들어 이 작업을 수행할 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e단계 3: 오디오 파일 업로드\u003c/h2\u003e\n\u003cp\u003e전사 및 명명된 엔티티를 추출할 오디오 파일은 URL을 통해 액세스할 수 있어야 합니다. 따라서 음성 대 텍스트 모델을 호출하기 전에 오디오 파일을 파일 호스팅 서비스에 업로드해야 합니다. 옵션으로는 AWS S3 버킷, SoundCloud와 같은 오디오 호스팅 서비스, 그리고 AssemblyAI의 자체 호스팅 서비스 등이 있습니다. 이번 튜토리얼에서는 오디오 파일을 SoundCloud에 업로드했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e단계 4: Entity 감지 및 분류 수행하기\u003c/h2\u003e\n\u003cp\u003e이제 이 단계에서 오디오 파일에서 엔티티를 감지하기 위한 모든 필수 사전 요구사항을 충족했습니다. 이제 API를 호출하여 명명된 엔티티를 추출할 수 있습니다. 이는 아래 절에서 보다 상세히 설명한 두 단계 프로세스입니다.\u003c/p\u003e\n\u003ch2\u003e단계 4.1: 전사를 위해 파일 제출하기\u003c/h2\u003e\n\u003cp\u003e첫 번째 단계는 HTTP Post 요청을 사용하여 음성 파일을 트리거하여 텍스트 모델을 활성화하는 것입니다. POST 요청은 오디오 파일을 audio_url로 사용하고 entity_detection 플래그를 사용하여 명명된 엔티티 인식을 수행하도록 모델에 지시합니다. 오디오 파일에는 여러 화자가 포함되어 있기 때문에 speaker_labels 플래그를 True로 설정했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e수신된 JSON 응답에 따르면, 포스트 요청의 상태가 대기 중인 것을 나타내며, 파일이 전사 대기열에 있는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e뿐만 아니라, JSON 응답에 entity_detection 플래그도 True인 것으로 나타납니다. 그러나 entities 키에 해당하는 값은 현재 대기 중인 상태이기 때문에 None입니다.\u003c/p\u003e\n\u003ch2\u003e단계 4.2: 전사 결과 가져오기\u003c/h2\u003e\n\u003cp\u003e우리의 POST 요청 상태를 확인하고 전사 결과를 보려면, 위에서 수신한 JSON 응답의 id 키를 사용하여 GET 요청을 해야 합니다. 우리는 POST 요청에서 받은 response_id를 전사 상태를 확인하기 위해 전달합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003eEntity Detection Results\u003c/h1\u003e\n\u003cp\u003e상태가 완료로 변경되면 아래와 유사한 응답을 받게 됩니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJSON 응답에서 상태를 완료로 확인합니다. 이는 오디오의 정상적인 전사를 나타냅니다.\u003c/li\u003e\n\u003cli\u003e텍스트 키에는 스피커 수준의 구분 없이 입력 오디오 파일의 전사가 문자열로 포함됩니다. 총 문장 수는 12개입니다.\u003c/li\u003e\n\u003cli\u003e오디오 파일은 여러 목소리로 구성되어 있기 때문에 단어 키 내에서 모든 스피커 키를 Not Null로 볼 수 있습니다. 스피커 키는 \"A\" 또는 \"B\"입니다.\u003c/li\u003e\n\u003cli\u003e확신 점수는 모델이 개별 단어와 전체 전사 텍스트를 전사하는 데 대한 확신도를 나타냅니다. 이는 0에서 1까지의 범위로, \"0\"이 가장 낮고 \"1\"이 가장 높습니다.\u003c/li\u003e\n\u003cli\u003e오디오의 각각의 12개 문장에서 감지된 엔터티에 액세스하려면 JSON 응답의 entities 키를 사용할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e오디오 파일에서 식별된 엔터티 수는 17개입니다.\u003c/li\u003e\n\u003cli\u003e각 엔터티에 대응하여 감지된 엔터티의 카테고리를 나타내는 entity_type을 얻을 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eEntity Detection Insights\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eJSON은 일반적으로 읽고 해석하기 어려운 편이기 때문에, 위의 엔티티 감지 결과를 DataFrame으로 변환하여 데이터를 시각적으로 보기 좋게 만들 수 있습니다. 이렇게 하면 추가적인 분석을 효과적으로 수행하는 데 도움이 될 것입니다. 우리는 텍스트, 문장의 지속 시간, 화자, 그리고 문장의 엔티티 수를 저장할 것입니다. 이를 아래 코드로 구현하였습니다:\u003c/p\u003e\n\u003cp\u003e위의 코드 스니펫으로 생성된 DataFrame은 아래 이미지에 나와 있습니다. 여기에는 오디오 파일에서 발화된 12개의 문장이 포함되어 있고, 해당 화자 레이블(\"A\" 및 \"B\"), 문장의 지속 시간(초), 그리고 문장의 엔티티 수를 나타내는 필드가 포함되어 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_3.png\" alt=\"DataFrame\"\u003e\u003c/p\u003e\n\u003cp\u003e다음으로, 오디오 파일에서 식별된 엔티티를 취하는 또 다른 DataFrame을 생성합니다. 이는 아래의 코드 블록을 따라 구현되었습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"/assets/img/2024-06-20-PerformingNamedEntityRecognitiononAudioData_4.png\"\u003e\n\u003ch2\u003e#1 Speaker-Sentence distribution\u003c/h2\u003e\n\u003cp\u003e먼저, 오디오 파일에서 각 화자가 말한 문장의 수를 계산해 봅시다. 아래와 같이 value_counts() 메서드를 사용하여 이 작업을 수행할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e각 화자의 분포를 백분율로 표시하려면 value_counts() 메서드에 normalize=True를 전달하면 됩니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch2\u003e#2 발화자별 시간 분포\u003c/h2\u003e\n\u003cp\u003e이제 개별 발화자들의 총 발화 시간을 찾아봅시다. 아래에서 확인할 수 있습니다:\u003c/p\u003e\n\u003cp\u003egroupby() 메서드를 사용하여 개별 문장의 지속 시간을 합산하여 총 지속 시간을 계산합니다. 발화자 \"B\"가 지속 시간 측면에서 우세한 발화자입니다.\u003c/p\u003e\n\u003ch2\u003e#3 발화자 엔터티-카운트 분포\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e오디오 파일에서 총 17개의 엔티티가 언급되었는데, 그 중 9개는 \"A\" 스피커에 의해 발화되었고 나머지는 \"B\" 스피커가 발화했습니다.\u003c/p\u003e\n\u003ch2\u003e#4 엔티티 유형 분포\u003c/h2\u003e\n\u003cp\u003e다음으로, 오디오 파일에서 발화된 개별 엔티티 유형의 분포를 분석해 보겠습니다. 아래에서 value_counts() 메서드를 사용하여 이를 구현했습니다:\u003c/p\u003e\n\u003ch2\u003e#5 스피커-엔티티 유형 분포\u003c/h2\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e마지막으로, 각 화자가 말한 각 Entity 유형의 수를 평가해 봅시다. 여기서는 groupby() 메소드 대신 시각화를 위해 crosstab()을 사용할 것입니다. 아래에서 이를 보여드리겠습니다:\u003c/p\u003e\n\u003cp\u003e이렇게 말씀드리면, 이 게시물에서는 AssemblyAI API를 사용하여 사전 녹음된 오디오 파일에서 Named Entity Recognition 모듈을 구축했습니다. 마지막으로, 감지된 Entity들에 대한 철저한 분석을 수행했습니다. API에서 얻은 결과는 입력 오디오 파일의 12개 개별 문장 내에서 식별된 17개 Entity를 강조했습니다.\u003c/p\u003e\n\u003cp\u003e독서해 주셔서 감사합니다!\u003c/p\u003e\n\u003cp\u003e🚀 내 매일 뉴스레터를 구독하시면 550페이지 이상의 무료 데이터 과학 PDF와 320편 이상의 게시물을 받아 보실 수 있습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/0*gtA9qrsOn5ZsnBXW.gif\"\u003e\n\u003cp\u003eDataDrivenInvestor.com에서 저희를 방문해주세요.\u003c/p\u003e\n\u003cp\u003eDDIntel을 여기서 구독하세요.\u003c/p\u003e\n\u003cp\u003e주요 기사:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우리의 창조자 생태계에 참여해보세요.\u003c/p\u003e\n\u003cp\u003eDDI 공식 텔레그램 채널: \u003ca href=\"https://t.me/+tafUp6ecEys4YjQ1\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://t.me/+tafUp6ecEys4YjQ1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eLinkedIn, Twitter, YouTube, 그리고 Facebook에서 팔로우해주세요.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-PerformingNamedEntityRecognitiononAudioData"},"buildId":"QH5Mz7n7Y6w0r4_gCGFQf","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>