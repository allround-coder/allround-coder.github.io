<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-15-HelloGPT-4o" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-HelloGPT-4o_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-15-HelloGPT-4o" data-gatsby-head="true"/><meta name="twitter:title" content="친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-HelloGPT-4o_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 03:08" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-a8eda6c93e0b14fe.js" defer=""></script><script src="/_next/static/R94iUTCf1NWeBC_VXjTJG/_buildManifest.js" defer=""></script><script src="/_next/static/R94iUTCf1NWeBC_VXjTJG/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">친근한 톤으로 번역 GPT-4를 소개합니다</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="친근한 톤으로 번역 GPT-4를 소개합니다" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/assets/profile.jpg"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">2<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-HelloGPT-4o&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>GPT-4o (&quot;o&quot; for &quot;omni&quot;)은 훨씬 자연스러운 인간-컴퓨터 상호작용을 위한 한 단계입니다. 이는 텍스트, 오디오 및 이미지의 어떤 조합이든 입력으로 받아들이고 어떤 조합이든 텍스트, 오디오 및 이미지 출력을 생성합니다. 이는 대화에서 인간의 응답 시간과 유사한 232밀리초 이내의 오디오 입력에 응답할 수 있으며, 평균 320밀리초로 응답할 수 있습니다. GPT-4 Turbo의 영문 텍스트와 코드에서의 성능과 비슷하며, 비영어 언어 텍스트에서는 상당한 향상을 보입니다. 또한 API에서 50% 빠르고 저렴합니다. GPT-4o는 기존 모델과 비교했을 때 비전 및 오디오 이해 능력이 특히 좋습니다.</p>
<h1>모델 기능</h1>
<p>두 대 GPT-4o가 상호작용하고 노래합니다.</p>
<p>인터뷰 준비요.</p>
<p>안녕하세요! 위에 표기된 사항들을 아래와 같이 번역해 드리겠습니다.</p>
<p>바위 가위 보 게임.</p>
<p>비꼼.</p>
<p>Sal과 Imran Khan과 함께 하는 수학.</p>
<p>둘의 GPT-4가 화음을 이루다.</p>
<p>더 궁금한 사항이 있으시면 언제든지 알려주세요!</p>
<p>Change the table tag to Markdown format.</p>
<p>| Point and learn Spanish |
| Meeting AI |
| Real-time translation |
| Lullaby |</p>
<p>더 빨리 말하기</p>
<p>생일 축하해.</p>
<p>개.</p>
<p>아빠 농담.</p>
<p>GPT-4o와 함께 런던에 있는 BeMyEyes의 Andy입니다.</p>
<p>고객 서비스 프로토타입.</p>
<p>GPT-4o 이전에는 Voice Mode를 사용하여 ChatGPT와 대화를 나눌 수 있었는데, 그 때의 대기 시간은 평균 2.8초(GPT-3.5) 및 5.4초(GPT-4)였습니다. 이를 위해 Voice Mode는 오디오를 텍스트로 변환하는 간단한 모델, 텍스트를 입력 받고 텍스트를 출력하는 GPT-3.5 또는 GPT-4, 그리고 이 텍스트를 다시 오디오로 변환하는 세 번째 간단한 모델의 파이프라인입니다. 이 과정은 주요 지능 소스인 GPT-4가 많은 정보를 잃게 되어, 톤, 다중 스피커, 배경 소음을 직접적으로 관찰할 수 없으며, 웃음소리, 노래, 감정을 표현할 수 없다는 것을 의미합니다.</p>
<p>GPT-4o를 통해 우리는 텍스트, 비전, 오디오를 모두 처리하는 단일 새 모델을 최종적으로 훈련시켰습니다. GPT-4o는 이러한 여러 모달리티를 결합한 첫 번째 모델이기 때문에, 우리는 아직 모델이 무엇을 할 수 있고 그 한계가 무엇인지 탐색하는 과정에서 원천적인 단계에 머물러 있습니다.</p>
<h1>모델 평가</h1>
<p>전통적인 기준에 따르면, GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 Turbo 수준의 성능을 달성하며, 동시에 다국어, 오디오 및 비전 능력에서 새로운 기록을 세우고 있습니다.</p>
<p><img src="/assets/img/2024-05-15-HelloGPT-4o_0.png" alt="이미지"/></p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"친근한 톤으로 번역 GPT-4를 소개합니다","description":"","date":"2024-05-15 03:08","slug":"2024-05-15-HelloGPT-4o","content":"\n\nGPT-4o (\"o\" for \"omni\")은 훨씬 자연스러운 인간-컴퓨터 상호작용을 위한 한 단계입니다. 이는 텍스트, 오디오 및 이미지의 어떤 조합이든 입력으로 받아들이고 어떤 조합이든 텍스트, 오디오 및 이미지 출력을 생성합니다. 이는 대화에서 인간의 응답 시간과 유사한 232밀리초 이내의 오디오 입력에 응답할 수 있으며, 평균 320밀리초로 응답할 수 있습니다. GPT-4 Turbo의 영문 텍스트와 코드에서의 성능과 비슷하며, 비영어 언어 텍스트에서는 상당한 향상을 보입니다. 또한 API에서 50% 빠르고 저렴합니다. GPT-4o는 기존 모델과 비교했을 때 비전 및 오디오 이해 능력이 특히 좋습니다.\n\n# 모델 기능\n\n두 대 GPT-4o가 상호작용하고 노래합니다.\n\n인터뷰 준비요.\n\n\n\n안녕하세요! 위에 표기된 사항들을 아래와 같이 번역해 드리겠습니다.\n\n\n바위 가위 보 게임.\n\n비꼼.\n\nSal과 Imran Khan과 함께 하는 수학.\n\n둘의 GPT-4가 화음을 이루다.\n \n\n더 궁금한 사항이 있으시면 언제든지 알려주세요!\n\n\n\nChange the table tag to Markdown format.\n\n| Point and learn Spanish |\n| Meeting AI |\n| Real-time translation |\n| Lullaby |\n\n\n\n더 빨리 말하기\n\n생일 축하해.\n\n개.\n\n아빠 농담.\n\n\n\nGPT-4o와 함께 런던에 있는 BeMyEyes의 Andy입니다.\n\n고객 서비스 프로토타입.\n\nGPT-4o 이전에는 Voice Mode를 사용하여 ChatGPT와 대화를 나눌 수 있었는데, 그 때의 대기 시간은 평균 2.8초(GPT-3.5) 및 5.4초(GPT-4)였습니다. 이를 위해 Voice Mode는 오디오를 텍스트로 변환하는 간단한 모델, 텍스트를 입력 받고 텍스트를 출력하는 GPT-3.5 또는 GPT-4, 그리고 이 텍스트를 다시 오디오로 변환하는 세 번째 간단한 모델의 파이프라인입니다. 이 과정은 주요 지능 소스인 GPT-4가 많은 정보를 잃게 되어, 톤, 다중 스피커, 배경 소음을 직접적으로 관찰할 수 없으며, 웃음소리, 노래, 감정을 표현할 수 없다는 것을 의미합니다.\n\nGPT-4o를 통해 우리는 텍스트, 비전, 오디오를 모두 처리하는 단일 새 모델을 최종적으로 훈련시켰습니다. GPT-4o는 이러한 여러 모달리티를 결합한 첫 번째 모델이기 때문에, 우리는 아직 모델이 무엇을 할 수 있고 그 한계가 무엇인지 탐색하는 과정에서 원천적인 단계에 머물러 있습니다.\n\n\n\n# 모델 평가\n\n전통적인 기준에 따르면, GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 Turbo 수준의 성능을 달성하며, 동시에 다국어, 오디오 및 비전 능력에서 새로운 기록을 세우고 있습니다.\n\n![이미지](/assets/img/2024-05-15-HelloGPT-4o_0.png)","ogImage":{"url":"/assets/img/2024-05-15-HelloGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-HelloGPT-4o_0.png","tag":["Tech"],"readingTime":2},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h1: \"h1\",\n    img: \"img\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"GPT-4o (\\\"o\\\" for \\\"omni\\\")은 훨씬 자연스러운 인간-컴퓨터 상호작용을 위한 한 단계입니다. 이는 텍스트, 오디오 및 이미지의 어떤 조합이든 입력으로 받아들이고 어떤 조합이든 텍스트, 오디오 및 이미지 출력을 생성합니다. 이는 대화에서 인간의 응답 시간과 유사한 232밀리초 이내의 오디오 입력에 응답할 수 있으며, 평균 320밀리초로 응답할 수 있습니다. GPT-4 Turbo의 영문 텍스트와 코드에서의 성능과 비슷하며, 비영어 언어 텍스트에서는 상당한 향상을 보입니다. 또한 API에서 50% 빠르고 저렴합니다. GPT-4o는 기존 모델과 비교했을 때 비전 및 오디오 이해 능력이 특히 좋습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"모델 기능\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"두 대 GPT-4o가 상호작용하고 노래합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"인터뷰 준비요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"안녕하세요! 위에 표기된 사항들을 아래와 같이 번역해 드리겠습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"바위 가위 보 게임.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"비꼼.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Sal과 Imran Khan과 함께 하는 수학.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"둘의 GPT-4가 화음을 이루다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"더 궁금한 사항이 있으시면 언제든지 알려주세요!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Change the table tag to Markdown format.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"| Point and learn Spanish |\\n| Meeting AI |\\n| Real-time translation |\\n| Lullaby |\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"더 빨리 말하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"생일 축하해.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"개.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아빠 농담.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"GPT-4o와 함께 런던에 있는 BeMyEyes의 Andy입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"고객 서비스 프로토타입.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"GPT-4o 이전에는 Voice Mode를 사용하여 ChatGPT와 대화를 나눌 수 있었는데, 그 때의 대기 시간은 평균 2.8초(GPT-3.5) 및 5.4초(GPT-4)였습니다. 이를 위해 Voice Mode는 오디오를 텍스트로 변환하는 간단한 모델, 텍스트를 입력 받고 텍스트를 출력하는 GPT-3.5 또는 GPT-4, 그리고 이 텍스트를 다시 오디오로 변환하는 세 번째 간단한 모델의 파이프라인입니다. 이 과정은 주요 지능 소스인 GPT-4가 많은 정보를 잃게 되어, 톤, 다중 스피커, 배경 소음을 직접적으로 관찰할 수 없으며, 웃음소리, 노래, 감정을 표현할 수 없다는 것을 의미합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"GPT-4o를 통해 우리는 텍스트, 비전, 오디오를 모두 처리하는 단일 새 모델을 최종적으로 훈련시켰습니다. GPT-4o는 이러한 여러 모달리티를 결합한 첫 번째 모델이기 때문에, 우리는 아직 모델이 무엇을 할 수 있고 그 한계가 무엇인지 탐색하는 과정에서 원천적인 단계에 머물러 있습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"모델 평가\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"전통적인 기준에 따르면, GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 Turbo 수준의 성능을 달성하며, 동시에 다국어, 오디오 및 비전 능력에서 새로운 기록을 세우고 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-15-HelloGPT-4o_0.png\",\n        alt: \"이미지\"\n      })\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-HelloGPT-4o"},"buildId":"R94iUTCf1NWeBC_VXjTJG","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>