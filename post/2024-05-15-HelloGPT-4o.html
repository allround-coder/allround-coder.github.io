<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-15-HelloGPT-4o" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-HelloGPT-4o_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-15-HelloGPT-4o" data-gatsby-head="true"/><meta name="twitter:title" content="친근한 톤으로 번역 GPT-4를 소개합니다 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-HelloGPT-4o_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 03:08" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_buildManifest.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">친근한 톤으로 번역 GPT-4를 소개합니다</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="친근한 톤으로 번역 GPT-4를 소개합니다" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">2<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-HelloGPT-4o&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>GPT-4o ("o" for "omni")은 훨씬 자연스러운 인간-컴퓨터 상호작용을 위한 한 단계입니다. 이는 텍스트, 오디오 및 이미지의 어떤 조합이든 입력으로 받아들이고 어떤 조합이든 텍스트, 오디오 및 이미지 출력을 생성합니다. 이는 대화에서 인간의 응답 시간과 유사한 232밀리초 이내의 오디오 입력에 응답할 수 있으며, 평균 320밀리초로 응답할 수 있습니다. GPT-4 Turbo의 영문 텍스트와 코드에서의 성능과 비슷하며, 비영어 언어 텍스트에서는 상당한 향상을 보입니다. 또한 API에서 50% 빠르고 저렴합니다. GPT-4o는 기존 모델과 비교했을 때 비전 및 오디오 이해 능력이 특히 좋습니다.</p>
<h1>모델 기능</h1>
<p>두 대 GPT-4o가 상호작용하고 노래합니다.</p>
<p>인터뷰 준비요.</p>
<p>안녕하세요! 위에 표기된 사항들을 아래와 같이 번역해 드리겠습니다.</p>
<p>바위 가위 보 게임.</p>
<p>비꼼.</p>
<p>Sal과 Imran Khan과 함께 하는 수학.</p>
<p>둘의 GPT-4가 화음을 이루다.</p>
<p>더 궁금한 사항이 있으시면 언제든지 알려주세요!</p>
<p>GPT-4o와 함께 런던에 있는 BeMyEyes의 Andy입니다.</p>
<p>고객 서비스 프로토타입.</p>
<p>GPT-4o 이전에는 Voice Mode를 사용하여 ChatGPT와 대화를 나눌 수 있었는데, 그 때의 대기 시간은 평균 2.8초(GPT-3.5) 및 5.4초(GPT-4)였습니다. 이를 위해 Voice Mode는 오디오를 텍스트로 변환하는 간단한 모델, 텍스트를 입력 받고 텍스트를 출력하는 GPT-3.5 또는 GPT-4, 그리고 이 텍스트를 다시 오디오로 변환하는 세 번째 간단한 모델의 파이프라인입니다. 이 과정은 주요 지능 소스인 GPT-4가 많은 정보를 잃게 되어, 톤, 다중 스피커, 배경 소음을 직접적으로 관찰할 수 없으며, 웃음소리, 노래, 감정을 표현할 수 없다는 것을 의미합니다.</p>
<p>GPT-4o를 통해 우리는 텍스트, 비전, 오디오를 모두 처리하는 단일 새 모델을 최종적으로 훈련시켰습니다. GPT-4o는 이러한 여러 모달리티를 결합한 첫 번째 모델이기 때문에, 우리는 아직 모델이 무엇을 할 수 있고 그 한계가 무엇인지 탐색하는 과정에서 원천적인 단계에 머물러 있습니다.</p>
<h1>모델 평가</h1>
<p>전통적인 기준에 따르면, GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 Turbo 수준의 성능을 달성하며, 동시에 다국어, 오디오 및 비전 능력에서 새로운 기록을 세우고 있습니다.</p>
<p><img src="/assets/img/2024-05-15-HelloGPT-4o_0.png" alt="이미지"></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"친근한 톤으로 번역 GPT-4를 소개합니다","description":"","date":"2024-05-15 03:08","slug":"2024-05-15-HelloGPT-4o","content":"\n\nGPT-4o (\"o\" for \"omni\")은 훨씬 자연스러운 인간-컴퓨터 상호작용을 위한 한 단계입니다. 이는 텍스트, 오디오 및 이미지의 어떤 조합이든 입력으로 받아들이고 어떤 조합이든 텍스트, 오디오 및 이미지 출력을 생성합니다. 이는 대화에서 인간의 응답 시간과 유사한 232밀리초 이내의 오디오 입력에 응답할 수 있으며, 평균 320밀리초로 응답할 수 있습니다. GPT-4 Turbo의 영문 텍스트와 코드에서의 성능과 비슷하며, 비영어 언어 텍스트에서는 상당한 향상을 보입니다. 또한 API에서 50% 빠르고 저렴합니다. GPT-4o는 기존 모델과 비교했을 때 비전 및 오디오 이해 능력이 특히 좋습니다.\n\n# 모델 기능\n\n두 대 GPT-4o가 상호작용하고 노래합니다.\n\n인터뷰 준비요.\n\n\n\n안녕하세요! 위에 표기된 사항들을 아래와 같이 번역해 드리겠습니다.\n\n\n바위 가위 보 게임.\n\n비꼼.\n\nSal과 Imran Khan과 함께 하는 수학.\n\n둘의 GPT-4가 화음을 이루다.\n \n\n더 궁금한 사항이 있으시면 언제든지 알려주세요!\n\n\nGPT-4o와 함께 런던에 있는 BeMyEyes의 Andy입니다.\n\n고객 서비스 프로토타입.\n\nGPT-4o 이전에는 Voice Mode를 사용하여 ChatGPT와 대화를 나눌 수 있었는데, 그 때의 대기 시간은 평균 2.8초(GPT-3.5) 및 5.4초(GPT-4)였습니다. 이를 위해 Voice Mode는 오디오를 텍스트로 변환하는 간단한 모델, 텍스트를 입력 받고 텍스트를 출력하는 GPT-3.5 또는 GPT-4, 그리고 이 텍스트를 다시 오디오로 변환하는 세 번째 간단한 모델의 파이프라인입니다. 이 과정은 주요 지능 소스인 GPT-4가 많은 정보를 잃게 되어, 톤, 다중 스피커, 배경 소음을 직접적으로 관찰할 수 없으며, 웃음소리, 노래, 감정을 표현할 수 없다는 것을 의미합니다.\n\nGPT-4o를 통해 우리는 텍스트, 비전, 오디오를 모두 처리하는 단일 새 모델을 최종적으로 훈련시켰습니다. GPT-4o는 이러한 여러 모달리티를 결합한 첫 번째 모델이기 때문에, 우리는 아직 모델이 무엇을 할 수 있고 그 한계가 무엇인지 탐색하는 과정에서 원천적인 단계에 머물러 있습니다.\n\n\n\n# 모델 평가\n\n전통적인 기준에 따르면, GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 Turbo 수준의 성능을 달성하며, 동시에 다국어, 오디오 및 비전 능력에서 새로운 기록을 세우고 있습니다.\n\n![이미지](/assets/img/2024-05-15-HelloGPT-4o_0.png)","ogImage":{"url":"/assets/img/2024-05-15-HelloGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-HelloGPT-4o_0.png","tag":["Tech"],"readingTime":2},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eGPT-4o (\"o\" for \"omni\")은 훨씬 자연스러운 인간-컴퓨터 상호작용을 위한 한 단계입니다. 이는 텍스트, 오디오 및 이미지의 어떤 조합이든 입력으로 받아들이고 어떤 조합이든 텍스트, 오디오 및 이미지 출력을 생성합니다. 이는 대화에서 인간의 응답 시간과 유사한 232밀리초 이내의 오디오 입력에 응답할 수 있으며, 평균 320밀리초로 응답할 수 있습니다. GPT-4 Turbo의 영문 텍스트와 코드에서의 성능과 비슷하며, 비영어 언어 텍스트에서는 상당한 향상을 보입니다. 또한 API에서 50% 빠르고 저렴합니다. GPT-4o는 기존 모델과 비교했을 때 비전 및 오디오 이해 능력이 특히 좋습니다.\u003c/p\u003e\n\u003ch1\u003e모델 기능\u003c/h1\u003e\n\u003cp\u003e두 대 GPT-4o가 상호작용하고 노래합니다.\u003c/p\u003e\n\u003cp\u003e인터뷰 준비요.\u003c/p\u003e\n\u003cp\u003e안녕하세요! 위에 표기된 사항들을 아래와 같이 번역해 드리겠습니다.\u003c/p\u003e\n\u003cp\u003e바위 가위 보 게임.\u003c/p\u003e\n\u003cp\u003e비꼼.\u003c/p\u003e\n\u003cp\u003eSal과 Imran Khan과 함께 하는 수학.\u003c/p\u003e\n\u003cp\u003e둘의 GPT-4가 화음을 이루다.\u003c/p\u003e\n\u003cp\u003e더 궁금한 사항이 있으시면 언제든지 알려주세요!\u003c/p\u003e\n\u003cp\u003eGPT-4o와 함께 런던에 있는 BeMyEyes의 Andy입니다.\u003c/p\u003e\n\u003cp\u003e고객 서비스 프로토타입.\u003c/p\u003e\n\u003cp\u003eGPT-4o 이전에는 Voice Mode를 사용하여 ChatGPT와 대화를 나눌 수 있었는데, 그 때의 대기 시간은 평균 2.8초(GPT-3.5) 및 5.4초(GPT-4)였습니다. 이를 위해 Voice Mode는 오디오를 텍스트로 변환하는 간단한 모델, 텍스트를 입력 받고 텍스트를 출력하는 GPT-3.5 또는 GPT-4, 그리고 이 텍스트를 다시 오디오로 변환하는 세 번째 간단한 모델의 파이프라인입니다. 이 과정은 주요 지능 소스인 GPT-4가 많은 정보를 잃게 되어, 톤, 다중 스피커, 배경 소음을 직접적으로 관찰할 수 없으며, 웃음소리, 노래, 감정을 표현할 수 없다는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003eGPT-4o를 통해 우리는 텍스트, 비전, 오디오를 모두 처리하는 단일 새 모델을 최종적으로 훈련시켰습니다. GPT-4o는 이러한 여러 모달리티를 결합한 첫 번째 모델이기 때문에, 우리는 아직 모델이 무엇을 할 수 있고 그 한계가 무엇인지 탐색하는 과정에서 원천적인 단계에 머물러 있습니다.\u003c/p\u003e\n\u003ch1\u003e모델 평가\u003c/h1\u003e\n\u003cp\u003e전통적인 기준에 따르면, GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 Turbo 수준의 성능을 달성하며, 동시에 다국어, 오디오 및 비전 능력에서 새로운 기록을 세우고 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-HelloGPT-4o_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-HelloGPT-4o"},"buildId":"aCCUs-qPrLLLWRnkN0AOd","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>