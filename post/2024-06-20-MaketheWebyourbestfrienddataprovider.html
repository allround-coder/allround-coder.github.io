<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>웹을 당신의 최고 친구 데이터 제공업체로 만들어 보세요 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-06-20-MaketheWebyourbestfrienddataprovider" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="웹을 당신의 최고 친구 데이터 제공업체로 만들어 보세요 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="웹을 당신의 최고 친구 데이터 제공업체로 만들어 보세요 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-06-20-MaketheWebyourbestfrienddataprovider" data-gatsby-head="true"/><meta name="twitter:title" content="웹을 당신의 최고 친구 데이터 제공업체로 만들어 보세요 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-20 04:38" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/QH5Mz7n7Y6w0r4_gCGFQf/_buildManifest.js" defer=""></script><script src="/_next/static/QH5Mz7n7Y6w0r4_gCGFQf/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">웹을 당신의 최고 친구 데이터 제공업체로 만들어 보세요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="웹을 당신의 최고 친구 데이터 제공업체로 만들어 보세요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 20, 2024</span><span class="posts_reading_time__f7YPP">14<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-20-MaketheWebyourbestfrienddataprovider&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_0.png">
<p>LLM을 도구로 갖추는 아이디어는 그리 새로운 것은 아닙니다. LangChain과 Llamaindex와 같은 강력하고 잘 알려진 프레임워크는 몇 달 전에 이미 이를 실행했습니다.</p>
<p>그들은 위험 부담에 대한 승리했어요!</p>
<p>지금 완전히 열광 중인 AI 에이전트 혁명은 도구 패러다임을 충분히 활용하여 놀라운 결과를 얻고 있어요.</p>
<div class="content-ad"></div>
<p>인공지능이 지금 새로운 벽을 맞이했습니다: 계산 요구 사항과 무료로 이용 가능한 데이터의 끝.</p>
<p>이 기사에서는 이러한 도전 과제를 다루고, 우리의 LLM 애플리케이션을 새로운 관련 데이터로 무료로 풍부하게 만드는 방법을 배울 것입니다!</p>
<pre><code class="hljs language-js"># 목차
- 중립적 <span class="hljs-variable constant_">LLM</span> 방향
- 검색 보강 생성: 여전히 최고의 도구
- 웹 검색 및 <span class="hljs-variable constant_">NLP</span> 뉴스는 도구입니다
- 단락에서 문서까지
- 그럼 이제 무엇을 해야 할까요? 직접 문서 저장소 만들기
</code></pre>
<h1>중립적 LLM</h1>
<div class="content-ad"></div>
<p>기술 발전은 이미 지난 해의 많은 작업을 이미 완료했습니다. 새로운 하드웨어와 GPU 세트는 이제 더 빠르고 저렴하게 복잡한 신경망 계산을 처리할 수 있습니다.</p>
<p>동시에 엣지-모바일 기기로의 명확한 이동이 있으며, 작은 언어 모델이 큰 모델과 경쟁할 수 있도록 하는 데 특별한 주의를 기울이고 있습니다. TinyLlama 프로젝트를 시작으로 목표는 모바일 전화 하드웨어에서 실행할만큼 충분히 작지만 환각하지 않고 유용한 모델을 만드는 것입니다.</p>
<p>규모 법칙 이상으로, 우리는 LLM이 추상화하고 이성적으로 추론하는 능력을 잃는 한계를 이해해야 합니다. 데이터 없이 신경망을 구축할 수 없지만 동시에 우리는 새로운 위키피디아를 만들고 있지는 않습니다, 맞죠?</p>
<p>AI 커뮤니티에서 진정한 권위자인 Cobus Greyling은 항상 이것을 강조합니다: 데이터 처리 및 데이터 처리를 LLM 응용 프로그램에서 분리하세요. 그에게는 LLM 응용 프로그램은 모델에 중립적이어야 하며 LLM을 유틸리티로 취급해야 한다 - 그러나 이는 신뢰할 수 있는 데이터, 좋은 소식 및 선별된 데이터 집합에 대해 작업해야 한다는 것을 의미합니다.</p>
<div class="content-ad"></div>
<p>우수한 데이터를 이용하면 작은 모델(20억 파라미터 미만)이 탁월한 성과를 거둘 수 있어요!</p>
<p><img src="/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_1.png" alt="image"></p>
<h2>검색 증강 생성: 여전히 최고의 도구</h2>
<p>좋은 말들이지만, 이를 어떻게 구현할까요?</p>
<div class="content-ad"></div>
<p>Retrieval Augmented Generation (RAG)은 생성된 텍스트의 품질을 향상시키기 위해 자연어 처리에서 사용되는 기술입니다.</p>
<p>마술은 없어요! 우리는 대규모 외부 지식 원본(데이터베이스나 문서 코퍼스 등)에서 관련 정보를 통합하여 LLM 프롬프트에 적용합니다. RAG에서는 쿼리나 프롬프트에 대한 응답을 생성할 때 모델이 매우 좋은 작업을 합니다:</p>
<ul>
<li>먼저 검색 메커니즘을 사용해 외부 지식 원본에서 관련 정보를 검색합니다.</li>
<li>검색된 정보는 그 후 생성 모델로 투입되어 자신의 지식을 보완하고</li>
<li>보다 정확하고 유익한 응답을 생성합니다.</li>
</ul>
<p>예를 들어, 특정 역사적 사건에 대한 질문을 하는 경우 RAG 모델은 먼저 역사 문서의 대규모 데이터베이스에서 해당 사건에 대한 정보를 검색하고, 그 정보를 사용하여 귀하의 질문에 대한 세부적이고 정확한 응답을 생성합니다.</p>
<div class="content-ad"></div>
<p>RAG은 순수 생성 모델과 구체적인 사실 정보에 제한을 받을 수 있는 것을 합하고, 오직 검색 기반 모델과 창의적인 대답을 생성하는 데 어려움을 겪을 수 있는 것을 합하면서 이 둘 간의 간극을 좁히는 데 도움을 줍니다.</p>
<p>그러니까 우리 수학 시간을 시작해 볼까요:</p>
<ul>
<li>좋은 이유를 제공하는 작은 LLM이 주어지면</li>
<li>좋고 최신 정보를 위해 RAG 파이프라인을 설정하면</li>
</ul>
<p>... 전례 없는 그리고 의미 있는 문서 세트를 만드는 방법이 필요합니다.</p>
<div class="content-ad"></div>
<h1>웹 검색 및 NLP 뉴스는 도구들입니다</h1>
<p>이전 글에서 LangChain을 DuckDuckGo 웹 검색과 함께 사용하는 방법에 대해 설명했었습니다.</p>
<p>최신 뉴스와 업데이트된 정보를 검색 엔진을 사용하여 가져오는 것은 어떤 반 데이터 모형으로 향하는 놀라운 방법입니다. 생성 모델의 마감일 한계가 우리가 필요한 모든 것과 외부 지식을 제공할 수 있기 때문에 우회됩니다.</p>
<p>확인되고 진실한 정보가 있는 경우, 우리의 LLM 응용 프로그램은 좋은 RAG 전략을 사용함으로써보다 효과적 일 수 있습니다.</p>
<div class="content-ad"></div>
<p>하지만 잘 알려진 웹 검색 도구에도 제한이 있습니다! 무료 Google Colab 노트북을 사용하여 확인해 보세요.</p>
<pre><code class="hljs language-js">%pip install --upgrade --quiet langchain langchain-community faiss-cpu 
%pip install tiktoken duckduckgo-search llama-cpp-agent newspaper3k
</code></pre>
<p>Langchain과 함께 duckduckgo-search와 newspaper3k(강력한 NLP 웹 HTML 문서 파서)를 설치합니다.</p>
<p>먼저, 웹 검색의 출력이 무엇인지 살펴봅시다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain_community.<span class="hljs-property">utilities</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">DuckDuckGoSearchAPIWrapper</span>
wrapper = <span class="hljs-title class_">DuckDuckGoSearchAPIWrapper</span>(region=<span class="hljs-string">'us-en'</span>, 
                                   time=<span class="hljs-string">"y"</span>, 
                                   max_results=<span class="hljs-number">10</span>) #time <span class="hljs-title class_">Options</span>: d, w, m, y

rawdb = wrapper.<span class="hljs-title function_">results</span>(<span class="hljs-string">'LLM Mixture of Agents'</span>,max_results=<span class="hljs-number">5</span>)
</code></pre>
<p>여기서는 에이전트 도구가 아닌 wrapper 자체를 사용하여 "LLM 혼합 에이전트"에 대한 정보를 찾습니다. 결과를 출력하고 rawdb 변수에 저장하면 다음과 같은 내용이 나타납니다:</p>
<pre><code class="hljs language-js">[
    {
        <span class="hljs-string">'snippet'</span>: <span class="hljs-string">'대형 언어 모델(LM)의 최근 발전은 자연어 이해 및 생성 작업에서 상당한 성능을 보여줍니다. 증가하는 LLM의 수로 인해 여러 LLM의 집단적 전문지식을 어떻게 활용할지는 흥미로운 개방 방향입니다. 이 목표를 향해, 우리는 Mixture-of-Agents (MoA) 방법을 통해 여러 LLM의 집단적 강점을 활용하는 새로운 접근 방식을 제안합니다.'</span>,
        <span class="hljs-string">'title'</span>: <span class="hljs-string">'Mixture-of-Agents Enhances Large Language Model Capabilities'</span>,
        <span class="hljs-string">'link'</span>: <span class="hljs-string">'https://arxiv.org/abs/2406.04692'</span>
    },
    {
        <span class="hljs-string">'snippet'</span>: <span class="hljs-string">'첫째로, 우리는 답안 제공자들에 의해 생성된 답 중 하나를 선택하기 위해 집계 모델을 사용하는 LLM 기반 순위 결정기를 Mixture-of-Agents와 비교합니다. 쓰기 결과는 Figure 4에 나와 있으며 MoA 접근 방식이 LLM-순위 결정층 베이스라인을 크게 능가함을 관찰할 수 있습니다.'</span>,
        <span class="hljs-string">'title'</span>: <span class="hljs-string">'Mixture-of-Agents Enhances Large Language Model Capabilities - arXiv.org'</span>,
        <span class="hljs-string">'link'</span>: <span class="hljs-string">'https://arxiv.org/html/2406.04692v1'</span>
    },
    {
        <span class="hljs-string">'snippet'</span>: <span class="hljs-string">'우리는 여러 LLM의 집단적 강점을 활용하여 최첨단 품질을 향상시키기 위한 Mixture of Agents (MoA) 접근 방식을 소개합니다. 그리고 우리는 상태-of-the-art의 품질을 향상하기 위해 여러 오픈소스 LLM 에이전트들을 활용하는 참조 구현인 Together MoA을 제공합니다. Together MoA는 AlpacaEval 2.0에서 65.1%의 점수를 달성하여 이전 리더 GPT-4o (57.5%)를 능가합니다.'</span>,
        <span class="hljs-string">'title'</span>: <span class="hljs-string">'Together MoA — collective intelligence of open-source models pushing ...'</span>,
        <span class="hljs-string">'link'</span>: <span class="hljs-string">'https://www.together.ai/blog/together-moa'</span>
    },
    {
        <span class="hljs-string">'snippet'</span>: <span class="hljs-string">'이 목표를 향해, 여러 LLM의 집단적 강점을 Mixture-of-Agents (MoA) 방법론을 통해 활용하는 새로운 접근 방식을 제안합니다. 우리의 접근 방식에서는 각 층이 여러 LLM 에이전트로 이루어진 층화식 MoA 아키텍처를 구성합니다. 각 에이전트는 이전 층에서 생성된 모든 출력을 보조 정보로 받습니다.'</span>,
        <span class="hljs-string">'title'</span>: <span class="hljs-string">'Mixture-of-Agents Enhances Large Language Model Capabilities'</span>,
        <span class="hljs-string">'link'</span>: <span class="hljs-string">'https://huggingface.co/papers/2406.04692'</span>
    },
    {
        <span class="hljs-string">'snippet'</span>: <span class="hljs-string">'Mixture of Agents (MoA)는 여러 LLM의 집단적 강점을 활용하여 성능을 향상시키는 혁신적인 접근 방식으로, 최첨단 결과를 달성합니다. 각 층이 여러 LLM 에이전트로 이루어진 층화 구조를 사용하여 MoA는 65.1%의 점수로 AlpacaEval 2.0에서 GPT-4 Omni의 57.5%를 크게 능가합니다.'</span>,
        <span class="hljs-string">'title'</span>: <span class="hljs-string">'GitHub - togethercomputer/MoA'</span>,
        <span class="hljs-string">'link'</span>: <span class="hljs-string">'https://github.com/togethercomputer/moa'</span>
    }
]
</code></pre>
<p>결과가 정확히 5개인 것을 확인할 수 있습니다. 하지만 텍스트가 짧네요! 이 짧은 조각 텍스트는 구체적인 사실적 질문에 사용할 수는 있지만 새로운 지식 베이스를 구축하기에는 부족합니다!</p>
<div class="content-ad"></div>
<p>좋은 점은 링크도 얻었다는 것이에요 🤣</p>
<p><img src="/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_2.png" alt="image"></p>
<h1>스니펫에서 문서로</h1>
<p>GitHub 레포지토리를 하나씩 검색하면서 쉽고 완벽한 웹 검색을 찾고 있었는데, newspaper3k를 만나기 전까지요.</p>
<div class="content-ad"></div>
<p>이 Python 라이브러리는 검색 엔진 결과에서 전체 텍스트를 얻기 위한 입구입니다. 그게 전부가 아닙니다!</p>
<p>nltk 라이브러리도 함께 사용되기 때문에 키워드, 설명, 이미지 URL 및 요약과 같은 다양한 메타데이터로 정보를 보완할 수 있습니다!</p>
<p>Google Colab에서 항상 살펴봅시다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> newspaper <span class="hljs-keyword">import</span> <span class="hljs-title class_">Article</span>
<span class="hljs-keyword">import</span> nltk
nltk.<span class="hljs-title function_">download</span>(<span class="hljs-string">'punkt'</span>)
</code></pre>
<div class="content-ad"></div>
<p>웹 페이지 텍스트 추출 함수들은 정말 쉽게 사용할 수 있어요. NLTK 기능을 포함하려면 우선 Punkt를 다운로드해야 해요.</p>
<p>NLTK에서 Punkt는 문장 토큰화에 사용되는 비지도 학습 가능한 모델이에요. 이 모델은 문장을 단어로 나누어 문장 시작 단어, 전치사구, 약어에 대한 모델을 개발함으로써 문장을 분리해요.</p>
<p>이제 우리가 작은 Neural toolkit를 준비했으니 웹 페이지에서 가능한 모든 것을 추출해 봅시다. 이전 DuckDuckGo 검색에서 추출된 링크 중 하나를 사용할 수 있어요: <a href="https://www.together.ai/blog/together-moa" rel="nofollow" target="_blank">https://www.together.ai/blog/together-moa</a></p>
<pre><code class="hljs language-js">url = <span class="hljs-string">'https://www.together.ai/blog/together-moa'</span>
article = <span class="hljs-title class_">Article</span>(url) # payload requests를 불러와요
article.<span class="hljs-title function_">download</span>()     # 로컬로 데이터와 메타데이터를 버퍼링해요
article.<span class="hljs-title function_">parse</span>()        # 텍스트 추출
article.<span class="hljs-title function_">nlp</span>()          # 키워드와 요약을 위해 nlp 도구 실행
</code></pre>
<div class="content-ad"></div>
<p>이제 모든 것이 처리되었으니 데이터를 살펴볼 수 있어요:</p>
<pre><code class="hljs language-js"><span class="hljs-title function_">print</span>(article.<span class="hljs-property">text</span>)
---
우린 <span class="hljs-title class_">Mixture</span> <span class="hljs-keyword">of</span> <span class="hljs-title class_">Agents</span> (<span class="hljs-title class_">MoA</span>)를 소개합니다. 이는 여러 <span class="hljs-variable constant_">LLM</span>들의 집합적인 강점을 활용하여 최신 기술 수준을 향상시키는 접근법입니다. 또한 우리는 몇몇 오픈 소스 <span class="hljs-variable constant_">LLM</span> 에이전트를 이용하여 <span class="hljs-number">65.1</span>%의 점수를 얻는 <span class="hljs-title class_">Together</span> <span class="hljs-title class_">MoA</span>라는 참조 구현을 제공합니다. 이로써 이전 리더인 <span class="hljs-variable constant_">GPT</span>-4o (<span class="hljs-number">57.5</span>%)를 능가했습니다.\n\n그림 <span class="hljs-number">1</span>: 에이전트 혼합 구조의 그림. 이 예시 ...


<span class="hljs-title function_">print</span>(article.<span class="hljs-property">authors</span>)

<span class="hljs-title function_">print</span>(article.<span class="hljs-property">keywords</span>)
---
[<span class="hljs-string">'에이전트'</span>, <span class="hljs-string">'20'</span>, <span class="hljs-string">'전선'</span>, <span class="hljs-string">'성능'</span>, <span class="hljs-string">'모델'</span>, <span class="hljs-string">'qwen1510bchat'</span>, <span class="hljs-string">'응답'</span>, <span class="hljs-string">'moa'</span>, <span class="hljs-string">'llm'</span>, <span class="hljs-string">'집단'</span>, <span class="hljs-string">'오픈소스'</span>, <span class="hljs-string">'alpacaeval'</span>, <span class="hljs-string">'제안자'</span>, <span class="hljs-string">'인텔리전스'</span>, <span class="hljs-string">'gpt4o'</span>, <span class="hljs-string">'능력강화'</span>]

<span class="hljs-title function_">print</span>(article.<span class="hljs-property">summary</span>)
---
우리는 <span class="hljs-title class_">Mixture</span> <span class="hljs-keyword">of</span> <span class="hljs-title class_">Agents</span> (<span class="hljs-title class_">MoA</span>)를 소개합니다. 이는 여러 <span class="hljs-variable constant_">LLM</span>들의 집단적인 강점을 활용하여 최신 기술 수준을 향상시키는 접근법입니다.
개요: 새로운 방법인 <span class="hljs-title class_">Mixture</span> <span class="hljs-keyword">of</span> <span class="hljs-title class_">Agents</span> (<span class="hljs-title class_">MoA</span>)를 소개하게 되어 기쁩니다. 이는 여러 <span class="hljs-variable constant_">LLM</span>들의 집단적인 강점을 활용하는 혁신적인 접근법입니다.
우리의 참조 구현인 <span class="hljs-title class_">Together</span> <span class="hljs-title class_">MoA</span>는 오직 오픈 소스 모델을 사용하여 <span class="hljs-title class_">AlpacaEval</span> <span class="hljs-number">2.0</span>에서 <span class="hljs-number">57.5</span>%인 <span class="hljs-variable constant_">GPT</span>-4o를 크게 능가하며 <span class="hljs-number">65.1</span>%의 점수를 얻습니다.
그림 <span class="hljs-number">2</span>는 각 모델이 <span class="hljs-title class_">AlpacaEval</span> <span class="hljs-number">2.0</span>에서 기존 점수를 크게 개선함을 보여줍니다. 특히, 오픈 소스 모델만을 사용하여 <span class="hljs-title class_">AlpacaEval</span> <span class="hljs-number">2.0</span>에서 우리는 <span class="hljs-number">57.5</span>% (<span class="hljs-variable constant_">GPT</span>-4o)에서 <span class="hljs-number">65.1</span>% (<span class="hljs-title class_">Together</span> <span class="hljs-title class_">MoA</span>)로 <span class="hljs-number">7.6</span>%의 절대 향상을 이룩했습니다.

<span class="hljs-title function_">print</span>(article.<span class="hljs-property">meta_description</span>)

<span class="hljs-title function_">print</span>(article.<span class="hljs-property">meta_img</span>)
<span class="hljs-attr">https</span>:<span class="hljs-comment">//cdn.prod.website-files.com/650c3b59079d92475f37b68f/6667e9c28da8569e846b4632_thumbnail.jpg</span>

<span class="hljs-title function_">print</span>(article.<span class="hljs-property">meta_keywords</span>)
[<span class="hljs-string">''</span>]
</code></pre>
<p>정말 멋지다고 생각해요!</p>
<p>이 몇 줄의 코드로 전체 텍스트와 중요한 메타데이터를 검색했어요. 웹 리소스가 정말 좋은 경우 날짜와 주 이미지도 간단히 이렇게 얻을 수 있답니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">article.<span class="hljs-property">top_image</span>
article.<span class="hljs-property">publish_date</span>
</code></pre>
<p><img src="/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_3.png" alt="image"></p>
<h1>그럼 이제 어떻게 하죠? 자체 문서 저장소 만들기</h1>
<p>지금까지 말한 모든 것들은 데이터/정보가 생성 AI 애플리케이션에 있어 핵심이라는 것을 고려할 때에만 관련이 있습니다. LLM은 이제 NLP 작업을 뛰어나게 수행하므로 좋은 텍스트가 필요합니다.</p>
<div class="content-ad"></div>
<p>여기서 더 나아가 봅시다! 웹 검색 쿼리를 입력하면 데이터를 풍부하게 모아서 표준 Langchain 형식으로 정리하는 파이프라인을 직접 만들 수 있습니다. 언제든지 사용할 준비가 완료되었어요!</p>
<h2>Wrapper 실행</h2>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> newspaper <span class="hljs-keyword">import</span> <span class="hljs-title class_">Article</span>
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">from</span> rich.<span class="hljs-property">markdown</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Markdown</span>
<span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> rich.<span class="hljs-property">console</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Console</span>
<span class="hljs-keyword">from</span> langchain.<span class="hljs-property">schema</span>.<span class="hljs-property">document</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Document</span>
<span class="hljs-variable language_">console</span> = <span class="hljs-title class_">Console</span>(width=<span class="hljs-number">90</span>)
<span class="hljs-keyword">import</span> nltk
# <span class="hljs-title class_">DuckDuckGo</span> 검색 엔진 래퍼 준비
<span class="hljs-keyword">from</span> langchain_community.<span class="hljs-property">utilities</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">DuckDuckGoSearchAPIWrapper</span>
wrapper = <span class="hljs-title class_">DuckDuckGoSearchAPIWrapper</span>(region=<span class="hljs-string">'us-en'</span>, time=<span class="hljs-string">"y"</span>, max_results=<span class="hljs-number">10</span>) #time parameter <span class="hljs-title class_">Options</span>: d, w, m, y
# 사용자 쿼리 요청
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">print</span>(f<span class="hljs-string">'[bold red1]무엇을 검색하시겠습니까?'</span>)
research = <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">input</span>(f<span class="hljs-string">'[bright_green]> '</span>)
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">print</span>(<span class="hljs-number">90</span>*<span class="hljs-string">'='</span>)
# <span class="hljs-title class_">Wrapper</span> 실행
rawdb = wrapper.<span class="hljs-title function_">results</span>(research,max_results=<span class="hljs-number">5</span>)
</code></pre>
<p>저는 아이작 아시모프의 원자 폭탄에 대한 이야기를 찾아보겠습니다. 그런 다음 예상 출력 형식을 사용하여 데이터베이스를 구축할 거에요. 다시 설명드리자면, 예상 형식은 다음과 같습니다:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">[
  {
    <span class="hljs-string">'snippet'</span>: <span class="hljs-string">'비디오 콜 - 아이작 아시모프 - 다양한 패운데이션 이야기 (1950년대+) 패운데이션 시리즈는 거대한 시간과 공간을 다룹니다. ... 원자폭탄 - H.G. 웰스 - The World Set Free (1914)'</span>,
    <span class="hljs-string">'title'</span>: <span class="hljs-string">'과학 소설이 미래 기술을 예언한 10가지 시간 - 디스트로이드'</span>,
    <span class="hljs-string">'link'</span>: <span class="hljs-string">'https://www.destructoid.com/10-times-sci-fi-predicted-the-future-of-technology/'</span>
  },
  {
    <span class="hljs-string">'snippet'</span>: <span class="hljs-string">'밤바람과 다른 이야기들. 아이작 아시모프의 최고의 과학 소설. 전체...'</span>
  }
]
</code></pre>
<p>여기에서 이미 황금 광산을 발견했어요: 우리는 DDG 검색 필드에서 제목, 스니펫 및 링크를 얻을 수 있어요.</p>
<h2>신문 NLP 도구와 Wrapper 결과 병합</h2>
<p>이제 rawdb 리스트를 반복하고 각 URL을 사용하여 newspaper3k를 실행할 수 있습니다. 더 똑똒하게 만들기 위해 반복 중 LangChain Document 객체를 만들어요.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-python">docdocs = []
<span class="hljs-keyword">for</span> items <span class="hljs-keyword">in</span> rawdb:
    url = items[<span class="hljs-string">"link"</span>]
    <span class="hljs-keyword">try</span>:  <span class="hljs-comment"># 만약 URL에 접속할 수 없다면 유용합니다</span>
        article = Article(url)
        article.download()
        article.parse()
        article.nlp()
        kw = []
        <span class="hljs-comment"># NLTK 키워드와 메타 웹페이지 키워드를 병합합니다</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> article.keywords + article.meta_keywords:
            <span class="hljs-keyword">if</span> i == <span class="hljs-string">''</span>:  <span class="hljs-comment"># 우리에게는 블랙 키워드가 없습니다</span>
                <span class="hljs-keyword">pass</span>
            <span class="hljs-keyword">else</span>:
                kw.append(i)
        <span class="hljs-keyword">if</span> article.text == <span class="hljs-string">''</span>:  <span class="hljs-comment"># 때로는 구문 분석할 텍스트가 없습니다. 그래서 스니펫을 사용합니다</span>
            docdocs.append(Document(page_content=items[<span class="hljs-string">"snippet"</span>], metadata={
                <span class="hljs-string">'source'</span>: items[<span class="hljs-string">"link"</span>],
                <span class="hljs-string">'title'</span>: items[<span class="hljs-string">"title"</span>],
                <span class="hljs-string">'snippet'</span>: items[<span class="hljs-string">"snippet"</span>],
                <span class="hljs-string">'author'</span>: article.authors,
                <span class="hljs-string">'keywords'</span>: kw,
                <span class="hljs-string">'meta_description'</span>: article.meta_description,
                <span class="hljs-string">'meta_img'</span>: article.meta_img,
                <span class="hljs-string">'top_image'</span>: article.top_image,
                <span class="hljs-string">'publish_date'</span>: article.publish_date,
                <span class="hljs-string">'summary'</span>: article.summary}))
        <span class="hljs-keyword">else</span>:
            docdocs.append(Document(page_content=article.text.replace(<span class="hljs-string">'\n\n'</span>, <span class="hljs-string">''</span>), metadata={
                <span class="hljs-string">'source'</span>: items[<span class="hljs-string">"link"</span>],
                <span class="hljs-string">'title'</span>: items[<span class="hljs-string">"title"</span>],
                <span class="hljs-string">'snippet'</span>: items[<span class="hljs-string">"snippet"</span>],
                <span class="hljs-string">'author'</span>: article.authors,
                <span class="hljs-string">'keywords'</span>: kw,
                <span class="hljs-string">'meta_description'</span>: article.meta_description,
                <span class="hljs-string">'meta_img'</span>: article.meta_img,
                <span class="hljs-string">'top_image'</span>: article.top_image,
                <span class="hljs-string">'publish_date'</span>: article.publish_date,
                <span class="hljs-string">'summary'</span>: article.summary}))
    <span class="hljs-keyword">except</span>:
        <span class="hljs-keyword">pass</span>
</code></pre>
<p>이제 변수 docdocs에는 메타데이터가 풍부한 LangChain 문서 목록이 있습니다. 이를 FAISS와 같은 벡터 데이터베이스에서 직접 사용하거나 로컬 파일에 저장할 수 있습니다. 저는 두 번째 옵션을 선호합니다. 그 이유는 언제든지 이러한 문서를 나중에 병합할 수 있기 때문입니다.</p>
<p>따라서 나는 이러한 문서를 저장하기 위해 피클 형식을 선택한 것입니다. pickle 모듈은 Python 표준 라이브러리에 포함되어 있습니다. Python 개체 구조를 직렬화하고 역 직렬화하기 위한 이진 프로토콜을 구현합니다.</p>
<p>“Pickling”은 Python 개체 계층 구조를 바이트 스트림으로 변환하는 프로세스이며, “unpickling”은 그 반대 작업으로, 바이너리 파일 또는 바이트류 객체에서 바이트 스트림을 개체 계층 구조로 변환하는 작업입니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">## 메타데이터와 함께 문서 세트를 pickle에 저장합니다.
lcdfilename = research.<span class="hljs-title function_">replace</span>(<span class="hljs-string">' '</span>,<span class="hljs-string">'_'</span>)+<span class="hljs-string">'.lcd'</span>
output = <span class="hljs-title function_">open</span>(lcdfilename, <span class="hljs-string">'wb'</span>)
pickle.<span class="hljs-title function_">dump</span>(docdocs, output)
output.<span class="hljs-title function_">close</span>()
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">print</span>(<span class="hljs-title class_">Markdown</span>(<span class="hljs-string">"> LangChain 문서 데이터가 저장되었습니다..."</span>))
</code></pre>
<p>이제 LangChain 문서를 가지고 로컬 모델을 실행해 볼 수 있어요. 여기에 하나의 도전 과제가 있네요: 왜 llama-cpp-python만 사용해서 시도해 보지 않으세요?</p>
<h1>결론</h1>
<p>Web 검색 및 자연어 처리(NLP) 도구를 이용하여 최신 및 관련 데이터로 언어 학습 모델(LLM)를 풍부하게 하는 혁신적인 접근 방식을 함께 탐색했습니다.</p>
<div class="content-ad"></div>
<p>이와 같은 전략들은 계산 요구 사항과 무료 데이터의 부족으로 인한 제한을 극복하는 데 중요합니다.</p>
<p>어고노스틱 LLM을 달성하기 위해, 외부 지식 소스를 통합하여 LLM 응답의 정확성과 신뢰도를 향상시키는 Retrieval Augmented Generation (RAG) 기술이 결정적인 기법으로 부상합니다.</p>
<p>저희는 웹 검색 엔진과 newspaper3k와 같은 NLP 라이브러리를 도구로 사용하여 온라인 리소스로부터 전체 텍스트와 가치 있는 메타데이터를 추출하고, 간략한 스니펫을 포괄적인 문서로 변환했습니다.</p>
<p>이 프로세스는 생성 모델의 잘라내기 날짜 제한을 우회하는 것뿐만 아니라 개인화된 문서 저장소를 생성하는 것을 용이하게 합니다.</p>
<div class="content-ad"></div>
<p>무서워하지 마시고 실험해보세요. 하지만 항상 기억하세요, 도구는 좋거나 나쁘지 않습니다! 도구는 사용하는 사람만큼 좋습니다!</p>
<p>만약 이 이야기가 가치 있었고 조금이라도 지원을 보내고 싶다면, 다음을 해볼 수 있습니다:</p>
<ul>
<li>이 이야기에 대해 많이 박수를 보내기</li>
<li>기억할 가치가 있는 부분을 강조하기 (나중에 찾기가 쉽고, 나는 더 나은 기사를 쓸 수 있습니다)</li>
<li>'자체 AI 구축 방법 배우기, 이 무료 eBook 다운로드하기'</li>
<li>내 링크를 통해 Medium 멤버십 가입하기 ($5/월로 무제한 Medium 이야기 읽기)</li>
<li>Medium에서 나를 팔로우하기</li>
<li>내 최신 기사를 읽기: <a href="https://medium.com/@fabio.matricardi" rel="nofollow" target="_blank">https://medium.com/@fabio.matricardi</a></li>
</ul>
<p>더 많은 내용을 보고 싶다면, 다음은 몇 가지 아이디어입니다:</p>
<div class="content-ad"></div>
<p>구글 Colab 노트북이 있는 GitHub 저장소</p>
<p><img src="/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_4.png" alt="GitHub Repository"></p>
<p>이 이야기는 Generative AI에서 발행되었습니다. 최신 AI 이야기를 알고 싶다면 LinkedIn에서 저희와 연결하고 Zeniteq를 팔로우하세요.</p>
<p>저희의 뉴스레터에 가입하여 최신 generative AI 뉴스와 업데이트를 받아보세요. 함께 AI의 미래를 함께 만들어 봅시다!</p>
<div class="content-ad"></div>
<p><code>![2024-06-20-MaketheWebyourbestfrienddataprovider_5.png](/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_5.png)</code></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"웹을 당신의 최고 친구 데이터 제공업체로 만들어 보세요","description":"","date":"2024-06-20 04:38","slug":"2024-06-20-MaketheWebyourbestfrienddataprovider","content":"\n\n\u003cimg src=\"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_0.png\" /\u003e\n\nLLM을 도구로 갖추는 아이디어는 그리 새로운 것은 아닙니다. LangChain과 Llamaindex와 같은 강력하고 잘 알려진 프레임워크는 몇 달 전에 이미 이를 실행했습니다.\n\n그들은 위험 부담에 대한 승리했어요!\n\n지금 완전히 열광 중인 AI 에이전트 혁명은 도구 패러다임을 충분히 활용하여 놀라운 결과를 얻고 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인공지능이 지금 새로운 벽을 맞이했습니다: 계산 요구 사항과 무료로 이용 가능한 데이터의 끝.\n\n이 기사에서는 이러한 도전 과제를 다루고, 우리의 LLM 애플리케이션을 새로운 관련 데이터로 무료로 풍부하게 만드는 방법을 배울 것입니다!\n\n```js\n# 목차\n- 중립적 LLM 방향\n- 검색 보강 생성: 여전히 최고의 도구\n- 웹 검색 및 NLP 뉴스는 도구입니다\n- 단락에서 문서까지\n- 그럼 이제 무엇을 해야 할까요? 직접 문서 저장소 만들기\n```\n\n# 중립적 LLM\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기술 발전은 이미 지난 해의 많은 작업을 이미 완료했습니다. 새로운 하드웨어와 GPU 세트는 이제 더 빠르고 저렴하게 복잡한 신경망 계산을 처리할 수 있습니다.\n\n동시에 엣지-모바일 기기로의 명확한 이동이 있으며, 작은 언어 모델이 큰 모델과 경쟁할 수 있도록 하는 데 특별한 주의를 기울이고 있습니다. TinyLlama 프로젝트를 시작으로 목표는 모바일 전화 하드웨어에서 실행할만큼 충분히 작지만 환각하지 않고 유용한 모델을 만드는 것입니다.\n\n규모 법칙 이상으로, 우리는 LLM이 추상화하고 이성적으로 추론하는 능력을 잃는 한계를 이해해야 합니다. 데이터 없이 신경망을 구축할 수 없지만 동시에 우리는 새로운 위키피디아를 만들고 있지는 않습니다, 맞죠?\n\nAI 커뮤니티에서 진정한 권위자인 Cobus Greyling은 항상 이것을 강조합니다: 데이터 처리 및 데이터 처리를 LLM 응용 프로그램에서 분리하세요. 그에게는 LLM 응용 프로그램은 모델에 중립적이어야 하며 LLM을 유틸리티로 취급해야 한다 - 그러나 이는 신뢰할 수 있는 데이터, 좋은 소식 및 선별된 데이터 집합에 대해 작업해야 한다는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우수한 데이터를 이용하면 작은 모델(20억 파라미터 미만)이 탁월한 성과를 거둘 수 있어요!\n\n![image](/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_1.png)\n\n## 검색 증강 생성: 여전히 최고의 도구\n\n좋은 말들이지만, 이를 어떻게 구현할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRetrieval Augmented Generation (RAG)은 생성된 텍스트의 품질을 향상시키기 위해 자연어 처리에서 사용되는 기술입니다.\n\n마술은 없어요! 우리는 대규모 외부 지식 원본(데이터베이스나 문서 코퍼스 등)에서 관련 정보를 통합하여 LLM 프롬프트에 적용합니다. RAG에서는 쿼리나 프롬프트에 대한 응답을 생성할 때 모델이 매우 좋은 작업을 합니다:\n\n- 먼저 검색 메커니즘을 사용해 외부 지식 원본에서 관련 정보를 검색합니다.\n- 검색된 정보는 그 후 생성 모델로 투입되어 자신의 지식을 보완하고\n- 보다 정확하고 유익한 응답을 생성합니다.\n\n예를 들어, 특정 역사적 사건에 대한 질문을 하는 경우 RAG 모델은 먼저 역사 문서의 대규모 데이터베이스에서 해당 사건에 대한 정보를 검색하고, 그 정보를 사용하여 귀하의 질문에 대한 세부적이고 정확한 응답을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG은 순수 생성 모델과 구체적인 사실 정보에 제한을 받을 수 있는 것을 합하고, 오직 검색 기반 모델과 창의적인 대답을 생성하는 데 어려움을 겪을 수 있는 것을 합하면서 이 둘 간의 간극을 좁히는 데 도움을 줍니다.\n\n그러니까 우리 수학 시간을 시작해 볼까요:\n\n- 좋은 이유를 제공하는 작은 LLM이 주어지면\n- 좋고 최신 정보를 위해 RAG 파이프라인을 설정하면\n\n... 전례 없는 그리고 의미 있는 문서 세트를 만드는 방법이 필요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 웹 검색 및 NLP 뉴스는 도구들입니다\n\n이전 글에서 LangChain을 DuckDuckGo 웹 검색과 함께 사용하는 방법에 대해 설명했었습니다.\n\n최신 뉴스와 업데이트된 정보를 검색 엔진을 사용하여 가져오는 것은 어떤 반 데이터 모형으로 향하는 놀라운 방법입니다. 생성 모델의 마감일 한계가 우리가 필요한 모든 것과 외부 지식을 제공할 수 있기 때문에 우회됩니다.\n\n확인되고 진실한 정보가 있는 경우, 우리의 LLM 응용 프로그램은 좋은 RAG 전략을 사용함으로써보다 효과적 일 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 잘 알려진 웹 검색 도구에도 제한이 있습니다! 무료 Google Colab 노트북을 사용하여 확인해 보세요.\n\n```js\n%pip install --upgrade --quiet langchain langchain-community faiss-cpu \n%pip install tiktoken duckduckgo-search llama-cpp-agent newspaper3k\n```\n\nLangchain과 함께 duckduckgo-search와 newspaper3k(강력한 NLP 웹 HTML 문서 파서)를 설치합니다.\n\n먼저, 웹 검색의 출력이 무엇인지 살펴봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom langchain_community.utilities import DuckDuckGoSearchAPIWrapper\nwrapper = DuckDuckGoSearchAPIWrapper(region='us-en', \n                                   time=\"y\", \n                                   max_results=10) #time Options: d, w, m, y\n\nrawdb = wrapper.results('LLM Mixture of Agents',max_results=5)\r\n```\n\n여기서는 에이전트 도구가 아닌 wrapper 자체를 사용하여 \"LLM 혼합 에이전트\"에 대한 정보를 찾습니다. 결과를 출력하고 rawdb 변수에 저장하면 다음과 같은 내용이 나타납니다:\n\n```js\n[\n    {\n        'snippet': '대형 언어 모델(LM)의 최근 발전은 자연어 이해 및 생성 작업에서 상당한 성능을 보여줍니다. 증가하는 LLM의 수로 인해 여러 LLM의 집단적 전문지식을 어떻게 활용할지는 흥미로운 개방 방향입니다. 이 목표를 향해, 우리는 Mixture-of-Agents (MoA) 방법을 통해 여러 LLM의 집단적 강점을 활용하는 새로운 접근 방식을 제안합니다.',\n        'title': 'Mixture-of-Agents Enhances Large Language Model Capabilities',\n        'link': 'https://arxiv.org/abs/2406.04692'\n    },\n    {\n        'snippet': '첫째로, 우리는 답안 제공자들에 의해 생성된 답 중 하나를 선택하기 위해 집계 모델을 사용하는 LLM 기반 순위 결정기를 Mixture-of-Agents와 비교합니다. 쓰기 결과는 Figure 4에 나와 있으며 MoA 접근 방식이 LLM-순위 결정층 베이스라인을 크게 능가함을 관찰할 수 있습니다.',\n        'title': 'Mixture-of-Agents Enhances Large Language Model Capabilities - arXiv.org',\n        'link': 'https://arxiv.org/html/2406.04692v1'\n    },\n    {\n        'snippet': '우리는 여러 LLM의 집단적 강점을 활용하여 최첨단 품질을 향상시키기 위한 Mixture of Agents (MoA) 접근 방식을 소개합니다. 그리고 우리는 상태-of-the-art의 품질을 향상하기 위해 여러 오픈소스 LLM 에이전트들을 활용하는 참조 구현인 Together MoA을 제공합니다. Together MoA는 AlpacaEval 2.0에서 65.1%의 점수를 달성하여 이전 리더 GPT-4o (57.5%)를 능가합니다.',\n        'title': 'Together MoA — collective intelligence of open-source models pushing ...',\n        'link': 'https://www.together.ai/blog/together-moa'\n    },\n    {\n        'snippet': '이 목표를 향해, 여러 LLM의 집단적 강점을 Mixture-of-Agents (MoA) 방법론을 통해 활용하는 새로운 접근 방식을 제안합니다. 우리의 접근 방식에서는 각 층이 여러 LLM 에이전트로 이루어진 층화식 MoA 아키텍처를 구성합니다. 각 에이전트는 이전 층에서 생성된 모든 출력을 보조 정보로 받습니다.',\n        'title': 'Mixture-of-Agents Enhances Large Language Model Capabilities',\n        'link': 'https://huggingface.co/papers/2406.04692'\n    },\n    {\n        'snippet': 'Mixture of Agents (MoA)는 여러 LLM의 집단적 강점을 활용하여 성능을 향상시키는 혁신적인 접근 방식으로, 최첨단 결과를 달성합니다. 각 층이 여러 LLM 에이전트로 이루어진 층화 구조를 사용하여 MoA는 65.1%의 점수로 AlpacaEval 2.0에서 GPT-4 Omni의 57.5%를 크게 능가합니다.',\n        'title': 'GitHub - togethercomputer/MoA',\n        'link': 'https://github.com/togethercomputer/moa'\n    }\n]\r\n```\n\n결과가 정확히 5개인 것을 확인할 수 있습니다. 하지만 텍스트가 짧네요! 이 짧은 조각 텍스트는 구체적인 사실적 질문에 사용할 수는 있지만 새로운 지식 베이스를 구축하기에는 부족합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n좋은 점은 링크도 얻었다는 것이에요 🤣\n\n![image](/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_2.png)\n\n# 스니펫에서 문서로\n\nGitHub 레포지토리를 하나씩 검색하면서 쉽고 완벽한 웹 검색을 찾고 있었는데, newspaper3k를 만나기 전까지요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 Python 라이브러리는 검색 엔진 결과에서 전체 텍스트를 얻기 위한 입구입니다. 그게 전부가 아닙니다!\n\nnltk 라이브러리도 함께 사용되기 때문에 키워드, 설명, 이미지 URL 및 요약과 같은 다양한 메타데이터로 정보를 보완할 수 있습니다!\n\nGoogle Colab에서 항상 살펴봅시다.\n\n```js\nfrom newspaper import Article\nimport nltk\nnltk.download('punkt')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n웹 페이지 텍스트 추출 함수들은 정말 쉽게 사용할 수 있어요. NLTK 기능을 포함하려면 우선 Punkt를 다운로드해야 해요.\n\nNLTK에서 Punkt는 문장 토큰화에 사용되는 비지도 학습 가능한 모델이에요. 이 모델은 문장을 단어로 나누어 문장 시작 단어, 전치사구, 약어에 대한 모델을 개발함으로써 문장을 분리해요.\n\n이제 우리가 작은 Neural toolkit를 준비했으니 웹 페이지에서 가능한 모든 것을 추출해 봅시다. 이전 DuckDuckGo 검색에서 추출된 링크 중 하나를 사용할 수 있어요: https://www.together.ai/blog/together-moa\n\n```js\nurl = 'https://www.together.ai/blog/together-moa'\narticle = Article(url) # payload requests를 불러와요\narticle.download()     # 로컬로 데이터와 메타데이터를 버퍼링해요\narticle.parse()        # 텍스트 추출\narticle.nlp()          # 키워드와 요약을 위해 nlp 도구 실행\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 모든 것이 처리되었으니 데이터를 살펴볼 수 있어요:\n\n```js\nprint(article.text)\n---\n우린 Mixture of Agents (MoA)를 소개합니다. 이는 여러 LLM들의 집합적인 강점을 활용하여 최신 기술 수준을 향상시키는 접근법입니다. 또한 우리는 몇몇 오픈 소스 LLM 에이전트를 이용하여 65.1%의 점수를 얻는 Together MoA라는 참조 구현을 제공합니다. 이로써 이전 리더인 GPT-4o (57.5%)를 능가했습니다.\\n\\n그림 1: 에이전트 혼합 구조의 그림. 이 예시 ...\n\n\nprint(article.authors)\n\nprint(article.keywords)\n---\n['에이전트', '20', '전선', '성능', '모델', 'qwen1510bchat', '응답', 'moa', 'llm', '집단', '오픈소스', 'alpacaeval', '제안자', '인텔리전스', 'gpt4o', '능력강화']\n\nprint(article.summary)\n---\n우리는 Mixture of Agents (MoA)를 소개합니다. 이는 여러 LLM들의 집단적인 강점을 활용하여 최신 기술 수준을 향상시키는 접근법입니다.\n개요: 새로운 방법인 Mixture of Agents (MoA)를 소개하게 되어 기쁩니다. 이는 여러 LLM들의 집단적인 강점을 활용하는 혁신적인 접근법입니다.\n우리의 참조 구현인 Together MoA는 오직 오픈 소스 모델을 사용하여 AlpacaEval 2.0에서 57.5%인 GPT-4o를 크게 능가하며 65.1%의 점수를 얻습니다.\n그림 2는 각 모델이 AlpacaEval 2.0에서 기존 점수를 크게 개선함을 보여줍니다. 특히, 오픈 소스 모델만을 사용하여 AlpacaEval 2.0에서 우리는 57.5% (GPT-4o)에서 65.1% (Together MoA)로 7.6%의 절대 향상을 이룩했습니다.\n\nprint(article.meta_description)\n\nprint(article.meta_img)\nhttps://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6667e9c28da8569e846b4632_thumbnail.jpg\n\nprint(article.meta_keywords)\n['']\n```\n\n정말 멋지다고 생각해요!\n\n이 몇 줄의 코드로 전체 텍스트와 중요한 메타데이터를 검색했어요. 웹 리소스가 정말 좋은 경우 날짜와 주 이미지도 간단히 이렇게 얻을 수 있답니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\r\narticle.top_image\r\narticle.publish_date\r\n```\r\n\r\n![image](/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_3.png)\r\n\r\n# 그럼 이제 어떻게 하죠? 자체 문서 저장소 만들기\r\n\r\n지금까지 말한 모든 것들은 데이터/정보가 생성 AI 애플리케이션에 있어 핵심이라는 것을 고려할 때에만 관련이 있습니다. LLM은 이제 NLP 작업을 뛰어나게 수행하므로 좋은 텍스트가 필요합니다.\r\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 더 나아가 봅시다! 웹 검색 쿼리를 입력하면 데이터를 풍부하게 모아서 표준 Langchain 형식으로 정리하는 파이프라인을 직접 만들 수 있습니다. 언제든지 사용할 준비가 완료되었어요!\n\n## Wrapper 실행\n\n```js\nfrom newspaper import Article\nimport pickle\nfrom rich.markdown import Markdown\nimport datetime\nfrom rich.console import Console\nfrom langchain.schema.document import Document\nconsole = Console(width=90)\nimport nltk\n# DuckDuckGo 검색 엔진 래퍼 준비\nfrom langchain_community.utilities import DuckDuckGoSearchAPIWrapper\nwrapper = DuckDuckGoSearchAPIWrapper(region='us-en', time=\"y\", max_results=10) #time parameter Options: d, w, m, y\n# 사용자 쿼리 요청\nconsole.print(f'[bold red1]무엇을 검색하시겠습니까?')\nresearch = console.input(f'[bright_green]\u003e ')\nconsole.print(90*'=')\n# Wrapper 실행\nrawdb = wrapper.results(research,max_results=5)\n```\n\n저는 아이작 아시모프의 원자 폭탄에 대한 이야기를 찾아보겠습니다. 그런 다음 예상 출력 형식을 사용하여 데이터베이스를 구축할 거에요. 다시 설명드리자면, 예상 형식은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n[\n  {\n    'snippet': '비디오 콜 - 아이작 아시모프 - 다양한 패운데이션 이야기 (1950년대+) 패운데이션 시리즈는 거대한 시간과 공간을 다룹니다. ... 원자폭탄 - H.G. 웰스 - The World Set Free (1914)',\n    'title': '과학 소설이 미래 기술을 예언한 10가지 시간 - 디스트로이드',\n    'link': 'https://www.destructoid.com/10-times-sci-fi-predicted-the-future-of-technology/'\n  },\n  {\n    'snippet': '밤바람과 다른 이야기들. 아이작 아시모프의 최고의 과학 소설. 전체...'\n  }\n]\n```\n\n여기에서 이미 황금 광산을 발견했어요: 우리는 DDG 검색 필드에서 제목, 스니펫 및 링크를 얻을 수 있어요.\n\n## 신문 NLP 도구와 Wrapper 결과 병합\n\n이제 rawdb 리스트를 반복하고 각 URL을 사용하여 newspaper3k를 실행할 수 있습니다. 더 똑똒하게 만들기 위해 반복 중 LangChain Document 객체를 만들어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndocdocs = []\nfor items in rawdb:\n    url = items[\"link\"]\n    try:  # 만약 URL에 접속할 수 없다면 유용합니다\n        article = Article(url)\n        article.download()\n        article.parse()\n        article.nlp()\n        kw = []\n        # NLTK 키워드와 메타 웹페이지 키워드를 병합합니다\n        for i in article.keywords + article.meta_keywords:\n            if i == '':  # 우리에게는 블랙 키워드가 없습니다\n                pass\n            else:\n                kw.append(i)\n        if article.text == '':  # 때로는 구문 분석할 텍스트가 없습니다. 그래서 스니펫을 사용합니다\n            docdocs.append(Document(page_content=items[\"snippet\"], metadata={\n                'source': items[\"link\"],\n                'title': items[\"title\"],\n                'snippet': items[\"snippet\"],\n                'author': article.authors,\n                'keywords': kw,\n                'meta_description': article.meta_description,\n                'meta_img': article.meta_img,\n                'top_image': article.top_image,\n                'publish_date': article.publish_date,\n                'summary': article.summary}))\n        else:\n            docdocs.append(Document(page_content=article.text.replace('\\n\\n', ''), metadata={\n                'source': items[\"link\"],\n                'title': items[\"title\"],\n                'snippet': items[\"snippet\"],\n                'author': article.authors,\n                'keywords': kw,\n                'meta_description': article.meta_description,\n                'meta_img': article.meta_img,\n                'top_image': article.top_image,\n                'publish_date': article.publish_date,\n                'summary': article.summary}))\n    except:\n        pass\n```\n\n이제 변수 docdocs에는 메타데이터가 풍부한 LangChain 문서 목록이 있습니다. 이를 FAISS와 같은 벡터 데이터베이스에서 직접 사용하거나 로컬 파일에 저장할 수 있습니다. 저는 두 번째 옵션을 선호합니다. 그 이유는 언제든지 이러한 문서를 나중에 병합할 수 있기 때문입니다.\n\n따라서 나는 이러한 문서를 저장하기 위해 피클 형식을 선택한 것입니다. pickle 모듈은 Python 표준 라이브러리에 포함되어 있습니다. Python 개체 구조를 직렬화하고 역 직렬화하기 위한 이진 프로토콜을 구현합니다.\n\n“Pickling”은 Python 개체 계층 구조를 바이트 스트림으로 변환하는 프로세스이며, “unpickling”은 그 반대 작업으로, 바이너리 파일 또는 바이트류 객체에서 바이트 스트림을 개체 계층 구조로 변환하는 작업입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n## 메타데이터와 함께 문서 세트를 pickle에 저장합니다.\nlcdfilename = research.replace(' ','_')+'.lcd'\noutput = open(lcdfilename, 'wb')\npickle.dump(docdocs, output)\noutput.close()\nconsole.print(Markdown(\"\u003e LangChain 문서 데이터가 저장되었습니다...\"))\n```\n\n이제 LangChain 문서를 가지고 로컬 모델을 실행해 볼 수 있어요. 여기에 하나의 도전 과제가 있네요: 왜 llama-cpp-python만 사용해서 시도해 보지 않으세요?\n\n# 결론\n\nWeb 검색 및 자연어 처리(NLP) 도구를 이용하여 최신 및 관련 데이터로 언어 학습 모델(LLM)를 풍부하게 하는 혁신적인 접근 방식을 함께 탐색했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이와 같은 전략들은 계산 요구 사항과 무료 데이터의 부족으로 인한 제한을 극복하는 데 중요합니다.\n\n어고노스틱 LLM을 달성하기 위해, 외부 지식 소스를 통합하여 LLM 응답의 정확성과 신뢰도를 향상시키는 Retrieval Augmented Generation (RAG) 기술이 결정적인 기법으로 부상합니다.\n\n저희는 웹 검색 엔진과 newspaper3k와 같은 NLP 라이브러리를 도구로 사용하여 온라인 리소스로부터 전체 텍스트와 가치 있는 메타데이터를 추출하고, 간략한 스니펫을 포괄적인 문서로 변환했습니다.\n\n이 프로세스는 생성 모델의 잘라내기 날짜 제한을 우회하는 것뿐만 아니라 개인화된 문서 저장소를 생성하는 것을 용이하게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n무서워하지 마시고 실험해보세요. 하지만 항상 기억하세요, 도구는 좋거나 나쁘지 않습니다! 도구는 사용하는 사람만큼 좋습니다!\n\n만약 이 이야기가 가치 있었고 조금이라도 지원을 보내고 싶다면, 다음을 해볼 수 있습니다:\n\n- 이 이야기에 대해 많이 박수를 보내기\n- 기억할 가치가 있는 부분을 강조하기 (나중에 찾기가 쉽고, 나는 더 나은 기사를 쓸 수 있습니다)\n- '자체 AI 구축 방법 배우기, 이 무료 eBook 다운로드하기'\n- 내 링크를 통해 Medium 멤버십 가입하기 ($5/월로 무제한 Medium 이야기 읽기)\n- Medium에서 나를 팔로우하기\n- 내 최신 기사를 읽기: https://medium.com/@fabio.matricardi\n\n더 많은 내용을 보고 싶다면, 다음은 몇 가지 아이디어입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n구글 Colab 노트북이 있는 GitHub 저장소\n\n![GitHub Repository](/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_4.png)\n\n이 이야기는 Generative AI에서 발행되었습니다. 최신 AI 이야기를 알고 싶다면 LinkedIn에서 저희와 연결하고 Zeniteq를 팔로우하세요.\n\n저희의 뉴스레터에 가입하여 최신 generative AI 뉴스와 업데이트를 받아보세요. 함께 AI의 미래를 함께 만들어 봅시다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`![2024-06-20-MaketheWebyourbestfrienddataprovider_5.png](/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_5.png)`","ogImage":{"url":"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_0.png"},"coverImage":"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_0.png","tag":["Tech"],"readingTime":14},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_0.png\"\u003e\n\u003cp\u003eLLM을 도구로 갖추는 아이디어는 그리 새로운 것은 아닙니다. LangChain과 Llamaindex와 같은 강력하고 잘 알려진 프레임워크는 몇 달 전에 이미 이를 실행했습니다.\u003c/p\u003e\n\u003cp\u003e그들은 위험 부담에 대한 승리했어요!\u003c/p\u003e\n\u003cp\u003e지금 완전히 열광 중인 AI 에이전트 혁명은 도구 패러다임을 충분히 활용하여 놀라운 결과를 얻고 있어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e인공지능이 지금 새로운 벽을 맞이했습니다: 계산 요구 사항과 무료로 이용 가능한 데이터의 끝.\u003c/p\u003e\n\u003cp\u003e이 기사에서는 이러한 도전 과제를 다루고, 우리의 LLM 애플리케이션을 새로운 관련 데이터로 무료로 풍부하게 만드는 방법을 배울 것입니다!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 목차\n- 중립적 \u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e 방향\n- 검색 보강 생성: 여전히 최고의 도구\n- 웹 검색 및 \u003cspan class=\"hljs-variable constant_\"\u003eNLP\u003c/span\u003e 뉴스는 도구입니다\n- 단락에서 문서까지\n- 그럼 이제 무엇을 해야 할까요? 직접 문서 저장소 만들기\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e중립적 LLM\u003c/h1\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e기술 발전은 이미 지난 해의 많은 작업을 이미 완료했습니다. 새로운 하드웨어와 GPU 세트는 이제 더 빠르고 저렴하게 복잡한 신경망 계산을 처리할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e동시에 엣지-모바일 기기로의 명확한 이동이 있으며, 작은 언어 모델이 큰 모델과 경쟁할 수 있도록 하는 데 특별한 주의를 기울이고 있습니다. TinyLlama 프로젝트를 시작으로 목표는 모바일 전화 하드웨어에서 실행할만큼 충분히 작지만 환각하지 않고 유용한 모델을 만드는 것입니다.\u003c/p\u003e\n\u003cp\u003e규모 법칙 이상으로, 우리는 LLM이 추상화하고 이성적으로 추론하는 능력을 잃는 한계를 이해해야 합니다. 데이터 없이 신경망을 구축할 수 없지만 동시에 우리는 새로운 위키피디아를 만들고 있지는 않습니다, 맞죠?\u003c/p\u003e\n\u003cp\u003eAI 커뮤니티에서 진정한 권위자인 Cobus Greyling은 항상 이것을 강조합니다: 데이터 처리 및 데이터 처리를 LLM 응용 프로그램에서 분리하세요. 그에게는 LLM 응용 프로그램은 모델에 중립적이어야 하며 LLM을 유틸리티로 취급해야 한다 - 그러나 이는 신뢰할 수 있는 데이터, 좋은 소식 및 선별된 데이터 집합에 대해 작업해야 한다는 것을 의미합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e우수한 데이터를 이용하면 작은 모델(20억 파라미터 미만)이 탁월한 성과를 거둘 수 있어요!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e검색 증강 생성: 여전히 최고의 도구\u003c/h2\u003e\n\u003cp\u003e좋은 말들이지만, 이를 어떻게 구현할까요?\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eRetrieval Augmented Generation (RAG)은 생성된 텍스트의 품질을 향상시키기 위해 자연어 처리에서 사용되는 기술입니다.\u003c/p\u003e\n\u003cp\u003e마술은 없어요! 우리는 대규모 외부 지식 원본(데이터베이스나 문서 코퍼스 등)에서 관련 정보를 통합하여 LLM 프롬프트에 적용합니다. RAG에서는 쿼리나 프롬프트에 대한 응답을 생성할 때 모델이 매우 좋은 작업을 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e먼저 검색 메커니즘을 사용해 외부 지식 원본에서 관련 정보를 검색합니다.\u003c/li\u003e\n\u003cli\u003e검색된 정보는 그 후 생성 모델로 투입되어 자신의 지식을 보완하고\u003c/li\u003e\n\u003cli\u003e보다 정확하고 유익한 응답을 생성합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e예를 들어, 특정 역사적 사건에 대한 질문을 하는 경우 RAG 모델은 먼저 역사 문서의 대규모 데이터베이스에서 해당 사건에 대한 정보를 검색하고, 그 정보를 사용하여 귀하의 질문에 대한 세부적이고 정확한 응답을 생성합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003eRAG은 순수 생성 모델과 구체적인 사실 정보에 제한을 받을 수 있는 것을 합하고, 오직 검색 기반 모델과 창의적인 대답을 생성하는 데 어려움을 겪을 수 있는 것을 합하면서 이 둘 간의 간극을 좁히는 데 도움을 줍니다.\u003c/p\u003e\n\u003cp\u003e그러니까 우리 수학 시간을 시작해 볼까요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e좋은 이유를 제공하는 작은 LLM이 주어지면\u003c/li\u003e\n\u003cli\u003e좋고 최신 정보를 위해 RAG 파이프라인을 설정하면\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e... 전례 없는 그리고 의미 있는 문서 세트를 만드는 방법이 필요합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003ch1\u003e웹 검색 및 NLP 뉴스는 도구들입니다\u003c/h1\u003e\n\u003cp\u003e이전 글에서 LangChain을 DuckDuckGo 웹 검색과 함께 사용하는 방법에 대해 설명했었습니다.\u003c/p\u003e\n\u003cp\u003e최신 뉴스와 업데이트된 정보를 검색 엔진을 사용하여 가져오는 것은 어떤 반 데이터 모형으로 향하는 놀라운 방법입니다. 생성 모델의 마감일 한계가 우리가 필요한 모든 것과 외부 지식을 제공할 수 있기 때문에 우회됩니다.\u003c/p\u003e\n\u003cp\u003e확인되고 진실한 정보가 있는 경우, 우리의 LLM 응용 프로그램은 좋은 RAG 전략을 사용함으로써보다 효과적 일 수 있습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e하지만 잘 알려진 웹 검색 도구에도 제한이 있습니다! 무료 Google Colab 노트북을 사용하여 확인해 보세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e%pip install --upgrade --quiet langchain langchain-community faiss-cpu \n%pip install tiktoken duckduckgo-search llama-cpp-agent newspaper3k\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLangchain과 함께 duckduckgo-search와 newspaper3k(강력한 NLP 웹 HTML 문서 파서)를 설치합니다.\u003c/p\u003e\n\u003cp\u003e먼저, 웹 검색의 출력이 무엇인지 살펴봅시다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain_community.\u003cspan class=\"hljs-property\"\u003eutilities\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDuckDuckGoSearchAPIWrapper\u003c/span\u003e\nwrapper = \u003cspan class=\"hljs-title class_\"\u003eDuckDuckGoSearchAPIWrapper\u003c/span\u003e(region=\u003cspan class=\"hljs-string\"\u003e'us-en'\u003c/span\u003e, \n                                   time=\u003cspan class=\"hljs-string\"\u003e\"y\"\u003c/span\u003e, \n                                   max_results=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e) #time \u003cspan class=\"hljs-title class_\"\u003eOptions\u003c/span\u003e: d, w, m, y\n\nrawdb = wrapper.\u003cspan class=\"hljs-title function_\"\u003eresults\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'LLM Mixture of Agents'\u003c/span\u003e,max_results=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기서는 에이전트 도구가 아닌 wrapper 자체를 사용하여 \"LLM 혼합 에이전트\"에 대한 정보를 찾습니다. 결과를 출력하고 rawdb 변수에 저장하면 다음과 같은 내용이 나타납니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e[\n    {\n        \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'대형 언어 모델(LM)의 최근 발전은 자연어 이해 및 생성 작업에서 상당한 성능을 보여줍니다. 증가하는 LLM의 수로 인해 여러 LLM의 집단적 전문지식을 어떻게 활용할지는 흥미로운 개방 방향입니다. 이 목표를 향해, 우리는 Mixture-of-Agents (MoA) 방법을 통해 여러 LLM의 집단적 강점을 활용하는 새로운 접근 방식을 제안합니다.'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Mixture-of-Agents Enhances Large Language Model Capabilities'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'link'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://arxiv.org/abs/2406.04692'\u003c/span\u003e\n    },\n    {\n        \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'첫째로, 우리는 답안 제공자들에 의해 생성된 답 중 하나를 선택하기 위해 집계 모델을 사용하는 LLM 기반 순위 결정기를 Mixture-of-Agents와 비교합니다. 쓰기 결과는 Figure 4에 나와 있으며 MoA 접근 방식이 LLM-순위 결정층 베이스라인을 크게 능가함을 관찰할 수 있습니다.'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Mixture-of-Agents Enhances Large Language Model Capabilities - arXiv.org'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'link'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://arxiv.org/html/2406.04692v1'\u003c/span\u003e\n    },\n    {\n        \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'우리는 여러 LLM의 집단적 강점을 활용하여 최첨단 품질을 향상시키기 위한 Mixture of Agents (MoA) 접근 방식을 소개합니다. 그리고 우리는 상태-of-the-art의 품질을 향상하기 위해 여러 오픈소스 LLM 에이전트들을 활용하는 참조 구현인 Together MoA을 제공합니다. Together MoA는 AlpacaEval 2.0에서 65.1%의 점수를 달성하여 이전 리더 GPT-4o (57.5%)를 능가합니다.'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Together MoA — collective intelligence of open-source models pushing ...'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'link'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://www.together.ai/blog/together-moa'\u003c/span\u003e\n    },\n    {\n        \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'이 목표를 향해, 여러 LLM의 집단적 강점을 Mixture-of-Agents (MoA) 방법론을 통해 활용하는 새로운 접근 방식을 제안합니다. 우리의 접근 방식에서는 각 층이 여러 LLM 에이전트로 이루어진 층화식 MoA 아키텍처를 구성합니다. 각 에이전트는 이전 층에서 생성된 모든 출력을 보조 정보로 받습니다.'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Mixture-of-Agents Enhances Large Language Model Capabilities'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'link'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://huggingface.co/papers/2406.04692'\u003c/span\u003e\n    },\n    {\n        \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Mixture of Agents (MoA)는 여러 LLM의 집단적 강점을 활용하여 성능을 향상시키는 혁신적인 접근 방식으로, 최첨단 결과를 달성합니다. 각 층이 여러 LLM 에이전트로 이루어진 층화 구조를 사용하여 MoA는 65.1%의 점수로 AlpacaEval 2.0에서 GPT-4 Omni의 57.5%를 크게 능가합니다.'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'GitHub - togethercomputer/MoA'\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'link'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://github.com/togethercomputer/moa'\u003c/span\u003e\n    }\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e결과가 정확히 5개인 것을 확인할 수 있습니다. 하지만 텍스트가 짧네요! 이 짧은 조각 텍스트는 구체적인 사실적 질문에 사용할 수는 있지만 새로운 지식 베이스를 구축하기에는 부족합니다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e좋은 점은 링크도 얻었다는 것이에요 🤣\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_2.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e스니펫에서 문서로\u003c/h1\u003e\n\u003cp\u003eGitHub 레포지토리를 하나씩 검색하면서 쉽고 완벽한 웹 검색을 찾고 있었는데, newspaper3k를 만나기 전까지요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이 Python 라이브러리는 검색 엔진 결과에서 전체 텍스트를 얻기 위한 입구입니다. 그게 전부가 아닙니다!\u003c/p\u003e\n\u003cp\u003enltk 라이브러리도 함께 사용되기 때문에 키워드, 설명, 이미지 URL 및 요약과 같은 다양한 메타데이터로 정보를 보완할 수 있습니다!\u003c/p\u003e\n\u003cp\u003eGoogle Colab에서 항상 살펴봅시다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e newspaper \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eArticle\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e nltk\nnltk.\u003cspan class=\"hljs-title function_\"\u003edownload\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'punkt'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e웹 페이지 텍스트 추출 함수들은 정말 쉽게 사용할 수 있어요. NLTK 기능을 포함하려면 우선 Punkt를 다운로드해야 해요.\u003c/p\u003e\n\u003cp\u003eNLTK에서 Punkt는 문장 토큰화에 사용되는 비지도 학습 가능한 모델이에요. 이 모델은 문장을 단어로 나누어 문장 시작 단어, 전치사구, 약어에 대한 모델을 개발함으로써 문장을 분리해요.\u003c/p\u003e\n\u003cp\u003e이제 우리가 작은 Neural toolkit를 준비했으니 웹 페이지에서 가능한 모든 것을 추출해 봅시다. 이전 DuckDuckGo 검색에서 추출된 링크 중 하나를 사용할 수 있어요: \u003ca href=\"https://www.together.ai/blog/together-moa\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://www.together.ai/blog/together-moa\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eurl = \u003cspan class=\"hljs-string\"\u003e'https://www.together.ai/blog/together-moa'\u003c/span\u003e\narticle = \u003cspan class=\"hljs-title class_\"\u003eArticle\u003c/span\u003e(url) # payload requests를 불러와요\narticle.\u003cspan class=\"hljs-title function_\"\u003edownload\u003c/span\u003e()     # 로컬로 데이터와 메타데이터를 버퍼링해요\narticle.\u003cspan class=\"hljs-title function_\"\u003eparse\u003c/span\u003e()        # 텍스트 추출\narticle.\u003cspan class=\"hljs-title function_\"\u003enlp\u003c/span\u003e()          # 키워드와 요약을 위해 nlp 도구 실행\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이제 모든 것이 처리되었으니 데이터를 살펴볼 수 있어요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(article.\u003cspan class=\"hljs-property\"\u003etext\u003c/span\u003e)\n---\n우린 \u003cspan class=\"hljs-title class_\"\u003eMixture\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAgents\u003c/span\u003e (\u003cspan class=\"hljs-title class_\"\u003eMoA\u003c/span\u003e)를 소개합니다. 이는 여러 \u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e들의 집합적인 강점을 활용하여 최신 기술 수준을 향상시키는 접근법입니다. 또한 우리는 몇몇 오픈 소스 \u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e 에이전트를 이용하여 \u003cspan class=\"hljs-number\"\u003e65.1\u003c/span\u003e%의 점수를 얻는 \u003cspan class=\"hljs-title class_\"\u003eTogether\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMoA\u003c/span\u003e라는 참조 구현을 제공합니다. 이로써 이전 리더인 \u003cspan class=\"hljs-variable constant_\"\u003eGPT\u003c/span\u003e-4o (\u003cspan class=\"hljs-number\"\u003e57.5\u003c/span\u003e%)를 능가했습니다.\\n\\n그림 \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e: 에이전트 혼합 구조의 그림. 이 예시 ...\n\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(article.\u003cspan class=\"hljs-property\"\u003eauthors\u003c/span\u003e)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(article.\u003cspan class=\"hljs-property\"\u003ekeywords\u003c/span\u003e)\n---\n[\u003cspan class=\"hljs-string\"\u003e'에이전트'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'20'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'전선'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'성능'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'모델'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'qwen1510bchat'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'응답'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'moa'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'llm'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'집단'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'오픈소스'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'alpacaeval'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'제안자'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'인텔리전스'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'gpt4o'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'능력강화'\u003c/span\u003e]\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(article.\u003cspan class=\"hljs-property\"\u003esummary\u003c/span\u003e)\n---\n우리는 \u003cspan class=\"hljs-title class_\"\u003eMixture\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAgents\u003c/span\u003e (\u003cspan class=\"hljs-title class_\"\u003eMoA\u003c/span\u003e)를 소개합니다. 이는 여러 \u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e들의 집단적인 강점을 활용하여 최신 기술 수준을 향상시키는 접근법입니다.\n개요: 새로운 방법인 \u003cspan class=\"hljs-title class_\"\u003eMixture\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAgents\u003c/span\u003e (\u003cspan class=\"hljs-title class_\"\u003eMoA\u003c/span\u003e)를 소개하게 되어 기쁩니다. 이는 여러 \u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e들의 집단적인 강점을 활용하는 혁신적인 접근법입니다.\n우리의 참조 구현인 \u003cspan class=\"hljs-title class_\"\u003eTogether\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMoA\u003c/span\u003e는 오직 오픈 소스 모델을 사용하여 \u003cspan class=\"hljs-title class_\"\u003eAlpacaEval\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e에서 \u003cspan class=\"hljs-number\"\u003e57.5\u003c/span\u003e%인 \u003cspan class=\"hljs-variable constant_\"\u003eGPT\u003c/span\u003e-4o를 크게 능가하며 \u003cspan class=\"hljs-number\"\u003e65.1\u003c/span\u003e%의 점수를 얻습니다.\n그림 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e는 각 모델이 \u003cspan class=\"hljs-title class_\"\u003eAlpacaEval\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e에서 기존 점수를 크게 개선함을 보여줍니다. 특히, 오픈 소스 모델만을 사용하여 \u003cspan class=\"hljs-title class_\"\u003eAlpacaEval\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e에서 우리는 \u003cspan class=\"hljs-number\"\u003e57.5\u003c/span\u003e% (\u003cspan class=\"hljs-variable constant_\"\u003eGPT\u003c/span\u003e-4o)에서 \u003cspan class=\"hljs-number\"\u003e65.1\u003c/span\u003e% (\u003cspan class=\"hljs-title class_\"\u003eTogether\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMoA\u003c/span\u003e)로 \u003cspan class=\"hljs-number\"\u003e7.6\u003c/span\u003e%의 절대 향상을 이룩했습니다.\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(article.\u003cspan class=\"hljs-property\"\u003emeta_description\u003c/span\u003e)\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(article.\u003cspan class=\"hljs-property\"\u003emeta_img\u003c/span\u003e)\n\u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//cdn.prod.website-files.com/650c3b59079d92475f37b68f/6667e9c28da8569e846b4632_thumbnail.jpg\u003c/span\u003e\n\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(article.\u003cspan class=\"hljs-property\"\u003emeta_keywords\u003c/span\u003e)\n[\u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e정말 멋지다고 생각해요!\u003c/p\u003e\n\u003cp\u003e이 몇 줄의 코드로 전체 텍스트와 중요한 메타데이터를 검색했어요. 웹 리소스가 정말 좋은 경우 날짜와 주 이미지도 간단히 이렇게 얻을 수 있답니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003earticle.\u003cspan class=\"hljs-property\"\u003etop_image\u003c/span\u003e\r\narticle.\u003cspan class=\"hljs-property\"\u003epublish_date\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e그럼 이제 어떻게 하죠? 자체 문서 저장소 만들기\u003c/h1\u003e\n\u003cp\u003e지금까지 말한 모든 것들은 데이터/정보가 생성 AI 애플리케이션에 있어 핵심이라는 것을 고려할 때에만 관련이 있습니다. LLM은 이제 NLP 작업을 뛰어나게 수행하므로 좋은 텍스트가 필요합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e여기서 더 나아가 봅시다! 웹 검색 쿼리를 입력하면 데이터를 풍부하게 모아서 표준 Langchain 형식으로 정리하는 파이프라인을 직접 만들 수 있습니다. 언제든지 사용할 준비가 완료되었어요!\u003c/p\u003e\n\u003ch2\u003eWrapper 실행\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e newspaper \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eArticle\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pickle\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e rich.\u003cspan class=\"hljs-property\"\u003emarkdown\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMarkdown\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e datetime\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e rich.\u003cspan class=\"hljs-property\"\u003econsole\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eConsole\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.\u003cspan class=\"hljs-property\"\u003eschema\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003edocument\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDocument\u003c/span\u003e\n\u003cspan class=\"hljs-variable language_\"\u003econsole\u003c/span\u003e = \u003cspan class=\"hljs-title class_\"\u003eConsole\u003c/span\u003e(width=\u003cspan class=\"hljs-number\"\u003e90\u003c/span\u003e)\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e nltk\n# \u003cspan class=\"hljs-title class_\"\u003eDuckDuckGo\u003c/span\u003e 검색 엔진 래퍼 준비\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain_community.\u003cspan class=\"hljs-property\"\u003eutilities\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDuckDuckGoSearchAPIWrapper\u003c/span\u003e\nwrapper = \u003cspan class=\"hljs-title class_\"\u003eDuckDuckGoSearchAPIWrapper\u003c/span\u003e(region=\u003cspan class=\"hljs-string\"\u003e'us-en'\u003c/span\u003e, time=\u003cspan class=\"hljs-string\"\u003e\"y\"\u003c/span\u003e, max_results=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e) #time parameter \u003cspan class=\"hljs-title class_\"\u003eOptions\u003c/span\u003e: d, w, m, y\n# 사용자 쿼리 요청\n\u003cspan class=\"hljs-variable language_\"\u003econsole\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'[bold red1]무엇을 검색하시겠습니까?'\u003c/span\u003e)\nresearch = \u003cspan class=\"hljs-variable language_\"\u003econsole\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003einput\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'[bright_green]\u003e '\u003c/span\u003e)\n\u003cspan class=\"hljs-variable language_\"\u003econsole\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e90\u003c/span\u003e*\u003cspan class=\"hljs-string\"\u003e'='\u003c/span\u003e)\n# \u003cspan class=\"hljs-title class_\"\u003eWrapper\u003c/span\u003e 실행\nrawdb = wrapper.\u003cspan class=\"hljs-title function_\"\u003eresults\u003c/span\u003e(research,max_results=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e저는 아이작 아시모프의 원자 폭탄에 대한 이야기를 찾아보겠습니다. 그런 다음 예상 출력 형식을 사용하여 데이터베이스를 구축할 거에요. 다시 설명드리자면, 예상 형식은 다음과 같습니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e[\n  {\n    \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'비디오 콜 - 아이작 아시모프 - 다양한 패운데이션 이야기 (1950년대+) 패운데이션 시리즈는 거대한 시간과 공간을 다룹니다. ... 원자폭탄 - H.G. 웰스 - The World Set Free (1914)'\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'과학 소설이 미래 기술을 예언한 10가지 시간 - 디스트로이드'\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'link'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://www.destructoid.com/10-times-sci-fi-predicted-the-future-of-technology/'\u003c/span\u003e\n  },\n  {\n    \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'밤바람과 다른 이야기들. 아이작 아시모프의 최고의 과학 소설. 전체...'\u003c/span\u003e\n  }\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e여기에서 이미 황금 광산을 발견했어요: 우리는 DDG 검색 필드에서 제목, 스니펫 및 링크를 얻을 수 있어요.\u003c/p\u003e\n\u003ch2\u003e신문 NLP 도구와 Wrapper 결과 병합\u003c/h2\u003e\n\u003cp\u003e이제 rawdb 리스트를 반복하고 각 URL을 사용하여 newspaper3k를 실행할 수 있습니다. 더 똑똒하게 만들기 위해 반복 중 LangChain Document 객체를 만들어요.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003edocdocs = []\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e items \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e rawdb:\n    url = items[\u003cspan class=\"hljs-string\"\u003e\"link\"\u003c/span\u003e]\n    \u003cspan class=\"hljs-keyword\"\u003etry\u003c/span\u003e:  \u003cspan class=\"hljs-comment\"\u003e# 만약 URL에 접속할 수 없다면 유용합니다\u003c/span\u003e\n        article = Article(url)\n        article.download()\n        article.parse()\n        article.nlp()\n        kw = []\n        \u003cspan class=\"hljs-comment\"\u003e# NLTK 키워드와 메타 웹페이지 키워드를 병합합니다\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e article.keywords + article.meta_keywords:\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e i == \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e:  \u003cspan class=\"hljs-comment\"\u003e# 우리에게는 블랙 키워드가 없습니다\u003c/span\u003e\n                \u003cspan class=\"hljs-keyword\"\u003epass\u003c/span\u003e\n            \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n                kw.append(i)\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e article.text == \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e:  \u003cspan class=\"hljs-comment\"\u003e# 때로는 구문 분석할 텍스트가 없습니다. 그래서 스니펫을 사용합니다\u003c/span\u003e\n            docdocs.append(Document(page_content=items[\u003cspan class=\"hljs-string\"\u003e\"snippet\"\u003c/span\u003e], metadata={\n                \u003cspan class=\"hljs-string\"\u003e'source'\u003c/span\u003e: items[\u003cspan class=\"hljs-string\"\u003e\"link\"\u003c/span\u003e],\n                \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: items[\u003cspan class=\"hljs-string\"\u003e\"title\"\u003c/span\u003e],\n                \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: items[\u003cspan class=\"hljs-string\"\u003e\"snippet\"\u003c/span\u003e],\n                \u003cspan class=\"hljs-string\"\u003e'author'\u003c/span\u003e: article.authors,\n                \u003cspan class=\"hljs-string\"\u003e'keywords'\u003c/span\u003e: kw,\n                \u003cspan class=\"hljs-string\"\u003e'meta_description'\u003c/span\u003e: article.meta_description,\n                \u003cspan class=\"hljs-string\"\u003e'meta_img'\u003c/span\u003e: article.meta_img,\n                \u003cspan class=\"hljs-string\"\u003e'top_image'\u003c/span\u003e: article.top_image,\n                \u003cspan class=\"hljs-string\"\u003e'publish_date'\u003c/span\u003e: article.publish_date,\n                \u003cspan class=\"hljs-string\"\u003e'summary'\u003c/span\u003e: article.summary}))\n        \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n            docdocs.append(Document(page_content=article.text.replace(\u003cspan class=\"hljs-string\"\u003e'\\n\\n'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e), metadata={\n                \u003cspan class=\"hljs-string\"\u003e'source'\u003c/span\u003e: items[\u003cspan class=\"hljs-string\"\u003e\"link\"\u003c/span\u003e],\n                \u003cspan class=\"hljs-string\"\u003e'title'\u003c/span\u003e: items[\u003cspan class=\"hljs-string\"\u003e\"title\"\u003c/span\u003e],\n                \u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e: items[\u003cspan class=\"hljs-string\"\u003e\"snippet\"\u003c/span\u003e],\n                \u003cspan class=\"hljs-string\"\u003e'author'\u003c/span\u003e: article.authors,\n                \u003cspan class=\"hljs-string\"\u003e'keywords'\u003c/span\u003e: kw,\n                \u003cspan class=\"hljs-string\"\u003e'meta_description'\u003c/span\u003e: article.meta_description,\n                \u003cspan class=\"hljs-string\"\u003e'meta_img'\u003c/span\u003e: article.meta_img,\n                \u003cspan class=\"hljs-string\"\u003e'top_image'\u003c/span\u003e: article.top_image,\n                \u003cspan class=\"hljs-string\"\u003e'publish_date'\u003c/span\u003e: article.publish_date,\n                \u003cspan class=\"hljs-string\"\u003e'summary'\u003c/span\u003e: article.summary}))\n    \u003cspan class=\"hljs-keyword\"\u003eexcept\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003epass\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 변수 docdocs에는 메타데이터가 풍부한 LangChain 문서 목록이 있습니다. 이를 FAISS와 같은 벡터 데이터베이스에서 직접 사용하거나 로컬 파일에 저장할 수 있습니다. 저는 두 번째 옵션을 선호합니다. 그 이유는 언제든지 이러한 문서를 나중에 병합할 수 있기 때문입니다.\u003c/p\u003e\n\u003cp\u003e따라서 나는 이러한 문서를 저장하기 위해 피클 형식을 선택한 것입니다. pickle 모듈은 Python 표준 라이브러리에 포함되어 있습니다. Python 개체 구조를 직렬화하고 역 직렬화하기 위한 이진 프로토콜을 구현합니다.\u003c/p\u003e\n\u003cp\u003e“Pickling”은 Python 개체 계층 구조를 바이트 스트림으로 변환하는 프로세스이며, “unpickling”은 그 반대 작업으로, 바이너리 파일 또는 바이트류 객체에서 바이트 스트림을 개체 계층 구조로 변환하는 작업입니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e## 메타데이터와 함께 문서 세트를 pickle에 저장합니다.\nlcdfilename = research.\u003cspan class=\"hljs-title function_\"\u003ereplace\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e' '\u003c/span\u003e,\u003cspan class=\"hljs-string\"\u003e'_'\u003c/span\u003e)+\u003cspan class=\"hljs-string\"\u003e'.lcd'\u003c/span\u003e\noutput = \u003cspan class=\"hljs-title function_\"\u003eopen\u003c/span\u003e(lcdfilename, \u003cspan class=\"hljs-string\"\u003e'wb'\u003c/span\u003e)\npickle.\u003cspan class=\"hljs-title function_\"\u003edump\u003c/span\u003e(docdocs, output)\noutput.\u003cspan class=\"hljs-title function_\"\u003eclose\u003c/span\u003e()\n\u003cspan class=\"hljs-variable language_\"\u003econsole\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eMarkdown\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\u003e LangChain 문서 데이터가 저장되었습니다...\"\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 LangChain 문서를 가지고 로컬 모델을 실행해 볼 수 있어요. 여기에 하나의 도전 과제가 있네요: 왜 llama-cpp-python만 사용해서 시도해 보지 않으세요?\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003eWeb 검색 및 자연어 처리(NLP) 도구를 이용하여 최신 및 관련 데이터로 언어 학습 모델(LLM)를 풍부하게 하는 혁신적인 접근 방식을 함께 탐색했습니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e이와 같은 전략들은 계산 요구 사항과 무료 데이터의 부족으로 인한 제한을 극복하는 데 중요합니다.\u003c/p\u003e\n\u003cp\u003e어고노스틱 LLM을 달성하기 위해, 외부 지식 소스를 통합하여 LLM 응답의 정확성과 신뢰도를 향상시키는 Retrieval Augmented Generation (RAG) 기술이 결정적인 기법으로 부상합니다.\u003c/p\u003e\n\u003cp\u003e저희는 웹 검색 엔진과 newspaper3k와 같은 NLP 라이브러리를 도구로 사용하여 온라인 리소스로부터 전체 텍스트와 가치 있는 메타데이터를 추출하고, 간략한 스니펫을 포괄적인 문서로 변환했습니다.\u003c/p\u003e\n\u003cp\u003e이 프로세스는 생성 모델의 잘라내기 날짜 제한을 우회하는 것뿐만 아니라 개인화된 문서 저장소를 생성하는 것을 용이하게 합니다.\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e무서워하지 마시고 실험해보세요. 하지만 항상 기억하세요, 도구는 좋거나 나쁘지 않습니다! 도구는 사용하는 사람만큼 좋습니다!\u003c/p\u003e\n\u003cp\u003e만약 이 이야기가 가치 있었고 조금이라도 지원을 보내고 싶다면, 다음을 해볼 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e이 이야기에 대해 많이 박수를 보내기\u003c/li\u003e\n\u003cli\u003e기억할 가치가 있는 부분을 강조하기 (나중에 찾기가 쉽고, 나는 더 나은 기사를 쓸 수 있습니다)\u003c/li\u003e\n\u003cli\u003e'자체 AI 구축 방법 배우기, 이 무료 eBook 다운로드하기'\u003c/li\u003e\n\u003cli\u003e내 링크를 통해 Medium 멤버십 가입하기 ($5/월로 무제한 Medium 이야기 읽기)\u003c/li\u003e\n\u003cli\u003eMedium에서 나를 팔로우하기\u003c/li\u003e\n\u003cli\u003e내 최신 기사를 읽기: \u003ca href=\"https://medium.com/@fabio.matricardi\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://medium.com/@fabio.matricardi\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e더 많은 내용을 보고 싶다면, 다음은 몇 가지 아이디어입니다:\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e구글 Colab 노트북이 있는 GitHub 저장소\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_4.png\" alt=\"GitHub Repository\"\u003e\u003c/p\u003e\n\u003cp\u003e이 이야기는 Generative AI에서 발행되었습니다. 최신 AI 이야기를 알고 싶다면 LinkedIn에서 저희와 연결하고 Zeniteq를 팔로우하세요.\u003c/p\u003e\n\u003cp\u003e저희의 뉴스레터에 가입하여 최신 generative AI 뉴스와 업데이트를 받아보세요. 함께 AI의 미래를 함께 만들어 봅시다!\u003c/p\u003e\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003e![2024-06-20-MaketheWebyourbestfrienddataprovider_5.png](/assets/img/2024-06-20-MaketheWebyourbestfrienddataprovider_5.png)\u003c/code\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-20-MaketheWebyourbestfrienddataprovider"},"buildId":"QH5Mz7n7Y6w0r4_gCGFQf","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>