<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>RAG 모델 구축하기  Databrick으로 쉽게 만들기 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-15-RAGyourmodelSimplifiedwithDatabrick" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="RAG 모델 구축하기  Databrick으로 쉽게 만들기 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="RAG 모델 구축하기  Databrick으로 쉽게 만들기 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-15-RAGyourmodelSimplifiedwithDatabrick" data-gatsby-head="true"/><meta name="twitter:title" content="RAG 모델 구축하기  Databrick으로 쉽게 만들기 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 15:48" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_buildManifest.js" defer=""></script><script src="/_next/static/aCCUs-qPrLLLWRnkN0AOd/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">RAG 모델 구축하기  Databrick으로 쉽게 만들기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="RAG 모델 구축하기  Databrick으로 쉽게 만들기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">3<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-RAGyourmodelSimplifiedwithDatabrick&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>RAG 이론 (간단 버전)</h1>
<p>언어 모델이 지식을 배우는 몇 가지 방법이 있습니다. 전통적으로는 모델을 처음부터 훈련하거나 기존 모델을 세밀 조정하는 방법이 있습니다. 이는 모델 가중치를 업데이트하여 모델을 더 훈련시키는 것을 의미합니다. 다른 방법은 상대적으로 새롭고 직접적으로 프롬프트 공학과 연관이 있습니다. 여기서는 지식을 모델 입력값으로 전달합니다. 모델은 이를 문맥으로 편입하여 지식을 통합합니다. 왜 모델에 처음부터 문맥을 전달하는 걸까요? 모델에게 문맥을 전달하는 것은 모델에게 오픈 노트로 시험을 보는 것과 같습니다. 모델은 참고할 사실을 손에 넣게 됩니다. 다만 전달할 수 있는 문맥의 크기에는 제한이 있습니다. 보통 5 페이지이며, 이는 대부분의 산업용 사례에 부족할 정도입니다. 그래서 모델 문맥 문제용 새로운 모델 구조 - 'RAG'가 등장했습니다!</p>
<h1>RAG 아키텍처—</h1>
<p>Context 데이터/지식을 임베드된 벡터로 변환하고 이를 저장하는 모델(임베딩 모델)을 추가(Vector 스토어). 쿼리(프롬프트)가 전달되면 프롬프트를 캡처하여 임베드된 벡터로 변환하고, 저장소에서 유사한 벡터를 찾아 메인 모델에게 컨텍스트 강화된 프롬프트(쿼리 벡터 + 저장소로부터의 유사 벡터)를 전달합니다. 이것이 여러분이 제공한 문맥을 통합하여 정확한 응답을 생성하는 RAG입니다.</p>
<p>아래는 Markdown 형식으로 변경된 표입니다.</p>
<p><img src="/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png" alt="이미지"></p>
<h1>RAG의 세부 내용:</h1>
<p>RAG는 다음과 같은 네 가지 주요 구성 요소로 구성됩니다:-</p>
<ol>
<li>벡터 검색 - 주의할 점은 우리가 아무런 검색을 하기 전에 문맥/지식을 임베딩 벡터로 변환한다는 것입니다. 모든 데이터 객체인 오디오, 비디오 또는 텍스트는 임베드 벡터로 변환되어 벡터 저장소에 저장될 수 있습니다.</li>
</ol>
<p>두 가지 종류의 검색이 있습니다. 정확한 검색과 근사 검색. 이름에서 알 수 있듯이 정확한 검색은 가장 가까운 방법을 찾는 무차별 대입 방법입니다. 전통적인 KNN과 유사합니다. 반면에 근사 검색은 가장 가까운 이웃을 찾는 데 정확도가 낮지만 속도가 빠릅니다. 가장 인기 있는 벡터 검색 알고리즘은 ANN을 사용합니다.</p>
<p>공통의 인덱싱 알고리즘은 몇 가지 있습니다 — Spotify의 ANNOY(트리 기반), Facebook의 FAISS(클러스터링), LSH(해싱), 그리고 Google의 SCaNN(벡터 압축). 이러한 알고리즘들은 모두 효율적인 검색을 수행하기 위한 모든 필요한 정보를 보유한 vector Index라는 데이터 구조를 반환합니다.</p>
<ol start="2">
<li>Vector Store: 벡터를 저장하는 데 사용하는 두 가지 전략이 있습니다. 하나는 가벼운 벡터 라이브러리를 사용하는 것이고, 다른 하나는 고급 기능을 제공하는 벡터 데이터베이스를 사용하는 것입니다.</li>
</ol>
<ul>
<li>벡터 라이브러리: 벡터 인덱스를 생성하고 이 인덱스는 메모리에 저장됩니다. 라이브러리는 일반적으로 작고 정적인 데이터에 대해 충분합니다. 저장된 벡터에 대한 CRUD 작업을 지원하지 않습니다. 즉, 데이터가 변경될 때마다 인덱스를 다시 만들어야 합니다. 데이터 복제가 없습니다.</li>
</ul>
<p>b. 벡터 데이터베이스 — 다른 한편으로는 구조화되지 않은 데이터를 저장하는 데 특화된 데이터베이스입니다. 데이터베이스의 CRUD 속성을 상속하며 오프라인에서 색인을 사전 처리하고 나서 벡터를 데이터베이스에 저장하여 모델에 온라인으로 제공할 수 있도록 합니다.</p>
<ol start="3">
<li>필터링 — 필터링은 생성 프로세스에 통합하기 전에 검색된 지식 베이스에서 관련 정보를 선택하고 우선순위를 정하는 메커니즘을 참조합니다. 이를 통해 검색된 컨텍스트가 정보를 제공할 뿐만 아니라 일관성 있고 맥락에 부합하는 응답을 생성하는 데 도움이 됩니다.</li>
</ol>
<p>세 가지 전략 — Pre-Query, In-Query, Post-Query 필터링.</p>
<ol start="4">
<li>프롬프트 엔지니어링 — LLM에 응답 생성이나 작업 완료를 요청하는 텍스트입니다. 원하는 출력을 생성하도록 모델을 안내하는 명확하고 구체적이며 잘 구조화된 입력을 작성하는 것을 포함합니다. 모델이 사실을 찾지 못했을 때 억지로 내용을 만들지 않도록 안내하는 지침을 제공하는 것, 예시와 데모 사용, 그리고 더 중요한 것은 작업을 더 잘 이해할 수 있도록 맥락을 제공하는 것을 모두 포함합니다. 위에서 설명한 RAG 워크플로우에 주목하세요. 마지막으로, 우리는 맥락 문서를 프롬프트를 통해 모델에 전달하고 있기 때문에 RAG가 프롬프트 엔지니어링과 밀접하게 연관되어 있다고 할 수 있습니다.</li>
</ol>
<h1>RAG Nvidia/Llama3 model with Databricks</h1>
<h2>Assets -</h2>
<p>Model - nvidia/Llama3-ChatQA-1.5-8B (<a href="https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B" rel="nofollow" target="_blank">링크</a>)</p>
<p>Vector Database - ChromaDb</p>
<p>Databricks ML 런타임 -13.3.x-cpu-ml-scala2.12 / 13.3.x-gpu-ml-scala2.12</p>
<p>이 두 Databricks 런타임은 호환됩니다. 그러나이 모델은 8B 매개변수를 가지고 있어 노트북에서 추론을 위해 로드하기에 너무 큽니다. 충분한 메모리와 코어가 있는 CPU 머신에서 응답을 생성하는 데 오랜 시간이 걸렸습니다. 당신의 요구에 적합하고 예산에 맞는 런타임을 선택하세요.</p>
<h2>데모 -</h2>
<p>감사합니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"RAG 모델 구축하기  Databrick으로 쉽게 만들기","description":"","date":"2024-05-15 15:48","slug":"2024-05-15-RAGyourmodelSimplifiedwithDatabrick","content":"\n\n# RAG 이론 (간단 버전)\n\n언어 모델이 지식을 배우는 몇 가지 방법이 있습니다. 전통적으로는 모델을 처음부터 훈련하거나 기존 모델을 세밀 조정하는 방법이 있습니다. 이는 모델 가중치를 업데이트하여 모델을 더 훈련시키는 것을 의미합니다. 다른 방법은 상대적으로 새롭고 직접적으로 프롬프트 공학과 연관이 있습니다. 여기서는 지식을 모델 입력값으로 전달합니다. 모델은 이를 문맥으로 편입하여 지식을 통합합니다. 왜 모델에 처음부터 문맥을 전달하는 걸까요? 모델에게 문맥을 전달하는 것은 모델에게 오픈 노트로 시험을 보는 것과 같습니다. 모델은 참고할 사실을 손에 넣게 됩니다. 다만 전달할 수 있는 문맥의 크기에는 제한이 있습니다. 보통 5 페이지이며, 이는 대부분의 산업용 사례에 부족할 정도입니다. 그래서 모델 문맥 문제용 새로운 모델 구조 - 'RAG'가 등장했습니다!\n\n# RAG 아키텍처—\n\nContext 데이터/지식을 임베드된 벡터로 변환하고 이를 저장하는 모델(임베딩 모델)을 추가(Vector 스토어). 쿼리(프롬프트)가 전달되면 프롬프트를 캡처하여 임베드된 벡터로 변환하고, 저장소에서 유사한 벡터를 찾아 메인 모델에게 컨텍스트 강화된 프롬프트(쿼리 벡터 + 저장소로부터의 유사 벡터)를 전달합니다. 이것이 여러분이 제공한 문맥을 통합하여 정확한 응답을 생성하는 RAG입니다.\n\n\n\n아래는 Markdown 형식으로 변경된 표입니다.\n\n\n![이미지](/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png)\n\n# RAG의 세부 내용:\n\nRAG는 다음과 같은 네 가지 주요 구성 요소로 구성됩니다:-\n\n1. 벡터 검색 - 주의할 점은 우리가 아무런 검색을 하기 전에 문맥/지식을 임베딩 벡터로 변환한다는 것입니다. 모든 데이터 객체인 오디오, 비디오 또는 텍스트는 임베드 벡터로 변환되어 벡터 저장소에 저장될 수 있습니다.\n  \n\n\n\n두 가지 종류의 검색이 있습니다. 정확한 검색과 근사 검색. 이름에서 알 수 있듯이 정확한 검색은 가장 가까운 방법을 찾는 무차별 대입 방법입니다. 전통적인 KNN과 유사합니다. 반면에 근사 검색은 가장 가까운 이웃을 찾는 데 정확도가 낮지만 속도가 빠릅니다. 가장 인기 있는 벡터 검색 알고리즘은 ANN을 사용합니다.\n\n공통의 인덱싱 알고리즘은 몇 가지 있습니다 — Spotify의 ANNOY(트리 기반), Facebook의 FAISS(클러스터링), LSH(해싱), 그리고 Google의 SCaNN(벡터 압축). 이러한 알고리즘들은 모두 효율적인 검색을 수행하기 위한 모든 필요한 정보를 보유한 vector Index라는 데이터 구조를 반환합니다.\n\n2. Vector Store: 벡터를 저장하는 데 사용하는 두 가지 전략이 있습니다. 하나는 가벼운 벡터 라이브러리를 사용하는 것이고, 다른 하나는 고급 기능을 제공하는 벡터 데이터베이스를 사용하는 것입니다.\n\n- 벡터 라이브러리: 벡터 인덱스를 생성하고 이 인덱스는 메모리에 저장됩니다. 라이브러리는 일반적으로 작고 정적인 데이터에 대해 충분합니다. 저장된 벡터에 대한 CRUD 작업을 지원하지 않습니다. 즉, 데이터가 변경될 때마다 인덱스를 다시 만들어야 합니다. 데이터 복제가 없습니다.\n\n\n\nb. 벡터 데이터베이스 — 다른 한편으로는 구조화되지 않은 데이터를 저장하는 데 특화된 데이터베이스입니다. 데이터베이스의 CRUD 속성을 상속하며 오프라인에서 색인을 사전 처리하고 나서 벡터를 데이터베이스에 저장하여 모델에 온라인으로 제공할 수 있도록 합니다.\n\n3. 필터링 — 필터링은 생성 프로세스에 통합하기 전에 검색된 지식 베이스에서 관련 정보를 선택하고 우선순위를 정하는 메커니즘을 참조합니다. 이를 통해 검색된 컨텍스트가 정보를 제공할 뿐만 아니라 일관성 있고 맥락에 부합하는 응답을 생성하는 데 도움이 됩니다.\n\n세 가지 전략 — Pre-Query, In-Query, Post-Query 필터링.\n\n4. 프롬프트 엔지니어링 — LLM에 응답 생성이나 작업 완료를 요청하는 텍스트입니다. 원하는 출력을 생성하도록 모델을 안내하는 명확하고 구체적이며 잘 구조화된 입력을 작성하는 것을 포함합니다. 모델이 사실을 찾지 못했을 때 억지로 내용을 만들지 않도록 안내하는 지침을 제공하는 것, 예시와 데모 사용, 그리고 더 중요한 것은 작업을 더 잘 이해할 수 있도록 맥락을 제공하는 것을 모두 포함합니다. 위에서 설명한 RAG 워크플로우에 주목하세요. 마지막으로, 우리는 맥락 문서를 프롬프트를 통해 모델에 전달하고 있기 때문에 RAG가 프롬프트 엔지니어링과 밀접하게 연관되어 있다고 할 수 있습니다.\n\n\n\n# RAG Nvidia/Llama3 model with Databricks\n\n## Assets -\n\nModel - nvidia/Llama3-ChatQA-1.5-8B ([링크](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B))\n\nVector Database - ChromaDb\n\n\n\nDatabricks ML 런타임 -13.3.x-cpu-ml-scala2.12 / 13.3.x-gpu-ml-scala2.12\n\n이 두 Databricks 런타임은 호환됩니다. 그러나이 모델은 8B 매개변수를 가지고 있어 노트북에서 추론을 위해 로드하기에 너무 큽니다. 충분한 메모리와 코어가 있는 CPU 머신에서 응답을 생성하는 데 오랜 시간이 걸렸습니다. 당신의 요구에 적합하고 예산에 맞는 런타임을 선택하세요.\n\n## 데모 -\n\n감사합니다.","ogImage":{"url":"/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png"},"coverImage":"/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png","tag":["Tech"],"readingTime":3},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003eRAG 이론 (간단 버전)\u003c/h1\u003e\n\u003cp\u003e언어 모델이 지식을 배우는 몇 가지 방법이 있습니다. 전통적으로는 모델을 처음부터 훈련하거나 기존 모델을 세밀 조정하는 방법이 있습니다. 이는 모델 가중치를 업데이트하여 모델을 더 훈련시키는 것을 의미합니다. 다른 방법은 상대적으로 새롭고 직접적으로 프롬프트 공학과 연관이 있습니다. 여기서는 지식을 모델 입력값으로 전달합니다. 모델은 이를 문맥으로 편입하여 지식을 통합합니다. 왜 모델에 처음부터 문맥을 전달하는 걸까요? 모델에게 문맥을 전달하는 것은 모델에게 오픈 노트로 시험을 보는 것과 같습니다. 모델은 참고할 사실을 손에 넣게 됩니다. 다만 전달할 수 있는 문맥의 크기에는 제한이 있습니다. 보통 5 페이지이며, 이는 대부분의 산업용 사례에 부족할 정도입니다. 그래서 모델 문맥 문제용 새로운 모델 구조 - 'RAG'가 등장했습니다!\u003c/p\u003e\n\u003ch1\u003eRAG 아키텍처—\u003c/h1\u003e\n\u003cp\u003eContext 데이터/지식을 임베드된 벡터로 변환하고 이를 저장하는 모델(임베딩 모델)을 추가(Vector 스토어). 쿼리(프롬프트)가 전달되면 프롬프트를 캡처하여 임베드된 벡터로 변환하고, 저장소에서 유사한 벡터를 찾아 메인 모델에게 컨텍스트 강화된 프롬프트(쿼리 벡터 + 저장소로부터의 유사 벡터)를 전달합니다. 이것이 여러분이 제공한 문맥을 통합하여 정확한 응답을 생성하는 RAG입니다.\u003c/p\u003e\n\u003cp\u003e아래는 Markdown 형식으로 변경된 표입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-RAGyourmodelSimplifiedwithDatabrick_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003eRAG의 세부 내용:\u003c/h1\u003e\n\u003cp\u003eRAG는 다음과 같은 네 가지 주요 구성 요소로 구성됩니다:-\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e벡터 검색 - 주의할 점은 우리가 아무런 검색을 하기 전에 문맥/지식을 임베딩 벡터로 변환한다는 것입니다. 모든 데이터 객체인 오디오, 비디오 또는 텍스트는 임베드 벡터로 변환되어 벡터 저장소에 저장될 수 있습니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e두 가지 종류의 검색이 있습니다. 정확한 검색과 근사 검색. 이름에서 알 수 있듯이 정확한 검색은 가장 가까운 방법을 찾는 무차별 대입 방법입니다. 전통적인 KNN과 유사합니다. 반면에 근사 검색은 가장 가까운 이웃을 찾는 데 정확도가 낮지만 속도가 빠릅니다. 가장 인기 있는 벡터 검색 알고리즘은 ANN을 사용합니다.\u003c/p\u003e\n\u003cp\u003e공통의 인덱싱 알고리즘은 몇 가지 있습니다 — Spotify의 ANNOY(트리 기반), Facebook의 FAISS(클러스터링), LSH(해싱), 그리고 Google의 SCaNN(벡터 압축). 이러한 알고리즘들은 모두 효율적인 검색을 수행하기 위한 모든 필요한 정보를 보유한 vector Index라는 데이터 구조를 반환합니다.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eVector Store: 벡터를 저장하는 데 사용하는 두 가지 전략이 있습니다. 하나는 가벼운 벡터 라이브러리를 사용하는 것이고, 다른 하나는 고급 기능을 제공하는 벡터 데이터베이스를 사용하는 것입니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e벡터 라이브러리: 벡터 인덱스를 생성하고 이 인덱스는 메모리에 저장됩니다. 라이브러리는 일반적으로 작고 정적인 데이터에 대해 충분합니다. 저장된 벡터에 대한 CRUD 작업을 지원하지 않습니다. 즉, 데이터가 변경될 때마다 인덱스를 다시 만들어야 합니다. 데이터 복제가 없습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eb. 벡터 데이터베이스 — 다른 한편으로는 구조화되지 않은 데이터를 저장하는 데 특화된 데이터베이스입니다. 데이터베이스의 CRUD 속성을 상속하며 오프라인에서 색인을 사전 처리하고 나서 벡터를 데이터베이스에 저장하여 모델에 온라인으로 제공할 수 있도록 합니다.\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e필터링 — 필터링은 생성 프로세스에 통합하기 전에 검색된 지식 베이스에서 관련 정보를 선택하고 우선순위를 정하는 메커니즘을 참조합니다. 이를 통해 검색된 컨텍스트가 정보를 제공할 뿐만 아니라 일관성 있고 맥락에 부합하는 응답을 생성하는 데 도움이 됩니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e세 가지 전략 — Pre-Query, In-Query, Post-Query 필터링.\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e프롬프트 엔지니어링 — LLM에 응답 생성이나 작업 완료를 요청하는 텍스트입니다. 원하는 출력을 생성하도록 모델을 안내하는 명확하고 구체적이며 잘 구조화된 입력을 작성하는 것을 포함합니다. 모델이 사실을 찾지 못했을 때 억지로 내용을 만들지 않도록 안내하는 지침을 제공하는 것, 예시와 데모 사용, 그리고 더 중요한 것은 작업을 더 잘 이해할 수 있도록 맥락을 제공하는 것을 모두 포함합니다. 위에서 설명한 RAG 워크플로우에 주목하세요. 마지막으로, 우리는 맥락 문서를 프롬프트를 통해 모델에 전달하고 있기 때문에 RAG가 프롬프트 엔지니어링과 밀접하게 연관되어 있다고 할 수 있습니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003eRAG Nvidia/Llama3 model with Databricks\u003c/h1\u003e\n\u003ch2\u003eAssets -\u003c/h2\u003e\n\u003cp\u003eModel - nvidia/Llama3-ChatQA-1.5-8B (\u003ca href=\"https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B\" rel=\"nofollow\" target=\"_blank\"\u003e링크\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eVector Database - ChromaDb\u003c/p\u003e\n\u003cp\u003eDatabricks ML 런타임 -13.3.x-cpu-ml-scala2.12 / 13.3.x-gpu-ml-scala2.12\u003c/p\u003e\n\u003cp\u003e이 두 Databricks 런타임은 호환됩니다. 그러나이 모델은 8B 매개변수를 가지고 있어 노트북에서 추론을 위해 로드하기에 너무 큽니다. 충분한 메모리와 코어가 있는 CPU 머신에서 응답을 생성하는 데 오랜 시간이 걸렸습니다. 당신의 요구에 적합하고 예산에 맞는 런타임을 선택하세요.\u003c/p\u003e\n\u003ch2\u003e데모 -\u003c/h2\u003e\n\u003cp\u003e감사합니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-RAGyourmodelSimplifiedwithDatabrick"},"buildId":"aCCUs-qPrLLLWRnkN0AOd","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>