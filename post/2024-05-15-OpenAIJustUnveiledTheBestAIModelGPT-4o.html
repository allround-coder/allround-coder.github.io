<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다 | allround-coder</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://allround-coder.github.io///post/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다 | allround-coder" data-gatsby-head="true"/><meta property="og:title" content="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다 | allround-coder" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://allround-coder.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://allround-coder.github.io///post/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o" data-gatsby-head="true"/><meta name="twitter:title" content="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다 | allround-coder" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | allround-coder" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-15 16:33" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ZFDEQ947R4"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
  
            gtag('config', 'G-ZFDEQ947R4');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-b088bc509ff5c497.js" defer=""></script><script src="/_next/static/OFpTzInQeZKWBaqJEukNX/_buildManifest.js" defer=""></script><script src="/_next/static/OFpTzInQeZKWBaqJEukNX/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">Allround Coder</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">Allround Coder</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 15, 2024</span><span class="posts_reading_time__f7YPP">4<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png">
<p>OpenAI가 드디어 GPT-4o를 공개했어요! 이 새로운 모델은 오디오, 비전, 텍스트에서 실시간 다중 모달 기능을 제공하지만 이제는 상당히 개선되었어요. 무료로 사용할 수 있어서 새로운 사용자들을 유치하고 모델 훈련을 더 확장하는 GPT 3.5와 비슷한 전략을 보여줍니다.</p>
<p>Mia Murati에 따르면 GPT-4o 모델의 주목할만한 기능 중 하나는 성능인데, 이전 모델인 GPT-4보다 최대 2배 빠르다고 해요. 또한 비용을 최대 50%까지 줄일 수 있다고 합니다. 이 발전은 새로운 개선 사항을 활용하면서 대규모 AI 프로젝트를 계속해서 배포할 수 있도록 해 줄 것입니다.</p>
<p>그러나 기술적인 부분은 여기까지 하고, 이 새로운 모델이 우리에게 무엇을 할 수 있는지 살펴봅시다!</p>
<h2>실시간으로 바라보는 비전</h2>
<p>이것은 이미지를 업로드하고 상호 작용하는 기본 기능을 넘어섭니다. OpenAI는 이제 우리에게 음성 어시스턴트를 통해 ChatGPT와 상호 작용할 수 있게 하고, 심지어 우리의 컴퓨터나 스마트폰에서 콘텐츠를 공유할 수도 있습니다. 응답은 실시간으로 생성되어 다양한 분석을 가능하게 하여 다양한 종류와 복잡성 수준의 분석이 가능합니다.</p>
<p>아래 데모에서 ChatGPT는 수학 가르쳐주는 선생님이 되었어요 (제 머릿속을 폭발시켰어요!).</p>
<p>우리는 여기서 ChatGPT의 전체 능력의 일부를 볼 수 있습니다. 이는 수학 문제를 해결할 뿐만 아니라 우리가 해결책으로 이끌어주며 전체 프로세스를 더 교육적이고 설명적인 방식으로 이해하게 도와주는 명확한 지침과 권고를 제안합니다.</p>
<p>목소리와 비전 능력이 문제를 매끄럽게 인식하고 해석하는 데 얼마나 놀라운지 정말 대단하죠.</p>
<h2>실시간 대화형 음성</h2>
<p>OpenAI는 유창함, 억양 및 논리적 순서와 같은 능력에 주목하여 자연스러운 방식으로 계속 대화할 수 있도록 노력했습니다.</p>
<p>GPT-4o 프레젠테이션 중에는 이 모델이 부드러운 대화를 나눌 수 있고, 진정으로 친근한 톤으로 권고사항을 제공할 수 있었습니다. 이 모델은 다양한 감성 스타일로 목소리를 만들어내며, 보다 강렬하거나 진지하고 공식적인 톤을 선택할 수 있도록 합니다.</p>
<p>실시간 대화 기능과 음성 번역을 결합한 데모를 확인해보세요.</p>
<p>ChatGPT에 프롬프트를 제공하는 과정이 보다 복잡하다는 사실을 알고 계셨나요? 이는 영어와 스페인어로 이루어진 이중 언어 대화를 원활하게 해석하고, 양쪽 언어를 인식하여 적절히 응답을 생성해야 합니다.</p>
<p>이 플랫폼의 응답 정확도와 유창함에 감명받았습니다. 설정된 목표를 쉽게 달성함과 동시에 어색한 일시 정지를 피할 수 있어 다른 AI 시스템에서 자주 볼 수 있는 문제를 효과적으로 해결합니다.</p>
<h2>구글과 마찬가지로 데모들이 실질적인가요?</h2>
<p>많은 데모에서 OpenAI는 비디오가 재치 있게 편집된 것이 아니라 실시간으로 발생하고 있다는 것을 보여주려고 노력합니다. 아래 비디오는 다중 모달 기능이 어떻게 상호 작용하여 보이고 들리는 것에 기반한 정확한 응답을 제공하는지 보여줍니다.</p>
<p>다음은 강조하고 싶은 몇 가지 포인트입니다:</p>
<ul>
<li>ChatGPT가 정확하게 식별하고 세부 요소를 설명하는 데 얼마나 놀라운지에 대해 강조하고 싶습니다. 사람들이 추가되어 외부 환경이 더 복잡해지더라도, ChatGPT는 성공적으로 그들을 인식했습니다.</li>
<li>이 새로운 모델이 일정 조건에 맞는 노래를 만들 수 있다는 것이 놀라운 일입니다. 그는 쉽게 멜로디를 생성했습니다!</li>
<li>두 GPT 모델 간의 상호 작용은 거의 미래의 한 눈살까지 보이는 것 같았습니다. 명시적으로 언급된 것은 아니지만, OpenAI가 향하고 있는 방향으로 보입니다. GPT-4o가 보여준 능력으로 보아, 다음 단계는 AI 시스템끼리 상호 작용하는 것입니다. 이는 한 AI가 다른 AI를 훈련하고 우리가 상상조차 할 수 없는 다른 발전을 이끌 수 있습니다.</li>
</ul>
<h1>GPT-4o가 다른 AI 모델을 능가했습니다</h1>
<h2>텍스트 평가</h2>
<p>OpenAI가 공유한 이미지는 GPT-4o가 다른 모델들보다 특히 수학 및 HumanEval과 같은 영역에서 뛰어나다는 것을 명확히 보여줍니다. 이러한 특성들은 더 매끄럽고 인간과 유사한 대화를 가능케 함으로써 사용자들에 의해 높이 평가됩니다.</p>
<p>게다가, GPT-4o는 영어를 포함하여 추가 20여 개 이상의 언어를 대상으로 한 응답 능력을 확장했습니다. 이 언어 토큰화의 강화는 더 넓은 전 세계 관객에게 도달하도록 설계되었습니다.</p>
<p><img src="/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_1.png" alt="이미지"></p>
<h2>오디오 번역 성능</h2>
<p>GPT-4o의 향상된 능력과 텍스트 평가 기능을 결합하여 더 많은 사람들과 연결할 수 있는 기회를 제공합니다. 이는 언어가 단순히 의사 소통을 넘어 장벽으로 작용하는 경우가 많음을 인식합니다.</p>
<p>그래프는 GPT-4o가 Gemini와 Whisper-v3와 같은 다른 인공지능 시스템보다 성능이 우수함을 명확히 보여줍니다.</p>
<p><img src="/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_2.png" alt="GPT-4o performance"></p>
<h2>작은 업데이트 이상</h2>
<p>저에게 있어, ChatGPT에 대한 새로운 업데이트 이상을 의미합니다. 이는 AI를 환경과 연결하고 그 잠재력을 극대화하는 데 상당한 영향을 미칩니다. 더 나아가, OpenAI로부터 예상했던 것과 정확히 일치합니다. 이들은 사용자를 중심으로 한 제품을 처음부터 현실적이고 구체적인 조치를 통해 제공하고자 하는 노력을 계속하고 있습니다. 다중 모더(멀티모달리티)의 역할은 여기서 중요하며, 그들은 이를 인식하고 이를 강화하여 다양한 현실 세계 상황에서 보다 정확한 응답을 제공하기 위해 노력했습니다.</p>
<p>이제 우리가 요구하는 것 중 일부를 충족하는 "인공적이지 않은" 제품이 있습니다. GPT-4o는 GPT-5가 될 것을 염두에 두고 OpenAI가 이 AI를 새로운 맥락에서 활용하도록 사용자를 격려하는 첫걸음 중 하나입니다.</p>
<p>제 뉴스레터에 가입하고 35,000명 이상의 구독자들과 함께 무료 치트 시트를 받아보세요: ChatGPT, 웹 스크래핑, 데이터 과학을 위한 Python, 자동화 등의 주제들을 다룹니다!</p>
<p>이와 같은 이야기를 즐기고 작가로서 제를 지원하고 싶다면, 내 Substack를 구독해주세요. Substack에서는 내가 콘텐츠를 만드는 다른 플랫폼에서 찾을 수 없는 기사들을 발행하고 있습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"오픈AI, 최고의 AI 모델 GPT-4o를 공개했습니다","description":"","date":"2024-05-15 16:33","slug":"2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o","content":"\n\n\u003cimg src=\"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png\" /\u003e\n\nOpenAI가 드디어 GPT-4o를 공개했어요! 이 새로운 모델은 오디오, 비전, 텍스트에서 실시간 다중 모달 기능을 제공하지만 이제는 상당히 개선되었어요. 무료로 사용할 수 있어서 새로운 사용자들을 유치하고 모델 훈련을 더 확장하는 GPT 3.5와 비슷한 전략을 보여줍니다.\n\nMia Murati에 따르면 GPT-4o 모델의 주목할만한 기능 중 하나는 성능인데, 이전 모델인 GPT-4보다 최대 2배 빠르다고 해요. 또한 비용을 최대 50%까지 줄일 수 있다고 합니다. 이 발전은 새로운 개선 사항을 활용하면서 대규모 AI 프로젝트를 계속해서 배포할 수 있도록 해 줄 것입니다.\n\n그러나 기술적인 부분은 여기까지 하고, 이 새로운 모델이 우리에게 무엇을 할 수 있는지 살펴봅시다!\n\n\n\n## 실시간으로 바라보는 비전\n\n이것은 이미지를 업로드하고 상호 작용하는 기본 기능을 넘어섭니다. OpenAI는 이제 우리에게 음성 어시스턴트를 통해 ChatGPT와 상호 작용할 수 있게 하고, 심지어 우리의 컴퓨터나 스마트폰에서 콘텐츠를 공유할 수도 있습니다. 응답은 실시간으로 생성되어 다양한 분석을 가능하게 하여 다양한 종류와 복잡성 수준의 분석이 가능합니다.\n\n아래 데모에서 ChatGPT는 수학 가르쳐주는 선생님이 되었어요 (제 머릿속을 폭발시켰어요!).\n\n우리는 여기서 ChatGPT의 전체 능력의 일부를 볼 수 있습니다. 이는 수학 문제를 해결할 뿐만 아니라 우리가 해결책으로 이끌어주며 전체 프로세스를 더 교육적이고 설명적인 방식으로 이해하게 도와주는 명확한 지침과 권고를 제안합니다.\n\n\n\n목소리와 비전 능력이 문제를 매끄럽게 인식하고 해석하는 데 얼마나 놀라운지 정말 대단하죠.\n\n## 실시간 대화형 음성\n\nOpenAI는 유창함, 억양 및 논리적 순서와 같은 능력에 주목하여 자연스러운 방식으로 계속 대화할 수 있도록 노력했습니다.\n\nGPT-4o 프레젠테이션 중에는 이 모델이 부드러운 대화를 나눌 수 있고, 진정으로 친근한 톤으로 권고사항을 제공할 수 있었습니다. 이 모델은 다양한 감성 스타일로 목소리를 만들어내며, 보다 강렬하거나 진지하고 공식적인 톤을 선택할 수 있도록 합니다.\n\n\n\n실시간 대화 기능과 음성 번역을 결합한 데모를 확인해보세요.\n\nChatGPT에 프롬프트를 제공하는 과정이 보다 복잡하다는 사실을 알고 계셨나요? 이는 영어와 스페인어로 이루어진 이중 언어 대화를 원활하게 해석하고, 양쪽 언어를 인식하여 적절히 응답을 생성해야 합니다.\n\n이 플랫폼의 응답 정확도와 유창함에 감명받았습니다. 설정된 목표를 쉽게 달성함과 동시에 어색한 일시 정지를 피할 수 있어 다른 AI 시스템에서 자주 볼 수 있는 문제를 효과적으로 해결합니다.\n\n## 구글과 마찬가지로 데모들이 실질적인가요?\n\n\n\n많은 데모에서 OpenAI는 비디오가 재치 있게 편집된 것이 아니라 실시간으로 발생하고 있다는 것을 보여주려고 노력합니다. 아래 비디오는 다중 모달 기능이 어떻게 상호 작용하여 보이고 들리는 것에 기반한 정확한 응답을 제공하는지 보여줍니다.\n\n다음은 강조하고 싶은 몇 가지 포인트입니다:\n\n- ChatGPT가 정확하게 식별하고 세부 요소를 설명하는 데 얼마나 놀라운지에 대해 강조하고 싶습니다. 사람들이 추가되어 외부 환경이 더 복잡해지더라도, ChatGPT는 성공적으로 그들을 인식했습니다.\n- 이 새로운 모델이 일정 조건에 맞는 노래를 만들 수 있다는 것이 놀라운 일입니다. 그는 쉽게 멜로디를 생성했습니다!\n- 두 GPT 모델 간의 상호 작용은 거의 미래의 한 눈살까지 보이는 것 같았습니다. 명시적으로 언급된 것은 아니지만, OpenAI가 향하고 있는 방향으로 보입니다. GPT-4o가 보여준 능력으로 보아, 다음 단계는 AI 시스템끼리 상호 작용하는 것입니다. 이는 한 AI가 다른 AI를 훈련하고 우리가 상상조차 할 수 없는 다른 발전을 이끌 수 있습니다.\n\n# GPT-4o가 다른 AI 모델을 능가했습니다\n\n\n\n## 텍스트 평가\n\nOpenAI가 공유한 이미지는 GPT-4o가 다른 모델들보다 특히 수학 및 HumanEval과 같은 영역에서 뛰어나다는 것을 명확히 보여줍니다. 이러한 특성들은 더 매끄럽고 인간과 유사한 대화를 가능케 함으로써 사용자들에 의해 높이 평가됩니다.\n\n게다가, GPT-4o는 영어를 포함하여 추가 20여 개 이상의 언어를 대상으로 한 응답 능력을 확장했습니다. 이 언어 토큰화의 강화는 더 넓은 전 세계 관객에게 도달하도록 설계되었습니다.\n\n![이미지](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_1.png)\n\n\n\n## 오디오 번역 성능\n\nGPT-4o의 향상된 능력과 텍스트 평가 기능을 결합하여 더 많은 사람들과 연결할 수 있는 기회를 제공합니다. 이는 언어가 단순히 의사 소통을 넘어 장벽으로 작용하는 경우가 많음을 인식합니다.\n\n그래프는 GPT-4o가 Gemini와 Whisper-v3와 같은 다른 인공지능 시스템보다 성능이 우수함을 명확히 보여줍니다.\n\n![GPT-4o performance](/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_2.png)\n\n\n\n## 작은 업데이트 이상\n\n저에게 있어, ChatGPT에 대한 새로운 업데이트 이상을 의미합니다. 이는 AI를 환경과 연결하고 그 잠재력을 극대화하는 데 상당한 영향을 미칩니다. 더 나아가, OpenAI로부터 예상했던 것과 정확히 일치합니다. 이들은 사용자를 중심으로 한 제품을 처음부터 현실적이고 구체적인 조치를 통해 제공하고자 하는 노력을 계속하고 있습니다. 다중 모더(멀티모달리티)의 역할은 여기서 중요하며, 그들은 이를 인식하고 이를 강화하여 다양한 현실 세계 상황에서 보다 정확한 응답을 제공하기 위해 노력했습니다.\n\n이제 우리가 요구하는 것 중 일부를 충족하는 \"인공적이지 않은\" 제품이 있습니다. GPT-4o는 GPT-5가 될 것을 염두에 두고 OpenAI가 이 AI를 새로운 맥락에서 활용하도록 사용자를 격려하는 첫걸음 중 하나입니다.\n\n제 뉴스레터에 가입하고 35,000명 이상의 구독자들과 함께 무료 치트 시트를 받아보세요: ChatGPT, 웹 스크래핑, 데이터 과학을 위한 Python, 자동화 등의 주제들을 다룹니다!\n\n\n\n이와 같은 이야기를 즐기고 작가로서 제를 지원하고 싶다면, 내 Substack를 구독해주세요. Substack에서는 내가 콘텐츠를 만드는 다른 플랫폼에서 찾을 수 없는 기사들을 발행하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png"},"coverImage":"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png","tag":["Tech"],"readingTime":4},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_0.png\"\u003e\n\u003cp\u003eOpenAI가 드디어 GPT-4o를 공개했어요! 이 새로운 모델은 오디오, 비전, 텍스트에서 실시간 다중 모달 기능을 제공하지만 이제는 상당히 개선되었어요. 무료로 사용할 수 있어서 새로운 사용자들을 유치하고 모델 훈련을 더 확장하는 GPT 3.5와 비슷한 전략을 보여줍니다.\u003c/p\u003e\n\u003cp\u003eMia Murati에 따르면 GPT-4o 모델의 주목할만한 기능 중 하나는 성능인데, 이전 모델인 GPT-4보다 최대 2배 빠르다고 해요. 또한 비용을 최대 50%까지 줄일 수 있다고 합니다. 이 발전은 새로운 개선 사항을 활용하면서 대규모 AI 프로젝트를 계속해서 배포할 수 있도록 해 줄 것입니다.\u003c/p\u003e\n\u003cp\u003e그러나 기술적인 부분은 여기까지 하고, 이 새로운 모델이 우리에게 무엇을 할 수 있는지 살펴봅시다!\u003c/p\u003e\n\u003ch2\u003e실시간으로 바라보는 비전\u003c/h2\u003e\n\u003cp\u003e이것은 이미지를 업로드하고 상호 작용하는 기본 기능을 넘어섭니다. OpenAI는 이제 우리에게 음성 어시스턴트를 통해 ChatGPT와 상호 작용할 수 있게 하고, 심지어 우리의 컴퓨터나 스마트폰에서 콘텐츠를 공유할 수도 있습니다. 응답은 실시간으로 생성되어 다양한 분석을 가능하게 하여 다양한 종류와 복잡성 수준의 분석이 가능합니다.\u003c/p\u003e\n\u003cp\u003e아래 데모에서 ChatGPT는 수학 가르쳐주는 선생님이 되었어요 (제 머릿속을 폭발시켰어요!).\u003c/p\u003e\n\u003cp\u003e우리는 여기서 ChatGPT의 전체 능력의 일부를 볼 수 있습니다. 이는 수학 문제를 해결할 뿐만 아니라 우리가 해결책으로 이끌어주며 전체 프로세스를 더 교육적이고 설명적인 방식으로 이해하게 도와주는 명확한 지침과 권고를 제안합니다.\u003c/p\u003e\n\u003cp\u003e목소리와 비전 능력이 문제를 매끄럽게 인식하고 해석하는 데 얼마나 놀라운지 정말 대단하죠.\u003c/p\u003e\n\u003ch2\u003e실시간 대화형 음성\u003c/h2\u003e\n\u003cp\u003eOpenAI는 유창함, 억양 및 논리적 순서와 같은 능력에 주목하여 자연스러운 방식으로 계속 대화할 수 있도록 노력했습니다.\u003c/p\u003e\n\u003cp\u003eGPT-4o 프레젠테이션 중에는 이 모델이 부드러운 대화를 나눌 수 있고, 진정으로 친근한 톤으로 권고사항을 제공할 수 있었습니다. 이 모델은 다양한 감성 스타일로 목소리를 만들어내며, 보다 강렬하거나 진지하고 공식적인 톤을 선택할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e실시간 대화 기능과 음성 번역을 결합한 데모를 확인해보세요.\u003c/p\u003e\n\u003cp\u003eChatGPT에 프롬프트를 제공하는 과정이 보다 복잡하다는 사실을 알고 계셨나요? 이는 영어와 스페인어로 이루어진 이중 언어 대화를 원활하게 해석하고, 양쪽 언어를 인식하여 적절히 응답을 생성해야 합니다.\u003c/p\u003e\n\u003cp\u003e이 플랫폼의 응답 정확도와 유창함에 감명받았습니다. 설정된 목표를 쉽게 달성함과 동시에 어색한 일시 정지를 피할 수 있어 다른 AI 시스템에서 자주 볼 수 있는 문제를 효과적으로 해결합니다.\u003c/p\u003e\n\u003ch2\u003e구글과 마찬가지로 데모들이 실질적인가요?\u003c/h2\u003e\n\u003cp\u003e많은 데모에서 OpenAI는 비디오가 재치 있게 편집된 것이 아니라 실시간으로 발생하고 있다는 것을 보여주려고 노력합니다. 아래 비디오는 다중 모달 기능이 어떻게 상호 작용하여 보이고 들리는 것에 기반한 정확한 응답을 제공하는지 보여줍니다.\u003c/p\u003e\n\u003cp\u003e다음은 강조하고 싶은 몇 가지 포인트입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChatGPT가 정확하게 식별하고 세부 요소를 설명하는 데 얼마나 놀라운지에 대해 강조하고 싶습니다. 사람들이 추가되어 외부 환경이 더 복잡해지더라도, ChatGPT는 성공적으로 그들을 인식했습니다.\u003c/li\u003e\n\u003cli\u003e이 새로운 모델이 일정 조건에 맞는 노래를 만들 수 있다는 것이 놀라운 일입니다. 그는 쉽게 멜로디를 생성했습니다!\u003c/li\u003e\n\u003cli\u003e두 GPT 모델 간의 상호 작용은 거의 미래의 한 눈살까지 보이는 것 같았습니다. 명시적으로 언급된 것은 아니지만, OpenAI가 향하고 있는 방향으로 보입니다. GPT-4o가 보여준 능력으로 보아, 다음 단계는 AI 시스템끼리 상호 작용하는 것입니다. 이는 한 AI가 다른 AI를 훈련하고 우리가 상상조차 할 수 없는 다른 발전을 이끌 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eGPT-4o가 다른 AI 모델을 능가했습니다\u003c/h1\u003e\n\u003ch2\u003e텍스트 평가\u003c/h2\u003e\n\u003cp\u003eOpenAI가 공유한 이미지는 GPT-4o가 다른 모델들보다 특히 수학 및 HumanEval과 같은 영역에서 뛰어나다는 것을 명확히 보여줍니다. 이러한 특성들은 더 매끄럽고 인간과 유사한 대화를 가능케 함으로써 사용자들에 의해 높이 평가됩니다.\u003c/p\u003e\n\u003cp\u003e게다가, GPT-4o는 영어를 포함하여 추가 20여 개 이상의 언어를 대상으로 한 응답 능력을 확장했습니다. 이 언어 토큰화의 강화는 더 넓은 전 세계 관객에게 도달하도록 설계되었습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e오디오 번역 성능\u003c/h2\u003e\n\u003cp\u003eGPT-4o의 향상된 능력과 텍스트 평가 기능을 결합하여 더 많은 사람들과 연결할 수 있는 기회를 제공합니다. 이는 언어가 단순히 의사 소통을 넘어 장벽으로 작용하는 경우가 많음을 인식합니다.\u003c/p\u003e\n\u003cp\u003e그래프는 GPT-4o가 Gemini와 Whisper-v3와 같은 다른 인공지능 시스템보다 성능이 우수함을 명확히 보여줍니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o_2.png\" alt=\"GPT-4o performance\"\u003e\u003c/p\u003e\n\u003ch2\u003e작은 업데이트 이상\u003c/h2\u003e\n\u003cp\u003e저에게 있어, ChatGPT에 대한 새로운 업데이트 이상을 의미합니다. 이는 AI를 환경과 연결하고 그 잠재력을 극대화하는 데 상당한 영향을 미칩니다. 더 나아가, OpenAI로부터 예상했던 것과 정확히 일치합니다. 이들은 사용자를 중심으로 한 제품을 처음부터 현실적이고 구체적인 조치를 통해 제공하고자 하는 노력을 계속하고 있습니다. 다중 모더(멀티모달리티)의 역할은 여기서 중요하며, 그들은 이를 인식하고 이를 강화하여 다양한 현실 세계 상황에서 보다 정확한 응답을 제공하기 위해 노력했습니다.\u003c/p\u003e\n\u003cp\u003e이제 우리가 요구하는 것 중 일부를 충족하는 \"인공적이지 않은\" 제품이 있습니다. GPT-4o는 GPT-5가 될 것을 염두에 두고 OpenAI가 이 AI를 새로운 맥락에서 활용하도록 사용자를 격려하는 첫걸음 중 하나입니다.\u003c/p\u003e\n\u003cp\u003e제 뉴스레터에 가입하고 35,000명 이상의 구독자들과 함께 무료 치트 시트를 받아보세요: ChatGPT, 웹 스크래핑, 데이터 과학을 위한 Python, 자동화 등의 주제들을 다룹니다!\u003c/p\u003e\n\u003cp\u003e이와 같은 이야기를 즐기고 작가로서 제를 지원하고 싶다면, 내 Substack를 구독해주세요. Substack에서는 내가 콘텐츠를 만드는 다른 플랫폼에서 찾을 수 없는 기사들을 발행하고 있습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-15-OpenAIJustUnveiledTheBestAIModelGPT-4o"},"buildId":"OFpTzInQeZKWBaqJEukNX","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>